{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ACGAN_CVAE_pytorch.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["TPiMSZkOg-JG","xF15EJdcS4sz","aACg2yM0x2ln","IfBjaXVv7IS0"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/shackste/galaxy-generator/blob/separate_py_files/ACGAN_CVAE_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ZEiOsqeXFUJk"},"source":["PyTorch implementation of combined Conditional Variable Auto Encoder (CVAE) and Auxiliary-Classifier Generative Adversarial Network (ACGAN)\n","\n","continuation of work by Mohamad Dia"]},{"cell_type":"markdown","metadata":{"id":"OqYSQwo1yU4Q"},"source":["# Environment Setup"]},{"cell_type":"markdown","metadata":{"id":"FiixZLra8_zZ"},"source":["## Modules"]},{"cell_type":"code","metadata":{"id":"n_TbcQTbUyvl"},"source":["!pip install torchviz\n","!pip install photutils\n","!pip install statmorph\n","!pip install wandb -qqq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQjam8HTF3NX"},"source":["import sys \n","from time import time\n","from pdb import set_trace\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch import cat, add, ones, zeros, tensor_split\n","import torchvision.models as models\n","\n","## include separate module files\n","#!git clone -b separate_py_files https://github.com/shackste/galaxy-generator.git\n","#sys.path.insert(0,\"/content/galaxy-generator/python_modules/\")\n","\n","from google.colab import drive\n","drive.mount(\"/drive\")\n","\n","sys.path.insert(0,\"/drive/MyDrive/FHNW/galaxy_generator/galaxy-generator/python_modules/\")\n","\n","from parameter import labels_dim, input_size, parameter\n","from file_system import root, folder_results\n","from helpful_functions import summarize, write_generated_galaxy_images_iteration\n","from sampler import make_training_sample_generator\n","from dataset import get_x_train, get_labels_train\n","from loss import loss_discriminator, loss_generator\n","\n","import pipeline\n","from pipeline import VAE, VAEGAN\n","from discriminator import Discriminator4\n","from encoder import Encoder4\n","from decoder import Decoder4\n","\n","track_hyperparameter = False\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TPiMSZkOg-JG"},"source":["# Track hyperparameter search via [wandb.ai](https://wandb.ai/shackste/galaxy-generator)"]},{"cell_type":"code","metadata":{"id":"nbhdMEZvg-x2"},"source":["track_hyperparameter = True\n","\n","if track_hyperparameter:\n","    import wandb\n","    !wandb login\n","\n","\n","\n","\"\"\"  USAGE\n","### the following has to be adjusted for every training run\n","### since hyperparameters can be changed on the fly, you find this in Training section\n","\n","wandb.init(project=\"galaxy-generator\", # top level identifier\n","           group=\"first\", # second level identifier, to seperate several groups of tests\n","           job_type=\"training\", # third level identifier, organize different jobs like training and evaluation\n","           tags=[\"first\"], # temporary tags to organize different tasks together\n","           name=\"first\", # bottom level identifier, label of graph in UI\n","           config=parameter.return_parameter_dict()  # here we fill the hyperparameters\n",")\n","\n","## to follow evolution of loss or other measures wich epoch, after each iteration use:\n","wandb.log({\"loss\":loss})  ## here we usually pass loss and accuracy measures\n","\n","## after training is done and all measures are written, finalize with\n","wandb.finish()\n","\"\"\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2fJ_oRuH8XW"},"source":["# new architecture"]},{"cell_type":"code","metadata":{"id":"V5_RkQfpBSyn"},"source":["from torch import rand\n","batch_size = 64\n","labels_dim = 37\n","\n","images = rand(batch_size, 3, 64, 64).cuda()\n","labels = rand(batch_size, labels_dim).cuda()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgUbrAYmp38_"},"source":["import torchvision.models as models\n","net = models.resnet50()\n","net\n","### no pretrained inverse available...\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-KDgM9GH1RD"},"source":["from functools import wraps, partial\n","import numpy as np\n","\n","from torch import cat, randn, rand, tensor, sum, mean, abs, all, squeeze\n","from torch.nn import Sequential, \\\n","                     Conv2d, ConvTranspose2d, Linear, \\\n","                     LeakyReLU, Sigmoid, Softmax, Tanh, Softplus, \\\n","                     BatchNorm1d, BatchNorm2d, Flatten\n","\n","\n","from neuralnetwork import NeuralNetwork, update_networks_on_loss\n","from parameter import colors_dim, labels_dim, parameter\n","from additional_layers import Reshape\n","from sampler import gaussian_sampler, generate_latent, generate_noise, generate_galaxy_labels\n","from loss import loss_adversarial, loss_reconstruction, loss_kl, loss_class, loss_metric, loss_latent\n","from labeling import labels_dim, label_group_sizes, class_groups, class_groups_indices, make_galaxy_labels_hierarchical\n","from weight_functions import constant_weight, rising_weight, falling_weight, cyclical_weight\n","from accuracy_measures import accuracy_discriminator, accuracy_classifier\n","from decorators import loss_to_value\n","\n","## prebuilt model for Encoder\n","resnet = models.resnet18(pretrained=False)\n","N_resnet = 512\n","#resnet = models.resnet50(pretrained=False)\n","#N_resnet = 2048\n","\n","## 3x3 kernel\n","\n","kernel_size = 3\n","stride = 2\n","padding = NeuralNetwork.same_padding(kernel_size)\n","output_padding = padding\n","\n","## 5x5 kernel\n","'''\n","kernel_size = 5\n","stride = 2\n","padding = NeuralNetwork.same_padding(kernel_size)\n","output_padding = 1\n","#'''\n","\n","groups_classified = [1, 2, 7]\n","\n","ixs_labels = []\n","for g in groups_classified:\n","    ixs_labels.extend(class_groups_indices[g]-1)\n","ixs_labels = tensor(ixs_labels)\n","\n","\n","\n","\n","\n","## weights\n","\n","weight_1 = partial(constant_weight, weight=1.)\n","weight_0 = partial(constant_weight, weight=0.)\n","weight_rise_0_3k__1_4k = partial(rising_weight, iteration_rise=3000, iteration_max=4000, max_weight=1., min_weight=0.)\n","weight_rise_0_6k__1_7k = partial(rising_weight, iteration_rise=6000, iteration_max=7000, max_weight=1., min_weight=0.)\n","weight_rise_0_10k__1_12k = partial(rising_weight, iteration_rise=10000, iteration_max=12000, max_weight=1., min_weight=0.)\n","\n","weights = {\n","    # discriminator\n","    \"discriminate_real\" : weight_0, # weight_1,\n","    \"discriminate_reproduced\" : weight_0, # weight_1,\n","    \"discriminate_latent\" : weight_0, # weight_1,\n","    \"discriminate_labels\" : weight_0, # weight_rise_0_10k__1_12k,\n","    \"discriminate_fake_labels\" : weight_0, # weight_rise_0_10k__1_12k,\n","    # generator\n","    \"regularize_latent\" : weight_0, # partial(cyclical_weight, full_cycle=2000, rise_cycle=1000, max_weight=1.),\n","    \"reproduce_image\" : weight_0, # partial(constant_weight, weight=1e-2),\n","    \"trick_reproduced\" : weight_0, # partial(constant_weight, weight=1e1),\n","    \"reproduce_image_labels\" : weight_0, # weight_1, # weight_rise_0_6k__1_7k,\n","    \"trick_labels\" : weight_0, # weight_rise_0_10k__1_12k,\n","    \"reproduce_labels\" : weight_0, # weight_rise_0_10k__1_12k,\n","    \"trick_fake_labels\" : weight_0, # weight_rise_0_10k__1_12k,\n","    \"reproduce_fake_labels\" : weight_0, # weight_rise_0_10k__1_12k,\n","    # image classifier\n","    \"image_label\" : weight_0,\n","    # classifier\n","    \"label_image\" : weight_1, # weight_rise_0_3k__1_4k,\n","    \"label_reproduced_image\" : weight_0, # weight_rise_0_3k__1_4k,\n","    \"label_labels\" : weight_0, # weight_rise_0_10k__1_12k,\n","    \"label_fake_labels\" : weight_0, # weight_rise_0_10k__1_12k,\n","    # inverse classifier\n","    \"unity_labels\" : weight_0, # weight_rise_0_6k__1_7k,\n","    \"unity_latent\" : weight_0, # weight_rise_0_6k__1_7k,\n","\n","\n","}\n","\n","\n","\n","\n","## layer creators\n","\n","def create_conv_layer(input_dim, output_dim, kernel_size=kernel_size, stride=stride, padding=padding):\n","    return Sequential(\n","        Conv2d(input_dim, output_dim, kernel_size=kernel_size, stride=stride, padding=padding),\n","        BatchNorm2d(output_dim, momentum=parameter.momentum),\n","        LeakyReLU(negative_slope=parameter.negative_slope),\n","    )\n","\n","def create_iconv_layer(input_dim, output_dim, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding):\n","    return Sequential(\n","        ConvTranspose2d(input_dim, output_dim, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding),\n","        BatchNorm2d(output_dim, momentum=parameter.momentum),\n","        LeakyReLU(negative_slope=parameter.negative_slope),\n","    )\n","\n","def create_dense_layer(input_dim, output_dim):\n","    return Sequential(\n","        Linear(input_dim, output_dim),\n","        BatchNorm1d(output_dim, momentum=parameter.momentum),\n","        LeakyReLU(negative_slope=parameter.negative_slope),\n","    )\n","\n","accuracy = {}\n","\n","\n","## useful functions\n","\n","\n","\n","## useful decorators\n","\n","losses = {}\n","\n","\n","def weighted_loss(name):\n","    def actual_decorator(func):\n","        @wraps(func)\n","        def wrapper(*args, **kwargs):\n","            weight = weights[name](args[0].iteration)\n","            if not weight:\n","                return tensor(0.).cuda()\n","            loss = func(*args, **kwargs)\n","            loss *= weight\n","            losses[name] = loss.item()\n","            return loss\n","        return wrapper\n","    return actual_decorator\n","\n","\n","## my architecture\n","\n","\n","\n","class Encoder(NeuralNetwork):\n","    \"\"\" transforms image to latent vector (distribution) \"\"\"\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        N_features = 8\n","\n","        self.conv0 = Sequential(\n","            Conv2d(colors_dim, N_features, kernel_size=1, stride=1),\n","            LeakyReLU(negative_slope=parameter.negative_slope),\n","        )\n","        self.conv1 = create_conv_layer(N_features, 2*N_features)\n","        self.conv2 = create_conv_layer(2*N_features, 2**2*N_features)\n","        self.conv3 = create_conv_layer(2**2*N_features, 2**3*N_features)\n","        self.conv4 = create_conv_layer(2**3*N_features, 2**4*N_features)\n","        self.conv5 = create_conv_layer(2**4*N_features, 2**5*N_features, kernel_size=4, stride=1, padding=0)\n","\n","        self.flat = Flatten()\n","        self.dense = create_dense_layer(2**5*N_features, 2**6*N_features)\n","\n","        ## the following take the same input\n","        self.dense_z_mu = Sequential(\n","            Linear(2**6*N_features, parameter.latent_dim),\n","#            Tanh(),\n","        )\n","        self.dense_z_std = Sequential(\n","            Linear(2**6*N_features, parameter.latent_dim),\n","            Softplus()\n","#            Sigmoid(),\n","        )\n","        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n","\n","\n","\n","\n","    def forward(self, images):\n","        x = self.conv0(images)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.conv5(x)\n","        x = self.flat(x)\n","        x = self.dense(x)\n","        z_mu = self.dense_z_mu(x)\n","        z_std = self.dense_z_std(x)\n","        return z_mu, z_std\n","\n","\n","class Encoder(NeuralNetwork):\n","    \"\"\" transforms image to latent vector (distribution) \"\"\"\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        self.conv = Sequential(*(list(resnet.children())[:-1]))\n","        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n","\n","    def forward(self, images):\n","        latent = self.conv(images)\n","        latent = squeeze(latent)\n","        latent_mu, latent_std = tensor_split(latent, 2, dim=1)\n","        return latent_mu, latent_std\n","\n","\n","class Decoder(NeuralNetwork):\n","    \"\"\" generates image from latent vector \"\"\"\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","        N_features = 8\n","\n","        self.dense1 = create_dense_layer(parameter.latent_dim, 2**6*N_features)\n","        self.dense2 = create_dense_layer(2**6*N_features, 2**5*N_features)\n","        self.reshape = Reshape(2**5*N_features,1,1)\n","\n","        self.iconv1 = create_iconv_layer(2**5*N_features, 2**4*N_features, kernel_size=4, stride=1, padding=0, output_padding=0)\n","        self.iconv2 = create_iconv_layer(2**4*N_features, 2**3*N_features)\n","        self.iconv3 = create_iconv_layer(2**3*N_features, 2**2*N_features)\n","        self.iconv4 = create_iconv_layer(2**2*N_features, 2*N_features)\n","        self.iconv5 = create_iconv_layer(2*N_features, N_features)\n","        self.conv_out = Sequential(\n","            Conv2d(N_features, colors_dim, kernel_size=1, stride=1),\n","            Sigmoid(),\n","        )\n","\n","        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n","\n","\n","\n","    def forward(self, latent):\n","        x = self.dense1(latent)\n","        x = self.dense2(x)\n","        x = self.reshape(x)\n","        x = self.iconv1(x)\n","        x = self.iconv2(x)\n","        x = self.iconv3(x)\n","        x = self.iconv4(x)\n","        x = self.iconv5(x)\n","        x = self.conv_out(x)\n","        return x\n","\n","\n","class Discriminator(NeuralNetwork):\n","    \"\"\" discriminates real and fake images \"\"\"\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        N_features = 8\n","        self.conv0 = Sequential(\n","            Conv2d(colors_dim, N_features, kernel_size=1, stride=1),\n","            LeakyReLU(negative_slope=parameter.negative_slope),\n","        )\n","        self.conv1 = create_conv_layer(N_features, 2*N_features)\n","        self.conv2 = create_conv_layer(2*N_features, 2**2*N_features)\n","        self.conv3 = create_conv_layer(2**2*N_features, 2**3*N_features)\n","        self.conv4 = create_conv_layer(2**3*N_features, 2**4*N_features)\n","        self.conv5 = create_conv_layer(2**4*N_features, 2**5*N_features, kernel_size=4, stride=1, padding=0)\n","\n","        self.flat = Flatten()\n","        self.dense1 = create_dense_layer(2**5*N_features, 2**5*N_features)\n","        self.dense_out = Sequential(\n","            Linear(2**5*N_features, 1),\n","            Sigmoid()\n","        )\n","        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n","\n","    def forward(self, images):\n","        x = self.conv0(images)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.conv5(x)\n","        x = self.flat(x)\n","        x = self.dense1(x)\n","        x = self.dense_out(x)\n","        return x\n","\n","\n","class ImageClassifier(NeuralNetwork):\n","    def __init__(self):\n","        super(ImageClassifier, self).__init__()\n","        self.conv = Sequential(*(list(resnet.children())[:-1]))\n","        self.dense_groups = []\n","        for i, N_label in enumerate(label_group_sizes):\n","            layer = f\"dense_group{i}\"\n","            setattr(self, layer,\n","                Sequential(\n","                   Linear(N_resnet, N_label),\n","                    Softmax(dim=1)\n","                )\n","            )\n","            self.dense_groups.append( getattr(self, layer) )\n","        self.dense_noise = Linear(N_resnet, labels_dim)\n","\n","        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n","\n","    def forward(self, image):\n","        x = self.conv(image)\n","        x = squeeze(x)\n","        noise = self.dense_noise(x)\n","        label_groups = [dense(x) for dense in self.dense_groups]\n","        ## renormalize label groups to fit settings of Galaxy zoo\n","        labels = make_galaxy_labels_hierarchical(label_groups)\n","        return labels, noise\n","\n","\n","class Classifier(NeuralNetwork):\n","    \"\"\" classifies image from latent vector (distribution).\n","    Reproduce classification scheme of Galaxy Zoo, see\n","    https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/overview/the-galaxy-zoo-decision-tree\n","    \"\"\"\n","    def __init__(self):\n","        super(Classifier, self).__init__()\n","        N_hidden = 2*parameter.latent_dim\n","        self.dense1 = create_dense_layer(parameter.latent_dim, N_hidden)\n","        self.dense2 = create_dense_layer(N_hidden, N_hidden)\n","        self.dense3 = create_dense_layer(N_hidden, N_hidden)\n","        self.dense4 = create_dense_layer(N_hidden, N_hidden)\n","        ## the following take the same input\n","        self.dense_groups = []\n","        for i, N_label in enumerate(label_group_sizes):\n","            layer = f\"dense_group{i}\"\n","            setattr(self, layer,\n","                Sequential(\n","                   Linear(N_hidden, N_label),\n","                    Softmax(dim=1)\n","                )\n","            )\n","            self.dense_groups.append( getattr(self, layer) )\n","        self.dense_noise = Linear(N_hidden, labels_dim)\n","\n","\n","        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n","\n","    def forward(self, latent_mu, latent_sigma):\n","#        x = self.dense1(cat((latent_mu, latent_sigma), dim=1))\n","        x = gaussian_sampler(latent_mu, latent_sigma)\n","        x = self.dense1(x)\n","        x = self.dense2(x)\n","        x = self.dense3(x)\n","        x = self.dense4(x)\n","        noise = self.dense_noise(x)\n","        label_groups = [dense(x) for dense in self.dense_groups]\n","        ## renormalize label groups to fit settings of Galaxy zoo\n","        labels = make_galaxy_labels_hierarchical(label_groups)\n","        return labels, noise\n","\n","class InverseClassifier(NeuralNetwork):\n","    \"\"\" transforms class info to latent vector (distribution) \"\"\"\n","    def __init__(self):\n","        super(InverseClassifier, self).__init__()\n","        N_hidden = 4*labels_dim\n","        self.dense1 = create_dense_layer(2*labels_dim, N_hidden)\n","        self.dense2 = create_dense_layer(N_hidden, N_hidden)\n","\n","        ## the following take the same input\n","        self.dense_mu= Sequential(\n","            Linear(4*labels_dim, parameter.latent_dim),\n","#            Tanh()\n","        )\n","        self.dense_sigma= Sequential(\n","            Linear(4*labels_dim, parameter.latent_dim),\n","            Softplus()\n","#            Sigmoid()\n","        )\n","        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n","\n","    def forward(self, labels, noise):\n","        x = self.dense1(cat((labels,noise),dim=1))\n","        x = self.dense2(x)\n","        z_mu = self.dense_mu(x)\n","        z_sigma = self.dense_sigma(x)\n","        return z_mu, z_sigma\n","\n","\n","\n","\n","### DCGAN architecture (Redford et al 2015)\n","\n","\n","class Encoder_(NeuralNetwork):\n","    \"\"\" transforms image to latent vector (distribution) \"\"\"\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        N_features = 8\n","\n","        self.conv1 = create_conv_layer(colors_dim, 128)\n","        self.conv2 = create_conv_layer(128, 256)\n","        self.conv3 = create_conv_layer(256, 512)\n","        self.conv4 = create_conv_layer(512, 1024)\n","\n","        self.flat = Flatten()\n","\n","        ## the following take the same input\n","        self.dense_z_mu = Sequential(\n","            Linear(1024*4*4, parameter.latent_dim),\n","#            Tanh(),\n","        )\n","        self.dense_z_std = Sequential(\n","            Linear(1024*4*4, parameter.latent_dim),\n","            Softplus()\n","#            Sigmoid(),\n","        )\n","        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n","\n","\n","\n","\n","    def forward(self, images):\n","        x = self.conv1(images)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.flat(x)\n","        z_mu = self.dense_z_mu(x)\n","        z_std = self.dense_z_std(x)\n","        return z_mu, z_std\n","\n","\n","class Decoder_(NeuralNetwork):\n","    \"\"\" generates image from latent vector \"\"\"\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","        N_features = 8\n","\n","        self.dense1 = create_dense_layer(parameter.latent_dim, 4*4*1024)\n","        self.reshape = Reshape(1024,4,4)\n","\n","        self.iconv1 = create_iconv_layer(1024, 512)\n","        self.iconv2 = create_iconv_layer(512, 256)\n","        self.iconv3 = create_iconv_layer(256, 128)\n","        self.iconv4 = create_iconv_layer(128, 3)\n","\n","        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n","\n","\n","\n","    def forward(self, latent):\n","        x = self.dense1(latent)\n","        x = self.reshape(x)\n","        x = self.iconv1(x)\n","        x = self.iconv2(x)\n","        x = self.iconv3(x)\n","        x = self.iconv4(x)\n","        return x\n","\n","class Discriminator_(NeuralNetwork):\n","    \"\"\" discriminate real and fake images \"\"\"\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        N_features = 8\n","\n","        self.conv1 = create_conv_layer(colors_dim, 128)\n","        self.conv2 = create_conv_layer(128, 256)\n","        self.conv3 = create_conv_layer(256, 512)\n","        self.conv4 = create_conv_layer(512, 1024)\n","\n","        self.flat = Flatten()\n","\n","        self.dense = Sequential(\n","            Linear(1024*4*4, 1),\n","            Sigmoid(),\n","        )\n","        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n","\n","    def forward(self, images):\n","        x = self.conv1(images)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.flat(x)\n","        genuine = self.dense(x)\n","        return genuine\n","\n","\n","\n","class Pipeline:\n","    def __init__(self):\n","        self.encoder = Encoder().cuda()\n","        self.decoder = Decoder().cuda()\n","        self.discriminator = Discriminator().cuda()\n","        self.classifier = Classifier().cuda()\n","        self.image_classifier = ImageClassifier().cuda()\n","        self.inverse_classifier = InverseClassifier().cuda()\n","        self.iteration = 0\n","        self.discriminator_train_interval = 1\n","\n","    def discriminate(self, images):\n","        \"\"\" find if image is real or fake \"\"\"\n","        genuine = self.discriminator(images)\n","        return genuine\n","\n","\n","    def classify(self, images, detach=False):\n","        \"\"\" find class vector describing image \"\"\"\n","        latent_mu, latent_sigma = self.encoder(images)\n","        if detach:\n","            latent_mu = latent_mu.detach()\n","            latent_sigma = latent_sigma.detach()\n","        labels, noise = self.labels_from_latent(latent_mu, latent_sigma)\n","        return labels, noise\n","\n","    def classify_image(self, images, detach=False):\n","        \"\"\" find class vector describing image \"\"\"\n","        labels, noise = self.image_classifier(images)\n","        return labels, noise\n","\n","    def labels_from_latent(self, latent_mu, latent_sigma):\n","        \"\"\" find class vector corresponding to latent vector distribution \"\"\"\n","        labels, noise = self.classifier(latent_mu, latent_sigma)\n","        return labels, noise\n","\n","    def latent_from_images(self, images):\n","        \"\"\" obtain latent distribution from images \"\"\"\n","        latent_mu, latent_sigma = self.encoder(images)\n","        return latent_mu, latent_sigma\n","\n","    def latent_from_labels(self, labels, noise):\n","        \"\"\" create latent vector distribution from labels \"\"\"\n","        latent_mu, latent_sigma = self.inverse_classifier(labels, noise)\n","        return latent_mu, latent_sigma\n","\n","    def reproduce(self, images, detach=False):\n","        \"\"\" recreate given images \"\"\"\n","        latent_mu, latent_sigma = self.encoder(images)\n","        latent = gaussian_sampler(latent_mu, latent_sigma)\n","        if detach:\n","            latent = latent.detach()\n","        generated_images = self.generate_from_latent(latent)\n","        return generated_images\n","\n","    def generate_from_latent(self, latent):\n","        \"\"\" generate images from latent vector \"\"\"\n","        generated_images = self.decoder(latent)\n","        return generated_images\n","\n","    def generate_from_latent_distribution(self, latent_mu, latent_sigma):\n","        \"\"\" generate images from latent distribution \"\"\"\n","        latent = gaussian_sampler(latent_mu, latent_sigma)\n","        generated_images = self.generate_from_latent(latent)\n","        return generated_images\n","\n","    def generate_from_labels(self, labels, noise):\n","        \"\"\" generate images from class labels \"\"\"\n","        latent = self.latent_from_labels(labels, noise)\n","        generated_images = self.generate_from_latent_distribution(*latent)\n","        return generated_images\n","\n","    ## training\n","    def train_step(self, images, labels):\n","        \"\"\" train the architecture on given images and corresponding labels \"\"\"\n","        batch_size = images.shape[0]\n","        noise_gen = generate_noise(batch_size)\n","        labels_gen = generate_galaxy_labels(batch_size)\n","        latent_gen = generate_latent(batch_size)\n","        self.fake = zeros((batch_size,1)).cuda() # target values\n","        self.real = ones((batch_size,1)).cuda() # target values\n","\n","        if weights[\"image_label\"](self.iteration):\n","            self.train_image_classifier(images, labels)\n","        self.train_classifier(images, labels, noise_gen, labels_gen, latent_gen)\n","        self.train_inverse_classifier(images, labels, noise_gen, labels_gen, latent_gen)\n","        self.train_generator(images, labels, noise_gen, labels_gen, latent_gen)\n","        if not self.iteration % self.discriminator_train_interval:\n","            self.train_discriminator(images, labels, noise_gen, labels_gen, latent_gen)\n","        self.iteration += 1\n","\n","\n","    def estimate_accuracy(self, images, labels):\n","        \"\"\" train the architecture on given images and corresponding labels \"\"\"\n","        batch_size = images.shape[0]\n","        noise_gen = generate_noise(batch_size)\n","        labels_gen = generate_galaxy_labels(batch_size)\n","        latent_gen = generate_latent(batch_size)\n","        self.fake = zeros((batch_size,1)).cuda() # target values\n","        self.real = ones((batch_size,1)).cuda() # target values\n","\n","        if weights[\"image_label\"](self.iteration):\n","            self.train_image_classifier(images, labels, train=False)\n","        self.train_classifier(images, labels, noise_gen, labels_gen, latent_gen, train=False)\n","        self.train_inverse_classifier(images, labels, noise_gen, labels_gen, latent_gen, train=False)\n","        self.train_generator(images, labels, noise_gen, labels_gen, latent_gen, train=False)\n","        self.train_discriminator(images, labels, noise_gen, labels_gen, latent_gen, train=False)\n","        \n","\n","    def train_step_fails(self, images, labels):\n","        \"\"\" train the architecture on given images and corresponding labels \"\"\"\n","        batch_size = images.shape[0]\n","        noise_gen = generate_noise(batch_size) if weights[\"trick_labels\"](self.iteration) else 0\n","        labels_gen = generate_galaxy_labels(batch_size) if weights[\"unity_labels\"](self.iteration) else 0\n","        latent_gen = generate_latent(batch_size)\n","\n","        # starting from images\n","        d1 = self.discriminate(images)\n","        latent1 = self.latent_from_images(images)\n","        labels1, _ = self.labels_from_latent(*latent1)\n","        images1 = self.generate_from_latent_distribution(*latent1)\n","        d2 = self.discriminate(images1)\n","        labels2, _ = self.classify(images1) if weights[\"label_reproduced_image\"](self.iteration) else (0, 0)\n","\n","        # starting from labels\n","        imgs = self.generate_from_labels(labels, noise_gen) if weights[\"trick_labels\"](self.iteration) else 0\n","        d3 = self.discriminate(imgs) if weights[\"trick_labels\"](self.iteration) else 0\n","        labels3, _ = self.classify(imgs) if weights[\"label_labels\"](self.iteration) else 0\n","\n","        # starting from generated labels\n","        z = self.latent_from_labels(labels_gen, noise_gen) if weights[\"unity_labels\"](self.iteration) else 0\n","        labels4, noise4 = self.labels_from_latent(*z) if weights[\"unity_labels\"](self.iteration) else (0, 0)\n","        imgs = self.generate_from_latent_distribution(*z) if weights[\"trick_fake_labels\"](self.iteration) else 0\n","        d4 = self.discriminate(imgs) if weights[\"trick_fake_labels\"](self.iteration) else 0\n","        labels5, _ = self.classify(imgs) if weights[\"reproduce_fake_labels\"](self.iteration) else (0, 0)\n","\n","        # starting from generated latent\n","        latent2 = self.latent_from_labels(*self.labels_from_latent(*latent_gen)) if weights[\"unity_latent\"](self.iteration) else 0\n","        d5 = self.discriminate(self.generate_from_latent_distribution(*latent_gen))\n","\n","        self.fake = zeros((batch_size,1)).cuda() # target values\n","        self.real = ones((batch_size,1)).cuda() # target values\n","        loss_discriminator = self.get_loss_discriminator(d1, d2, d3, d4, d5)\n","        loss_generator = self.get_loss_generator(images, labels, labels_gen, images1, latent1, d2, d3, d4, labels2, labels3, labels5)\n","        loss_classifier = self.get_loss_classifier(labels, labels_gen, labels1, labels2, labels3, labels5)\n","        loss_inverse_classifier = self.get_loss_inverse_classifier(latent_gen, labels_gen, noise_gen, latent2, labels4, noise4)\n","\n","        update_networks_on_loss(loss_discriminator, self.discriminator)\n","        update_networks_on_loss(loss_generator, self.encoder, self.decoder)\n","        update_networks_on_loss(loss_classifier, self.classifier)\n","        update_networks_on_loss(loss_inverse_classifier, self.inverse_classifier)\n","\n","        self.iteration += 1\n","\n","    ## discriminator\n","    def train_discriminator(self, images, labels, noise_gen, labels_gen, latent_gen, train=True):\n","        # from images\n","        d1 = self.discriminate(images) if weights[\"discriminate_real\"](self.iteration) else 0\n","        if weights[\"discriminate_reproduced\"](self.iteration):\n","            images1 = self.reproduce(images)\n","            d2 = self.discriminate(images1) \n","        else:\n","            d2 = 0\n","        # from labels\n","        if weights[\"discriminate_labels\"](self.iteration):\n","            imgs = self.generate_from_labels(labels, noise_gen)\n","            d3 = self.discriminate(imgs)\n","        else:\n","            d3 = 0\n","        # from fake labels\n","        if weights[\"discriminate_fake_labels\"](self.iteration):\n","            imgs = self.generate_from_labels(labels_gen, noise_gen)\n","            d4 = self.discriminate(imgs)\n","        else:\n","            d4 = 0\n","        # from latent\n","        if weights[\"discriminate_latent\"](self.iteration):\n","            d5 = self.discriminate(self.generate_from_latent_distribution(*latent_gen))\n","        else:\n","            d5 = 0\n","\n","        if train:\n","            loss = self.get_loss_discriminator(d1, d2, d3, d4, d5)\n","            update_networks_on_loss(loss, self.discriminator)\n","            losses[\"discriminator\"] = loss.item()\n","        else:\n","            accuracy[\"discriminate_real\"] = accuracy_discriminator(self.real, d1)\n","            accuracy[\"discriminate_reproduced\"] = accuracy_discriminator(self.fake, d2)\n","            accuracy[\"discriminate_labels\"] = accuracy_discriminator(self.fake, d3)\n","            accuracy[\"discriminate_fake_labels\"] = accuracy_discriminator(self.fake, d4)\n","            accuracy[\"discriminate_latent\"] = accuracy_discriminator(self.fake, d5)\n","\n","\n","\n","    def get_loss_discriminator(self, d1, d2, d3, d4, d5):\n","        loss = self.get_loss_discriminate_real(d1)\n","        loss += self.get_loss_discriminate_reproduced(d2)\n","        loss += self.get_loss_discriminate_labels(d3)\n","        loss += self.get_loss_discriminate_fake_labels(d4)\n","        loss += self.get_loss_discriminate_latent(d5)\n","        return loss\n","\n","    @weighted_loss(\"discriminate_real\")\n","    def get_loss_discriminate_real(self, d1):\n","        return loss_adversarial(self.real, d1)\n","\n","    @weighted_loss(\"discriminate_reproduced\")\n","    def get_loss_discriminate_reproduced(self, d2):\n","        return loss_adversarial(self.fake, d2)\n","\n","    @weighted_loss(\"discriminate_labels\")\n","    def get_loss_discriminate_labels(self, d3):\n","        return loss_adversarial(self.fake, d3)\n","\n","    @weighted_loss(\"discriminate_fake_labels\")\n","    def get_loss_discriminate_fake_labels(self, d4):\n","        return loss_adversarial(self.fake, d4)\n","\n","    @weighted_loss(\"discriminate_latent\")\n","    def get_loss_discriminate_latent(self, d5):\n","        return loss_adversarial(self.fake, d5)\n","\n","\n","    ## generator\n","    def train_generator(self, images, labels, noise_gen, labels_gen, latent_gen, train=True):\n","        # from images\n","        latent1 = self.latent_from_images(images)\n","        images1 = self.generate_from_latent_distribution(*latent1)\n","        d2 = self.discriminate(images1) if weights[\"trick_reproduced\"](self.iteration) else 0\n","        labels2, _ = self.classify(images1) if weights[\"reproduce_image_labels\"](self.iteration) else (0, 0)\n","        # from labels\n","        if weights[\"trick_labels\"](self.iteration):\n","            imgs = self.generate_from_labels(labels, noise_gen)\n","            d3 = self.discriminate(imgs)\n","            labels3, _ = self.classify(imgs)\n","        else:\n","            d3, labels3 = 0, 0\n","        # from generated labels\n","        if weights[\"trick_fake_labels\"](self.iteration):\n","            z = self.latent_from_labels(labels_gen, noise_gen)\n","            imgs = self.generate_from_latent_distribution(*z)\n","            d4 = self.discriminate(imgs)\n","            labels5, _ = self.classify(imgs)\n","        else:\n","            d4, labels5 = 0, 0\n","\n","        if train:\n","            loss = self.get_loss_generator(images, labels, labels_gen, images1, latent1, d2, d3, d4, labels2, labels3, labels5)\n","            update_networks_on_loss(loss, self.encoder, self.decoder)\n","            losses[\"generator\"] = loss.item()\n","            if np.isnan(loss.item()):\n","                set_trace()\n","        else:\n","            accuracy[\"trick_reproduced\"] = accuracy_discriminator(self.real, d2)\n","            accuracy[\"reproduce_image_labels\"] = accuracy_classifier(labels, labels2)\n","            accuracy[\"trick_labels\"] = accuracy_discriminator(self.real, d3)\n","            accuracy[\"reproduce_labels\"] = accuracy_classifier(labels, labels3)\n","            accuracy[\"trick_fake_labels\"] = accuracy_discriminator(self.real, d4)\n","            accuracy[\"reproduce_fake_labels\"] = accuracy_classifier(labels_gen, labels5)\n","\n","\n","    def get_loss_generator(self, images, labels, labels_gen, images1, latent1, d2, d3, d4, labels2, labels3, labels5):\n","        loss = self.get_loss_regularize_latent(latent1)\n","        loss += self.get_loss_reproduce_image(images, images1)\n","        loss += self.get_loss_trick_reproduced(d2)\n","        loss += self.get_loss_reproduce_image_labels(labels, labels2)\n","        loss += self.get_loss_trick_labels(d3)\n","        loss += self.get_loss_reproduce_labels(labels, labels3)\n","        loss += self.get_loss_trick_fake_labels(d4)\n","        loss += self.get_loss_reproduce_fake_labels(labels_gen, labels5)\n","        return loss\n","\n","    @weighted_loss(\"regularize_latent\")\n","    def get_loss_regularize_latent(self, latent1):\n","        return loss_kl(latent1)\n","\n","    @weighted_loss(\"reproduce_image\")\n","    def get_loss_reproduce_image(self, images, images1):\n","        return loss_reconstruction(images, images1)\n","\n","    @weighted_loss(\"trick_reproduced\")\n","    def get_loss_trick_reproduced(self, d2):\n","        return loss_adversarial(self.real, d2)\n","\n","    @weighted_loss(\"trick_labels\")\n","    def get_loss_trick_labels(self, d3):\n","        return loss_adversarial(self.real, d3)\n","\n","    @weighted_loss(\"trick_fake_labels\")\n","    def get_loss_trick_fake_labels(self, d4):\n","        return loss_adversarial(self.real, d4)\n","\n","    @weighted_loss(\"reproduce_image_labels\")\n","    def get_loss_reproduce_image_labels(self, labels, labels2):\n","        return loss_class(labels, labels2)\n","\n","    @weighted_loss(\"reproduce_labels\")\n","    def get_loss_reproduce_labels(self, labels, labels3):\n","        return loss_class(labels, labels3)\n","\n","    @weighted_loss(\"reproduce_fake_labels\")\n","    def get_loss_reproduce_fake_labels(self, labels, labels5):\n","        return loss_class(labels, labels5)\n","\n","\n","    ## image classifier\n","    def train_image_classifier(self, images, labels, train=True):\n","        labels1, _ = self.classify_image(images)\n","\n","        if train:\n","#            loss = loss_class(labels, labels1)\n","            loss = loss_class(labels[:,ixs_labels], labels1[:,ixs_labels])\n","            update_networks_on_loss(loss, self.image_classifier)\n","            losses[\"image_classifier\"] = loss.item()\n","        else:\n","#            accuracy[\"image_label\"] = accuracy_classifier(labels, labels1)\n","            accuracy[\"image_label\"] = accuracy_classifier(labels[:,ixs_labels], labels1[:,ixs_labels])\n","\n","\n","\n","    ## classifier\n","    def train_classifier(self, images, labels, noise_gen, labels_gen, latent_gen, train=True):\n","        # starting from images\n","        if weights[\"label_image\"](self.iteration):\n","            latent1 = self.latent_from_images(images)\n","            labels1, _ = self.labels_from_latent(*latent1)\n","            images1 = self.generate_from_latent_distribution(*latent1)\n","            labels2, _ = self.classify(images1)\n","        else:\n","            labels1, labels2 = 0, 0\n","        # starting from labels\n","        if weights[\"label_labels\"](self.iteration):\n","            imgs = self.generate_from_labels(labels, noise_gen)\n","            labels3, _ = self.classify(imgs)\n","        else:\n","            labels3 = 0\n","\n","        # starting from generated labels\n","        if weights[\"label_fake_labels\"](self.iteration):\n","            z = self.latent_from_labels(labels_gen, noise_gen)\n","            imgs = self.generate_from_latent_distribution(*z)\n","            labels5, _ = self.classify(imgs)\n","        else:\n","            labels5 = 0\n","\n","        if train:\n","            loss = self.get_loss_classifier(labels, labels_gen, labels1, labels2, labels3, labels5)\n","            update_networks_on_loss(loss, self.classifier, self.encoder)\n","            losses[\"classifier\"] = loss.item()\n","            if np.isnan(loss.item()):\n","                set_trace()\n","                print(labels1, labels2, labels3, labels5)\n","        else:\n","#            accuracy[\"label_image\"] = accuracy_classifier(labels, labels1)\n","            accuracy[\"label_image\"] = accuracy_classifier(labels[:,ixs_labels], labels1[:,ixs_labels])\n","            accuracy[\"label_reproduced_image\"] = accuracy_classifier(labels, labels2)\n","            accuracy[\"label_labels\"] = accuracy_classifier(labels, labels3)\n","            accuracy[\"label_fake_labels\"] = accuracy_classifier(labels_gen, labels5)\n","            if False:\n","                self.plot_mislabeled_galaxies(images, labels, labels1)\n","\n","    def plot_mislabeled_galaxies(self, images, labels, prediction, N=10, p=0.1):\n","        accurate = abs(labels[:,ixs_labels] - prediction[:,ixs_labels]) < p\n","        accurate = all(accurate, dim=1)\n","        i = 0\n","        for ix, acc in enumerate(accurate):\n","            if not acc:\n","                plt.imshow(np.transpose(images[ix].cpu(), (1,2,0)))\n","                plt.show()\n","                print((labels[ix,ixs_labels]*100).round() / 100)\n","                print((prediction[ix,ixs_labels]*100).round() / 100)\n","                print((abs(prediction[ix,ixs_labels]-labels[ix,ixs_labels])*100).round() / 100)\n","                i += 1\n","                if i >= N:\n","                    break\n","\n","\n","    def get_loss_classifier(self, labels, labels_gen, labels1, labels2, labels3, labels5):\n","#        loss = self.get_loss_label_image(labels, labels1)\n","        loss = self.get_loss_label_image(labels[:,ixs_labels], labels1[:,ixs_labels])\n","        loss += self.get_loss_label_reproduced_image(labels, labels2)\n","        loss += self.get_loss_label_labels(labels, labels3)\n","        loss += self.get_loss_label_fake_labels(labels_gen, labels5)\n","        return loss\n","\n","    @weighted_loss(\"label_image\")\n","    def get_loss_label_image(self, labels, labels1):\n","        return loss_class(labels, labels1)\n","\n","    @weighted_loss(\"label_reproduced_image\")\n","    def get_loss_label_reproduced_image(self, labels, labels2):\n","        return loss_class(labels, labels2)\n","\n","    @weighted_loss(\"label_labels\")\n","    def get_loss_label_labels(self, labels, labels3):\n","        return loss_class(labels, labels3)\n","\n","    @weighted_loss(\"label_fake_labels\")\n","    def get_loss_label_fake_labels(self, labels, labels5):\n","        return loss_class(labels, labels5)\n","\n","    ## inverse classifier\n","    def train_inverse_classifier(self, images, labels, noise_gen, labels_gen, latent_gen, train=True):\n","        # from generated labels\n","        if weights[\"unity_labels\"](self.iteration):\n","            labels4, noise4 = self.labels_from_latent(*self.latent_from_labels(labels_gen, noise_gen))\n","        else:\n","            labels4, noise4 = 0, 0\n","        # from latent\n","        if weights[\"unity_latent\"](self.iteration):\n","            label, noise = self.labels_from_latent(*latent_gen)\n","            latent2 = self.latent_from_labels(label, noise)\n","            latent2 = self.latent_from_labels(*self.labels_from_latent(*latent_gen))\n","        else:\n","            latent2 = 0\n","        if train:\n","            loss = self.get_loss_inverse_classifier(latent_gen, labels_gen, noise_gen, latent2, labels4, noise4)\n","            update_networks_on_loss(loss, self.inverse_classifier)\n","            losses[\"inverse_classifier\"] = loss.item()\n","        else:\n","            accuracy[\"unity_labels\"] = accuracy_classifier(labels_gen, labels4)\n","            accuracy[\"unity_latent\"] = accuracy_classifier(latent_gen, latent2)\n","#            accuracy[\"unity_latent\"] = accuracy_classifier(cat(latent_gen, dim=1), cat(latent2, dim=1))\n","\n","\n","    def get_loss_inverse_classifier(self, latent_gen, labels_gen, noise_gen, latent2, labels4, noise4):\n","        loss = self.get_loss_unity_labels(labels_gen, noise_gen, labels4, noise4)\n","        loss += self.get_loss_unity_latent(latent_gen, latent2)\n","        return loss\n","\n","    @weighted_loss(\"unity_latent\")\n","    def get_loss_unity_latent(self, latent_gen, latent2):\n","        return loss_latent(latent_gen, latent2)\n","\n","    @weighted_loss(\"unity_labels\")\n","    def get_loss_unity_labels(self, labels_gen, noise_gen, labels4, noise4):\n","        return loss_class(labels_gen, labels4) + loss_metric(noise_gen, noise4)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rkkAioqzn-GM"},"source":["pipe = Pipeline()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jltqr7mPeNOK"},"source":["batch_size = 64\n","\n","pipe.iteration = 20001\n","\n","images = rand(batch_size, 3, 64, 64).cuda()\n","labels = rand(batch_size, labels_dim).cuda()\n","\n","pipe.train_step(images, labels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqrGgxbzq7mn"},"source":["latent = pipe.latent_from_images(images)\n","for l in latent:\n","    print(l.device)\n","loss_kl(latent).device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xF15EJdcS4sz"},"source":["# basic test Neural Networks\n","check whether the pipeline components work on their own"]},{"cell_type":"code","metadata":{"id":"R9nQvGR3nP2d"},"source":["summarize(Discriminator4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rF6t2XHZn28G"},"source":["import numpy as np\n","from torch import rand\n","\n","\n","net = Encoder()\n","\n","print(\"N_parameters:\", np.sum([np.prod(p.size()) for p in net.parameters()]))\n","input_dummy = rand(3, *input_size)\n","label_dummy = rand(3, labels_dim)\n","\n","net(input_dummy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnAFagCuTpgn"},"source":["net = Decoder()\n","\n","print(\"N_parameters:\", np.sum([np.prod(p.size()) for p in net.parameters()]))\n","input_dummy = rand(3, parameter.latent_dim)\n","label_dummy = rand(3, labels_dim)\n","\n","net(input_dummy).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFU3aEjsa1fO"},"source":["net = Discriminator()\n","\n","print(\"N_parameters:\", np.sum([np.prod(p.size()) for p in net.parameters()]))\n","input_dummy = rand(3, parameter.latent_dim)\n","label_dummy = rand(3, labels_dim)\n","\n","net(input_dummy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQWWIPTjbIi1"},"source":["net = Classifier()\n","\n","print(\"N_parameters:\", np.sum([np.prod(p.size()) for p in net.parameters()]))\n","input_dummy = rand(3, parameter.latent_dim)\n","label_dummy = rand(3, labels_dim)\n","\n","net(input_dummy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bSxX80AGbrRa"},"source":["net = InverseClassifier()\n","\n","print(\"N_parameters:\", np.sum([np.prod(p.size()) for p in net.parameters()]))\n","input_dummy = rand(3, labels_dim)\n","label_dummy = rand(3, labels_dim)\n","\n","net(input_dummy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SoUTAxuN_47Z"},"source":["net = VAE()\n","images_dummy = rand(5,3,64,64).cuda()\n","labels_dummy = rand(5,3).cuda()\n","pred = net(images_dummy, labels_dummy)\n","pred.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JVoRvJap_402"},"source":["net = VAEGAN()\n","input_dummy = rand(3,3,64,64).cuda()\n","labels_dummy = rand(3,3).cuda()\n","pred = net(input_dummy, labels_dummy)\n","pred.shape, pred[:,0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJ4i89gkVnDl"},"source":["# Training Data\n","load data from files at google drive"]},{"cell_type":"code","metadata":{"id":"y60D2u2BVmwj"},"source":["x_train = get_x_train()\n","labels_train = get_labels_train()\n","N_samples = x_train.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxHuUIcb3qmK"},"source":["from torch import tensor\n","from torch.utils.data import Dataset, random_split\n","\n","N_test = 3000\n","\n","np.random.seed(N_test)\n","ixs_train = np.arange(x_train.shape[0])\n","np.random.shuffle(ixs_train)\n","ixs_test, ixs_train = np.split(ixs_train, (N_test,))\n","\n","np.random.seed()\n","\n","x_test = x_train[ixs_test]\n","labels_test = labels_train[ixs_test]\n","\n","x_train = x_train[ixs_train]\n","labels_train = labels_train[ixs_train]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dix3TOy4tByW"},"source":["# Training new network"]},{"cell_type":"code","metadata":{"id":"Pcv74cShtEpJ"},"source":["## if you want to change any parameter:\n","parameter.learning_rate = 0.002 #0.01\n","parameter.latent_dim = 2048\n","parameter.latent_dim = N_resnet // 2\n","\n","pipe = Pipeline()\n","\n","#'''  load pretrained models\n","pipe.encoder.load()\n","pipe.classifier.load()\n","#'''\n","\n","#'''  choose which class label groups to train on\n","groups_classified = [6, 8] #[1, 2, 6, 7]\n","ixs_labels = []\n","for g in groups_classified:\n","    ixs_labels.extend(class_groups_indices[g]-1)\n","ixs_labels = tensor(ixs_labels)\n","#'''\n","\n","pipe.discriminator_train_interval = 1 #10\n","\n","if track_hyperparameter:\n","    wandb.init(project=\"galaxy-generator\", # top level identifier\n","            group=\"check_losses\", # second level identifier, to seperate several groups of tests\n","            job_type=\"training\", # third level identifier, organize different jobs like training and evaluation\n","            tags=[\"all one\"], # temporary tags to organize different tasks together\n","            name=f\"no class loss, delta=0\", # bottom level identifier, label of graph in UI\n","            config=parameter.return_parameter_dict()  # here we fill the hyperparameters\n","    )\n","\n","epochs = 50\n","batch_size = 64\n","steps = x_train.shape[0] // batch_size\n","save_interval = 500\n","loss_interval = 100\n","\n","losses_series = {name:[] for name in list(weights.keys())+[\"discriminator\", \"generator\", \"classifier\",\"inverse_classifier\", \"image_classifier\"]}\n","accuracies = [\n","              # discriminator\n","              \"discriminate_real\", \"discriminate_reproduced\", \"discriminate_latent\", \"discriminate_labels\", \"discriminate_fake_labels\",\n","              # generator\n","              \"trick_reproduced\", \"trick_labels\", \"trick_fake_labels\",\n","              \"reproduce_image_labels\", \"reproduce_labels\", \"reproduce_fake_labels\",\n","              # classifier\n","              \"label_images\", \"label_reproduced_image\", \"label_labels\", \"label_fake_labels\",\n","              # inverse classifier\n","              \"unity_labels\", \"unity_latent\"\n","              ]\n","accuracy_series = {name:[] for name in list(weights.keys())}\n","tests = [\n","         \"N_pairs_real\", \"N_pairs_reproduced\", \"N_pairs_fake\", \n","         \"average_residual_real\", \"average_residual_reproduced\", \"average_residual_fake\", \n","         \"intensity_real\", \"intensity_reproduced\", \"intensity_fake\", \n","         \"bluriness_real\", \"bluriness_reproduced\", \"bluriness_fake\"\n","        ]\n","test_series = {name:[] for name in tests}\n","losses = {}\n","\n","from statistical_tests import get_image_pair_residual_statistics, compute_residual_average_image, compute_average_intensity, compute_average_bluriness\n","\n","discriminator_losses = []\n","generator_losses = []\n","encoder_losses = []\n","classifier_losses = []\n","inverse_classifier_losses = []\n","\n","iteration = 1\n","epoch = 1\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hzGkfyltzmN"},"source":["t0 = time()\n","while epoch <= epochs:\n","    sample_generator = make_training_sample_generator(batch_size, x_train, labels_train)\n","    step = 1\n","    while step <= steps:\n","        images, labels = next(sample_generator)\n","        pipe.train_step(images, labels)\n","\n","        if not pipe.iteration % loss_interval:\n","            t1 = time()\n","            print(f\"iteration {pipe.iteration}, epoch {epoch}, batch {step+1}/{steps}, took {t1-t0:.1f} s\")\n","            t0 = t1\n","            ixs = np.random.choice(np.arange(x_test.shape[0]), size=300, replace=False )\n","            pipe.estimate_accuracy(images=x_test[ixs].cuda(), labels=labels_test[ixs].cuda())\n","            for name in accuracy_series.keys():\n","                try:\n","                    accuracy_series[name].append(accuracy[name])\n","                except:\n","                    accuracy_series[name].append(0.)\n","            for name in losses_series.keys():\n","                try:\n","                    losses_series[name].append(losses[name])\n","                except:\n","                    losses_series[name].append(0.)\n","                if name in [\"discriminator\", \"generator\", \"classifier\",\"inverse_classifier\", \"image_classifier\"]: #[\"discriminate_real\", \"discriminate_generated\", \"generator_from_labels\", \"generator_reproduced\"]:\n","                    try:\n","                        print(f\"{name}: {losses[name]:.2}\", end=\", \")\n","                    except:\n","                        pass\n","            else:\n","                print(\"\")\n","            \n","#            print(f\"iteration {iteration}, epoch {epoch}, batch {step}/{steps}, \" + \\\n","#                f\"disc_loss {losses['discriminator']:.5},  gen_loss {losses['generator']:.5}\")\n","\n","        if not pipe.iteration % save_interval:\n","            write_generated_galaxy_images_iteration(iteration=pipe.iteration-1, images=images.detach().cpu().numpy())\n","            reproduced_images = pipe.reproduce(images)\n","            write_generated_galaxy_images_iteration(iteration=pipe.iteration, images=reproduced_images.detach().cpu().numpy())\n","            noise = generate_noise(batch_size)\n","            generated_images = pipe.generate_from_labels(labels, noise)\n","            write_generated_galaxy_images_iteration(iteration=pipe.iteration+1, images=generated_images.detach().cpu().numpy())\n","            test_series[\"N_pairs_real\"].append(get_image_pair_residual_statistics(images)[0])\n","            test_series[\"N_pairs_reproduced\"].append(get_image_pair_residual_statistics(reproduced_images)[0])\n","            test_series[\"N_pairs_fake\"].append(get_image_pair_residual_statistics(generated_images)[0])\n","            test_series[\"average_residual_real\"].append(compute_residual_average_image(images, last_images))\n","            test_series[\"average_residual_reproduced\"].append(compute_residual_average_image(images, reproduced_images))\n","            test_series[\"average_residual_fake\"].append(compute_residual_average_image(images, generated_images))\n","            test_series[\"intensity_real\"].append(compute_average_intensity(images.detach().cpu().numpy()))\n","#            test_series[\"intensity_reproduced\"].append(compute_average_intensity(reproduced_images.detach().cpu().numpy()))\n","#            test_series[\"intensity_fake\"].append(compute_average_intensity(generated_images.detach().cpu().numpy()))\n","            test_series[\"bluriness_real\"].append(compute_average_bluriness(images.detach().cpu()))\n","            test_series[\"bluriness_reproduced\"].append(compute_average_bluriness(reproduced_images.detach().cpu()))\n","            test_series[\"bluriness_fake\"].append(compute_average_bluriness(generated_images.detach().cpu()))\n","        step += 1\n","        iteration += 1\n","        last_images = images\n","    epoch += 1\n","\n","    # save a plot of the costs\n","    fig, axs = plt.subplots(1, 6, figsize=(30,8))\n","    '''\n","    for name in [\"discriminator\", \"generator\", \"classifier\",\"inverse_classifier\"]:\n","        ax.plot(np.arange(loss_interval,iteration,loss_interval), losses_series[name], label=name)\n","    ax.set_yscale(\"log\")\n","    ax.legend()\n","    plt.savefig(folder_results+\"cost_vs_iteration.png\")\n","    plt.show()\n","    plt.close()\n","    '''\n","\n","    for name, loss_series in losses_series.items():\n","        if not loss_series:\n","            continue\n","        if name in [\"discriminator\", \"generator\", \"classifier\",\"inverse_classifier\", \"image_classifier\"]:\n","            ax = axs[0]\n","        elif \"discriminate\" in name:\n","            ax = axs[1]\n","        elif \"label_\" in name:\n","            ax = axs[3]\n","        elif \"unity_\" in name:\n","            ax = axs[4]\n","        elif \"image_\" in name:\n","            ax = axs[5]\n","        else:\n","            ax = axs[2]\n","#        ax.plot(np.arange(loss_interval,pipe.iteration,loss_interval), loss_series, label=name)\n","        ax.plot(np.arange(1,len(loss_series)+1) * loss_interval, loss_series, label=name)\n","    axs[0].set_title(\"total loss\")\n","    axs[1].set_title(\"discriminator\")\n","    axs[2].set_title(\"generator\")\n","    axs[3].set_title(\"classifier\")\n","    axs[4].set_title(\"inverse classifier\")\n","    axs[5].set_title(\"image classifier\")\n","    for ax in axs:\n","        ax.set_yscale(\"log\")\n","        ax.legend()\n","    plt.savefig(folder_results+\"cost_vs_iteration.png\")\n","    plt.show()\n","    plt.close()\n","\n","    # save plot of tests\n","    fig, axs = plt.subplots(1, 4, figsize=(30,8))\n","    for name, series in test_series.items():\n","        if not series:\n","            continue\n","        if \"N_pairs\" in name:\n","            ax = axs[0]\n","        elif \"residual\" in name:\n","            ax = axs[1]\n","        elif \"intensity\" in name:\n","            ax = axs[2]\n","        elif \"bluriness\" in name:\n","            ax = axs[3]\n","#        ax.plot(np.arange(save_interval,pipe.iteration,save_interval), series, label=name, linestyle=\":\" if \"real\" in name else \"-\")\n","        ax.plot(np.arange(1,len(series)+1) * loss_interval, series, label=name, linestyle=\":\" if \"real\" in name else \"-\")\n","    axs[0].set_title(\"N_pairs\")\n","    axs[1].set_title(\"average residual\")\n","    axs[2].set_title(\"intensity\")\n","    axs[3].set_title(\"bluriness\")\n","    for ax in axs:\n","        ax.set_yscale(\"log\")\n","        ax.legend()\n","    plt.savefig(folder_results+\"tests_vs_iteration.png\")\n","    plt.show()\n","    plt.close()\n","\n","    # save plot of accuracies\n","    fig, axs = plt.subplots(1, 5, figsize=(30,8))\n","    for name, series in accuracy_series.items():\n","        linestyle = \"-\"\n","        if not series:\n","            continue\n","        if \"discriminate\" in name:\n","            ax = axs[0]\n","        elif \"reproduce_\" in name:\n","            ax = axs[1]\n","        elif \"label_\" in name:\n","            ax = axs[2]\n","        elif \"unity_\" in name:\n","            ax = axs[3]\n","        elif \"image_\" in name:\n","#            ax = axs[4]\n","            ax = axs[2]\n","            linestyle = \"-.\"\n","        else:\n","            ax = axs[1]\n","#        ax.plot(np.arange(loss_interval,pipe.iteration,loss_interval), series, label=name, linestyle=linestyle)\n","        ax.plot(np.arange(1,len(series)+1) * loss_interval, series, label=name, linestyle=linestyle)\n","    axs[0].set_title(\"discriminator\")\n","    axs[1].set_title(\"generator\")\n","    axs[2].set_title(\"classifier\")\n","    axs[3].set_title(\"inverse classifier\")\n","    axs[4].set_title(\"image classifier\")\n","    for ax in axs:\n","        ax.legend()\n","        ax.set_ylim(0,1)\n","    plt.savefig(folder_results+\"accuracy_vs_iteration.png\")\n","    plt.show()\n","    plt.close()\n","\n","\n","\n","\n","    ### really save?\n","    #pipeline.decoder.save()\n","    pipe.encoder.save()\n","    pipe.classifier.save()\n","    #pipeline.discriminator.save()\n","\n","\n","if track_hyperparameter:\n","    ## after training is done and all measures are written, finalize with\n","    wandb.finish()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FbRPCQ1_aVI"},"source":["fig, axs = plt.subplots(1, 3, figsize=(12,3))\n","for name, loss_series in losses_series.items():\n","    if not loss_series:\n","        continue\n","    if \"discriminate\" in name:\n","        ax = axs[0]\n","    elif \"generator\" in name:\n","        if len(name) > 22:\n","            continue\n","        ax = axs[1]\n","    else:\n","        ax = axs[2]\n","    ax.plot(np.arange(loss_interval,iteration,loss_interval), loss_series, label=name)\n","for ax in axs:\n","    ax.set_yscale(\"log\")\n","    ax.legend()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ywYMJ8togGHF"},"source":["def dot_flat(a, b):\n","    \"\"\" multiply each element in a to each element in b, return results in flat array \"\"\"\n","    result = a[...,None] @ b[None,...]\n","    return result.flatten()\n","\n","\n","def label_leaves_likelihood(labels):\n","    \"\"\" compute likelihood for all leaves in the hierarchical label tree \"\"\"\n","    def P_group(g):\n","        return labels[class_groups_indices[g]-1]\n","    P3 = P_group(3).sum()\n","    P5 = P_group(5).sum()\n","    P6 = P_group(6).sum()\n","#    P6 = P_group(1)[:2].sum()   \n","    P11 = P_group(11).sum()\n","    P3 = P3 if P3 else 1.\n","    P5 = P5 if P5 else 1.\n","    P6 = P6 if P6 else 1.\n","    P11 = P11 if P11 else 1.\n","    ## artifact                 1\n","    leaves = [P_group(1)[2]]\n","    ## round regular            3\n","    l = P_group(7) * P_group(6)[1] / P6\n","    leaves += list(l)\n","    ## round odd                21\n","    l = dot_flat(P_group(7), P_group(8)) / P6\n","    leaves += list(l)\n","    ## disk edge regular        3\n","    l = P_group(9) * P_group(6)[1] / P6\n","    leaves += list(l)\n","    ## disk edge odd            21\n","    l = dot_flat(P_group(9), P_group(8)) / P6\n","    leaves += list(l)\n","    ## disk arms regular        144\n","    l = dot_flat(dot_flat(dot_flat(P_group(3), P_group(10)), P_group(11)), P_group(5)) * P_group(6)[1] / P3 / P11 / P5 / P6 \n","    leaves += list(l)\n","    ## disk arms odd            1008\n","    l = dot_flat(dot_flat(dot_flat(dot_flat(P_group(3), P_group(10)), P_group(11)), P_group(5)), P_group(8)) / P3 / P11 / P5 / P6 \n","    leaves += list(l)\n","    ## disk no arms regular     8\n","    l = dot_flat(P_group(3), P_group(5)) * P_group(4)[1] * P_group(6)[1] / P3 / P5 / P6\n","    leaves += list(l)\n","    ## disk no arms odd         65\n","    l = dot_flat(dot_flat(P_group(3), P_group(5)), P_group(8)) * P_group(4)[1] / P3 / P5 / P6\n","    leaves += list(l)\n","    ## total                    1265\n","    return tensor(leaves)\n","\n","N = 2\n","ix = np.random.randint(x_train.shape[0]-5)\n","\n","image = x_train[ix:ix+2].cuda()\n","\n","label, noise = pipe.classify_image(image)\n","label = generate_galaxy_labels(2)\n","\n","print(f\"{label_leaves_likelihood(label[0].cpu()).sum().item():.4} == {1.:.4}\")\n","print(f\"{label[0,class_groups_indices[1]-1].sum().item():.4} == {1.:.4}\")\n","print(f\"{label[0,class_groups_indices[2]-1].sum().item():.4} == {label[0,1].item():.4}\")\n","print(f\"{label[0,class_groups_indices[3]-1].sum().item():.4} == {label[0,4].item():.4}\")\n","print(f\"{label[0,class_groups_indices[4]-1].sum().item():.4} == {label[0,4].item():.4}\")\n","print(f\"{label[0,class_groups_indices[5]-1].sum().item():.4} == {label[0,4].item():.4}\")\n","print(f\"{label[0,class_groups_indices[6]-1].sum().item():.4} == {1.:.4}\")\n","print(f\"{label[0,class_groups_indices[7]-1].sum().item():.4} == {label[0,0].item():.4}\")\n","print(f\"{label[0,class_groups_indices[8]-1].sum().item():.4} == {label[0,13].item():.4}\")\n","print(f\"{label[0,class_groups_indices[9]-1].sum().item():.4} == {label[0,3].item():.4}\")\n","print(f\"{label[0,class_groups_indices[10]-1].sum().item():.4} == {label[0,7].item():.4}\")\n","print(f\"{label[0,class_groups_indices[11]-1].sum().item():.4} == {label[0,7].item():.4}\")\n","print(label[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aACg2yM0x2ln"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"Vj-UnIeJx7eU"},"source":["## if you want to change any parameter:\n","parameter.learning_rate = 0.0002\n","## the following can also be changed during runtime\n","parameter.alpha = 1.\n","parameter.beta = 1.\n","parameter.gamma = 1.\n","parameter.delta = 1.\n","parameter.zeta = 1.\n","\n","\n","## start training with fresh, untrained networks\n","pipeline.decoder = Decoder4().cuda()\n","pipeline.encoder = Encoder4().cuda()\n","pipeline.discriminator = Discriminator4().cuda()\n","\n","## if you want to change discriminator, encoder or decoder network:\n","#pipeline.decoder = YourDecoder().cuda()\n","## before you create the VAE and VAEGAN\n","\n","if track_hyperparameter:\n","    wandb.init(project=\"galaxy-generator\", # top level identifier\n","            group=\"check_losses\", # second level identifier, to seperate several groups of tests\n","            job_type=\"training\", # third level identifier, organize different jobs like training and evaluation\n","            tags=[\"all one\"], # temporary tags to organize different tasks together\n","            name=f\"no class loss, delta=0\", # bottom level identifier, label of graph in UI\n","            config=parameter.return_parameter_dict()  # here we fill the hyperparameters\n","    )\n","\n","\n","\n","\n","epochs = 20\n","batch_size = 64\n","steps = N_samples // batch_size\n","save_interval = 200\n","\n","discriminator_losses = []\n","discriminator_losses_real = []\n","discriminator_losses_fake = []\n","generator_losses = []\n","\n","valid = ones((batch_size,1)).cuda()\n","fake = zeros((batch_size,1)).cuda()\n","\n","vae = VAE()\n","vaegan = VAEGAN()\n","\n","iteration = 0\n","epoch = 0\n","step = 0\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDiXsx8Mxfti"},"source":["def training_step(images, labels):\n","    global iteration\n","\n","    # -------------------\n","    # Train Discriminator\n","    # -------------------\n","    vae.train(False)\n","    pipeline.discriminator.train(True)\n","    pipeline.discriminator.zero_grad()\n","\n","    generated_images = vae(images, labels)\n","    target_real = cat((valid,labels), dim=1)\n","    prediction_real = pipeline.discriminator(images)[:,:1+labels_dim]\n","    target_fake = cat((fake, labels), dim=1)\n","    prediction_fake = pipeline.discriminator(generated_images)[:,:1+labels_dim]\n","\n","    d_loss_real = loss_discriminator(target_real, prediction_real)\n","    d_loss_fake = loss_discriminator(target_fake, prediction_fake)\n","    d_loss = 0.5 * add(d_loss_fake, d_loss_real)\n","    discriminator_losses.append(d_loss)\n","    discriminator_losses_fake.append(d_loss_fake)\n","    discriminator_losses_real.append(d_loss_real)\n","\n","    d_loss_real.backward()\n","    d_loss_fake.backward()\n","    pipeline.discriminator.optimizer.step()\n","\n","    # ---------------\n","    # Train Generator\n","    # ---------------\n","    vae.train(True)\n","    pipeline.discriminator.train(False)\n","    pipeline.encoder.zero_grad()\n","    pipeline.decoder.zero_grad()\n","\n","    generated_images = vae(images, labels)\n","    target = pipeline.discriminator(images)\n","    target[:,0] = 1\n","    target[:,1:1+labels_dim] = labels\n","    target = target.detach()\n","    prediction = pipeline.discriminator(generated_images)\n","    latent = pipeline.encoder(images, labels)\n","\n","    g_loss = loss_generator(target, prediction, images, generated_images, latent)\n","    g_loss.backward()\n","    pipeline.encoder.optimizer.step()\n","    pipeline.decoder.optimizer.step()\n","    generator_losses.append(g_loss)\n","\n","    if track_hyperparameter:\n","        ## save measures to wandb.ai\n","        wandb.log({\"loss discriminator\":d_loss, \"loss generator\":g_loss}) \n","\n","\n","    iteration += 1\n","\n","    print(f\"iteration {iteration}, epoch {epoch+1}, batch {step+1}/{steps},\" + \\\n","          f\"disc_loss {d_loss:.5}, (real {d_loss_real:.5}, fake {d_loss_fake:.5} ) gen_loss {g_loss:.5}\")\n","\n","    if not iteration % save_interval:\n","        write_generated_galaxy_images_iteration(iteration=iteration, images=generated_images.detach().cpu().numpy())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u3zDzxauAqqH"},"source":["while epoch < epochs:\n","    sample_generator = make_training_sample_generator(batch_size, x_train, labels_train)\n","    step = 0\n","    while step < steps:\n","        images, labels = next(sample_generator)\n","        training_step(images, labels)\n","        step += 1\n","    epoch += 1\n","\n","    # save a plot of the costs\n","    plt.clf()\n","    plt.plot(discriminator_losses, label='discriminator cost')\n","    plt.plot(generator_losses, label='generator cost')\n","    plt.plot(discriminator_losses_fake, label='discriminator cost fake', linestyle=\":\")\n","    plt.plot(discriminator_losses_real, label='discriminator cost real', linestyle=\"-.\")\n","    plt.yscale(\"log\")\n","    plt.legend()\n","    plt.savefig(folder_results+\"cost_vs_iteration.png\")\n","    plt.close()\n","\n","\n","    ### really save?\n","    #pipeline.decoder.save()\n","    #pipeline.encoder.save()\n","    #pipeline.discriminator.save()\n","\n","\n","if track_hyperparameter:\n","    ## after training is done and all measures are written, finalize with\n","    wandb.finish()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfBjaXVv7IS0"},"source":["# Testing"]},{"cell_type":"code","metadata":{"id":"tFcJlt6Hh8Qh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPlmAHXbVZac"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnvpRlGSbVfF"},"source":[""],"execution_count":null,"outputs":[]}]}