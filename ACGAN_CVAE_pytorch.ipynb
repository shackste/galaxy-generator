{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACGAN_CVAE_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OqYSQwo1yU4Q",
        "FiixZLra8_zZ",
        "y4fZsGw58csw",
        "OteyUsuqGmlc",
        "VE6IwXKiOnry",
        "xF15EJdcS4sz",
        "jCOi36MsLVcF",
        "CriG5eqZLfCl",
        "kLOAJzCYbKaM",
        "8H2tHpYFj5IZ",
        "-ce7yr0Xtn7s",
        "zYA_JRZkQPuU",
        "PflxcKvJXJSG",
        "H_tBcygt5wPW",
        "q5ZSz-KWLrur",
        "9FiNNQMGt-3_",
        "9pPB3sMNtMuP",
        "jHyPd1gGoxo2",
        "N0KpZUUgvS5l",
        "kg1oCdQAQ4k9",
        "npjGgL35UtbP",
        "RijOcfATXiAk",
        "MPQboez8Xmoa",
        "BVfQKAZvahOX",
        "KHE-7GAJRwy0",
        "_WizX5x6yEPr"
      ],
      "authorship_tag": "ABX9TyMB48aNWHOrJvoneobfcIVo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shackste/galaxy-generator/blob/separate_py_files/ACGAN_CVAE_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEiOsqeXFUJk"
      },
      "source": [
        "PyTorch implementation of combined Conditional Variable Auto Encoder (CVAE) and Auxiliary-Classifier Generative Adversarial Network (ACGAN)\n",
        "\n",
        "continuation of work by Mohamad Dia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqYSQwo1yU4Q"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiixZLra8_zZ"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_TbcQTbUyvl",
        "outputId": "31571efc-58c9-4498-c662-14ff1888fa44"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 28.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.8.0+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp37-none-any.whl size=3520 sha256=1f7dd6ea90d371e94cbfe5ae5b60f89324c60608d2bd0c219356122aec960574\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQjam8HTF3NX",
        "outputId": "1fcbb325-6b86-469a-cc9a-2c2aa5885a0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "import sys \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import cat, add, ones, zeros\n",
        "\n",
        "\n",
        "## include separate module files\n",
        "!git clone -b separate_py_files https://github.com/shackste/galaxy-generator.git\n",
        "\n",
        "sys.path.insert(0,\"/content/galaxy-generator/python_modules/\")\n",
        "\n",
        "from parameter import labels_dim, input_size, parameter\n",
        "from file_system import root, folder_results\n",
        "from helpful_functions import summarize, write_generated_galaxy_images_iteration\n",
        "from sampler import make_training_sample_generator\n",
        "from dataset import get_x_train, get_labels_train\n",
        "from loss import loss_discriminator, loss_generator\n",
        "\n",
        "import pipeline\n",
        "from pipeline import VAE, VAEGAN\n",
        "from discriminator import Discriminator4\n",
        "from encoder import Encoder4\n",
        "from decoder import Decoder4\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'galaxy-generator'...\n",
            "remote: Enumerating objects: 162, done.\u001b[K\n",
            "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "remote: Total 162 (delta 103), reused 98 (delta 47), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (162/162), 3.16 MiB | 30.54 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n",
            "\n",
            "!!!!!!!!!!\n",
            "\n",
            "galaxyzoo_data_cropped_nonnormalized.npy and training_solutions_rev1.csv must be placed in google drive under FHNW/galaxy_generator/\n",
            "the results will be placed there, too.\n",
            "\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF15EJdcS4sz"
      },
      "source": [
        "# basic test Neural Networks\n",
        "check whether the pipeline components work on their own"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9nQvGR3nP2d",
        "outputId": "eaf5f358-b4f3-41e9-8504-eccb5170df2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "summarize(Discriminator4)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size (3, 64, 64)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 64, 64]              64\n",
            "         LeakyReLU-2           [-1, 16, 64, 64]               0\n",
            "            Conv2d-3           [-1, 32, 32, 32]           4,640\n",
            "       BatchNorm2d-4           [-1, 32, 32, 32]              64\n",
            "         LeakyReLU-5           [-1, 32, 32, 32]               0\n",
            "            Conv2d-6           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-7           [-1, 64, 16, 16]             128\n",
            "         LeakyReLU-8           [-1, 64, 16, 16]               0\n",
            "            Conv2d-9            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-10            [-1, 128, 8, 8]             256\n",
            "        LeakyReLU-11            [-1, 128, 8, 8]               0\n",
            "          Flatten-12                 [-1, 8192]               0\n",
            "           Linear-13                 [-1, 1024]       8,389,632\n",
            "        LeakyReLU-14                 [-1, 1024]               0\n",
            "           Linear-15                    [-1, 1]           1,025\n",
            "          Sigmoid-16                    [-1, 1]               0\n",
            "           Linear-17                    [-1, 3]           3,075\n",
            "          Softmax-18                    [-1, 3]               0\n",
            "================================================================\n",
            "Total params: 8,491,236\n",
            "Trainable params: 8,491,236\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 2.39\n",
            "Params size (MB): 32.39\n",
            "Estimated Total Size (MB): 34.83\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF6t2XHZn28G",
        "outputId": "967c2c40-21b3-46d7-ed26-440b398a1ad6"
      },
      "source": [
        "import numpy as np\n",
        "from torch import rand\n",
        "\n",
        "\n",
        "net = Encoder4()\n",
        "\n",
        "print(np.sum([np.prod(p.size()) for p in net.parameters()]) == 8508656)\n",
        "input_dummy = rand(3, *input_size)\n",
        "label_dummy = rand(3, labels_dim)\n",
        "\n",
        "net(input_dummy, label_dummy)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1421, -0.2423,  0.0967,  0.3992, -0.2161,  0.3253,  0.1492, -0.2746],\n",
              "         [-0.3237, -0.1038, -0.5041, -0.3167, -0.3121, -0.7072, -0.0351, -0.2488],\n",
              "         [ 0.2505, -0.0255, -0.2822, -0.2313, -0.4929, -0.0486, -0.2691,  0.4607]],\n",
              "        grad_fn=<AddmmBackward>),\n",
              " tensor([[0.4920, 0.7047, 0.7827, 0.4335, 1.0000, 0.7958, 0.9132, 0.7743],\n",
              "         [1.0283, 0.6942, 0.4761, 0.4233, 1.2211, 1.0086, 1.2905, 0.5837],\n",
              "         [0.5365, 0.7953, 0.6508, 0.5631, 0.3876, 0.7153, 0.7852, 1.0525]],\n",
              "        grad_fn=<SoftplusBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnAFagCuTpgn",
        "outputId": "dfc27f38-83a1-4b14-9450-93ee5866f9d0"
      },
      "source": [
        "net = Decoder4()\n",
        "\n",
        "print(np.sum([np.prod(p.size()) for p in net.parameters()]))\n",
        "input_dummy = rand(3, parameter.latent_dim)\n",
        "label_dummy = rand(3, labels_dim)\n",
        "\n",
        "net(input_dummy, label_dummy).shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8524675\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoUTAxuN_47Z",
        "outputId": "f8992261-5e72-4a88-f7d2-1c2298073a3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net = VAE()\n",
        "images_dummy = rand(5,3,64,64).cuda()\n",
        "labels_dummy = rand(5,3).cuda()\n",
        "pred = net(images_dummy, labels_dummy)\n",
        "pred.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVoRvJap_402",
        "outputId": "4fd1603b-2425-4e9c-ef1b-1fdb89864389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net = VAEGAN()\n",
        "input_dummy = rand(3,3,64,64).cuda()\n",
        "labels_dummy = rand(3,3).cuda()\n",
        "pred = net(input_dummy, labels_dummy)\n",
        "pred.shape, pred[:,0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 1028]),\n",
              " tensor([0.5037, 0.5509, 0.5390], device='cuda:0', grad_fn=<SelectBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ4i89gkVnDl"
      },
      "source": [
        "# Training Data\n",
        "load data from files at google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y60D2u2BVmwj"
      },
      "source": [
        "x_train = get_x_train()\n",
        "labels_train = get_labels_train()\n",
        "N_samples = x_train.shape[0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aACg2yM0x2ln"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj-UnIeJx7eU"
      },
      "source": [
        "epochs = 20\n",
        "batch_size = 128\n",
        "steps = N_samples // batch_size\n",
        "save_interval = 200\n",
        "\n",
        "discriminator_losses = []\n",
        "discriminator_losses_real = []\n",
        "discriminator_losses_fake = []\n",
        "generator_losses = []\n",
        "\n",
        "valid = ones((batch_size,1)).cuda()\n",
        "fake = zeros((batch_size,1)).cuda()\n",
        "\n",
        "## if you want to change discriminator, encoder or decoder network:\n",
        "#pipeline.decoder = YourDecoder().cuda()\n",
        "## before you create the VAE and VAEGAN\n",
        "\n",
        "## if you want to change any parameter:\n",
        "#parameter.alpha = 1\n",
        "## this can be done during runtime\n",
        "\n",
        "vae = VAE()\n",
        "vaegan = VAEGAN()\n",
        "\n",
        "\n",
        "iteration = 0\n",
        "epoch = 0\n",
        "step = 0\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDiXsx8Mxfti"
      },
      "source": [
        "def training_step():\n",
        "    global iteration\n",
        "    images, labels = next(sample_generator)\n",
        "\n",
        "    # -------------------\n",
        "    # Train Discriminator\n",
        "    # -------------------\n",
        "    vae.train(False)\n",
        "    pipeline.discriminator.train(True)\n",
        "    pipeline.discriminator.zero_grad()\n",
        "\n",
        "    generated_images = vae(images, labels)\n",
        "    target_real = cat((valid,labels), dim=1)\n",
        "    prediction_real = pipeline.discriminator(images)[:,:1+labels_dim]\n",
        "    target_fake = cat((fake, labels), dim=1)\n",
        "    prediction_fake = pipeline.discriminator(generated_images)[:,:1+labels_dim]\n",
        "\n",
        "    d_loss_real = loss_discriminator(target_real, prediction_real)\n",
        "    d_loss_fake = loss_discriminator(target_fake, prediction_fake)\n",
        "    d_loss = 0.5 * add(d_loss_fake, d_loss_real)\n",
        "    discriminator_losses.append(d_loss)\n",
        "    discriminator_losses_fake.append(d_loss_fake)\n",
        "    discriminator_losses_real.append(d_loss_real)\n",
        "\n",
        "    d_loss_real.backward()\n",
        "    d_loss_fake.backward()\n",
        "    pipeline.discriminator.optimizer.step()\n",
        "\n",
        "    # ---------------\n",
        "    # Train Generator\n",
        "    # ---------------\n",
        "    vae.train(True)\n",
        "    pipeline.discriminator.train(False)\n",
        "    pipeline.encoder.zero_grad()\n",
        "    pipeline.decoder.zero_grad()\n",
        "\n",
        "    generated_images = vae(images, labels)\n",
        "    target = pipeline.discriminator(images)\n",
        "    target[:,0] = 1\n",
        "    target[:,1:1+labels_dim] = labels\n",
        "    target = target.detach()\n",
        "    prediction = pipeline.discriminator(generated_images)\n",
        "    latent = pipeline.encoder(images, labels)\n",
        "\n",
        "    g_loss = loss_generator(target, prediction, images, generated_images, latent)\n",
        "    g_loss.backward()\n",
        "    pipeline.encoder.optimizer.step()\n",
        "    pipeline.decoder.optimizer.step()\n",
        "    generator_losses.append(g_loss)\n",
        "\n",
        "\n",
        "    iteration += 1\n",
        "\n",
        "    print(f\"iteration {iteration}, epoch {epoch+1}, batch {step+1}/{steps},\" + \\\n",
        "          f\"disc_loss {d_loss:.5}, (real {d_loss_real:.5}, fake {d_loss_fake:.5} ) gen_loss {g_loss:.5}\")\n",
        "\n",
        "    if not iteration % save_interval:\n",
        "        write_generated_galaxy_images_iteration(iteration=iteration, images=generated_images.detach().cpu().numpy())\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3zDzxauAqqH",
        "outputId": "965d547c-92b0-40c9-d990-145244cafc56"
      },
      "source": [
        "while epoch < epochs:\n",
        "    sample_generator = make_training_sample_generator(batch_size, x_train, labels_train)\n",
        "    step = 0\n",
        "    while step < steps:\n",
        "        training_step()\n",
        "        step += 1\n",
        "    epoch += 1\n",
        "\n",
        "    # save a plot of the costs\n",
        "    plt.clf()\n",
        "    plt.plot(discriminator_losses, label='discriminator cost')\n",
        "    plt.plot(generator_losses, label='generator cost')\n",
        "    plt.plot(discriminator_losses_fake, label='discriminator cost fake', linestyle=\":\")\n",
        "    plt.plot(discriminator_losses_real, label='discriminator cost real', linestyle=\"-.\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    plt.savefig(folder_results+\"cost_vs_iteration.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    ### really save?\n",
        "    pipeline.decoder.save()\n",
        "    pipeline.encoder.save()\n",
        "    pipeline.discriminator.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 1, epoch 1, batch 1/481,disc_loss 142.28, (real 138.34, fake 146.23 ) gen_loss 772.25\n",
            "iteration 2, epoch 1, batch 2/481,disc_loss 135.46, (real 130.95, fake 139.96 ) gen_loss 2221.5\n",
            "iteration 3, epoch 1, batch 3/481,disc_loss 247.76, (real 279.3, fake 216.22 ) gen_loss 1509.5\n",
            "iteration 4, epoch 1, batch 4/481,disc_loss 128.71, (real 126.66, fake 130.77 ) gen_loss 1184.3\n",
            "iteration 5, epoch 1, batch 5/481,disc_loss 120.39, (real 125.02, fake 115.76 ) gen_loss 1026.8\n",
            "iteration 6, epoch 1, batch 6/481,disc_loss 101.81, (real 105.59, fake 98.025 ) gen_loss 875.14\n",
            "iteration 7, epoch 1, batch 7/481,disc_loss 91.274, (real 96.622, fake 85.927 ) gen_loss 827.23\n",
            "iteration 8, epoch 1, batch 8/481,disc_loss 103.93, (real 111.41, fake 96.455 ) gen_loss 703.81\n",
            "iteration 9, epoch 1, batch 9/481,disc_loss 91.88, (real 98.718, fake 85.043 ) gen_loss 683.74\n",
            "iteration 10, epoch 1, batch 10/481,disc_loss 94.382, (real 101.65, fake 87.112 ) gen_loss 646.13\n",
            "iteration 11, epoch 1, batch 11/481,disc_loss 89.706, (real 96.385, fake 83.027 ) gen_loss 600.23\n",
            "iteration 12, epoch 1, batch 12/481,disc_loss 87.337, (real 92.817, fake 81.857 ) gen_loss 586.48\n",
            "iteration 13, epoch 1, batch 13/481,disc_loss 87.407, (real 94.244, fake 80.57 ) gen_loss 583.56\n",
            "iteration 14, epoch 1, batch 14/481,disc_loss 85.782, (real 92.154, fake 79.409 ) gen_loss 571.57\n",
            "iteration 15, epoch 1, batch 15/481,disc_loss 84.849, (real 91.632, fake 78.066 ) gen_loss 559.65\n",
            "iteration 16, epoch 1, batch 16/481,disc_loss 86.68, (real 97.001, fake 76.359 ) gen_loss 550.8\n",
            "iteration 17, epoch 1, batch 17/481,disc_loss 87.314, (real 96.159, fake 78.468 ) gen_loss 529.92\n",
            "iteration 18, epoch 1, batch 18/481,disc_loss 84.071, (real 90.148, fake 77.994 ) gen_loss 546.28\n",
            "iteration 19, epoch 1, batch 19/481,disc_loss 85.224, (real 90.887, fake 79.561 ) gen_loss 529.4\n",
            "iteration 20, epoch 1, batch 20/481,disc_loss 90.299, (real 98.574, fake 82.023 ) gen_loss 533.03\n",
            "iteration 21, epoch 1, batch 21/481,disc_loss 85.138, (real 92.123, fake 78.153 ) gen_loss 528.93\n",
            "iteration 22, epoch 1, batch 22/481,disc_loss 85.655, (real 93.53, fake 77.78 ) gen_loss 539.09\n",
            "iteration 23, epoch 1, batch 23/481,disc_loss 83.344, (real 88.345, fake 78.342 ) gen_loss 531.38\n",
            "iteration 24, epoch 1, batch 24/481,disc_loss 89.051, (real 91.108, fake 86.994 ) gen_loss 542.82\n",
            "iteration 25, epoch 1, batch 25/481,disc_loss 96.329, (real 103.22, fake 89.433 ) gen_loss 574.44\n",
            "iteration 26, epoch 1, batch 26/481,disc_loss 109.4, (real 112.17, fake 106.63 ) gen_loss 545.98\n",
            "iteration 27, epoch 1, batch 27/481,disc_loss 105.36, (real 113.78, fake 96.951 ) gen_loss 496.68\n",
            "iteration 28, epoch 1, batch 28/481,disc_loss 98.705, (real 110.0, fake 87.407 ) gen_loss 482.04\n",
            "iteration 29, epoch 1, batch 29/481,disc_loss 87.384, (real 95.442, fake 79.326 ) gen_loss 463.34\n",
            "iteration 30, epoch 1, batch 30/481,disc_loss 87.999, (real 98.574, fake 77.424 ) gen_loss 470.77\n",
            "iteration 31, epoch 1, batch 31/481,disc_loss 83.266, (real 90.605, fake 75.928 ) gen_loss 467.52\n",
            "iteration 32, epoch 1, batch 32/481,disc_loss 85.937, (real 91.849, fake 80.024 ) gen_loss 473.92\n",
            "iteration 33, epoch 1, batch 33/481,disc_loss 81.768, (real 88.811, fake 74.726 ) gen_loss 467.29\n",
            "iteration 34, epoch 1, batch 34/481,disc_loss 84.46, (real 90.906, fake 78.014 ) gen_loss 466.74\n",
            "iteration 35, epoch 1, batch 35/481,disc_loss 82.257, (real 89.194, fake 75.32 ) gen_loss 463.35\n",
            "iteration 36, epoch 1, batch 36/481,disc_loss 88.417, (real 95.732, fake 81.102 ) gen_loss 453.59\n",
            "iteration 37, epoch 1, batch 37/481,disc_loss 79.975, (real 87.52, fake 72.43 ) gen_loss 453.63\n",
            "iteration 38, epoch 1, batch 38/481,disc_loss 85.001, (real 91.205, fake 78.796 ) gen_loss 453.02\n",
            "iteration 39, epoch 1, batch 39/481,disc_loss 82.817, (real 89.316, fake 76.319 ) gen_loss 456.96\n",
            "iteration 40, epoch 1, batch 40/481,disc_loss 82.932, (real 91.563, fake 74.3 ) gen_loss 447.87\n",
            "iteration 41, epoch 1, batch 41/481,disc_loss 85.622, (real 93.397, fake 77.847 ) gen_loss 456.93\n",
            "iteration 42, epoch 1, batch 42/481,disc_loss 81.213, (real 87.436, fake 74.99 ) gen_loss 448.81\n",
            "iteration 43, epoch 1, batch 43/481,disc_loss 86.915, (real 93.349, fake 80.481 ) gen_loss 464.87\n",
            "iteration 44, epoch 1, batch 44/481,disc_loss 83.317, (real 89.425, fake 77.209 ) gen_loss 429.79\n",
            "iteration 45, epoch 1, batch 45/481,disc_loss 86.89, (real 97.565, fake 76.214 ) gen_loss 452.81\n",
            "iteration 46, epoch 1, batch 46/481,disc_loss 89.304, (real 99.49, fake 79.119 ) gen_loss 469.56\n",
            "iteration 47, epoch 1, batch 47/481,disc_loss 87.053, (real 94.356, fake 79.75 ) gen_loss 457.28\n",
            "iteration 48, epoch 1, batch 48/481,disc_loss 83.638, (real 90.751, fake 76.524 ) gen_loss 450.7\n",
            "iteration 49, epoch 1, batch 49/481,disc_loss 87.608, (real 94.131, fake 81.085 ) gen_loss 453.45\n",
            "iteration 50, epoch 1, batch 50/481,disc_loss 85.014, (real 90.27, fake 79.758 ) gen_loss 447.04\n",
            "iteration 51, epoch 1, batch 51/481,disc_loss 84.306, (real 91.607, fake 77.005 ) gen_loss 448.56\n",
            "iteration 52, epoch 1, batch 52/481,disc_loss 79.463, (real 83.222, fake 75.704 ) gen_loss 442.22\n",
            "iteration 53, epoch 1, batch 53/481,disc_loss 82.557, (real 88.255, fake 76.858 ) gen_loss 459.13\n",
            "iteration 54, epoch 1, batch 54/481,disc_loss 84.419, (real 93.796, fake 75.042 ) gen_loss 453.3\n",
            "iteration 55, epoch 1, batch 55/481,disc_loss 91.453, (real 101.61, fake 81.298 ) gen_loss 463.85\n",
            "iteration 56, epoch 1, batch 56/481,disc_loss 97.483, (real 105.39, fake 89.573 ) gen_loss 475.95\n",
            "iteration 57, epoch 1, batch 57/481,disc_loss 95.164, (real 97.447, fake 92.882 ) gen_loss 437.16\n",
            "iteration 58, epoch 1, batch 58/481,disc_loss 82.013, (real 85.425, fake 78.602 ) gen_loss 431.07\n",
            "iteration 59, epoch 1, batch 59/481,disc_loss 82.398, (real 88.787, fake 76.01 ) gen_loss 420.63\n",
            "iteration 60, epoch 1, batch 60/481,disc_loss 83.325, (real 88.457, fake 78.192 ) gen_loss 441.09\n",
            "iteration 61, epoch 1, batch 61/481,disc_loss 83.295, (real 91.225, fake 75.366 ) gen_loss 424.79\n",
            "iteration 62, epoch 1, batch 62/481,disc_loss 84.958, (real 91.04, fake 78.875 ) gen_loss 421.88\n",
            "iteration 63, epoch 1, batch 63/481,disc_loss 84.288, (real 89.569, fake 79.008 ) gen_loss 416.08\n",
            "iteration 64, epoch 1, batch 64/481,disc_loss 83.454, (real 88.012, fake 78.895 ) gen_loss 411.52\n",
            "iteration 65, epoch 1, batch 65/481,disc_loss 83.346, (real 87.896, fake 78.796 ) gen_loss 412.23\n",
            "iteration 66, epoch 1, batch 66/481,disc_loss 83.502, (real 88.569, fake 78.435 ) gen_loss 418.69\n",
            "iteration 67, epoch 1, batch 67/481,disc_loss 85.222, (real 92.265, fake 78.179 ) gen_loss 409.97\n",
            "iteration 68, epoch 1, batch 68/481,disc_loss 90.819, (real 97.812, fake 83.826 ) gen_loss 424.36\n",
            "iteration 69, epoch 1, batch 69/481,disc_loss 84.906, (real 89.932, fake 79.881 ) gen_loss 407.49\n",
            "iteration 70, epoch 1, batch 70/481,disc_loss 84.8, (real 91.223, fake 78.377 ) gen_loss 430.28\n",
            "iteration 71, epoch 1, batch 71/481,disc_loss 82.028, (real 87.209, fake 76.847 ) gen_loss 445.53\n",
            "iteration 72, epoch 1, batch 72/481,disc_loss 87.22, (real 92.381, fake 82.06 ) gen_loss 432.44\n",
            "iteration 73, epoch 1, batch 73/481,disc_loss 89.142, (real 94.88, fake 83.404 ) gen_loss 417.95\n",
            "iteration 74, epoch 1, batch 74/481,disc_loss 86.284, (real 91.213, fake 81.356 ) gen_loss 419.55\n",
            "iteration 75, epoch 1, batch 75/481,disc_loss 84.167, (real 89.149, fake 79.185 ) gen_loss 420.17\n",
            "iteration 76, epoch 1, batch 76/481,disc_loss 87.58, (real 94.305, fake 80.854 ) gen_loss 421.07\n",
            "iteration 77, epoch 1, batch 77/481,disc_loss 86.84, (real 91.585, fake 82.096 ) gen_loss 423.34\n",
            "iteration 78, epoch 1, batch 78/481,disc_loss 89.055, (real 93.476, fake 84.633 ) gen_loss 427.36\n",
            "iteration 79, epoch 1, batch 79/481,disc_loss 88.355, (real 95.256, fake 81.455 ) gen_loss 403.29\n",
            "iteration 80, epoch 1, batch 80/481,disc_loss 84.31, (real 89.96, fake 78.659 ) gen_loss 404.79\n",
            "iteration 81, epoch 1, batch 81/481,disc_loss 84.859, (real 91.675, fake 78.043 ) gen_loss 398.33\n",
            "iteration 82, epoch 1, batch 82/481,disc_loss 79.076, (real 86.08, fake 72.072 ) gen_loss 374.22\n",
            "iteration 83, epoch 1, batch 83/481,disc_loss 86.131, (real 92.194, fake 80.068 ) gen_loss 387.09\n",
            "iteration 84, epoch 1, batch 84/481,disc_loss 85.95, (real 90.121, fake 81.778 ) gen_loss 393.75\n",
            "iteration 85, epoch 1, batch 85/481,disc_loss 80.62, (real 85.962, fake 75.278 ) gen_loss 385.86\n",
            "iteration 86, epoch 1, batch 86/481,disc_loss 82.307, (real 89.166, fake 75.447 ) gen_loss 389.09\n",
            "iteration 87, epoch 1, batch 87/481,disc_loss 85.302, (real 90.532, fake 80.072 ) gen_loss 383.84\n",
            "iteration 88, epoch 1, batch 88/481,disc_loss 87.168, (real 96.424, fake 77.913 ) gen_loss 392.21\n",
            "iteration 89, epoch 1, batch 89/481,disc_loss 86.1, (real 93.933, fake 78.267 ) gen_loss 390.31\n",
            "iteration 90, epoch 1, batch 90/481,disc_loss 83.304, (real 88.724, fake 77.884 ) gen_loss 384.02\n",
            "iteration 91, epoch 1, batch 91/481,disc_loss 84.768, (real 90.279, fake 79.258 ) gen_loss 385.79\n",
            "iteration 92, epoch 1, batch 92/481,disc_loss 81.507, (real 88.471, fake 74.543 ) gen_loss 368.88\n",
            "iteration 93, epoch 1, batch 93/481,disc_loss 86.106, (real 92.897, fake 79.314 ) gen_loss 381.98\n",
            "iteration 94, epoch 1, batch 94/481,disc_loss 82.152, (real 87.626, fake 76.679 ) gen_loss 385.77\n",
            "iteration 95, epoch 1, batch 95/481,disc_loss 84.616, (real 93.512, fake 75.719 ) gen_loss 389.86\n",
            "iteration 96, epoch 1, batch 96/481,disc_loss 78.966, (real 85.856, fake 72.077 ) gen_loss 367.77\n",
            "iteration 97, epoch 1, batch 97/481,disc_loss 82.75, (real 88.833, fake 76.668 ) gen_loss 364.13\n",
            "iteration 98, epoch 1, batch 98/481,disc_loss 80.933, (real 86.547, fake 75.32 ) gen_loss 366.25\n",
            "iteration 99, epoch 1, batch 99/481,disc_loss 80.865, (real 90.146, fake 71.585 ) gen_loss 372.08\n",
            "iteration 100, epoch 1, batch 100/481,disc_loss 83.042, (real 88.033, fake 78.051 ) gen_loss 367.12\n",
            "iteration 101, epoch 1, batch 101/481,disc_loss 83.832, (real 89.924, fake 77.741 ) gen_loss 383.67\n",
            "iteration 102, epoch 1, batch 102/481,disc_loss 87.849, (real 98.405, fake 77.293 ) gen_loss 369.59\n",
            "iteration 103, epoch 1, batch 103/481,disc_loss 85.164, (real 90.048, fake 80.28 ) gen_loss 372.79\n",
            "iteration 104, epoch 1, batch 104/481,disc_loss 82.872, (real 87.831, fake 77.912 ) gen_loss 373.41\n",
            "iteration 105, epoch 1, batch 105/481,disc_loss 85.185, (real 88.843, fake 81.526 ) gen_loss 382.3\n",
            "iteration 106, epoch 1, batch 106/481,disc_loss 84.395, (real 92.258, fake 76.533 ) gen_loss 356.85\n",
            "iteration 107, epoch 1, batch 107/481,disc_loss 83.004, (real 88.368, fake 77.64 ) gen_loss 363.91\n",
            "iteration 108, epoch 1, batch 108/481,disc_loss 81.475, (real 86.872, fake 76.079 ) gen_loss 375.11\n",
            "iteration 109, epoch 1, batch 109/481,disc_loss 83.412, (real 89.128, fake 77.695 ) gen_loss 378.1\n",
            "iteration 110, epoch 1, batch 110/481,disc_loss 78.82, (real 83.335, fake 74.304 ) gen_loss 380.04\n",
            "iteration 111, epoch 1, batch 111/481,disc_loss 85.782, (real 90.175, fake 81.39 ) gen_loss 396.59\n",
            "iteration 112, epoch 1, batch 112/481,disc_loss 85.164, (real 94.467, fake 75.861 ) gen_loss 374.67\n",
            "iteration 113, epoch 1, batch 113/481,disc_loss 78.612, (real 84.899, fake 72.326 ) gen_loss 368.84\n",
            "iteration 114, epoch 1, batch 114/481,disc_loss 88.905, (real 94.684, fake 83.126 ) gen_loss 375.46\n",
            "iteration 115, epoch 1, batch 115/481,disc_loss 79.603, (real 85.784, fake 73.422 ) gen_loss 367.79\n",
            "iteration 116, epoch 1, batch 116/481,disc_loss 84.599, (real 91.47, fake 77.728 ) gen_loss 383.62\n",
            "iteration 117, epoch 1, batch 117/481,disc_loss 83.514, (real 87.436, fake 79.592 ) gen_loss 397.58\n",
            "iteration 118, epoch 1, batch 118/481,disc_loss 84.26, (real 89.097, fake 79.423 ) gen_loss 371.81\n",
            "iteration 119, epoch 1, batch 119/481,disc_loss 82.227, (real 84.492, fake 79.961 ) gen_loss 381.34\n",
            "iteration 120, epoch 1, batch 120/481,disc_loss 83.875, (real 88.277, fake 79.473 ) gen_loss 395.77\n",
            "iteration 121, epoch 1, batch 121/481,disc_loss 85.087, (real 93.28, fake 76.894 ) gen_loss 386.41\n",
            "iteration 122, epoch 1, batch 122/481,disc_loss 90.311, (real 98.464, fake 82.158 ) gen_loss 379.69\n",
            "iteration 123, epoch 1, batch 123/481,disc_loss 91.966, (real 98.486, fake 85.445 ) gen_loss 374.87\n",
            "iteration 124, epoch 1, batch 124/481,disc_loss 86.823, (real 92.468, fake 81.178 ) gen_loss 354.69\n",
            "iteration 125, epoch 1, batch 125/481,disc_loss 89.708, (real 94.768, fake 84.648 ) gen_loss 357.57\n",
            "iteration 126, epoch 1, batch 126/481,disc_loss 83.223, (real 88.047, fake 78.399 ) gen_loss 353.34\n",
            "iteration 127, epoch 1, batch 127/481,disc_loss 90.627, (real 99.342, fake 81.913 ) gen_loss 338.03\n",
            "iteration 128, epoch 1, batch 128/481,disc_loss 85.057, (real 91.314, fake 78.799 ) gen_loss 326.31\n",
            "iteration 129, epoch 1, batch 129/481,disc_loss 83.237, (real 89.073, fake 77.401 ) gen_loss 334.58\n",
            "iteration 130, epoch 1, batch 130/481,disc_loss 80.857, (real 86.708, fake 75.005 ) gen_loss 326.73\n",
            "iteration 131, epoch 1, batch 131/481,disc_loss 84.453, (real 89.558, fake 79.347 ) gen_loss 335.71\n",
            "iteration 132, epoch 1, batch 132/481,disc_loss 82.499, (real 87.692, fake 77.307 ) gen_loss 338.41\n",
            "iteration 133, epoch 1, batch 133/481,disc_loss 88.94, (real 96.008, fake 81.871 ) gen_loss 347.37\n",
            "iteration 134, epoch 1, batch 134/481,disc_loss 89.34, (real 94.549, fake 84.132 ) gen_loss 343.65\n",
            "iteration 135, epoch 1, batch 135/481,disc_loss 90.715, (real 96.701, fake 84.729 ) gen_loss 348.02\n",
            "iteration 136, epoch 1, batch 136/481,disc_loss 86.262, (real 93.848, fake 78.677 ) gen_loss 405.44\n",
            "iteration 137, epoch 1, batch 137/481,disc_loss 84.54, (real 89.785, fake 79.294 ) gen_loss 367.35\n",
            "iteration 138, epoch 1, batch 138/481,disc_loss 85.908, (real 92.523, fake 79.294 ) gen_loss 376.05\n",
            "iteration 139, epoch 1, batch 139/481,disc_loss 82.65, (real 90.915, fake 74.386 ) gen_loss 369.41\n",
            "iteration 140, epoch 1, batch 140/481,disc_loss 82.896, (real 88.232, fake 77.56 ) gen_loss 361.39\n",
            "iteration 141, epoch 1, batch 141/481,disc_loss 84.077, (real 89.819, fake 78.335 ) gen_loss 375.07\n",
            "iteration 142, epoch 1, batch 142/481,disc_loss 85.194, (real 91.742, fake 78.647 ) gen_loss 353.03\n",
            "iteration 143, epoch 1, batch 143/481,disc_loss 85.916, (real 92.421, fake 79.41 ) gen_loss 328.02\n",
            "iteration 144, epoch 1, batch 144/481,disc_loss 80.721, (real 86.536, fake 74.907 ) gen_loss 329.2\n",
            "iteration 145, epoch 1, batch 145/481,disc_loss 83.924, (real 90.549, fake 77.299 ) gen_loss 341.57\n",
            "iteration 146, epoch 1, batch 146/481,disc_loss 82.452, (real 89.818, fake 75.086 ) gen_loss 325.09\n",
            "iteration 147, epoch 1, batch 147/481,disc_loss 86.585, (real 94.436, fake 78.735 ) gen_loss 330.66\n",
            "iteration 148, epoch 1, batch 148/481,disc_loss 82.608, (real 88.208, fake 77.009 ) gen_loss 323.94\n",
            "iteration 149, epoch 1, batch 149/481,disc_loss 83.414, (real 89.884, fake 76.944 ) gen_loss 364.27\n",
            "iteration 150, epoch 1, batch 150/481,disc_loss 81.898, (real 87.651, fake 76.145 ) gen_loss 354.57\n",
            "iteration 151, epoch 1, batch 151/481,disc_loss 80.788, (real 88.394, fake 73.183 ) gen_loss 330.88\n",
            "iteration 152, epoch 1, batch 152/481,disc_loss 82.708, (real 87.761, fake 77.655 ) gen_loss 333.33\n",
            "iteration 153, epoch 1, batch 153/481,disc_loss 85.777, (real 91.107, fake 80.448 ) gen_loss 339.08\n",
            "iteration 154, epoch 1, batch 154/481,disc_loss 82.986, (real 88.2, fake 77.772 ) gen_loss 333.85\n",
            "iteration 155, epoch 1, batch 155/481,disc_loss 81.781, (real 86.338, fake 77.225 ) gen_loss 330.19\n",
            "iteration 156, epoch 1, batch 156/481,disc_loss 86.24, (real 91.709, fake 80.77 ) gen_loss 332.76\n",
            "iteration 157, epoch 1, batch 157/481,disc_loss 80.549, (real 85.521, fake 75.577 ) gen_loss 337.53\n",
            "iteration 158, epoch 1, batch 158/481,disc_loss 80.301, (real 86.396, fake 74.206 ) gen_loss 324.7\n",
            "iteration 159, epoch 1, batch 159/481,disc_loss 80.396, (real 84.797, fake 75.996 ) gen_loss 342.6\n",
            "iteration 160, epoch 1, batch 160/481,disc_loss 84.638, (real 90.505, fake 78.77 ) gen_loss 340.87\n",
            "iteration 161, epoch 1, batch 161/481,disc_loss 83.314, (real 90.937, fake 75.692 ) gen_loss 342.79\n",
            "iteration 162, epoch 1, batch 162/481,disc_loss 84.829, (real 90.751, fake 78.906 ) gen_loss 338.37\n",
            "iteration 163, epoch 1, batch 163/481,disc_loss 89.351, (real 92.964, fake 85.738 ) gen_loss 359.35\n",
            "iteration 164, epoch 1, batch 164/481,disc_loss 87.725, (real 96.839, fake 78.611 ) gen_loss 344.87\n",
            "iteration 165, epoch 1, batch 165/481,disc_loss 91.235, (real 98.916, fake 83.554 ) gen_loss 344.08\n",
            "iteration 166, epoch 1, batch 166/481,disc_loss 85.217, (real 91.192, fake 79.242 ) gen_loss 321.22\n",
            "iteration 167, epoch 1, batch 167/481,disc_loss 83.057, (real 87.328, fake 78.785 ) gen_loss 326.27\n",
            "iteration 168, epoch 1, batch 168/481,disc_loss 84.645, (real 90.451, fake 78.838 ) gen_loss 323.16\n",
            "iteration 169, epoch 1, batch 169/481,disc_loss 80.283, (real 85.557, fake 75.009 ) gen_loss 326.96\n",
            "iteration 170, epoch 1, batch 170/481,disc_loss 85.192, (real 90.605, fake 79.779 ) gen_loss 326.9\n",
            "iteration 171, epoch 1, batch 171/481,disc_loss 81.957, (real 87.308, fake 76.606 ) gen_loss 323.25\n",
            "iteration 172, epoch 1, batch 172/481,disc_loss 82.853, (real 87.833, fake 77.873 ) gen_loss 324.54\n",
            "iteration 173, epoch 1, batch 173/481,disc_loss 84.165, (real 90.002, fake 78.328 ) gen_loss 323.25\n",
            "iteration 174, epoch 1, batch 174/481,disc_loss 84.633, (real 90.1, fake 79.166 ) gen_loss 327.07\n",
            "iteration 175, epoch 1, batch 175/481,disc_loss 84.963, (real 89.59, fake 80.336 ) gen_loss 322.08\n",
            "iteration 176, epoch 1, batch 176/481,disc_loss 83.204, (real 87.987, fake 78.421 ) gen_loss 313.01\n",
            "iteration 177, epoch 1, batch 177/481,disc_loss 78.567, (real 83.666, fake 73.468 ) gen_loss 316.9\n",
            "iteration 178, epoch 1, batch 178/481,disc_loss 82.5, (real 87.216, fake 77.784 ) gen_loss 335.48\n",
            "iteration 179, epoch 1, batch 179/481,disc_loss 83.454, (real 88.585, fake 78.322 ) gen_loss 345.83\n",
            "iteration 180, epoch 1, batch 180/481,disc_loss 80.313, (real 84.625, fake 76.0 ) gen_loss 329.29\n",
            "iteration 181, epoch 1, batch 181/481,disc_loss 79.425, (real 84.423, fake 74.427 ) gen_loss 321.15\n",
            "iteration 182, epoch 1, batch 182/481,disc_loss 78.963, (real 84.39, fake 73.536 ) gen_loss 315.76\n",
            "iteration 183, epoch 1, batch 183/481,disc_loss 84.023, (real 89.89, fake 78.156 ) gen_loss 317.96\n",
            "iteration 184, epoch 1, batch 184/481,disc_loss 84.082, (real 89.712, fake 78.452 ) gen_loss 324.65\n",
            "iteration 185, epoch 1, batch 185/481,disc_loss 80.484, (real 85.711, fake 75.257 ) gen_loss 328.56\n",
            "iteration 186, epoch 1, batch 186/481,disc_loss 76.721, (real 81.786, fake 71.657 ) gen_loss 309.4\n",
            "iteration 187, epoch 1, batch 187/481,disc_loss 84.572, (real 92.051, fake 77.093 ) gen_loss 322.98\n",
            "iteration 188, epoch 1, batch 188/481,disc_loss 85.592, (real 91.304, fake 79.881 ) gen_loss 329.05\n",
            "iteration 189, epoch 1, batch 189/481,disc_loss 88.887, (real 97.826, fake 79.948 ) gen_loss 327.76\n",
            "iteration 190, epoch 1, batch 190/481,disc_loss 86.771, (real 95.228, fake 78.315 ) gen_loss 335.35\n",
            "iteration 191, epoch 1, batch 191/481,disc_loss 78.31, (real 83.982, fake 72.637 ) gen_loss 295.48\n",
            "iteration 192, epoch 1, batch 192/481,disc_loss 76.757, (real 81.889, fake 71.624 ) gen_loss 296.7\n",
            "iteration 193, epoch 1, batch 193/481,disc_loss 84.155, (real 89.271, fake 79.039 ) gen_loss 307.9\n",
            "iteration 194, epoch 1, batch 194/481,disc_loss 82.367, (real 86.676, fake 78.058 ) gen_loss 300.54\n",
            "iteration 195, epoch 1, batch 195/481,disc_loss 82.154, (real 87.565, fake 76.742 ) gen_loss 307.79\n",
            "iteration 196, epoch 1, batch 196/481,disc_loss 87.995, (real 94.804, fake 81.186 ) gen_loss 302.63\n",
            "iteration 197, epoch 1, batch 197/481,disc_loss 86.406, (real 91.99, fake 80.823 ) gen_loss 303.26\n",
            "iteration 198, epoch 1, batch 198/481,disc_loss 84.316, (real 89.223, fake 79.408 ) gen_loss 303.26\n",
            "iteration 199, epoch 1, batch 199/481,disc_loss 84.231, (real 89.658, fake 78.804 ) gen_loss 304.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 200, epoch 1, batch 200/481,disc_loss 82.981, (real 89.79, fake 76.173 ) gen_loss 307.26\n",
            "iteration 201, epoch 1, batch 201/481,disc_loss 85.92, (real 91.435, fake 80.406 ) gen_loss 315.42\n",
            "iteration 202, epoch 1, batch 202/481,disc_loss 86.646, (real 91.206, fake 82.086 ) gen_loss 327.78\n",
            "iteration 203, epoch 1, batch 203/481,disc_loss 81.776, (real 89.709, fake 73.842 ) gen_loss 317.79\n",
            "iteration 204, epoch 1, batch 204/481,disc_loss 82.505, (real 89.883, fake 75.127 ) gen_loss 336.92\n",
            "iteration 205, epoch 1, batch 205/481,disc_loss 77.415, (real 84.123, fake 70.707 ) gen_loss 315.64\n",
            "iteration 206, epoch 1, batch 206/481,disc_loss 81.903, (real 87.929, fake 75.877 ) gen_loss 337.74\n",
            "iteration 207, epoch 1, batch 207/481,disc_loss 82.124, (real 86.978, fake 77.269 ) gen_loss 343.68\n",
            "iteration 208, epoch 1, batch 208/481,disc_loss 87.515, (real 95.323, fake 79.706 ) gen_loss 336.55\n",
            "iteration 209, epoch 1, batch 209/481,disc_loss 83.095, (real 87.984, fake 78.206 ) gen_loss 319.51\n",
            "iteration 210, epoch 1, batch 210/481,disc_loss 82.37, (real 88.558, fake 76.182 ) gen_loss 340.85\n",
            "iteration 211, epoch 1, batch 211/481,disc_loss 81.818, (real 87.686, fake 75.949 ) gen_loss 324.31\n",
            "iteration 212, epoch 1, batch 212/481,disc_loss 81.251, (real 83.075, fake 79.426 ) gen_loss 325.54\n",
            "iteration 213, epoch 1, batch 213/481,disc_loss 86.417, (real 91.591, fake 81.242 ) gen_loss 336.36\n",
            "iteration 214, epoch 1, batch 214/481,disc_loss 77.551, (real 84.391, fake 70.711 ) gen_loss 296.84\n",
            "iteration 215, epoch 1, batch 215/481,disc_loss 79.785, (real 86.791, fake 72.78 ) gen_loss 323.63\n",
            "iteration 216, epoch 1, batch 216/481,disc_loss 83.351, (real 89.071, fake 77.632 ) gen_loss 319.53\n",
            "iteration 217, epoch 1, batch 217/481,disc_loss 76.254, (real 80.473, fake 72.035 ) gen_loss 321.26\n",
            "iteration 218, epoch 1, batch 218/481,disc_loss 84.917, (real 91.574, fake 78.26 ) gen_loss 328.84\n",
            "iteration 219, epoch 1, batch 219/481,disc_loss 85.993, (real 92.735, fake 79.251 ) gen_loss 313.25\n",
            "iteration 220, epoch 1, batch 220/481,disc_loss 83.839, (real 89.521, fake 78.157 ) gen_loss 311.88\n",
            "iteration 221, epoch 1, batch 221/481,disc_loss 85.908, (real 93.308, fake 78.509 ) gen_loss 322.38\n",
            "iteration 222, epoch 1, batch 222/481,disc_loss 82.95, (real 86.928, fake 78.973 ) gen_loss 325.98\n",
            "iteration 223, epoch 1, batch 223/481,disc_loss 84.612, (real 93.207, fake 76.017 ) gen_loss 326.38\n",
            "iteration 224, epoch 1, batch 224/481,disc_loss 79.895, (real 85.99, fake 73.799 ) gen_loss 298.02\n",
            "iteration 225, epoch 1, batch 225/481,disc_loss 85.278, (real 98.56, fake 71.996 ) gen_loss 332.58\n",
            "iteration 226, epoch 1, batch 226/481,disc_loss 84.807, (real 93.077, fake 76.537 ) gen_loss 319.09\n",
            "iteration 227, epoch 1, batch 227/481,disc_loss 81.158, (real 86.135, fake 76.18 ) gen_loss 308.38\n",
            "iteration 228, epoch 1, batch 228/481,disc_loss 81.895, (real 88.084, fake 75.705 ) gen_loss 304.36\n",
            "iteration 229, epoch 1, batch 229/481,disc_loss 86.037, (real 91.711, fake 80.363 ) gen_loss 312.29\n",
            "iteration 230, epoch 1, batch 230/481,disc_loss 81.648, (real 86.399, fake 76.898 ) gen_loss 293.58\n",
            "iteration 231, epoch 1, batch 231/481,disc_loss 80.038, (real 85.676, fake 74.4 ) gen_loss 303.61\n",
            "iteration 232, epoch 1, batch 232/481,disc_loss 87.074, (real 94.812, fake 79.336 ) gen_loss 327.36\n",
            "iteration 233, epoch 1, batch 233/481,disc_loss 82.911, (real 91.626, fake 74.196 ) gen_loss 295.67\n",
            "iteration 234, epoch 1, batch 234/481,disc_loss 82.801, (real 87.44, fake 78.163 ) gen_loss 301.28\n",
            "iteration 235, epoch 1, batch 235/481,disc_loss 82.176, (real 87.676, fake 76.675 ) gen_loss 300.31\n",
            "iteration 236, epoch 1, batch 236/481,disc_loss 83.133, (real 88.528, fake 77.739 ) gen_loss 306.46\n",
            "iteration 237, epoch 1, batch 237/481,disc_loss 80.653, (real 85.921, fake 75.385 ) gen_loss 305.77\n",
            "iteration 238, epoch 1, batch 238/481,disc_loss 80.504, (real 86.775, fake 74.233 ) gen_loss 331.44\n",
            "iteration 239, epoch 1, batch 239/481,disc_loss 83.909, (real 88.451, fake 79.367 ) gen_loss 299.02\n",
            "iteration 240, epoch 1, batch 240/481,disc_loss 85.023, (real 91.377, fake 78.668 ) gen_loss 328.05\n",
            "iteration 241, epoch 1, batch 241/481,disc_loss 81.174, (real 85.63, fake 76.718 ) gen_loss 307.48\n",
            "iteration 242, epoch 1, batch 242/481,disc_loss 81.55, (real 86.171, fake 76.93 ) gen_loss 306.21\n",
            "iteration 243, epoch 1, batch 243/481,disc_loss 82.741, (real 88.98, fake 76.502 ) gen_loss 296.19\n",
            "iteration 244, epoch 1, batch 244/481,disc_loss 85.654, (real 92.074, fake 79.234 ) gen_loss 304.75\n",
            "iteration 245, epoch 1, batch 245/481,disc_loss 83.201, (real 88.704, fake 77.698 ) gen_loss 315.97\n",
            "iteration 246, epoch 1, batch 246/481,disc_loss 82.389, (real 89.195, fake 75.584 ) gen_loss 328.41\n",
            "iteration 247, epoch 1, batch 247/481,disc_loss 84.329, (real 88.959, fake 79.699 ) gen_loss 332.55\n",
            "iteration 248, epoch 1, batch 248/481,disc_loss 78.842, (real 85.729, fake 71.956 ) gen_loss 300.85\n",
            "iteration 249, epoch 1, batch 249/481,disc_loss 86.19, (real 92.02, fake 80.36 ) gen_loss 294.65\n",
            "iteration 250, epoch 1, batch 250/481,disc_loss 78.909, (real 84.171, fake 73.647 ) gen_loss 303.73\n",
            "iteration 251, epoch 1, batch 251/481,disc_loss 85.455, (real 91.891, fake 79.018 ) gen_loss 349.67\n",
            "iteration 252, epoch 1, batch 252/481,disc_loss 81.145, (real 85.9, fake 76.391 ) gen_loss 330.31\n",
            "iteration 253, epoch 1, batch 253/481,disc_loss 86.151, (real 93.499, fake 78.803 ) gen_loss 318.52\n",
            "iteration 254, epoch 1, batch 254/481,disc_loss 83.913, (real 88.017, fake 79.809 ) gen_loss 353.88\n",
            "iteration 255, epoch 1, batch 255/481,disc_loss 81.512, (real 87.929, fake 75.095 ) gen_loss 344.83\n",
            "iteration 256, epoch 1, batch 256/481,disc_loss 84.012, (real 90.914, fake 77.111 ) gen_loss 298.75\n",
            "iteration 257, epoch 1, batch 257/481,disc_loss 83.234, (real 87.311, fake 79.158 ) gen_loss 318.91\n",
            "iteration 258, epoch 1, batch 258/481,disc_loss 84.099, (real 89.414, fake 78.785 ) gen_loss 326.41\n",
            "iteration 259, epoch 1, batch 259/481,disc_loss 82.8, (real 89.034, fake 76.566 ) gen_loss 318.73\n",
            "iteration 260, epoch 1, batch 260/481,disc_loss 81.891, (real 85.602, fake 78.181 ) gen_loss 301.98\n",
            "iteration 261, epoch 1, batch 261/481,disc_loss 87.587, (real 95.695, fake 79.479 ) gen_loss 304.17\n",
            "iteration 262, epoch 1, batch 262/481,disc_loss 83.679, (real 89.618, fake 77.739 ) gen_loss 296.54\n",
            "iteration 263, epoch 1, batch 263/481,disc_loss 79.747, (real 85.699, fake 73.795 ) gen_loss 303.68\n",
            "iteration 264, epoch 1, batch 264/481,disc_loss 79.115, (real 86.07, fake 72.16 ) gen_loss 292.66\n",
            "iteration 265, epoch 1, batch 265/481,disc_loss 78.738, (real 85.608, fake 71.868 ) gen_loss 289.71\n",
            "iteration 266, epoch 1, batch 266/481,disc_loss 82.352, (real 88.621, fake 76.082 ) gen_loss 299.91\n",
            "iteration 267, epoch 1, batch 267/481,disc_loss 84.482, (real 91.252, fake 77.712 ) gen_loss 299.64\n",
            "iteration 268, epoch 1, batch 268/481,disc_loss 81.632, (real 85.564, fake 77.7 ) gen_loss 303.75\n",
            "iteration 269, epoch 1, batch 269/481,disc_loss 85.398, (real 90.596, fake 80.199 ) gen_loss 331.43\n",
            "iteration 270, epoch 1, batch 270/481,disc_loss 77.727, (real 81.907, fake 73.547 ) gen_loss 311.73\n",
            "iteration 271, epoch 1, batch 271/481,disc_loss 85.947, (real 90.561, fake 81.332 ) gen_loss 313.08\n",
            "iteration 272, epoch 1, batch 272/481,disc_loss 80.687, (real 87.095, fake 74.278 ) gen_loss 295.44\n",
            "iteration 273, epoch 1, batch 273/481,disc_loss 85.759, (real 90.711, fake 80.807 ) gen_loss 326.54\n",
            "iteration 274, epoch 1, batch 274/481,disc_loss 79.494, (real 86.089, fake 72.898 ) gen_loss 294.6\n",
            "iteration 275, epoch 1, batch 275/481,disc_loss 79.227, (real 84.402, fake 74.053 ) gen_loss 311.1\n",
            "iteration 276, epoch 1, batch 276/481,disc_loss 80.348, (real 85.919, fake 74.777 ) gen_loss 325.41\n",
            "iteration 277, epoch 1, batch 277/481,disc_loss 82.159, (real 86.488, fake 77.831 ) gen_loss 294.02\n",
            "iteration 278, epoch 1, batch 278/481,disc_loss 80.543, (real 85.297, fake 75.789 ) gen_loss 292.57\n",
            "iteration 279, epoch 1, batch 279/481,disc_loss 85.37, (real 91.18, fake 79.56 ) gen_loss 305.95\n",
            "iteration 280, epoch 1, batch 280/481,disc_loss 80.681, (real 86.164, fake 75.197 ) gen_loss 279.61\n",
            "iteration 281, epoch 1, batch 281/481,disc_loss 81.29, (real 87.473, fake 75.107 ) gen_loss 303.42\n",
            "iteration 282, epoch 1, batch 282/481,disc_loss 81.333, (real 86.525, fake 76.14 ) gen_loss 311.57\n",
            "iteration 283, epoch 1, batch 283/481,disc_loss 80.463, (real 86.501, fake 74.425 ) gen_loss 295.16\n",
            "iteration 284, epoch 1, batch 284/481,disc_loss 76.88, (real 81.435, fake 72.324 ) gen_loss 295.57\n",
            "iteration 285, epoch 1, batch 285/481,disc_loss 84.015, (real 88.371, fake 79.659 ) gen_loss 358.44\n",
            "iteration 286, epoch 1, batch 286/481,disc_loss 83.311, (real 91.503, fake 75.119 ) gen_loss 389.45\n",
            "iteration 287, epoch 1, batch 287/481,disc_loss 78.215, (real 83.13, fake 73.3 ) gen_loss 293.68\n",
            "iteration 288, epoch 1, batch 288/481,disc_loss 79.908, (real 85.922, fake 73.895 ) gen_loss 298.96\n",
            "iteration 289, epoch 1, batch 289/481,disc_loss 84.127, (real 89.02, fake 79.235 ) gen_loss 313.98\n",
            "iteration 290, epoch 1, batch 290/481,disc_loss 82.649, (real 88.265, fake 77.034 ) gen_loss 289.21\n",
            "iteration 291, epoch 1, batch 291/481,disc_loss 84.105, (real 89.033, fake 79.177 ) gen_loss 311.99\n",
            "iteration 292, epoch 1, batch 292/481,disc_loss 88.804, (real 96.159, fake 81.45 ) gen_loss 303.74\n",
            "iteration 293, epoch 1, batch 293/481,disc_loss 88.504, (real 94.382, fake 82.626 ) gen_loss 303.07\n",
            "iteration 294, epoch 1, batch 294/481,disc_loss 84.333, (real 89.511, fake 79.155 ) gen_loss 294.12\n",
            "iteration 295, epoch 1, batch 295/481,disc_loss 81.45, (real 88.216, fake 74.685 ) gen_loss 284.28\n",
            "iteration 296, epoch 1, batch 296/481,disc_loss 85.33, (real 91.05, fake 79.611 ) gen_loss 301.36\n",
            "iteration 297, epoch 1, batch 297/481,disc_loss 84.106, (real 90.262, fake 77.949 ) gen_loss 289.6\n",
            "iteration 298, epoch 1, batch 298/481,disc_loss 83.109, (real 89.421, fake 76.798 ) gen_loss 287.54\n",
            "iteration 299, epoch 1, batch 299/481,disc_loss 83.869, (real 88.618, fake 79.12 ) gen_loss 284.66\n",
            "iteration 300, epoch 1, batch 300/481,disc_loss 81.312, (real 85.346, fake 77.278 ) gen_loss 295.37\n",
            "iteration 301, epoch 1, batch 301/481,disc_loss 78.044, (real 82.829, fake 73.258 ) gen_loss 321.06\n",
            "iteration 302, epoch 1, batch 302/481,disc_loss 82.493, (real 89.157, fake 75.83 ) gen_loss 295.66\n",
            "iteration 303, epoch 1, batch 303/481,disc_loss 81.204, (real 86.065, fake 76.342 ) gen_loss 294.53\n",
            "iteration 304, epoch 1, batch 304/481,disc_loss 81.55, (real 87.026, fake 76.074 ) gen_loss 290.26\n",
            "iteration 305, epoch 1, batch 305/481,disc_loss 85.095, (real 89.301, fake 80.889 ) gen_loss 292.41\n",
            "iteration 306, epoch 1, batch 306/481,disc_loss 78.094, (real 83.26, fake 72.928 ) gen_loss 307.99\n",
            "iteration 307, epoch 1, batch 307/481,disc_loss 82.639, (real 88.361, fake 76.918 ) gen_loss 323.18\n",
            "iteration 308, epoch 1, batch 308/481,disc_loss 86.715, (real 97.663, fake 75.768 ) gen_loss 293.62\n",
            "iteration 309, epoch 1, batch 309/481,disc_loss 77.561, (real 82.275, fake 72.848 ) gen_loss 319.0\n",
            "iteration 310, epoch 1, batch 310/481,disc_loss 81.879, (real 88.826, fake 74.933 ) gen_loss 314.47\n",
            "iteration 311, epoch 1, batch 311/481,disc_loss 84.783, (real 89.635, fake 79.931 ) gen_loss 296.54\n",
            "iteration 312, epoch 1, batch 312/481,disc_loss 83.788, (real 88.549, fake 79.028 ) gen_loss 283.58\n",
            "iteration 313, epoch 1, batch 313/481,disc_loss 80.704, (real 85.772, fake 75.635 ) gen_loss 277.82\n",
            "iteration 314, epoch 1, batch 314/481,disc_loss 77.051, (real 82.229, fake 71.873 ) gen_loss 277.19\n",
            "iteration 315, epoch 1, batch 315/481,disc_loss 84.475, (real 90.481, fake 78.469 ) gen_loss 281.1\n",
            "iteration 316, epoch 1, batch 316/481,disc_loss 81.155, (real 88.829, fake 73.481 ) gen_loss 279.13\n",
            "iteration 317, epoch 1, batch 317/481,disc_loss 86.759, (real 93.453, fake 80.065 ) gen_loss 293.04\n",
            "iteration 318, epoch 1, batch 318/481,disc_loss 79.375, (real 85.605, fake 73.145 ) gen_loss 289.33\n",
            "iteration 319, epoch 1, batch 319/481,disc_loss 83.092, (real 87.457, fake 78.726 ) gen_loss 285.98\n",
            "iteration 320, epoch 1, batch 320/481,disc_loss 83.648, (real 90.097, fake 77.2 ) gen_loss 274.31\n",
            "iteration 321, epoch 1, batch 321/481,disc_loss 84.45, (real 88.633, fake 80.268 ) gen_loss 284.3\n",
            "iteration 322, epoch 1, batch 322/481,disc_loss 81.497, (real 88.146, fake 74.848 ) gen_loss 296.76\n",
            "iteration 323, epoch 1, batch 323/481,disc_loss 78.338, (real 82.773, fake 73.902 ) gen_loss 302.46\n",
            "iteration 324, epoch 1, batch 324/481,disc_loss 77.606, (real 82.964, fake 72.249 ) gen_loss 343.27\n",
            "iteration 325, epoch 1, batch 325/481,disc_loss 79.93, (real 85.21, fake 74.65 ) gen_loss 311.61\n",
            "iteration 326, epoch 1, batch 326/481,disc_loss 81.424, (real 87.856, fake 74.992 ) gen_loss 314.51\n",
            "iteration 327, epoch 1, batch 327/481,disc_loss 78.886, (real 83.272, fake 74.499 ) gen_loss 274.16\n",
            "iteration 328, epoch 1, batch 328/481,disc_loss 83.709, (real 87.837, fake 79.58 ) gen_loss 313.3\n",
            "iteration 329, epoch 1, batch 329/481,disc_loss 83.338, (real 91.153, fake 75.523 ) gen_loss 307.2\n",
            "iteration 330, epoch 1, batch 330/481,disc_loss 82.271, (real 89.402, fake 75.141 ) gen_loss 279.64\n",
            "iteration 331, epoch 1, batch 331/481,disc_loss 78.978, (real 84.353, fake 73.604 ) gen_loss 284.25\n",
            "iteration 332, epoch 1, batch 332/481,disc_loss 81.86, (real 87.102, fake 76.619 ) gen_loss 286.23\n",
            "iteration 333, epoch 1, batch 333/481,disc_loss 76.855, (real 83.191, fake 70.519 ) gen_loss 302.07\n",
            "iteration 334, epoch 1, batch 334/481,disc_loss 83.67, (real 88.018, fake 79.322 ) gen_loss 314.01\n",
            "iteration 335, epoch 1, batch 335/481,disc_loss 81.031, (real 86.541, fake 75.522 ) gen_loss 286.28\n",
            "iteration 336, epoch 1, batch 336/481,disc_loss 83.638, (real 90.078, fake 77.198 ) gen_loss 273.04\n",
            "iteration 337, epoch 1, batch 337/481,disc_loss 82.502, (real 87.492, fake 77.511 ) gen_loss 307.85\n",
            "iteration 338, epoch 1, batch 338/481,disc_loss 80.049, (real 84.103, fake 75.995 ) gen_loss 320.58\n",
            "iteration 339, epoch 1, batch 339/481,disc_loss 81.78, (real 91.537, fake 72.023 ) gen_loss 323.43\n",
            "iteration 340, epoch 1, batch 340/481,disc_loss 82.22, (real 88.815, fake 75.626 ) gen_loss 386.74\n",
            "iteration 341, epoch 1, batch 341/481,disc_loss 85.628, (real 90.359, fake 80.897 ) gen_loss 330.72\n",
            "iteration 342, epoch 1, batch 342/481,disc_loss 83.241, (real 88.357, fake 78.125 ) gen_loss 304.07\n",
            "iteration 343, epoch 1, batch 343/481,disc_loss 83.896, (real 90.695, fake 77.096 ) gen_loss 296.7\n",
            "iteration 344, epoch 1, batch 344/481,disc_loss 83.582, (real 89.345, fake 77.82 ) gen_loss 284.16\n",
            "iteration 345, epoch 1, batch 345/481,disc_loss 83.689, (real 87.752, fake 79.626 ) gen_loss 311.97\n",
            "iteration 346, epoch 1, batch 346/481,disc_loss 87.629, (real 95.328, fake 79.93 ) gen_loss 297.0\n",
            "iteration 347, epoch 1, batch 347/481,disc_loss 82.969, (real 87.197, fake 78.741 ) gen_loss 315.53\n",
            "iteration 348, epoch 1, batch 348/481,disc_loss 80.007, (real 84.147, fake 75.868 ) gen_loss 295.37\n",
            "iteration 349, epoch 1, batch 349/481,disc_loss 85.626, (real 92.701, fake 78.552 ) gen_loss 303.81\n",
            "iteration 350, epoch 1, batch 350/481,disc_loss 82.872, (real 88.727, fake 77.016 ) gen_loss 286.46\n",
            "iteration 351, epoch 1, batch 351/481,disc_loss 80.898, (real 86.322, fake 75.474 ) gen_loss 280.95\n",
            "iteration 352, epoch 1, batch 352/481,disc_loss 79.807, (real 84.063, fake 75.55 ) gen_loss 287.79\n",
            "iteration 353, epoch 1, batch 353/481,disc_loss 82.061, (real 88.561, fake 75.561 ) gen_loss 285.48\n",
            "iteration 354, epoch 1, batch 354/481,disc_loss 84.751, (real 90.409, fake 79.092 ) gen_loss 284.72\n",
            "iteration 355, epoch 1, batch 355/481,disc_loss 81.523, (real 86.896, fake 76.15 ) gen_loss 293.49\n",
            "iteration 356, epoch 1, batch 356/481,disc_loss 80.036, (real 85.601, fake 74.472 ) gen_loss 284.3\n",
            "iteration 357, epoch 1, batch 357/481,disc_loss 80.097, (real 85.9, fake 74.293 ) gen_loss 290.5\n",
            "iteration 358, epoch 1, batch 358/481,disc_loss 79.035, (real 85.404, fake 72.667 ) gen_loss 303.78\n",
            "iteration 359, epoch 1, batch 359/481,disc_loss 80.34, (real 84.999, fake 75.682 ) gen_loss 299.55\n",
            "iteration 360, epoch 1, batch 360/481,disc_loss 79.877, (real 85.71, fake 74.044 ) gen_loss 288.36\n",
            "iteration 361, epoch 1, batch 361/481,disc_loss 80.975, (real 85.84, fake 76.11 ) gen_loss 273.7\n",
            "iteration 362, epoch 1, batch 362/481,disc_loss 82.04, (real 88.904, fake 75.175 ) gen_loss 291.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfBjaXVv7IS0"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFcJlt6Hh8Qh"
      },
      "source": [
        "!pip install line_profiler\n",
        "%load_ext line_profiler\n",
        "%lprun -f training_step training_step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPlmAHXbVZac"
      },
      "source": [
        "labels_train.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnvpRlGSbVfF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ZBhhRlJ_gS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQQHP5dDJPVw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}