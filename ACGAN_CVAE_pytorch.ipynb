{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACGAN_CVAE_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OqYSQwo1yU4Q",
        "FiixZLra8_zZ",
        "y4fZsGw58csw",
        "OteyUsuqGmlc",
        "VE6IwXKiOnry",
        "xF15EJdcS4sz",
        "jCOi36MsLVcF",
        "CriG5eqZLfCl",
        "kLOAJzCYbKaM",
        "8H2tHpYFj5IZ",
        "-ce7yr0Xtn7s",
        "zYA_JRZkQPuU",
        "PflxcKvJXJSG",
        "H_tBcygt5wPW",
        "q5ZSz-KWLrur",
        "9FiNNQMGt-3_",
        "9pPB3sMNtMuP",
        "jHyPd1gGoxo2",
        "N0KpZUUgvS5l",
        "kg1oCdQAQ4k9",
        "npjGgL35UtbP",
        "RijOcfATXiAk",
        "MPQboez8Xmoa",
        "BVfQKAZvahOX",
        "KHE-7GAJRwy0",
        "_WizX5x6yEPr"
      ],
      "authorship_tag": "ABX9TyNRij3C0IwW1YLMzN1Ero6d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shackste/galaxy-generator/blob/main/ACGAN_CVAE_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEiOsqeXFUJk"
      },
      "source": [
        "PyTorch implementation of combined Conditional Variable Auto Encoder (CVAE) and Auxiliary-Classifier Generative Adversarial Network (ACGAN)\n",
        "\n",
        "continuation of work by Mohamad Dia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqYSQwo1yU4Q"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiixZLra8_zZ"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_TbcQTbUyvl",
        "outputId": "57529025-1e3b-4c16-9885-8602144f1220"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCTcfYGzFI1Q"
      },
      "source": [
        "import os\n",
        "import imageio\n",
        "from datetime import datetime\n",
        "from pdb import set_trace\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import Image, display\n",
        "import torch\n",
        "from torch import rand, cat, tensor, log, square, sum, mean, add, from_numpy\n",
        "from torch.nn import Module, Sequential, \\\n",
        "                     Conv2d, ConvTranspose2d, Linear, \\\n",
        "                     ReLU, LeakyReLU, Sigmoid, Softmax, Softplus, \\\n",
        "                     BatchNorm1d, BatchNorm2d, Flatten, ZeroPad2d, \\\n",
        "                     MSELoss, BCELoss, CrossEntropyLoss                    \n",
        "from torch.optim import Adam\n",
        "from torchsummary import summary\n",
        "from torchviz import make_dot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4fZsGw58csw"
      },
      "source": [
        "## File system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FBqRTkN8J34",
        "outputId": "ec40d1ca-0e59-460b-a103-b08e2d28a970"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/drive\")\n",
        "\n",
        "root = \"/drive/MyDrive/FHNW/galaxy_generator/\"\n",
        "\n",
        "## the following files need to be downloaded to your google drive in root folder defined above\n",
        "## download from https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/data\n",
        "file_galaxy_images = root + \"galaxyzoo_data_cropped_nonnormalized.npy\"\n",
        "file_galaxy_labels = root + \"training_solutions_rev1.csv\"\n",
        "\n",
        "folder_results = root + \"results_pytorch/\"\n",
        "\n",
        "if not os.path.exists(folder_results):\n",
        "    os.mkdir(folder_results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OteyUsuqGmlc"
      },
      "source": [
        "## Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jje7X-YCGk1Y"
      },
      "source": [
        "# Input\n",
        "image_dim = 64\n",
        "colors_dim = 3\n",
        "labels_dim = 3\n",
        "input_size = (colors_dim,image_dim,image_dim)\n",
        "\n",
        "# Encoder/Decoder\n",
        "latent_dim = 8\n",
        "decoder_dim = latent_dim  # differs from latent_dim if PCA applied before decoder\n",
        "\n",
        "# General\n",
        "learning_rate = 0.0002\n",
        "betas = (0.5,0.999)  ## 0.999 is default beta2 in tensorflow\n",
        "optimizer = Adam\n",
        "negative_slope = 0.2 # for LeakyReLU\n",
        "momentum = 0.99 # for BatchNorm\n",
        "\n",
        "# Loss weights\n",
        "alpha = 1 # switch VAE (1) / AE (0)\n",
        "beta = 1 # weight for KL-loss\n",
        "zeta = 0.5 # weight for MSE-loss\n",
        "delta = 1 # weight for class-loss\n",
        "gamma = 1024 # weight for learned-metric-loss (https://arxiv.org/pdf/1512.09300.pdf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE6IwXKiOnry"
      },
      "source": [
        "# helpful functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBlFPHM8OnX_"
      },
      "source": [
        "def summarize(network, input_size=input_size, graph=False, labels=False):\n",
        "    net = network()\n",
        "    net = net.cuda()\n",
        "    print(\"input size\", input_size)\n",
        "    input_dummy = rand(3, *input_size).cuda()\n",
        "    if labels:\n",
        "        summary(net, input_size)\n",
        "        labels_dummy = rand(labels_dim).cuda()\n",
        "        y = net(input_dummy, labels_dummy)\n",
        "    else:\n",
        "        summary(net, input_size)\n",
        "        y = net(input_dummy)\n",
        "    if graph:\n",
        "        display(make_dot(y, params=dict(list(net.named_parameters()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QuVirPN1ecH"
      },
      "source": [
        "## create module to reshape in Sequential\n",
        "class Reshape(Module):\n",
        "    def __init__(self, *shape):\n",
        "        super(Reshape, self).__init__()\n",
        "        self.shape = shape\n",
        "    def forward(self, input):\n",
        "        return input.view(input.shape[0], *self.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHGyIu-Bzf-L"
      },
      "source": [
        "## create module to check shape is Sequential\n",
        "class PrintShape(Module):\n",
        "    def __init__(self):\n",
        "        super(PrintShape, self).__init__()\n",
        "    def forward(self, input):\n",
        "        print(input.shape)\n",
        "        return input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBzU4x5GLeww"
      },
      "source": [
        "def same_padding(*, kernel_size=1, stride=1):\n",
        "    return (kernel_size - stride)//2 + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Es77S5v9PLl"
      },
      "source": [
        "def write_generated_galaxy_image(*, image=None, filename=None):\n",
        "    assert image is not None, \"provide generated galaxy image\"\n",
        "    assert filename is not None, \"provide filename\"\n",
        "    if not image.shape[2] == 3:\n",
        "        image = np.rollaxis(image, 0, 3)\n",
        "    imageio.imwrite(folder_results+filename, image)\n",
        "\n",
        "\n",
        "def write_generated_galaxy_images_iteration(*, iteration=None, images=None):\n",
        "    assert type(iteration) is int, \"provide iteration as integer\"\n",
        "    assert images is not None, \"provide generated galaxy images\"\n",
        "\n",
        "    w, h = 8, 8  ## !! find where these come from\n",
        "    d = image_dim\n",
        "\n",
        "    flat_image = np.empty((3, w*d, h*d))\n",
        "    k = 0\n",
        "    for iw in range(w):\n",
        "        for ih in range(h):\n",
        "            flat_image[:,iw*d:(iw+1)*d, ih*d:(ih+1)*d] = images[k]\n",
        "            k += 1\n",
        "    write_generated_galaxy_image(image=flat_image, filename=f\"samples_iter{iteration}.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2XotYa5Jp4y"
      },
      "source": [
        "def return_batch(sample, i, size):\n",
        "    \"\"\" from sample return the i'th batch if size \"\"\"\n",
        "    return sample[i*size : (i+1)*size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF15EJdcS4sz"
      },
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBV0yDeUhyD6"
      },
      "source": [
        "class NeuralNetwork(Module):\n",
        "    \"\"\" base class with convenient procedures used by all NN\"\"\"\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.parameter_file = f\"{folder_results}parameter_state_dict_{self._get_name()}.pth\"\n",
        "        # self.cuda() ## all NN shall run on cuda ### doesnt seem to work\n",
        "    \n",
        "    def save(self):\n",
        "        torch.save(self.state_dict(), self.parameter_file)\n",
        "\n",
        "    def load(self):\n",
        "        self.load_state_dict(torch.load(self.parameter_file))\n",
        "        self.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCOi36MsLVcF"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG5eqZLfCl"
      },
      "source": [
        "### Discriminator 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4alTcqEnKVg_"
      },
      "source": [
        "class Discriminator1(NeuralNetwork):\n",
        "    def __init__(self):\n",
        "        super(Discriminator1, self).__init__()\n",
        "        kernel_size = 3\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv1 = Sequential(\n",
        "            Conv2d(colors_dim, 8, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            Conv2d(8, 16, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(16),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv3 = Sequential(\n",
        "            Conv2d(16, 32, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv4 = Sequential(\n",
        "            Conv2d(32, 64, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.dense1 = Sequential(\n",
        "            Flatten(),\n",
        "            Linear(1024,256),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.dense2 = Sequential(\n",
        "            Linear(256,1),\n",
        "            Sigmoid(),\n",
        "        )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oks7AXbSMVvi",
        "outputId": "d96cfd96-dbd1-4b23-b361-7dd2fc0f9e8e"
      },
      "source": [
        "summarize(Discriminator1, graph=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size (3, 64, 64)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 32, 32]             224\n",
            "              ReLU-2            [-1, 8, 32, 32]               0\n",
            "            Conv2d-3           [-1, 16, 16, 16]           1,168\n",
            "       BatchNorm2d-4           [-1, 16, 16, 16]              32\n",
            "              ReLU-5           [-1, 16, 16, 16]               0\n",
            "            Conv2d-6             [-1, 32, 8, 8]           4,640\n",
            "              ReLU-7             [-1, 32, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 4, 4]          18,496\n",
            "              ReLU-9             [-1, 64, 4, 4]               0\n",
            "          Flatten-10                 [-1, 1024]               0\n",
            "           Linear-11                  [-1, 256]         262,400\n",
            "             ReLU-12                  [-1, 256]               0\n",
            "           Linear-13                    [-1, 1]             257\n",
            "          Sigmoid-14                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 287,217\n",
            "Trainable params: 287,217\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 0.28\n",
            "Params size (MB): 1.10\n",
            "Estimated Total Size (MB): 1.42\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLOAJzCYbKaM"
      },
      "source": [
        "### Discriminator 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVh_Z1rEYW_c"
      },
      "source": [
        "class Discriminator3(NeuralNetwork):\n",
        "    def __init__(self):\n",
        "        super(Discriminator3, self).__init__()\n",
        "        kernel_size = 5\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.conv0 = Sequential(\n",
        "            Conv2d(colors_dim, 32, kernel_size=1, stride=1),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.conv1 = Sequential(\n",
        "            Conv2d(32, 64, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(64, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            Conv2d(64, 128, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(128, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.dense1 = Sequential(\n",
        "            Flatten(),\n",
        "            Linear(32768,1024),\n",
        "            BatchNorm1d(1024, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.dense2 = Sequential(\n",
        "            Linear(1024,1),\n",
        "            Sigmoid(),\n",
        "        )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yDxd_e6b97W",
        "outputId": "e7787572-9dab-409d-8b7b-fdd0d488c2f5"
      },
      "source": [
        "summarize(Discriminator3, graph=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size (3, 64, 64)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 64, 64]             128\n",
            "         LeakyReLU-2           [-1, 32, 64, 64]               0\n",
            "            Conv2d-3           [-1, 64, 32, 32]          51,264\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "         LeakyReLU-5           [-1, 64, 32, 32]               0\n",
            "            Conv2d-6          [-1, 128, 16, 16]         204,928\n",
            "       BatchNorm2d-7          [-1, 128, 16, 16]             256\n",
            "         LeakyReLU-8          [-1, 128, 16, 16]               0\n",
            "           Flatten-9                [-1, 32768]               0\n",
            "           Linear-10                 [-1, 1024]      33,555,456\n",
            "      BatchNorm1d-11                 [-1, 1024]           2,048\n",
            "        LeakyReLU-12                 [-1, 1024]               0\n",
            "           Linear-13                    [-1, 1]           1,025\n",
            "          Sigmoid-14                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 33,815,233\n",
            "Trainable params: 33,815,233\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 4.52\n",
            "Params size (MB): 128.99\n",
            "Estimated Total Size (MB): 133.57\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H2tHpYFj5IZ"
      },
      "source": [
        "### Discriminator 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioup9Q3OkBe8"
      },
      "source": [
        "class Discriminator4(NeuralNetwork):\n",
        "    def __init__(self):\n",
        "        super(Discriminator4, self).__init__()\n",
        "        kernel_size = 3\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv0 = Sequential(\n",
        "            Conv2d(colors_dim, 16, kernel_size=1, stride=1),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.conv1 = Sequential(\n",
        "            Conv2d(16, 32, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(32, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope)\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            Conv2d(32, 64, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(64, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope)\n",
        "        )\n",
        "        self.conv3 = Sequential(\n",
        "            Conv2d(64, 128, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(128, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope)\n",
        "        )\n",
        "        self.dense1 = Sequential(\n",
        "            Flatten(),\n",
        "            Linear(8192, 1024),\n",
        "            LeakyReLU(negative_slope=negative_slope)\n",
        "        )   ## outputs metric\n",
        "\n",
        "        ## the following take metric as input\n",
        "        self.dense2_1 = Sequential(\n",
        "            Linear(1024,1),\n",
        "            Sigmoid(),\n",
        "        )   ## outputs true/fake\n",
        "        self.dense2_2 = Sequential(\n",
        "            Linear(1024, labels_dim),\n",
        "            Softmax(dim=1),\n",
        "        )   ## outputs labels\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "\n",
        "    def forward(self, images):\n",
        "        x = self.conv0(images)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        metric = self.dense1(x)\n",
        "        true_fake = self.dense2_1(metric)\n",
        "        labels = self.dense2_2(metric)\n",
        "        return cat((true_fake, labels, metric), dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOXPe0p8dKrN",
        "outputId": "6c6bbf4c-a7da-4187-bf87-c407447b4b01"
      },
      "source": [
        "summarize(Discriminator4, graph=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size (3, 64, 64)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 64, 64]              64\n",
            "         LeakyReLU-2           [-1, 16, 64, 64]               0\n",
            "            Conv2d-3           [-1, 32, 32, 32]           4,640\n",
            "       BatchNorm2d-4           [-1, 32, 32, 32]              64\n",
            "         LeakyReLU-5           [-1, 32, 32, 32]               0\n",
            "            Conv2d-6           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-7           [-1, 64, 16, 16]             128\n",
            "         LeakyReLU-8           [-1, 64, 16, 16]               0\n",
            "            Conv2d-9            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-10            [-1, 128, 8, 8]             256\n",
            "        LeakyReLU-11            [-1, 128, 8, 8]               0\n",
            "          Flatten-12                 [-1, 8192]               0\n",
            "           Linear-13                 [-1, 1024]       8,389,632\n",
            "        LeakyReLU-14                 [-1, 1024]               0\n",
            "           Linear-15                    [-1, 1]           1,025\n",
            "          Sigmoid-16                    [-1, 1]               0\n",
            "           Linear-17                    [-1, 3]           3,075\n",
            "          Softmax-18                    [-1, 3]               0\n",
            "================================================================\n",
            "Total params: 8,491,236\n",
            "Trainable params: 8,491,236\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 2.39\n",
            "Params size (MB): 32.39\n",
            "Estimated Total Size (MB): 34.83\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ce7yr0Xtn7s"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYA_JRZkQPuU"
      },
      "source": [
        "### Encoder 1\n",
        "basic convolutional network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GazjpfMrcJQ"
      },
      "source": [
        "class Encoder1(NeuralNetwork):\n",
        "    def __init__(self):\n",
        "        super(Encoder1, self).__init__()\n",
        "        kernel_size = 3\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv1 = Sequential(\n",
        "            Conv2d(colors_dim, 8, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            Conv2d(8, 16, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(16),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv3 = Sequential(\n",
        "            Conv2d(16, 32, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv4 = Sequential(\n",
        "            Conv2d(32, 64, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.dense1 = Sequential(\n",
        "            Flatten(),\n",
        "            Linear(1024, 256),\n",
        "            ReLU(),\n",
        "        )\n",
        "        \n",
        "        ## the following take the same input\n",
        "        self.dense_z_mu = Linear(256, latent_dim)\n",
        "        if alpha:\n",
        "            self.dense_z_std = Sequential(\n",
        "                Linear(256, latent_dim),\n",
        "                Softplus(),\n",
        "            )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.dense1(x)\n",
        "        z_mu = self.dense_z_mu(x)\n",
        "        if not alpha:\n",
        "            return z_mu\n",
        "        z_std = self.dense_z_std(x)\n",
        "        return cat((z_mu, z_std), dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNMo9hClUCfN",
        "outputId": "b964f44b-d45e-44a3-ea1b-9465e8d3686f"
      },
      "source": [
        "summarize(Encoder1, graph=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size (3, 64, 64)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 32, 32]             224\n",
            "              ReLU-2            [-1, 8, 32, 32]               0\n",
            "            Conv2d-3           [-1, 16, 16, 16]           1,168\n",
            "       BatchNorm2d-4           [-1, 16, 16, 16]              32\n",
            "              ReLU-5           [-1, 16, 16, 16]               0\n",
            "            Conv2d-6             [-1, 32, 8, 8]           4,640\n",
            "              ReLU-7             [-1, 32, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 4, 4]          18,496\n",
            "              ReLU-9             [-1, 64, 4, 4]               0\n",
            "          Flatten-10                 [-1, 1024]               0\n",
            "           Linear-11                  [-1, 256]         262,400\n",
            "             ReLU-12                  [-1, 256]               0\n",
            "           Linear-13                    [-1, 8]           2,056\n",
            "           Linear-14                    [-1, 8]           2,056\n",
            "         Softplus-15                    [-1, 8]               0\n",
            "================================================================\n",
            "Total params: 291,072\n",
            "Trainable params: 291,072\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 0.28\n",
            "Params size (MB): 1.11\n",
            "Estimated Total Size (MB): 1.43\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PflxcKvJXJSG"
      },
      "source": [
        "### Encoder 2\n",
        "inception convolutional network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxb5FWx6XNB4"
      },
      "source": [
        "class Encoder2(NeuralNetwork):\n",
        "    \"\"\" inception convolutional network \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Encoder2, self).__init__()\n",
        "        kernel_size = 3\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2\n",
        "\n",
        "        # Inception 1\n",
        "        self.inc1_1 = Sequential(\n",
        "            Conv2d(colors_dim, 4, kernel_size=1, stride=1),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.inc1_2 = Sequential(\n",
        "            Conv2d(colors_dim, 4, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv1 = Sequential(\n",
        "            Conv2d(8, 8, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "\n",
        "        # Inception 2\n",
        "        self.inc2_1 = Sequential(\n",
        "            Conv2d(8, 8, kernel_size=1, stride=1),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.inc2_2 = Sequential(\n",
        "            Conv2d(8, 8, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            Conv2d(16, 16, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(16),\n",
        "            ReLU(),\n",
        "        )\n",
        "\n",
        "        # Inception 3\n",
        "        self.inc3_1 = Sequential(\n",
        "            Conv2d(16, 16, kernel_size=1, stride=1),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.inc3_2 = Sequential(\n",
        "            Conv2d(16, 16, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv3 = Sequential(\n",
        "            Conv2d(32, 32, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "\n",
        "        # Inception 4\n",
        "        self.inc4_1 = Sequential(\n",
        "            Conv2d(32, 32, kernel_size=1, stride=1),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.inc4_2 = Sequential(\n",
        "            Conv2d(32, 32, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv4 = Sequential(\n",
        "            Conv2d(64, 64, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "\n",
        "        self.dense1 = Sequential(\n",
        "            Flatten(),\n",
        "            Linear(1024, 256),\n",
        "            ReLU(),\n",
        "        )\n",
        "\n",
        "        ## the following take the same input from dense1\n",
        "        self.dense_z_mu = Linear(256, latent_dim)\n",
        "        if alpha:\n",
        "            self.dense_z_std = Sequential(\n",
        "                Linear(256, latent_dim),\n",
        "                Softplus(),\n",
        "            )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inc1 = self.inc1_1(x)\n",
        "        inc2 = self.inc1_2(x)\n",
        "        x = self.conv1(cat((inc1,inc2), dim=1))\n",
        "        inc1 = self.inc2_1(x)\n",
        "        inc2 = self.inc2_2(x)\n",
        "        x = self.conv2(cat((inc1,inc2), dim=1))\n",
        "        inc1 = self.inc3_1(x)\n",
        "        inc2 = self.inc3_2(x)\n",
        "        x = self.conv3(cat((inc1,inc2), dim=1))\n",
        "        inc1 = self.inc4_1(x)\n",
        "        inc2 = self.inc4_2(x)\n",
        "        x = self.conv4(cat((inc1,inc2), dim=1))\n",
        "        x = self.dense1(x)\n",
        "        z_mu = self.dense_z_mu(x)\n",
        "        if not alpha:\n",
        "            return z_mu\n",
        "        z_std = self.dense_z_std(x)\n",
        "        return cat((z_mu, z_std), dim=1)\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbL2rhM-XNvj",
        "outputId": "ae16b8e3-e9cd-4d87-e06c-441401b78120"
      },
      "source": [
        "summarize(Encoder2, graph=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size (3, 64, 64)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 4, 64, 64]              16\n",
            "              ReLU-2            [-1, 4, 64, 64]               0\n",
            "            Conv2d-3            [-1, 4, 64, 64]             112\n",
            "              ReLU-4            [-1, 4, 64, 64]               0\n",
            "            Conv2d-5            [-1, 8, 32, 32]             584\n",
            "              ReLU-6            [-1, 8, 32, 32]               0\n",
            "            Conv2d-7            [-1, 8, 32, 32]              72\n",
            "              ReLU-8            [-1, 8, 32, 32]               0\n",
            "            Conv2d-9            [-1, 8, 32, 32]             584\n",
            "             ReLU-10            [-1, 8, 32, 32]               0\n",
            "           Conv2d-11           [-1, 16, 16, 16]           2,320\n",
            "      BatchNorm2d-12           [-1, 16, 16, 16]              32\n",
            "             ReLU-13           [-1, 16, 16, 16]               0\n",
            "           Conv2d-14           [-1, 16, 16, 16]             272\n",
            "             ReLU-15           [-1, 16, 16, 16]               0\n",
            "           Conv2d-16           [-1, 16, 16, 16]           2,320\n",
            "             ReLU-17           [-1, 16, 16, 16]               0\n",
            "           Conv2d-18             [-1, 32, 8, 8]           9,248\n",
            "             ReLU-19             [-1, 32, 8, 8]               0\n",
            "           Conv2d-20             [-1, 32, 8, 8]           1,056\n",
            "             ReLU-21             [-1, 32, 8, 8]               0\n",
            "           Conv2d-22             [-1, 32, 8, 8]           9,248\n",
            "             ReLU-23             [-1, 32, 8, 8]               0\n",
            "           Conv2d-24             [-1, 64, 4, 4]          36,928\n",
            "             ReLU-25             [-1, 64, 4, 4]               0\n",
            "          Flatten-26                 [-1, 1024]               0\n",
            "           Linear-27                  [-1, 256]         262,400\n",
            "             ReLU-28                  [-1, 256]               0\n",
            "           Linear-29                    [-1, 8]           2,056\n",
            "           Linear-30                    [-1, 8]           2,056\n",
            "         Softplus-31                    [-1, 8]               0\n",
            "================================================================\n",
            "Total params: 329,304\n",
            "Trainable params: 329,304\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 1.22\n",
            "Params size (MB): 1.26\n",
            "Estimated Total Size (MB): 2.52\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_tBcygt5wPW"
      },
      "source": [
        "### Encoder 3\n",
        "BatchNormed convolutional network with LeakyReLU and kernel=5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEap3yQD5z6o"
      },
      "source": [
        "class Encoder3(NeuralNetwork):\n",
        "    \"\"\" convolutional network with leaky ReLU \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Encoder3, self).__init__()\n",
        "        kernel_size = 5\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.conv0 = Sequential(\n",
        "            Conv2d(colors_dim, 32, kernel_size=1, stride=1),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.conv1 = Sequential(\n",
        "            Conv2d(32, 64, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(64, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            Conv2d(64, 128, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(128, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.dense1 = Sequential(\n",
        "            Flatten(),\n",
        "            Linear(32768, 1024),\n",
        "            BatchNorm1d(1024, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "\n",
        "        ## the following take the same input from dense1\n",
        "        self.dense_z_mu = Linear(1024, latent_dim)\n",
        "        if alpha:\n",
        "            self.dense_z_std = Sequential(\n",
        "                Linear(1024, latent_dim),\n",
        "                Softplus(),\n",
        "            )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.dense1(x)\n",
        "        z_mu = self.dense_z_mu(x)\n",
        "        if not alpha:\n",
        "            return z_mu\n",
        "        z_std = self.dense_z_std(x)\n",
        "        return cat((z_mu, z_std), dim=1)\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9JUxa_6Cvpk",
        "outputId": "43e43c43-ce80-4e3c-c186-606c7bfd95c2"
      },
      "source": [
        "summarize(Encoder3, graph=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size (3, 64, 64)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 64, 64]             128\n",
            "         LeakyReLU-2           [-1, 32, 64, 64]               0\n",
            "            Conv2d-3           [-1, 64, 32, 32]          51,264\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "         LeakyReLU-5           [-1, 64, 32, 32]               0\n",
            "            Conv2d-6          [-1, 128, 16, 16]         204,928\n",
            "       BatchNorm2d-7          [-1, 128, 16, 16]             256\n",
            "         LeakyReLU-8          [-1, 128, 16, 16]               0\n",
            "           Flatten-9                [-1, 32768]               0\n",
            "           Linear-10                 [-1, 1024]      33,555,456\n",
            "      BatchNorm1d-11                 [-1, 1024]           2,048\n",
            "        LeakyReLU-12                 [-1, 1024]               0\n",
            "           Linear-13                    [-1, 8]           8,200\n",
            "           Linear-14                    [-1, 8]           8,200\n",
            "         Softplus-15                    [-1, 8]               0\n",
            "================================================================\n",
            "Total params: 33,830,608\n",
            "Trainable params: 33,830,608\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 4.52\n",
            "Params size (MB): 129.05\n",
            "Estimated Total Size (MB): 133.62\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5ZSz-KWLrur"
      },
      "source": [
        "### Encoder 4\n",
        "Conditional BatchNormed convolutional network with LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhqbA2iBLveZ"
      },
      "source": [
        "class Encoder4(NeuralNetwork):\n",
        "    \"\"\" convolutional network with BatchNorm and LeakyReLU \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Encoder4, self).__init__()\n",
        "        kernel_size = 3\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.conv0 = Sequential(\n",
        "            Conv2d(colors_dim, 16, kernel_size=1, stride=1),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.conv1 = Sequential(\n",
        "            Conv2d(16, 32, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(32, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            Conv2d(32, 64, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(64, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "        )\n",
        "        self.conv3 = Sequential(\n",
        "            Conv2d(64,128, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            BatchNorm2d(128, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope),\n",
        "            Flatten(), # next layer takes flat input with labels appended\n",
        "        )\n",
        "        self.dense1 = Sequential(\n",
        "            Linear(8192+labels_dim, 1024),\n",
        "            BatchNorm1d(1024, momentum=momentum),\n",
        "            LeakyReLU(negative_slope=negative_slope)\n",
        "        )\n",
        "\n",
        "        ## the following take the same input from dense1\n",
        "        self.dense_z_mu = Linear(1024, latent_dim)\n",
        "        if alpha:\n",
        "            self.dense_z_std = Sequential(\n",
        "                Linear(1024, latent_dim),\n",
        "                Softplus(),\n",
        "            )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "    def forward(self, images, labels):\n",
        "        x = self.conv0(images)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = cat((x, labels), dim=1)\n",
        "        x = self.dense1(x)\n",
        "        z_mu = self.dense_z_mu(x)\n",
        "        if not alpha:\n",
        "            return z_mu\n",
        "        z_std = self.dense_z_std(x)\n",
        "        return z_mu, z_std\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF6t2XHZn28G",
        "outputId": "34ea8dc8-1212-4598-dea6-46aed2387e00"
      },
      "source": [
        "net = Encoder4()\n",
        "\n",
        "print(np.sum(np.prod(p.size()) for p in net.parameters()) == 8508656)\n",
        "input_dummy = rand(3, *input_size)\n",
        "label_dummy = rand(3, labels_dim)\n",
        "\n",
        "net(input_dummy, label_dummy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0611, -0.2827, -0.0415,  0.4707, -0.4764,  0.3596,  0.8293,  0.1872],\n",
              "         [ 0.4531, -0.3066, -0.5375, -0.2680, -0.3986, -0.2112, -0.7741, -0.6823],\n",
              "         [-0.1973,  0.4452,  0.1442,  0.5908,  0.5428, -0.2410,  0.4880, -0.0132]],\n",
              "        grad_fn=<AddmmBackward>),\n",
              " tensor([[0.5300, 0.5976, 1.1684, 0.9888, 0.6516, 0.5743, 0.7925, 0.7538],\n",
              "         [0.7084, 0.6592, 0.4728, 0.6019, 0.5940, 0.5222, 0.6214, 0.3699],\n",
              "         [0.4528, 0.7144, 0.5324, 0.3958, 0.2968, 0.5435, 0.4786, 0.6071]],\n",
              "        grad_fn=<SoftplusBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FiNNQMGt-3_"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjOHvFCetNTM"
      },
      "source": [
        "flat_dim = 8192\n",
        "orig_dim = (128, 8, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pPB3sMNtMuP"
      },
      "source": [
        "### Decoder 1\n",
        "Inverse basic convolutional network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd0MysPAuAgq"
      },
      "source": [
        "class Decoder1(NeuralNetwork):\n",
        "    \"\"\" reverse basic convolutional network \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Decoder1, self).__init__()\n",
        "        kernel_size = 3\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2 \n",
        "        self.dense1 = Sequential(\n",
        "            Linear(latent_dim, flat_dim),\n",
        "            BatchNorm1d(flat_dim),\n",
        "            ReLU(),\n",
        "            Reshape(*orig_dim)\n",
        "        )\n",
        "        self.conv1 = Sequential(\n",
        "            ConvTranspose2d(orig_dim[0], 32, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            ConvTranspose2d(32, 16, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv3 = Sequential(\n",
        "            ConvTranspose2d(16, 8, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=padding),\n",
        "            BatchNorm2d(8),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv4 = Sequential(\n",
        "            ConvTranspose2d(8, colors_dim, kernel_size=1, stride=1),\n",
        "            Sigmoid(),\n",
        "        )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb5xoVuvuNs-",
        "outputId": "fdd82a97-9a21-420d-c329-07917791bcc3"
      },
      "source": [
        "summarize(Decoder1, input_size=(latent_dim,))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size (8,)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 8192]          73,728\n",
            "       BatchNorm1d-2                 [-1, 8192]          16,384\n",
            "              ReLU-3                 [-1, 8192]               0\n",
            "           Reshape-4            [-1, 128, 8, 8]               0\n",
            "   ConvTranspose2d-5           [-1, 32, 16, 16]          36,896\n",
            "              ReLU-6           [-1, 32, 16, 16]               0\n",
            "   ConvTranspose2d-7           [-1, 16, 32, 32]           4,624\n",
            "              ReLU-8           [-1, 16, 32, 32]               0\n",
            "   ConvTranspose2d-9            [-1, 8, 64, 64]           1,160\n",
            "      BatchNorm2d-10            [-1, 8, 64, 64]              16\n",
            "             ReLU-11            [-1, 8, 64, 64]               0\n",
            "  ConvTranspose2d-12            [-1, 3, 64, 64]              27\n",
            "          Sigmoid-13            [-1, 3, 64, 64]               0\n",
            "================================================================\n",
            "Total params: 132,835\n",
            "Trainable params: 132,835\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.56\n",
            "Params size (MB): 0.51\n",
            "Estimated Total Size (MB): 2.07\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHyPd1gGoxo2"
      },
      "source": [
        "### Decoder 2\n",
        "Inverse inception convolutional network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQUcUNjjo0Z-"
      },
      "source": [
        "class Decoder2(NeuralNetwork):\n",
        "    \"\"\" reverse inception convolutional network \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Decoder2, self).__init__()\n",
        "        kernel_size = 3\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2 \n",
        "        self.dense1 = Sequential(\n",
        "            Linear(latent_dim, flat_dim),\n",
        "            BatchNorm1d(flat_dim),\n",
        "            ReLU(),\n",
        "            Reshape(*orig_dim)\n",
        "        )\n",
        "        # inception 1\n",
        "        self.inc1_1 = Sequential(\n",
        "            Conv2d(orig_dim[0], 16, kernel_size=1, stride=1),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.inc1_2 = Sequential(\n",
        "            Conv2d(orig_dim[0], 16, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv1 = Sequential(\n",
        "            ConvTranspose2d(32, 32, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        # inception 2\n",
        "        self.inc2_1 = Sequential(\n",
        "            Conv2d(32, 8, kernel_size=1, stride=1),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.inc2_2 = Sequential(\n",
        "            Conv2d(32, 8, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            ConvTranspose2d(16, 16, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        # inception 3\n",
        "        self.inc3_1 = Sequential(\n",
        "            Conv2d(16, 4, kernel_size=1, stride=1),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.inc3_2 = Sequential(\n",
        "            Conv2d(16, 4, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv3 = Sequential(\n",
        "            ConvTranspose2d(8, 8, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=padding),\n",
        "            BatchNorm2d(8),\n",
        "            ReLU(),\n",
        "        )\n",
        "        # inception 4\n",
        "        self.inc4_1 = Sequential(\n",
        "            Conv2d(8, 2, kernel_size=1, stride=1),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.inc4_2 = Sequential(\n",
        "            Conv2d(8, 2, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv4 = Sequential(\n",
        "            ConvTranspose2d(4, 4, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=padding),\n",
        "            Sigmoid(),\n",
        "        )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x1 = self.inc1_1(x)\n",
        "        x2 = self.inc1_2(x)\n",
        "        x = self.conv1(cat((x1, x2), dim=1))\n",
        "        x1 = self.inc2_1(x)\n",
        "        x2 = self.inc2_2(x)\n",
        "        x = self.conv2(cat((x1, x2), dim=1))\n",
        "        x1 = self.inc3_1(x)\n",
        "        x2 = self.inc3_2(x)\n",
        "        x = self.conv3(cat((x1, x2), dim=1))\n",
        "        x1 = self.inc4_1(x)\n",
        "        x2 = self.inc4_2(x)\n",
        "        x = self.conv4(cat((x1, x2), dim=1))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S7E4kFTswAy",
        "outputId": "49b9e81a-426f-4bb3-8317-e4030bb29813"
      },
      "source": [
        "summarize(Decoder2, input_size=(latent_dim,))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size (8,)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 8192]          73,728\n",
            "       BatchNorm1d-2                 [-1, 8192]          16,384\n",
            "              ReLU-3                 [-1, 8192]               0\n",
            "           Reshape-4            [-1, 128, 8, 8]               0\n",
            "            Conv2d-5             [-1, 16, 8, 8]           2,064\n",
            "              ReLU-6             [-1, 16, 8, 8]               0\n",
            "            Conv2d-7             [-1, 16, 8, 8]          18,448\n",
            "              ReLU-8             [-1, 16, 8, 8]               0\n",
            "   ConvTranspose2d-9           [-1, 32, 16, 16]           9,248\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11            [-1, 8, 16, 16]             264\n",
            "             ReLU-12            [-1, 8, 16, 16]               0\n",
            "           Conv2d-13            [-1, 8, 16, 16]           2,312\n",
            "             ReLU-14            [-1, 8, 16, 16]               0\n",
            "  ConvTranspose2d-15           [-1, 16, 32, 32]           2,320\n",
            "             ReLU-16           [-1, 16, 32, 32]               0\n",
            "           Conv2d-17            [-1, 4, 32, 32]              68\n",
            "             ReLU-18            [-1, 4, 32, 32]               0\n",
            "           Conv2d-19            [-1, 4, 32, 32]             580\n",
            "             ReLU-20            [-1, 4, 32, 32]               0\n",
            "  ConvTranspose2d-21            [-1, 8, 64, 64]             584\n",
            "      BatchNorm2d-22            [-1, 8, 64, 64]              16\n",
            "             ReLU-23            [-1, 8, 64, 64]               0\n",
            "           Conv2d-24            [-1, 2, 64, 64]              18\n",
            "             ReLU-25            [-1, 2, 64, 64]               0\n",
            "           Conv2d-26            [-1, 2, 64, 64]             146\n",
            "             ReLU-27            [-1, 2, 64, 64]               0\n",
            "  ConvTranspose2d-28          [-1, 4, 128, 128]             148\n",
            "          Sigmoid-29          [-1, 4, 128, 128]               0\n",
            "================================================================\n",
            "Total params: 126,328\n",
            "Trainable params: 126,328\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 2.84\n",
            "Params size (MB): 0.48\n",
            "Estimated Total Size (MB): 3.33\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0KpZUUgvS5l"
      },
      "source": [
        "### Decoder 3\n",
        "Inverse BatchNormed convolutional network with LeakyReLU and kernel=5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4t5s1F2vR_n"
      },
      "source": [
        "class Decoder3(NeuralNetwork):\n",
        "    \"\"\" reverse convolutional network with kernel of 5\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Decoder3, self).__init__()\n",
        "        kernel_size = 5\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2 \n",
        "        self.dense1 = Sequential(\n",
        "            Linear(latent_dim, 1024),\n",
        "            BatchNorm1d(1024, momentum=momentum),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.dense2= Sequential(\n",
        "            Linear(1024, 8*8*128),\n",
        "            BatchNorm1d(8*8*128, momentum=momentum),\n",
        "            ReLU(),\n",
        "            Reshape(128, 8, 8)\n",
        "        )\n",
        "        self.conv1 = Sequential(\n",
        "            ConvTranspose2d(128, 64, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1),\n",
        "            BatchNorm2d(64, momentum=momentum),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            ConvTranspose2d(64, 32, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1),\n",
        "            BatchNorm2d(32, momentum=momentum),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv3 = Sequential(\n",
        "            ConvTranspose2d(32, 16, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=1),\n",
        "            BatchNorm2d(16, momentum=momentum),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv4 = Sequential(\n",
        "            ConvTranspose2d(16, colors_dim, kernel_size=1),\n",
        "            Sigmoid()\n",
        "        )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7drdrW3SxbWt",
        "outputId": "77f08603-bc50-459b-fda1-7630cd972f49"
      },
      "source": [
        "summarize(Decoder3, input_size=(latent_dim,))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size (8,)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1024]           9,216\n",
            "       BatchNorm1d-2                 [-1, 1024]           2,048\n",
            "              ReLU-3                 [-1, 1024]               0\n",
            "            Linear-4                 [-1, 8192]       8,396,800\n",
            "       BatchNorm1d-5                 [-1, 8192]          16,384\n",
            "              ReLU-6                 [-1, 8192]               0\n",
            "           Reshape-7            [-1, 128, 8, 8]               0\n",
            "   ConvTranspose2d-8           [-1, 64, 16, 16]         204,864\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "  ConvTranspose2d-11           [-1, 32, 32, 32]          51,232\n",
            "      BatchNorm2d-12           [-1, 32, 32, 32]              64\n",
            "             ReLU-13           [-1, 32, 32, 32]               0\n",
            "  ConvTranspose2d-14           [-1, 16, 64, 64]          12,816\n",
            "      BatchNorm2d-15           [-1, 16, 64, 64]              32\n",
            "             ReLU-16           [-1, 16, 64, 64]               0\n",
            "  ConvTranspose2d-17            [-1, 3, 64, 64]              51\n",
            "          Sigmoid-18            [-1, 3, 64, 64]               0\n",
            "================================================================\n",
            "Total params: 8,693,635\n",
            "Trainable params: 8,693,635\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 3.09\n",
            "Params size (MB): 33.16\n",
            "Estimated Total Size (MB): 36.25\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg1oCdQAQ4k9"
      },
      "source": [
        "### Decoder 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DctYFm8Q3rv"
      },
      "source": [
        "class Decoder4(NeuralNetwork):\n",
        "    \"\"\" reverse convolutional network with kernel of 5\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Decoder4, self).__init__()\n",
        "        kernel_size = 3\n",
        "        stride = 2\n",
        "        padding = (kernel_size - 1) // 2 \n",
        "        self.dense1 = Sequential(\n",
        "            Linear(latent_dim + labels_dim, 1024),\n",
        "            BatchNorm1d(1024, momentum=momentum),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.dense2 = Sequential(\n",
        "            Linear(1024, flat_dim),\n",
        "            BatchNorm1d(flat_dim, momentum=momentum),\n",
        "            ReLU(),\n",
        "            Reshape(*orig_dim),\n",
        "        )\n",
        "        self.conv1 = Sequential(\n",
        "            ConvTranspose2d(orig_dim[0], 64, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=padding),\n",
        "            BatchNorm2d(64, momentum=momentum),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv2 = Sequential(\n",
        "            ConvTranspose2d(64, 32, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=padding),\n",
        "            BatchNorm2d(32, momentum=momentum),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv3 = Sequential(\n",
        "            ConvTranspose2d(32, 16, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=padding),\n",
        "            BatchNorm2d(16, momentum=momentum),\n",
        "            ReLU(),\n",
        "        )\n",
        "        self.conv4 = Sequential(\n",
        "            ConvTranspose2d(16, colors_dim, kernel_size=1, stride=1),\n",
        "            Sigmoid(),\n",
        "        )\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate, betas=betas)\n",
        "\n",
        "    def forward(self, latent, labels):\n",
        "        x = self.dense1(cat((latent, labels), dim=1))\n",
        "        x = self.dense2(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        return x\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnAFagCuTpgn",
        "outputId": "3f3d0e03-4a83-4cac-e1d6-fc2a07b7731e"
      },
      "source": [
        "net = Decoder4()\n",
        "\n",
        "print(np.sum(np.prod(p.size()) for p in net.parameters()) )\n",
        "input_dummy = rand(3, latent_dim)\n",
        "label_dummy = rand(3, labels_dim)\n",
        "\n",
        "net(input_dummy, label_dummy).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8524675\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npjGgL35UtbP"
      },
      "source": [
        "# Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mTgqq1OUt08"
      },
      "source": [
        "# latent space\n",
        "def gaussian_sampler(mu, sigma):\n",
        "    batch = mu.shape[0]\n",
        "    dim = mu.shape[1]\n",
        "    return mu + sigma*torch.randn(batch, dim, requires_grad=True).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RijOcfATXiAk"
      },
      "source": [
        "# Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1c8jfSeFEql"
      },
      "source": [
        "## all pipelines use the same networks\n",
        "encoder = Encoder4().cuda()\n",
        "try:\n",
        "    encoder.load()\n",
        "except:\n",
        "    print(\"fresh encoder\")\n",
        "decoder = Decoder4().cuda()\n",
        "try:\n",
        "    decoder.load()\n",
        "except:\n",
        "    print(\"fresh decoder\")\n",
        "discriminator = Discriminator4().cuda()\n",
        "try:\n",
        "    discriminator.load()\n",
        "except:\n",
        "    print(\"fresh discriminator\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPQboez8Xmoa"
      },
      "source": [
        "## CVAE\n",
        "Encoder + Decoder with labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO_31ivTXigq"
      },
      "source": [
        "class VAE(NeuralNetwork):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.sampler = gaussian_sampler\n",
        "\n",
        "    def forward(self, images, labels):\n",
        "        latent = self.encoder(images, labels)\n",
        "        if alpha: ## in VAE mode, replace z by random sample\n",
        "            latent = self.sampler(*latent)\n",
        "        x_hat = self.decoder(latent,labels)\n",
        "        return x_hat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bwPVOlaF2tl",
        "outputId": "3511a6c5-d40a-4f0c-e895-313706814c9b"
      },
      "source": [
        "net = VAE()\n",
        "images = rand(5,3,64,64).cuda()\n",
        "labels = rand(5,3).cuda()\n",
        "pred = net(images, labels)\n",
        "pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVfQKAZvahOX"
      },
      "source": [
        "## VAE-GAN\n",
        "VAE + Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6hFfejlahD3"
      },
      "source": [
        "\n",
        "class VAEGAN(NeuralNetwork):\n",
        "    def __init__(self):\n",
        "        super(VAEGAN, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.discriminator = discriminator\n",
        "        self.sampler = gaussian_sampler\n",
        "\n",
        "    def forward(self, images, labels):\n",
        "        latent = self.encoder(images, labels)\n",
        "        if alpha:\n",
        "            latent = self.sampler(*latent)\n",
        "        generated_images = self.decoder(latent,labels)\n",
        "\n",
        "        output = self.discriminator(generated_images)\n",
        "        return output # true/fake, labels, metric\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB16UaY7Q-ge",
        "outputId": "4d762d5b-2f1e-4ef2-f5e5-9004ca6fbc5b"
      },
      "source": [
        "net = VAEGAN()\n",
        "input = rand(3,3,64,64).cuda()\n",
        "labels = rand(3,3).cuda()\n",
        "pred = net(input, labels)\n",
        "pred.shape, pred[:,0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 1028]),\n",
              " tensor([0.0064, 0.0347, 0.0162], device='cuda:0', grad_fn=<SelectBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHE-7GAJRwy0"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhcBuXqxVpBO"
      },
      "source": [
        "## pytorch has no categorical crossentropy for e. g. target = [0.3, 0.2, 0.5] instead of [0,0,1] or actually 2\n",
        "## use the same loss as tensorflow: - sum target * log(prediction)\n",
        "def categorical_crossentropy(target, prediction):\n",
        "    \"\"\" calculate loss for one hot encoded labels with uncertain target \"\"\"\n",
        "    loss = - sum(target * log(prediction))\n",
        "    return loss\n",
        "\n",
        "mse = MSELoss()\n",
        "bce = BCELoss()\n",
        "cross_entropy = categorical_crossentropy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mADH2cyRvnX"
      },
      "source": [
        "def loss_reconstruction(image, generated_image):\n",
        "    \"\"\" divergence of generated image from input image \"\"\"\n",
        "    return mse(image, generated_image) * image_dim**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkxyQ79wYMRR"
      },
      "source": [
        "def loss_kl(latent):\n",
        "    \"\"\" divergence of recontstructed latent distribution from true distribution, assumed to be unit gaussian \"\"\"\n",
        "    loss = 1 + 2*log(latent[1]) - square(latent[0]) - square(latent[1])\n",
        "    loss = -0.5 * sum(loss) #, axis=-1)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5qv6vp4aAoR"
      },
      "source": [
        "def loss_VAE(image, generated_image, latent_mean, latent_std):\n",
        "    \"\"\" total loss of VAE \"\"\"\n",
        "    ## !! care for correct handling if latent in loss_kl\n",
        "    loss = loss_reconstruction(image, generated_image)\n",
        "    if alpha:\n",
        "        loss += loss_kl(latent_mean, latent_std)\n",
        "    return mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEwUEEXrD7Sw"
      },
      "source": [
        "def loss_adversarial(target, prediction):\n",
        "    \"\"\" divergence of classification of whether image is part of sample distribution \"\"\"\n",
        "    return bce(prediction, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cB3N6yKGUr4"
      },
      "source": [
        "def loss_class(target, prediction):\n",
        "    \"\"\" divergence of classification of subclasses in sample distribution \"\"\"\n",
        "    return cross_entropy(target, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0XP_Yw6GUiD"
      },
      "source": [
        "def loss_metric(target, prediction):\n",
        "    \"\"\" divergence of internal metric \"\"\"\n",
        "    return mse(target, prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZJydiKXGUUT"
      },
      "source": [
        "def loss_GAN(target, prediction, image, generated_image, latent):\n",
        "    \"\"\" total loss of GAN\n",
        "    \n",
        "    target and prediction contain (axis=1)\n",
        "        0               ; binary classification\n",
        "        1:labels_dim    ; label classification\n",
        "        labels_dim+1:-1 ; metric\n",
        "    \"\"\"\n",
        "    loss = loss_adversarial(target[:,0], prediction[:,0])\n",
        "    loss += delta * loss_class(target[:,1:1+labels_dim], prediction[:,1:1+labels_dim])\n",
        "    loss += gamma * loss_metric(target[:,2+labels_dim:], prediction[:,2+labels_dim:])\n",
        "    loss += zeta * loss_reconstruction(image, generated_image)\n",
        "    if alpha:\n",
        "        loss += beta * loss_kl(latent)\n",
        "    return mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV4HUV4fJnBz"
      },
      "source": [
        "def loss_discriminator(target, prediction):\n",
        "    \"\"\" total loss of discriminator \"\"\"\n",
        "    loss = loss_adversarial(target[:,0], prediction[:,0])\n",
        "    loss += delta * loss_class(target[:,1:1+labels_dim], prediction[:,1:1+labels_dim])\n",
        "    return mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WizX5x6yEPr"
      },
      "source": [
        "# Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFdGZoaC0eNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df1d1c1-d35f-4552-9cb5-c88ce3c5dbd8"
      },
      "source": [
        "# galaxy images\n",
        "x_train = np.load(file_galaxy_images)\n",
        "x_train = x_train/255.0 ## rescale to 0<x<1\n",
        "x_train = np.rollaxis(x_train, -1, 1)\n",
        "N_samples = x_train.shape[0]\n",
        "x_train = from_numpy(x_train).cuda()\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([61578, 3, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "30YdFCVxyEtt",
        "outputId": "1ff2e63b-1910-4cba-85db-0b2eefd71599"
      },
      "source": [
        "# hierarchical galaxy labels\n",
        "df_galaxy_labels =  pd.read_csv(file_galaxy_labels)\n",
        "display(df_galaxy_labels.head())\n",
        "labels_train = df_galaxy_labels[df_galaxy_labels.columns[1:4]].values\n",
        "labels_train = from_numpy(labels_train).float().cuda()\n",
        "labels_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GalaxyID</th>\n",
              "      <th>Class1.1</th>\n",
              "      <th>Class1.2</th>\n",
              "      <th>Class1.3</th>\n",
              "      <th>Class2.1</th>\n",
              "      <th>Class2.2</th>\n",
              "      <th>Class3.1</th>\n",
              "      <th>Class3.2</th>\n",
              "      <th>Class4.1</th>\n",
              "      <th>Class4.2</th>\n",
              "      <th>Class5.1</th>\n",
              "      <th>Class5.2</th>\n",
              "      <th>Class5.3</th>\n",
              "      <th>Class5.4</th>\n",
              "      <th>Class6.1</th>\n",
              "      <th>Class6.2</th>\n",
              "      <th>Class7.1</th>\n",
              "      <th>Class7.2</th>\n",
              "      <th>Class7.3</th>\n",
              "      <th>Class8.1</th>\n",
              "      <th>Class8.2</th>\n",
              "      <th>Class8.3</th>\n",
              "      <th>Class8.4</th>\n",
              "      <th>Class8.5</th>\n",
              "      <th>Class8.6</th>\n",
              "      <th>Class8.7</th>\n",
              "      <th>Class9.1</th>\n",
              "      <th>Class9.2</th>\n",
              "      <th>Class9.3</th>\n",
              "      <th>Class10.1</th>\n",
              "      <th>Class10.2</th>\n",
              "      <th>Class10.3</th>\n",
              "      <th>Class11.1</th>\n",
              "      <th>Class11.2</th>\n",
              "      <th>Class11.3</th>\n",
              "      <th>Class11.4</th>\n",
              "      <th>Class11.5</th>\n",
              "      <th>Class11.6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100008</td>\n",
              "      <td>0.383147</td>\n",
              "      <td>0.616853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.616853</td>\n",
              "      <td>0.038452</td>\n",
              "      <td>0.578401</td>\n",
              "      <td>0.418398</td>\n",
              "      <td>0.198455</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.104752</td>\n",
              "      <td>0.512101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054453</td>\n",
              "      <td>0.945547</td>\n",
              "      <td>0.201463</td>\n",
              "      <td>0.181684</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.027226</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027226</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.279952</td>\n",
              "      <td>0.138445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.092886</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100023</td>\n",
              "      <td>0.327001</td>\n",
              "      <td>0.663777</td>\n",
              "      <td>0.009222</td>\n",
              "      <td>0.031178</td>\n",
              "      <td>0.632599</td>\n",
              "      <td>0.467370</td>\n",
              "      <td>0.165229</td>\n",
              "      <td>0.591328</td>\n",
              "      <td>0.041271</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.236781</td>\n",
              "      <td>0.160941</td>\n",
              "      <td>0.234877</td>\n",
              "      <td>0.189149</td>\n",
              "      <td>0.810851</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.135082</td>\n",
              "      <td>0.191919</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.140353</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048796</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.012414</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.018764</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.131378</td>\n",
              "      <td>0.459950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.591328</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100053</td>\n",
              "      <td>0.765717</td>\n",
              "      <td>0.177352</td>\n",
              "      <td>0.056931</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.177352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.177352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.177352</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.117790</td>\n",
              "      <td>0.059562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.741864</td>\n",
              "      <td>0.023853</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100078</td>\n",
              "      <td>0.693377</td>\n",
              "      <td>0.238564</td>\n",
              "      <td>0.068059</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.238564</td>\n",
              "      <td>0.109493</td>\n",
              "      <td>0.129071</td>\n",
              "      <td>0.189098</td>\n",
              "      <td>0.049466</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113284</td>\n",
              "      <td>0.125280</td>\n",
              "      <td>0.320398</td>\n",
              "      <td>0.679602</td>\n",
              "      <td>0.408599</td>\n",
              "      <td>0.284778</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.096119</td>\n",
              "      <td>0.096119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128159</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.094549</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.094549</td>\n",
              "      <td>0.189098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100090</td>\n",
              "      <td>0.933839</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066161</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029383</td>\n",
              "      <td>0.970617</td>\n",
              "      <td>0.494587</td>\n",
              "      <td>0.439252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029383</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   GalaxyID  Class1.1  Class1.2  ...  Class11.4  Class11.5  Class11.6\n",
              "0    100008  0.383147  0.616853  ...        0.0        0.0   0.325512\n",
              "1    100023  0.327001  0.663777  ...        0.0        0.0   0.000000\n",
              "2    100053  0.765717  0.177352  ...        0.0        0.0   0.000000\n",
              "3    100078  0.693377  0.238564  ...        0.0        0.0   0.000000\n",
              "4    100090  0.933839  0.000000  ...        0.0        0.0   0.000000\n",
              "\n",
              "[5 rows x 38 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([61578, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIh_w1o9DcFm"
      },
      "source": [
        "## completely random batch each next()\n",
        "def make_training_sample_generator(batch_size):\n",
        "    while True:\n",
        "        idx = np.random.choice(range(N_samples), size=batch_size, replace=False)\n",
        "        yield x_train[idx], labels_train[idx]\n",
        "\n",
        "## randomized batches with each sample chosen at most once per epoch\n",
        "def make_training_sample_generator(batch_size):\n",
        "    indices = np.random.permutation(N_samples)\n",
        "    for i in range(int(N_samples/batch_size)):\n",
        "        idx = return_batch(indices, i, batch_size)\n",
        "        yield x_train[idx], labels_train[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aACg2yM0x2ln"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj-UnIeJx7eU"
      },
      "source": [
        "epochs = 20\n",
        "batch_size = 128\n",
        "steps = N_samples // batch_size\n",
        "save_interval = 200\n",
        "\n",
        "discriminator_losses = []\n",
        "discriminator_losses_real = []\n",
        "discriminator_losses_fake = []\n",
        "generator_losses = []\n",
        "\n",
        "valid = torch.ones((batch_size,1)).cuda()\n",
        "fake = torch.zeros((batch_size,1)).cuda()\n",
        "\n",
        "vae = VAE()\n",
        "vaegan = VAEGAN()\n",
        "\n",
        "iteration = 0\n",
        "epoch = 0\n",
        "step = 0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDiXsx8Mxfti"
      },
      "source": [
        "def training_step():\n",
        "    global iteration\n",
        "    images, labels = next(sample_generator)\n",
        "\n",
        "    # -------------------\n",
        "    # Train Discriminator\n",
        "    # -------------------\n",
        "    vae.train(False)\n",
        "    discriminator.train(True)\n",
        "    discriminator.zero_grad()\n",
        "\n",
        "    generated_images = vae(images, labels)\n",
        "    target_real = cat((valid,labels), dim=1)\n",
        "    prediction_real = discriminator(images)[:,:1+labels_dim]\n",
        "    target_fake = cat((fake, labels), dim=1)\n",
        "    prediction_fake = discriminator(generated_images)[:,:1+labels_dim]\n",
        "\n",
        "    d_loss_real = loss_discriminator(target_real, prediction_real)\n",
        "    d_loss_fake = loss_discriminator(target_fake, prediction_fake)\n",
        "    d_loss = 0.5 * add(d_loss_fake, d_loss_real)\n",
        "    discriminator_losses.append(d_loss)\n",
        "    discriminator_losses_fake.append(d_loss_fake)\n",
        "    discriminator_losses_real.append(d_loss_real)\n",
        "\n",
        "    d_loss_real.backward()\n",
        "    d_loss_fake.backward()\n",
        "    discriminator.optimizer.step()\n",
        "\n",
        "    # ---------------\n",
        "    # Train Generator\n",
        "    # ---------------\n",
        "    vae.train(True)\n",
        "    discriminator.train(False)\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "\n",
        "    generated_images = vae(images, labels)\n",
        "    target = discriminator(images)\n",
        "    target[:,0] = 1\n",
        "    target[:,1:1+labels_dim] = labels\n",
        "    target = target.detach()\n",
        "    prediction = discriminator(generated_images)\n",
        "    latent = encoder(images, labels)\n",
        "\n",
        "    g_loss = loss_GAN(target, prediction, images, generated_images, latent)\n",
        "    g_loss.backward()\n",
        "    encoder.optimizer.step()\n",
        "    decoder.optimizer.step()\n",
        "    generator_losses.append(g_loss)\n",
        "\n",
        "\n",
        "    iteration += 1\n",
        "\n",
        "    print(f\"iteration {iteration}, epoch {epoch+1}, batch {step+1}/{steps},\" + \\\n",
        "          f\"disc_loss {d_loss:.5}, (real {d_loss_real:.5}, fake {d_loss_fake:.5} ) gen_loss {g_loss:.5}\")\n",
        "\n",
        "    if not iteration % save_interval:\n",
        "        write_generated_galaxy_images_iteration(iteration=iteration, images=generated_images.detach().cpu().numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3zDzxauAqqH",
        "outputId": "8ed07b0b-42fd-4b6d-cd9b-456d27224e91"
      },
      "source": [
        "while epoch < epochs:\n",
        "    sample_generator = make_training_sample_generator(batch_size)\n",
        "    step = 0\n",
        "    while step < steps:\n",
        "        training_step()\n",
        "        step += 1\n",
        "    epoch += 1\n",
        "\n",
        "    # save a plot of the costs\n",
        "    plt.clf()\n",
        "    plt.plot(discriminator_losses, label='discriminator cost')\n",
        "    plt.plot(generator_losses, label='generator cost')\n",
        "    plt.plot(discriminator_losses_fake, label='discriminator cost fake', linestyle=\":\")\n",
        "    plt.plot(discriminator_losses_real, label='discriminator cost real', linestyle=\"-.\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    plt.savefig(folder_results+\"cost_vs_iteration.png\")\n",
        "    plt.close()\n",
        "\n",
        "    decoder.save()\n",
        "    encoder.save()\n",
        "    discriminator.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 201, epoch 1, batch 1/481,disc_loss 79.212, (real 83.769, fake 74.655 ) gen_loss 310.68\n",
            "iteration 202, epoch 1, batch 2/481,disc_loss 79.836, (real 86.036, fake 73.636 ) gen_loss 293.07\n",
            "iteration 203, epoch 1, batch 3/481,disc_loss 78.791, (real 82.5, fake 75.083 ) gen_loss 304.12\n",
            "iteration 204, epoch 1, batch 4/481,disc_loss 80.133, (real 84.288, fake 75.977 ) gen_loss 297.68\n",
            "iteration 205, epoch 1, batch 5/481,disc_loss 78.573, (real 84.24, fake 72.906 ) gen_loss 321.8\n",
            "iteration 206, epoch 1, batch 6/481,disc_loss 85.417, (real 89.631, fake 81.204 ) gen_loss 311.72\n",
            "iteration 207, epoch 1, batch 7/481,disc_loss 84.245, (real 90.055, fake 78.434 ) gen_loss 304.15\n",
            "iteration 208, epoch 1, batch 8/481,disc_loss 82.226, (real 88.576, fake 75.876 ) gen_loss 280.0\n",
            "iteration 209, epoch 1, batch 9/481,disc_loss 81.437, (real 89.204, fake 73.671 ) gen_loss 311.29\n",
            "iteration 210, epoch 1, batch 10/481,disc_loss 88.37, (real 92.969, fake 83.772 ) gen_loss 300.66\n",
            "iteration 211, epoch 1, batch 11/481,disc_loss 75.549, (real 80.51, fake 70.588 ) gen_loss 325.58\n",
            "iteration 212, epoch 1, batch 12/481,disc_loss 86.069, (real 90.412, fake 81.726 ) gen_loss 290.35\n",
            "iteration 213, epoch 1, batch 13/481,disc_loss 79.011, (real 84.292, fake 73.73 ) gen_loss 300.81\n",
            "iteration 214, epoch 1, batch 14/481,disc_loss 78.56, (real 84.55, fake 72.569 ) gen_loss 312.23\n",
            "iteration 215, epoch 1, batch 15/481,disc_loss 81.677, (real 87.189, fake 76.165 ) gen_loss 316.34\n",
            "iteration 216, epoch 1, batch 16/481,disc_loss 82.694, (real 86.291, fake 79.097 ) gen_loss 301.53\n",
            "iteration 217, epoch 1, batch 17/481,disc_loss 83.41, (real 90.157, fake 76.664 ) gen_loss 285.53\n",
            "iteration 218, epoch 1, batch 18/481,disc_loss 82.217, (real 87.265, fake 77.17 ) gen_loss 283.33\n",
            "iteration 219, epoch 1, batch 19/481,disc_loss 81.803, (real 85.36, fake 78.246 ) gen_loss 299.0\n",
            "iteration 220, epoch 1, batch 20/481,disc_loss 81.52, (real 86.613, fake 76.427 ) gen_loss 300.94\n",
            "iteration 221, epoch 1, batch 21/481,disc_loss 80.297, (real 84.157, fake 76.436 ) gen_loss 298.11\n",
            "iteration 222, epoch 1, batch 22/481,disc_loss 74.577, (real 80.404, fake 68.75 ) gen_loss 286.24\n",
            "iteration 223, epoch 1, batch 23/481,disc_loss 81.787, (real 86.898, fake 76.676 ) gen_loss 318.49\n",
            "iteration 224, epoch 1, batch 24/481,disc_loss 84.881, (real 89.737, fake 80.025 ) gen_loss 298.44\n",
            "iteration 225, epoch 1, batch 25/481,disc_loss 83.016, (real 89.041, fake 76.991 ) gen_loss 286.75\n",
            "iteration 226, epoch 1, batch 26/481,disc_loss 81.907, (real 87.566, fake 76.249 ) gen_loss 302.18\n",
            "iteration 227, epoch 1, batch 27/481,disc_loss 80.773, (real 85.556, fake 75.99 ) gen_loss 318.55\n",
            "iteration 228, epoch 1, batch 28/481,disc_loss 83.81, (real 89.434, fake 78.186 ) gen_loss 340.75\n",
            "iteration 229, epoch 1, batch 29/481,disc_loss 74.036, (real 77.974, fake 70.098 ) gen_loss 320.5\n",
            "iteration 230, epoch 1, batch 30/481,disc_loss 88.31, (real 93.539, fake 83.08 ) gen_loss 349.82\n",
            "iteration 231, epoch 1, batch 31/481,disc_loss 84.601, (real 90.985, fake 78.218 ) gen_loss 326.61\n",
            "iteration 232, epoch 1, batch 32/481,disc_loss 82.64, (real 87.009, fake 78.271 ) gen_loss 348.24\n",
            "iteration 233, epoch 1, batch 33/481,disc_loss 78.842, (real 83.428, fake 74.255 ) gen_loss 303.78\n",
            "iteration 234, epoch 1, batch 34/481,disc_loss 80.185, (real 86.55, fake 73.821 ) gen_loss 293.69\n",
            "iteration 235, epoch 1, batch 35/481,disc_loss 78.959, (real 84.955, fake 72.963 ) gen_loss 319.58\n",
            "iteration 236, epoch 1, batch 36/481,disc_loss 78.254, (real 82.237, fake 74.272 ) gen_loss 305.14\n",
            "iteration 237, epoch 1, batch 37/481,disc_loss 83.805, (real 88.572, fake 79.037 ) gen_loss 328.82\n",
            "iteration 238, epoch 1, batch 38/481,disc_loss 88.704, (real 94.894, fake 82.514 ) gen_loss 363.88\n",
            "iteration 239, epoch 1, batch 39/481,disc_loss 81.727, (real 88.78, fake 74.674 ) gen_loss 317.11\n",
            "iteration 240, epoch 1, batch 40/481,disc_loss 83.148, (real 88.122, fake 78.174 ) gen_loss 329.52\n",
            "iteration 241, epoch 1, batch 41/481,disc_loss 81.926, (real 87.614, fake 76.238 ) gen_loss 286.81\n",
            "iteration 242, epoch 1, batch 42/481,disc_loss 82.719, (real 86.127, fake 79.311 ) gen_loss 338.16\n",
            "iteration 243, epoch 1, batch 43/481,disc_loss 84.344, (real 89.504, fake 79.185 ) gen_loss 316.15\n",
            "iteration 244, epoch 1, batch 44/481,disc_loss 85.126, (real 91.465, fake 78.787 ) gen_loss 320.21\n",
            "iteration 245, epoch 1, batch 45/481,disc_loss 80.754, (real 85.307, fake 76.201 ) gen_loss 325.82\n",
            "iteration 246, epoch 1, batch 46/481,disc_loss 79.464, (real 85.456, fake 73.471 ) gen_loss 298.85\n",
            "iteration 247, epoch 1, batch 47/481,disc_loss 80.565, (real 84.155, fake 76.975 ) gen_loss 322.96\n",
            "iteration 248, epoch 1, batch 48/481,disc_loss 81.157, (real 86.665, fake 75.649 ) gen_loss 338.87\n",
            "iteration 249, epoch 1, batch 49/481,disc_loss 79.55, (real 83.428, fake 75.672 ) gen_loss 316.37\n",
            "iteration 250, epoch 1, batch 50/481,disc_loss 79.659, (real 84.028, fake 75.289 ) gen_loss 329.78\n",
            "iteration 251, epoch 1, batch 51/481,disc_loss 80.521, (real 83.733, fake 77.308 ) gen_loss 304.24\n",
            "iteration 252, epoch 1, batch 52/481,disc_loss 83.508, (real 89.922, fake 77.094 ) gen_loss 311.7\n",
            "iteration 253, epoch 1, batch 53/481,disc_loss 81.432, (real 85.993, fake 76.871 ) gen_loss 308.92\n",
            "iteration 254, epoch 1, batch 54/481,disc_loss 79.35, (real 85.146, fake 73.554 ) gen_loss 285.61\n",
            "iteration 255, epoch 1, batch 55/481,disc_loss 81.762, (real 87.859, fake 75.665 ) gen_loss 320.74\n",
            "iteration 256, epoch 1, batch 56/481,disc_loss 84.509, (real 91.817, fake 77.2 ) gen_loss 306.48\n",
            "iteration 257, epoch 1, batch 57/481,disc_loss 89.268, (real 94.21, fake 84.326 ) gen_loss 311.64\n",
            "iteration 258, epoch 1, batch 58/481,disc_loss 81.365, (real 85.101, fake 77.629 ) gen_loss 354.16\n",
            "iteration 259, epoch 1, batch 59/481,disc_loss 80.049, (real 84.999, fake 75.1 ) gen_loss 317.63\n",
            "iteration 260, epoch 1, batch 60/481,disc_loss 78.799, (real 83.483, fake 74.115 ) gen_loss 336.76\n",
            "iteration 261, epoch 1, batch 61/481,disc_loss 77.846, (real 83.884, fake 71.808 ) gen_loss 363.18\n",
            "iteration 262, epoch 1, batch 62/481,disc_loss 79.829, (real 85.373, fake 74.285 ) gen_loss 290.73\n",
            "iteration 263, epoch 1, batch 63/481,disc_loss 81.008, (real 85.019, fake 76.998 ) gen_loss 306.93\n",
            "iteration 264, epoch 1, batch 64/481,disc_loss 83.58, (real 86.277, fake 80.883 ) gen_loss 340.19\n",
            "iteration 265, epoch 1, batch 65/481,disc_loss 78.382, (real 84.367, fake 72.396 ) gen_loss 312.59\n",
            "iteration 266, epoch 1, batch 66/481,disc_loss 82.379, (real 88.382, fake 76.376 ) gen_loss 315.1\n",
            "iteration 267, epoch 1, batch 67/481,disc_loss 85.175, (real 89.888, fake 80.462 ) gen_loss 341.71\n",
            "iteration 268, epoch 1, batch 68/481,disc_loss 81.244, (real 86.421, fake 76.067 ) gen_loss 301.49\n",
            "iteration 269, epoch 1, batch 69/481,disc_loss 82.065, (real 86.508, fake 77.623 ) gen_loss 326.66\n",
            "iteration 270, epoch 1, batch 70/481,disc_loss 83.355, (real 89.068, fake 77.641 ) gen_loss 311.12\n",
            "iteration 271, epoch 1, batch 71/481,disc_loss 81.973, (real 86.498, fake 77.447 ) gen_loss 333.55\n",
            "iteration 272, epoch 1, batch 72/481,disc_loss 82.523, (real 87.699, fake 77.347 ) gen_loss 318.62\n",
            "iteration 273, epoch 1, batch 73/481,disc_loss 80.251, (real 85.146, fake 75.356 ) gen_loss 285.53\n",
            "iteration 274, epoch 1, batch 74/481,disc_loss 78.83, (real 83.221, fake 74.439 ) gen_loss 297.16\n",
            "iteration 275, epoch 1, batch 75/481,disc_loss 80.083, (real 84.482, fake 75.684 ) gen_loss 299.78\n",
            "iteration 276, epoch 1, batch 76/481,disc_loss 83.319, (real 88.358, fake 78.28 ) gen_loss 343.19\n",
            "iteration 277, epoch 1, batch 77/481,disc_loss 83.886, (real 90.326, fake 77.446 ) gen_loss 304.76\n",
            "iteration 278, epoch 1, batch 78/481,disc_loss 84.761, (real 89.38, fake 80.143 ) gen_loss 300.69\n",
            "iteration 279, epoch 1, batch 79/481,disc_loss 80.083, (real 85.645, fake 74.521 ) gen_loss 317.19\n",
            "iteration 280, epoch 1, batch 80/481,disc_loss 79.819, (real 84.763, fake 74.875 ) gen_loss 327.43\n",
            "iteration 281, epoch 1, batch 81/481,disc_loss 82.065, (real 87.187, fake 76.942 ) gen_loss 353.56\n",
            "iteration 282, epoch 1, batch 82/481,disc_loss 85.474, (real 91.679, fake 79.27 ) gen_loss 322.99\n",
            "iteration 283, epoch 1, batch 83/481,disc_loss 86.293, (real 98.02, fake 74.565 ) gen_loss 292.5\n",
            "iteration 284, epoch 1, batch 84/481,disc_loss 81.909, (real 86.102, fake 77.715 ) gen_loss 403.38\n",
            "iteration 285, epoch 1, batch 85/481,disc_loss 87.278, (real 92.741, fake 81.815 ) gen_loss 313.92\n",
            "iteration 286, epoch 1, batch 86/481,disc_loss 80.896, (real 85.431, fake 76.36 ) gen_loss 311.76\n",
            "iteration 287, epoch 1, batch 87/481,disc_loss 81.749, (real 86.234, fake 77.264 ) gen_loss 339.27\n",
            "iteration 288, epoch 1, batch 88/481,disc_loss 74.991, (real 82.248, fake 67.734 ) gen_loss 305.08\n",
            "iteration 289, epoch 1, batch 89/481,disc_loss 81.584, (real 84.959, fake 78.208 ) gen_loss 313.85\n",
            "iteration 290, epoch 1, batch 90/481,disc_loss 81.704, (real 87.961, fake 75.447 ) gen_loss 324.99\n",
            "iteration 291, epoch 1, batch 91/481,disc_loss 82.443, (real 86.586, fake 78.3 ) gen_loss 334.92\n",
            "iteration 292, epoch 1, batch 92/481,disc_loss 82.24, (real 88.044, fake 76.436 ) gen_loss 317.79\n",
            "iteration 293, epoch 1, batch 93/481,disc_loss 78.962, (real 84.411, fake 73.513 ) gen_loss 349.21\n",
            "iteration 294, epoch 1, batch 94/481,disc_loss 77.145, (real 82.085, fake 72.205 ) gen_loss 290.36\n",
            "iteration 295, epoch 1, batch 95/481,disc_loss 82.682, (real 87.052, fake 78.312 ) gen_loss 305.13\n",
            "iteration 296, epoch 1, batch 96/481,disc_loss 76.02, (real 80.204, fake 71.837 ) gen_loss 305.68\n",
            "iteration 297, epoch 1, batch 97/481,disc_loss 76.774, (real 83.155, fake 70.394 ) gen_loss 319.93\n",
            "iteration 298, epoch 1, batch 98/481,disc_loss 80.366, (real 84.855, fake 75.877 ) gen_loss 299.98\n",
            "iteration 299, epoch 1, batch 99/481,disc_loss 79.43, (real 84.54, fake 74.32 ) gen_loss 312.35\n",
            "iteration 300, epoch 1, batch 100/481,disc_loss 77.384, (real 81.657, fake 73.11 ) gen_loss 322.44\n",
            "iteration 301, epoch 1, batch 101/481,disc_loss 76.452, (real 83.554, fake 69.351 ) gen_loss 352.04\n",
            "iteration 302, epoch 1, batch 102/481,disc_loss 80.487, (real 85.314, fake 75.661 ) gen_loss 313.64\n",
            "iteration 303, epoch 1, batch 103/481,disc_loss 84.446, (real 87.993, fake 80.899 ) gen_loss 318.47\n",
            "iteration 304, epoch 1, batch 104/481,disc_loss 77.949, (real 82.525, fake 73.374 ) gen_loss 343.24\n",
            "iteration 305, epoch 1, batch 105/481,disc_loss 81.728, (real 87.515, fake 75.94 ) gen_loss 318.93\n",
            "iteration 306, epoch 1, batch 106/481,disc_loss 89.454, (real 97.11, fake 81.799 ) gen_loss 336.72\n",
            "iteration 307, epoch 1, batch 107/481,disc_loss 86.586, (real 92.108, fake 81.064 ) gen_loss 325.73\n",
            "iteration 308, epoch 1, batch 108/481,disc_loss 78.965, (real 84.342, fake 73.588 ) gen_loss 318.38\n",
            "iteration 309, epoch 1, batch 109/481,disc_loss 81.339, (real 87.0, fake 75.678 ) gen_loss 330.89\n",
            "iteration 310, epoch 1, batch 110/481,disc_loss 81.559, (real 87.024, fake 76.094 ) gen_loss 349.05\n",
            "iteration 311, epoch 1, batch 111/481,disc_loss 80.695, (real 86.4, fake 74.991 ) gen_loss 445.03\n",
            "iteration 312, epoch 1, batch 112/481,disc_loss 85.255, (real 90.711, fake 79.799 ) gen_loss 346.74\n",
            "iteration 313, epoch 1, batch 113/481,disc_loss 80.897, (real 86.032, fake 75.761 ) gen_loss 351.91\n",
            "iteration 314, epoch 1, batch 114/481,disc_loss 80.598, (real 85.946, fake 75.249 ) gen_loss 332.57\n",
            "iteration 315, epoch 1, batch 115/481,disc_loss 81.321, (real 85.411, fake 77.231 ) gen_loss 437.96\n",
            "iteration 316, epoch 1, batch 116/481,disc_loss 83.025, (real 87.963, fake 78.086 ) gen_loss 352.56\n",
            "iteration 317, epoch 1, batch 117/481,disc_loss 78.93, (real 84.564, fake 73.296 ) gen_loss 303.02\n",
            "iteration 318, epoch 1, batch 118/481,disc_loss 82.13, (real 86.156, fake 78.103 ) gen_loss 307.67\n",
            "iteration 319, epoch 1, batch 119/481,disc_loss 81.902, (real 86.585, fake 77.218 ) gen_loss 329.51\n",
            "iteration 320, epoch 1, batch 120/481,disc_loss 81.631, (real 85.674, fake 77.589 ) gen_loss 312.72\n",
            "iteration 321, epoch 1, batch 121/481,disc_loss 80.091, (real 84.37, fake 75.812 ) gen_loss 315.45\n",
            "iteration 322, epoch 1, batch 122/481,disc_loss 80.659, (real 85.577, fake 75.741 ) gen_loss 296.07\n",
            "iteration 323, epoch 1, batch 123/481,disc_loss 80.519, (real 85.045, fake 75.993 ) gen_loss 306.27\n",
            "iteration 324, epoch 1, batch 124/481,disc_loss 81.228, (real 86.045, fake 76.411 ) gen_loss 312.6\n",
            "iteration 325, epoch 1, batch 125/481,disc_loss 88.47, (real 94.019, fake 82.921 ) gen_loss 335.31\n",
            "iteration 326, epoch 1, batch 126/481,disc_loss 84.3, (real 90.659, fake 77.941 ) gen_loss 331.85\n",
            "iteration 327, epoch 1, batch 127/481,disc_loss 80.289, (real 84.884, fake 75.694 ) gen_loss 363.89\n",
            "iteration 328, epoch 1, batch 128/481,disc_loss 80.426, (real 85.668, fake 75.185 ) gen_loss 329.28\n",
            "iteration 329, epoch 1, batch 129/481,disc_loss 85.279, (real 90.983, fake 79.575 ) gen_loss 362.68\n",
            "iteration 330, epoch 1, batch 130/481,disc_loss 82.651, (real 87.182, fake 78.119 ) gen_loss 383.65\n",
            "iteration 331, epoch 1, batch 131/481,disc_loss 81.341, (real 86.022, fake 76.661 ) gen_loss 365.65\n",
            "iteration 332, epoch 1, batch 132/481,disc_loss 85.108, (real 88.979, fake 81.237 ) gen_loss 365.93\n",
            "iteration 333, epoch 1, batch 133/481,disc_loss 83.237, (real 85.595, fake 80.879 ) gen_loss 322.39\n",
            "iteration 334, epoch 1, batch 134/481,disc_loss 81.848, (real 86.729, fake 76.967 ) gen_loss 329.76\n",
            "iteration 335, epoch 1, batch 135/481,disc_loss 85.607, (real 92.647, fake 78.568 ) gen_loss 328.18\n",
            "iteration 336, epoch 1, batch 136/481,disc_loss 78.472, (real 85.535, fake 71.409 ) gen_loss 344.11\n",
            "iteration 337, epoch 1, batch 137/481,disc_loss 81.866, (real 86.23, fake 77.502 ) gen_loss 339.16\n",
            "iteration 338, epoch 1, batch 138/481,disc_loss 76.293, (real 81.797, fake 70.789 ) gen_loss 369.6\n",
            "iteration 339, epoch 1, batch 139/481,disc_loss 85.45, (real 90.449, fake 80.45 ) gen_loss 363.99\n",
            "iteration 340, epoch 1, batch 140/481,disc_loss 83.53, (real 88.607, fake 78.453 ) gen_loss 354.87\n",
            "iteration 341, epoch 1, batch 141/481,disc_loss 81.553, (real 86.242, fake 76.863 ) gen_loss 345.07\n",
            "iteration 342, epoch 1, batch 142/481,disc_loss 78.489, (real 82.87, fake 74.109 ) gen_loss 322.14\n",
            "iteration 343, epoch 1, batch 143/481,disc_loss 81.428, (real 86.814, fake 76.043 ) gen_loss 364.9\n",
            "iteration 344, epoch 1, batch 144/481,disc_loss 79.951, (real 85.249, fake 74.652 ) gen_loss 312.7\n",
            "iteration 345, epoch 1, batch 145/481,disc_loss 79.802, (real 85.493, fake 74.111 ) gen_loss 337.77\n",
            "iteration 346, epoch 1, batch 146/481,disc_loss 80.692, (real 84.867, fake 76.517 ) gen_loss 343.09\n",
            "iteration 347, epoch 1, batch 147/481,disc_loss 80.366, (real 84.671, fake 76.062 ) gen_loss 320.72\n",
            "iteration 348, epoch 1, batch 148/481,disc_loss 78.503, (real 83.443, fake 73.563 ) gen_loss 344.0\n",
            "iteration 349, epoch 1, batch 149/481,disc_loss 82.174, (real 87.761, fake 76.587 ) gen_loss 335.36\n",
            "iteration 350, epoch 1, batch 150/481,disc_loss 80.661, (real 85.057, fake 76.265 ) gen_loss 346.12\n",
            "iteration 351, epoch 1, batch 151/481,disc_loss 80.825, (real 85.658, fake 75.993 ) gen_loss 313.29\n",
            "iteration 352, epoch 1, batch 152/481,disc_loss 79.792, (real 84.378, fake 75.207 ) gen_loss 326.86\n",
            "iteration 353, epoch 1, batch 153/481,disc_loss 83.723, (real 89.471, fake 77.975 ) gen_loss 326.13\n",
            "iteration 354, epoch 1, batch 154/481,disc_loss 82.77, (real 87.838, fake 77.702 ) gen_loss 350.31\n",
            "iteration 355, epoch 1, batch 155/481,disc_loss 82.933, (real 87.46, fake 78.405 ) gen_loss 333.42\n",
            "iteration 356, epoch 1, batch 156/481,disc_loss 79.942, (real 86.652, fake 73.231 ) gen_loss 336.28\n",
            "iteration 357, epoch 1, batch 157/481,disc_loss 85.79, (real 90.423, fake 81.158 ) gen_loss 344.03\n",
            "iteration 358, epoch 1, batch 158/481,disc_loss 80.607, (real 85.889, fake 75.325 ) gen_loss 354.7\n",
            "iteration 359, epoch 1, batch 159/481,disc_loss 79.489, (real 85.213, fake 73.765 ) gen_loss 361.77\n",
            "iteration 360, epoch 1, batch 160/481,disc_loss 82.719, (real 87.591, fake 77.847 ) gen_loss 371.41\n",
            "iteration 361, epoch 1, batch 161/481,disc_loss 84.99, (real 88.61, fake 81.371 ) gen_loss 458.09\n",
            "iteration 362, epoch 1, batch 162/481,disc_loss 82.803, (real 88.409, fake 77.197 ) gen_loss 348.89\n",
            "iteration 363, epoch 1, batch 163/481,disc_loss 79.0, (real 84.448, fake 73.553 ) gen_loss 368.28\n",
            "iteration 364, epoch 1, batch 164/481,disc_loss 81.057, (real 86.392, fake 75.723 ) gen_loss 371.89\n",
            "iteration 365, epoch 1, batch 165/481,disc_loss 80.945, (real 86.024, fake 75.867 ) gen_loss 428.2\n",
            "iteration 366, epoch 1, batch 166/481,disc_loss 81.859, (real 86.782, fake 76.937 ) gen_loss 384.08\n",
            "iteration 367, epoch 1, batch 167/481,disc_loss 84.503, (real 90.886, fake 78.121 ) gen_loss 330.75\n",
            "iteration 368, epoch 1, batch 168/481,disc_loss 83.21, (real 88.672, fake 77.749 ) gen_loss 343.92\n",
            "iteration 369, epoch 1, batch 169/481,disc_loss 83.398, (real 88.033, fake 78.763 ) gen_loss 323.87\n",
            "iteration 370, epoch 1, batch 170/481,disc_loss 82.838, (real 88.193, fake 77.483 ) gen_loss 321.39\n",
            "iteration 371, epoch 1, batch 171/481,disc_loss 74.782, (real 80.278, fake 69.286 ) gen_loss 345.86\n",
            "iteration 372, epoch 1, batch 172/481,disc_loss 78.145, (real 84.15, fake 72.14 ) gen_loss 309.03\n",
            "iteration 373, epoch 1, batch 173/481,disc_loss 79.632, (real 84.579, fake 74.685 ) gen_loss 343.54\n",
            "iteration 374, epoch 1, batch 174/481,disc_loss 80.155, (real 86.019, fake 74.291 ) gen_loss 355.46\n",
            "iteration 375, epoch 1, batch 175/481,disc_loss 79.581, (real 84.565, fake 74.597 ) gen_loss 360.33\n",
            "iteration 376, epoch 1, batch 176/481,disc_loss 87.759, (real 93.116, fake 82.402 ) gen_loss 404.12\n",
            "iteration 377, epoch 1, batch 177/481,disc_loss 82.491, (real 86.085, fake 78.896 ) gen_loss 337.22\n",
            "iteration 378, epoch 1, batch 178/481,disc_loss 79.111, (real 83.635, fake 74.587 ) gen_loss 392.44\n",
            "iteration 379, epoch 1, batch 179/481,disc_loss 75.118, (real 78.832, fake 71.404 ) gen_loss 337.36\n",
            "iteration 380, epoch 1, batch 180/481,disc_loss 79.916, (real 84.847, fake 74.985 ) gen_loss 408.46\n",
            "iteration 381, epoch 1, batch 181/481,disc_loss 82.972, (real 89.351, fake 76.592 ) gen_loss 367.49\n",
            "iteration 382, epoch 1, batch 182/481,disc_loss 83.148, (real 88.512, fake 77.784 ) gen_loss 301.51\n",
            "iteration 383, epoch 1, batch 183/481,disc_loss 82.852, (real 87.472, fake 78.232 ) gen_loss 352.54\n",
            "iteration 384, epoch 1, batch 184/481,disc_loss 79.358, (real 83.128, fake 75.587 ) gen_loss 359.97\n",
            "iteration 385, epoch 1, batch 185/481,disc_loss 82.485, (real 86.241, fake 78.729 ) gen_loss 335.23\n",
            "iteration 386, epoch 1, batch 186/481,disc_loss 76.486, (real 81.363, fake 71.608 ) gen_loss 322.95\n",
            "iteration 387, epoch 1, batch 187/481,disc_loss 79.257, (real 83.005, fake 75.509 ) gen_loss 336.16\n",
            "iteration 388, epoch 1, batch 188/481,disc_loss 81.633, (real 86.244, fake 77.022 ) gen_loss 375.75\n",
            "iteration 389, epoch 1, batch 189/481,disc_loss 84.451, (real 88.872, fake 80.029 ) gen_loss 333.77\n",
            "iteration 390, epoch 1, batch 190/481,disc_loss 78.311, (real 82.887, fake 73.735 ) gen_loss 327.34\n",
            "iteration 391, epoch 1, batch 191/481,disc_loss 81.217, (real 86.926, fake 75.508 ) gen_loss 311.29\n",
            "iteration 392, epoch 1, batch 192/481,disc_loss 78.474, (real 82.594, fake 74.354 ) gen_loss 342.57\n",
            "iteration 393, epoch 1, batch 193/481,disc_loss 83.718, (real 89.171, fake 78.265 ) gen_loss 384.85\n",
            "iteration 394, epoch 1, batch 194/481,disc_loss 79.484, (real 84.297, fake 74.671 ) gen_loss 339.17\n",
            "iteration 395, epoch 1, batch 195/481,disc_loss 81.031, (real 84.988, fake 77.074 ) gen_loss 333.72\n",
            "iteration 396, epoch 1, batch 196/481,disc_loss 85.313, (real 89.511, fake 81.114 ) gen_loss 374.78\n",
            "iteration 397, epoch 1, batch 197/481,disc_loss 79.904, (real 85.274, fake 74.534 ) gen_loss 394.33\n",
            "iteration 398, epoch 1, batch 198/481,disc_loss 80.156, (real 84.485, fake 75.826 ) gen_loss 305.57\n",
            "iteration 399, epoch 1, batch 199/481,disc_loss 80.805, (real 85.231, fake 76.379 ) gen_loss 314.38\n",
            "iteration 400, epoch 1, batch 200/481,disc_loss 83.919, (real 91.128, fake 76.71 ) gen_loss 313.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 401, epoch 1, batch 201/481,disc_loss 83.215, (real 89.635, fake 76.794 ) gen_loss 333.74\n",
            "iteration 402, epoch 1, batch 202/481,disc_loss 80.619, (real 86.054, fake 75.184 ) gen_loss 297.35\n",
            "iteration 403, epoch 1, batch 203/481,disc_loss 78.931, (real 84.339, fake 73.523 ) gen_loss 337.68\n",
            "iteration 404, epoch 1, batch 204/481,disc_loss 79.774, (real 86.124, fake 73.425 ) gen_loss 317.63\n",
            "iteration 405, epoch 1, batch 205/481,disc_loss 82.205, (real 86.413, fake 77.998 ) gen_loss 343.05\n",
            "iteration 406, epoch 1, batch 206/481,disc_loss 81.871, (real 85.836, fake 77.906 ) gen_loss 340.06\n",
            "iteration 407, epoch 1, batch 207/481,disc_loss 80.096, (real 83.302, fake 76.891 ) gen_loss 376.77\n",
            "iteration 408, epoch 1, batch 208/481,disc_loss 78.384, (real 83.798, fake 72.969 ) gen_loss 356.34\n",
            "iteration 409, epoch 1, batch 209/481,disc_loss 82.047, (real 86.648, fake 77.447 ) gen_loss 338.99\n",
            "iteration 410, epoch 1, batch 210/481,disc_loss 83.818, (real 89.163, fake 78.473 ) gen_loss 328.96\n",
            "iteration 411, epoch 1, batch 211/481,disc_loss 78.313, (real 83.297, fake 73.33 ) gen_loss 384.19\n",
            "iteration 412, epoch 1, batch 212/481,disc_loss 82.59, (real 87.612, fake 77.567 ) gen_loss 338.54\n",
            "iteration 413, epoch 1, batch 213/481,disc_loss 82.665, (real 87.487, fake 77.843 ) gen_loss 438.4\n",
            "iteration 414, epoch 1, batch 214/481,disc_loss 79.698, (real 83.983, fake 75.412 ) gen_loss 342.77\n",
            "iteration 415, epoch 1, batch 215/481,disc_loss 86.61, (real 95.629, fake 77.59 ) gen_loss 341.62\n",
            "iteration 416, epoch 1, batch 216/481,disc_loss 81.004, (real 85.22, fake 76.789 ) gen_loss 346.98\n",
            "iteration 417, epoch 1, batch 217/481,disc_loss 78.364, (real 83.929, fake 72.8 ) gen_loss 333.03\n",
            "iteration 418, epoch 1, batch 218/481,disc_loss 80.581, (real 85.506, fake 75.656 ) gen_loss 354.63\n",
            "iteration 419, epoch 1, batch 219/481,disc_loss 82.216, (real 86.714, fake 77.718 ) gen_loss 371.03\n",
            "iteration 420, epoch 1, batch 220/481,disc_loss 78.803, (real 83.785, fake 73.82 ) gen_loss 318.89\n",
            "iteration 421, epoch 1, batch 221/481,disc_loss 78.641, (real 85.251, fake 72.032 ) gen_loss 352.75\n",
            "iteration 422, epoch 1, batch 222/481,disc_loss 81.107, (real 85.691, fake 76.523 ) gen_loss 329.34\n",
            "iteration 423, epoch 1, batch 223/481,disc_loss 82.224, (real 87.879, fake 76.569 ) gen_loss 362.84\n",
            "iteration 424, epoch 1, batch 224/481,disc_loss 80.794, (real 84.282, fake 77.305 ) gen_loss 291.33\n",
            "iteration 425, epoch 1, batch 225/481,disc_loss 80.48, (real 86.603, fake 74.358 ) gen_loss 345.32\n",
            "iteration 426, epoch 1, batch 226/481,disc_loss 82.29, (real 88.732, fake 75.848 ) gen_loss 350.58\n",
            "iteration 427, epoch 1, batch 227/481,disc_loss 81.205, (real 85.833, fake 76.577 ) gen_loss 331.62\n",
            "iteration 428, epoch 1, batch 228/481,disc_loss 81.223, (real 85.48, fake 76.965 ) gen_loss 338.13\n",
            "iteration 429, epoch 1, batch 229/481,disc_loss 76.752, (real 81.627, fake 71.876 ) gen_loss 335.95\n",
            "iteration 430, epoch 1, batch 230/481,disc_loss 86.973, (real 90.792, fake 83.153 ) gen_loss 331.24\n",
            "iteration 431, epoch 1, batch 231/481,disc_loss 81.167, (real 85.381, fake 76.952 ) gen_loss 326.6\n",
            "iteration 432, epoch 1, batch 232/481,disc_loss 87.143, (real 91.617, fake 82.67 ) gen_loss 346.27\n",
            "iteration 433, epoch 1, batch 233/481,disc_loss 77.123, (real 82.672, fake 71.574 ) gen_loss 326.45\n",
            "iteration 434, epoch 1, batch 234/481,disc_loss 78.005, (real 82.62, fake 73.39 ) gen_loss 404.34\n",
            "iteration 435, epoch 1, batch 235/481,disc_loss 82.787, (real 87.802, fake 77.772 ) gen_loss 380.54\n",
            "iteration 436, epoch 1, batch 236/481,disc_loss 83.264, (real 87.752, fake 78.776 ) gen_loss 395.04\n",
            "iteration 437, epoch 1, batch 237/481,disc_loss 82.101, (real 85.55, fake 78.652 ) gen_loss 344.73\n",
            "iteration 438, epoch 1, batch 238/481,disc_loss 77.773, (real 81.957, fake 73.59 ) gen_loss 404.12\n",
            "iteration 439, epoch 1, batch 239/481,disc_loss 81.248, (real 86.574, fake 75.923 ) gen_loss 352.79\n",
            "iteration 440, epoch 1, batch 240/481,disc_loss 82.806, (real 89.423, fake 76.189 ) gen_loss 395.44\n",
            "iteration 441, epoch 1, batch 241/481,disc_loss 83.292, (real 88.272, fake 78.313 ) gen_loss 372.3\n",
            "iteration 442, epoch 1, batch 242/481,disc_loss 76.704, (real 81.197, fake 72.211 ) gen_loss 400.4\n",
            "iteration 443, epoch 1, batch 243/481,disc_loss 80.905, (real 85.424, fake 76.386 ) gen_loss 362.85\n",
            "iteration 444, epoch 1, batch 244/481,disc_loss 80.689, (real 86.213, fake 75.165 ) gen_loss 372.9\n",
            "iteration 445, epoch 1, batch 245/481,disc_loss 78.32, (real 83.708, fake 72.931 ) gen_loss 367.17\n",
            "iteration 446, epoch 1, batch 246/481,disc_loss 86.813, (real 91.62, fake 82.007 ) gen_loss 359.17\n",
            "iteration 447, epoch 1, batch 247/481,disc_loss 82.188, (real 88.714, fake 75.661 ) gen_loss 389.44\n",
            "iteration 448, epoch 1, batch 248/481,disc_loss 85.45, (real 91.493, fake 79.407 ) gen_loss 352.39\n",
            "iteration 449, epoch 1, batch 249/481,disc_loss 75.173, (real 79.278, fake 71.067 ) gen_loss 389.63\n",
            "iteration 450, epoch 1, batch 250/481,disc_loss 79.67, (real 85.679, fake 73.661 ) gen_loss 367.79\n",
            "iteration 451, epoch 1, batch 251/481,disc_loss 84.176, (real 92.246, fake 76.106 ) gen_loss 439.95\n",
            "iteration 452, epoch 1, batch 252/481,disc_loss 80.164, (real 84.153, fake 76.175 ) gen_loss 381.36\n",
            "iteration 453, epoch 1, batch 253/481,disc_loss 82.244, (real 86.566, fake 77.923 ) gen_loss 330.63\n",
            "iteration 454, epoch 1, batch 254/481,disc_loss 85.909, (real 91.641, fake 80.177 ) gen_loss 329.65\n",
            "iteration 455, epoch 1, batch 255/481,disc_loss 84.207, (real 91.555, fake 76.858 ) gen_loss 347.78\n",
            "iteration 456, epoch 1, batch 256/481,disc_loss 79.256, (real 84.661, fake 73.851 ) gen_loss 336.82\n",
            "iteration 457, epoch 1, batch 257/481,disc_loss 76.695, (real 83.311, fake 70.079 ) gen_loss 332.61\n",
            "iteration 458, epoch 1, batch 258/481,disc_loss 80.315, (real 84.363, fake 76.267 ) gen_loss 342.44\n",
            "iteration 459, epoch 1, batch 259/481,disc_loss 75.721, (real 79.078, fake 72.364 ) gen_loss 352.5\n",
            "iteration 460, epoch 1, batch 260/481,disc_loss 81.398, (real 87.072, fake 75.724 ) gen_loss 373.94\n",
            "iteration 461, epoch 1, batch 261/481,disc_loss 75.927, (real 81.456, fake 70.397 ) gen_loss 335.29\n",
            "iteration 462, epoch 1, batch 262/481,disc_loss 83.622, (real 89.449, fake 77.796 ) gen_loss 362.93\n",
            "iteration 463, epoch 1, batch 263/481,disc_loss 78.676, (real 82.727, fake 74.624 ) gen_loss 345.08\n",
            "iteration 464, epoch 1, batch 264/481,disc_loss 80.111, (real 83.769, fake 76.454 ) gen_loss 359.29\n",
            "iteration 465, epoch 1, batch 265/481,disc_loss 84.263, (real 87.767, fake 80.758 ) gen_loss 338.24\n",
            "iteration 466, epoch 1, batch 266/481,disc_loss 79.406, (real 84.456, fake 74.357 ) gen_loss 361.15\n",
            "iteration 467, epoch 1, batch 267/481,disc_loss 84.282, (real 90.401, fake 78.164 ) gen_loss 329.09\n",
            "iteration 468, epoch 1, batch 268/481,disc_loss 83.32, (real 89.151, fake 77.488 ) gen_loss 388.18\n",
            "iteration 469, epoch 1, batch 269/481,disc_loss 79.74, (real 86.097, fake 73.382 ) gen_loss 350.83\n",
            "iteration 470, epoch 1, batch 270/481,disc_loss 76.759, (real 81.751, fake 71.767 ) gen_loss 376.99\n",
            "iteration 471, epoch 1, batch 271/481,disc_loss 77.496, (real 82.796, fake 72.195 ) gen_loss 412.59\n",
            "iteration 472, epoch 1, batch 272/481,disc_loss 82.021, (real 86.76, fake 77.281 ) gen_loss 374.36\n",
            "iteration 473, epoch 1, batch 273/481,disc_loss 80.339, (real 84.735, fake 75.942 ) gen_loss 331.6\n",
            "iteration 474, epoch 1, batch 274/481,disc_loss 77.511, (real 82.599, fake 72.422 ) gen_loss 316.86\n",
            "iteration 475, epoch 1, batch 275/481,disc_loss 75.539, (real 79.139, fake 71.94 ) gen_loss 345.5\n",
            "iteration 476, epoch 1, batch 276/481,disc_loss 82.035, (real 87.076, fake 76.994 ) gen_loss 360.88\n",
            "iteration 477, epoch 1, batch 277/481,disc_loss 77.695, (real 81.473, fake 73.917 ) gen_loss 328.17\n",
            "iteration 478, epoch 1, batch 278/481,disc_loss 79.098, (real 83.55, fake 74.646 ) gen_loss 343.52\n",
            "iteration 479, epoch 1, batch 279/481,disc_loss 80.333, (real 84.744, fake 75.922 ) gen_loss 346.01\n",
            "iteration 480, epoch 1, batch 280/481,disc_loss 82.731, (real 87.139, fake 78.322 ) gen_loss 382.19\n",
            "iteration 481, epoch 1, batch 281/481,disc_loss 79.96, (real 85.204, fake 74.716 ) gen_loss 322.68\n",
            "iteration 482, epoch 1, batch 282/481,disc_loss 80.12, (real 85.874, fake 74.365 ) gen_loss 319.96\n",
            "iteration 483, epoch 1, batch 283/481,disc_loss 80.154, (real 84.117, fake 76.19 ) gen_loss 329.7\n",
            "iteration 484, epoch 1, batch 284/481,disc_loss 81.911, (real 86.737, fake 77.085 ) gen_loss 359.5\n",
            "iteration 485, epoch 1, batch 285/481,disc_loss 78.408, (real 83.892, fake 72.923 ) gen_loss 328.12\n",
            "iteration 486, epoch 1, batch 286/481,disc_loss 80.996, (real 86.964, fake 75.028 ) gen_loss 355.28\n",
            "iteration 487, epoch 1, batch 287/481,disc_loss 76.601, (real 81.9, fake 71.303 ) gen_loss 346.13\n",
            "iteration 488, epoch 1, batch 288/481,disc_loss 81.9, (real 87.572, fake 76.228 ) gen_loss 349.15\n",
            "iteration 489, epoch 1, batch 289/481,disc_loss 85.484, (real 89.241, fake 81.726 ) gen_loss 413.96\n",
            "iteration 490, epoch 1, batch 290/481,disc_loss 82.442, (real 89.097, fake 75.787 ) gen_loss 361.9\n",
            "iteration 491, epoch 1, batch 291/481,disc_loss 84.599, (real 88.879, fake 80.318 ) gen_loss 371.97\n",
            "iteration 492, epoch 1, batch 292/481,disc_loss 78.403, (real 82.553, fake 74.254 ) gen_loss 368.95\n",
            "iteration 493, epoch 1, batch 293/481,disc_loss 76.898, (real 81.447, fake 72.35 ) gen_loss 340.53\n",
            "iteration 494, epoch 1, batch 294/481,disc_loss 81.008, (real 86.538, fake 75.479 ) gen_loss 353.71\n",
            "iteration 495, epoch 1, batch 295/481,disc_loss 84.032, (real 89.849, fake 78.216 ) gen_loss 324.14\n",
            "iteration 496, epoch 1, batch 296/481,disc_loss 82.155, (real 87.193, fake 77.117 ) gen_loss 369.31\n",
            "iteration 497, epoch 1, batch 297/481,disc_loss 79.554, (real 83.564, fake 75.544 ) gen_loss 330.51\n",
            "iteration 498, epoch 1, batch 298/481,disc_loss 82.17, (real 88.727, fake 75.612 ) gen_loss 375.69\n",
            "iteration 499, epoch 1, batch 299/481,disc_loss 82.406, (real 86.744, fake 78.068 ) gen_loss 346.27\n",
            "iteration 500, epoch 1, batch 300/481,disc_loss 79.809, (real 84.737, fake 74.881 ) gen_loss 336.72\n",
            "iteration 501, epoch 1, batch 301/481,disc_loss 81.674, (real 87.033, fake 76.315 ) gen_loss 332.92\n",
            "iteration 502, epoch 1, batch 302/481,disc_loss 77.796, (real 82.792, fake 72.8 ) gen_loss 362.28\n",
            "iteration 503, epoch 1, batch 303/481,disc_loss 79.939, (real 83.022, fake 76.857 ) gen_loss 417.94\n",
            "iteration 504, epoch 1, batch 304/481,disc_loss 80.517, (real 85.165, fake 75.868 ) gen_loss 383.63\n",
            "iteration 505, epoch 1, batch 305/481,disc_loss 79.442, (real 83.293, fake 75.59 ) gen_loss 445.16\n",
            "iteration 506, epoch 1, batch 306/481,disc_loss 76.441, (real 81.0, fake 71.883 ) gen_loss 362.71\n",
            "iteration 507, epoch 1, batch 307/481,disc_loss 77.901, (real 81.666, fake 74.136 ) gen_loss 389.43\n",
            "iteration 508, epoch 1, batch 308/481,disc_loss 82.091, (real 86.77, fake 77.412 ) gen_loss 341.13\n",
            "iteration 509, epoch 1, batch 309/481,disc_loss 79.871, (real 84.252, fake 75.491 ) gen_loss 382.04\n",
            "iteration 510, epoch 1, batch 310/481,disc_loss 79.246, (real 84.023, fake 74.469 ) gen_loss 356.12\n",
            "iteration 511, epoch 1, batch 311/481,disc_loss 82.32, (real 86.516, fake 78.125 ) gen_loss 349.11\n",
            "iteration 512, epoch 1, batch 312/481,disc_loss 85.39, (real 89.926, fake 80.854 ) gen_loss 340.91\n",
            "iteration 513, epoch 1, batch 313/481,disc_loss 83.172, (real 89.336, fake 77.009 ) gen_loss 326.71\n",
            "iteration 514, epoch 1, batch 314/481,disc_loss 78.489, (real 83.495, fake 73.484 ) gen_loss 363.17\n",
            "iteration 515, epoch 1, batch 315/481,disc_loss 78.78, (real 82.279, fake 75.281 ) gen_loss 340.61\n",
            "iteration 516, epoch 1, batch 316/481,disc_loss 77.932, (real 81.784, fake 74.081 ) gen_loss 340.09\n",
            "iteration 517, epoch 1, batch 317/481,disc_loss 82.403, (real 87.27, fake 77.537 ) gen_loss 408.79\n",
            "iteration 518, epoch 1, batch 318/481,disc_loss 78.545, (real 84.783, fake 72.307 ) gen_loss 348.61\n",
            "iteration 519, epoch 1, batch 319/481,disc_loss 77.778, (real 83.133, fake 72.423 ) gen_loss 342.52\n",
            "iteration 520, epoch 1, batch 320/481,disc_loss 81.335, (real 87.722, fake 74.947 ) gen_loss 356.52\n",
            "iteration 521, epoch 1, batch 321/481,disc_loss 84.439, (real 89.782, fake 79.096 ) gen_loss 398.0\n",
            "iteration 522, epoch 1, batch 322/481,disc_loss 84.73, (real 89.819, fake 79.642 ) gen_loss 341.45\n",
            "iteration 523, epoch 1, batch 323/481,disc_loss 81.338, (real 87.391, fake 75.285 ) gen_loss 327.87\n",
            "iteration 524, epoch 1, batch 324/481,disc_loss 81.564, (real 87.157, fake 75.971 ) gen_loss 371.29\n",
            "iteration 525, epoch 1, batch 325/481,disc_loss 81.386, (real 87.113, fake 75.659 ) gen_loss 365.95\n",
            "iteration 526, epoch 1, batch 326/481,disc_loss 80.356, (real 85.994, fake 74.719 ) gen_loss 388.97\n",
            "iteration 527, epoch 1, batch 327/481,disc_loss 83.43, (real 88.336, fake 78.524 ) gen_loss 371.18\n",
            "iteration 528, epoch 1, batch 328/481,disc_loss 79.101, (real 82.766, fake 75.436 ) gen_loss 372.18\n",
            "iteration 529, epoch 1, batch 329/481,disc_loss 80.05, (real 84.661, fake 75.439 ) gen_loss 381.75\n",
            "iteration 530, epoch 1, batch 330/481,disc_loss 84.38, (real 90.837, fake 77.923 ) gen_loss 354.56\n",
            "iteration 531, epoch 1, batch 331/481,disc_loss 80.151, (real 86.385, fake 73.918 ) gen_loss 348.82\n",
            "iteration 532, epoch 1, batch 332/481,disc_loss 80.333, (real 85.063, fake 75.604 ) gen_loss 316.02\n",
            "iteration 533, epoch 1, batch 333/481,disc_loss 81.429, (real 87.603, fake 75.256 ) gen_loss 367.16\n",
            "iteration 534, epoch 1, batch 334/481,disc_loss 80.437, (real 86.324, fake 74.551 ) gen_loss 334.63\n",
            "iteration 535, epoch 1, batch 335/481,disc_loss 79.844, (real 84.467, fake 75.221 ) gen_loss 340.39\n",
            "iteration 536, epoch 1, batch 336/481,disc_loss 81.578, (real 86.912, fake 76.245 ) gen_loss 344.77\n",
            "iteration 537, epoch 1, batch 337/481,disc_loss 83.374, (real 90.325, fake 76.423 ) gen_loss 357.04\n",
            "iteration 538, epoch 1, batch 338/481,disc_loss 80.44, (real 85.63, fake 75.25 ) gen_loss 398.88\n",
            "iteration 539, epoch 1, batch 339/481,disc_loss 78.156, (real 81.206, fake 75.107 ) gen_loss 378.58\n",
            "iteration 540, epoch 1, batch 340/481,disc_loss 80.386, (real 86.506, fake 74.265 ) gen_loss 422.41\n",
            "iteration 541, epoch 1, batch 341/481,disc_loss 79.64, (real 85.338, fake 73.942 ) gen_loss 475.58\n",
            "iteration 542, epoch 1, batch 342/481,disc_loss 79.415, (real 83.307, fake 75.522 ) gen_loss 408.45\n",
            "iteration 543, epoch 1, batch 343/481,disc_loss 84.713, (real 90.51, fake 78.917 ) gen_loss 387.17\n",
            "iteration 544, epoch 1, batch 344/481,disc_loss 81.19, (real 85.894, fake 76.487 ) gen_loss 381.94\n",
            "iteration 545, epoch 1, batch 345/481,disc_loss 81.656, (real 86.969, fake 76.344 ) gen_loss 332.86\n",
            "iteration 546, epoch 1, batch 346/481,disc_loss 81.548, (real 86.29, fake 76.805 ) gen_loss 363.3\n",
            "iteration 547, epoch 1, batch 347/481,disc_loss 80.675, (real 84.382, fake 76.967 ) gen_loss 357.41\n",
            "iteration 548, epoch 1, batch 348/481,disc_loss 79.912, (real 86.423, fake 73.4 ) gen_loss 384.74\n",
            "iteration 549, epoch 1, batch 349/481,disc_loss 81.824, (real 85.942, fake 77.707 ) gen_loss 378.93\n",
            "iteration 550, epoch 1, batch 350/481,disc_loss 80.911, (real 85.955, fake 75.867 ) gen_loss 387.04\n",
            "iteration 551, epoch 1, batch 351/481,disc_loss 78.101, (real 81.841, fake 74.361 ) gen_loss 368.99\n",
            "iteration 552, epoch 1, batch 352/481,disc_loss 81.482, (real 85.39, fake 77.574 ) gen_loss 364.41\n",
            "iteration 553, epoch 1, batch 353/481,disc_loss 79.878, (real 83.489, fake 76.266 ) gen_loss 340.5\n",
            "iteration 554, epoch 1, batch 354/481,disc_loss 83.172, (real 86.307, fake 80.036 ) gen_loss 379.04\n",
            "iteration 555, epoch 1, batch 355/481,disc_loss 81.297, (real 86.199, fake 76.394 ) gen_loss 325.16\n",
            "iteration 556, epoch 1, batch 356/481,disc_loss 81.172, (real 85.232, fake 77.111 ) gen_loss 322.42\n",
            "iteration 557, epoch 1, batch 357/481,disc_loss 76.488, (real 80.18, fake 72.797 ) gen_loss 362.96\n",
            "iteration 558, epoch 1, batch 358/481,disc_loss 83.299, (real 88.008, fake 78.589 ) gen_loss 364.56\n",
            "iteration 559, epoch 1, batch 359/481,disc_loss 85.958, (real 91.005, fake 80.911 ) gen_loss 326.68\n",
            "iteration 560, epoch 1, batch 360/481,disc_loss 76.326, (real 79.979, fake 72.674 ) gen_loss 361.31\n",
            "iteration 561, epoch 1, batch 361/481,disc_loss 83.962, (real 89.848, fake 78.077 ) gen_loss 333.26\n",
            "iteration 562, epoch 1, batch 362/481,disc_loss 80.378, (real 85.867, fake 74.888 ) gen_loss 367.48\n",
            "iteration 563, epoch 1, batch 363/481,disc_loss 79.922, (real 84.649, fake 75.195 ) gen_loss 334.54\n",
            "iteration 564, epoch 1, batch 364/481,disc_loss 82.074, (real 85.894, fake 78.254 ) gen_loss 353.05\n",
            "iteration 565, epoch 1, batch 365/481,disc_loss 82.053, (real 86.306, fake 77.8 ) gen_loss 390.27\n",
            "iteration 566, epoch 1, batch 366/481,disc_loss 83.561, (real 88.523, fake 78.598 ) gen_loss 365.35\n",
            "iteration 567, epoch 1, batch 367/481,disc_loss 77.253, (real 80.974, fake 73.532 ) gen_loss 346.68\n",
            "iteration 568, epoch 1, batch 368/481,disc_loss 78.721, (real 82.936, fake 74.506 ) gen_loss 322.73\n",
            "iteration 569, epoch 1, batch 369/481,disc_loss 79.538, (real 84.646, fake 74.429 ) gen_loss 506.82\n",
            "iteration 570, epoch 1, batch 370/481,disc_loss 84.499, (real 89.668, fake 79.331 ) gen_loss 413.13\n",
            "iteration 571, epoch 1, batch 371/481,disc_loss 82.367, (real 88.433, fake 76.3 ) gen_loss 374.76\n",
            "iteration 572, epoch 1, batch 372/481,disc_loss 82.991, (real 86.627, fake 79.355 ) gen_loss 331.13\n",
            "iteration 573, epoch 1, batch 373/481,disc_loss 78.979, (real 83.903, fake 74.056 ) gen_loss 344.96\n",
            "iteration 574, epoch 1, batch 374/481,disc_loss 81.836, (real 85.646, fake 78.025 ) gen_loss 341.66\n",
            "iteration 575, epoch 1, batch 375/481,disc_loss 81.387, (real 85.873, fake 76.902 ) gen_loss 364.04\n",
            "iteration 576, epoch 1, batch 376/481,disc_loss 77.49, (real 81.171, fake 73.81 ) gen_loss 332.69\n",
            "iteration 577, epoch 1, batch 377/481,disc_loss 83.312, (real 87.008, fake 79.616 ) gen_loss 340.12\n",
            "iteration 578, epoch 1, batch 378/481,disc_loss 82.496, (real 87.155, fake 77.838 ) gen_loss 413.45\n",
            "iteration 579, epoch 1, batch 379/481,disc_loss 82.599, (real 88.265, fake 76.932 ) gen_loss 370.5\n",
            "iteration 580, epoch 1, batch 380/481,disc_loss 81.715, (real 86.302, fake 77.129 ) gen_loss 364.35\n",
            "iteration 581, epoch 1, batch 381/481,disc_loss 83.1, (real 87.357, fake 78.842 ) gen_loss 357.37\n",
            "iteration 582, epoch 1, batch 382/481,disc_loss 85.709, (real 91.749, fake 79.669 ) gen_loss 338.9\n",
            "iteration 583, epoch 1, batch 383/481,disc_loss 82.438, (real 87.477, fake 77.398 ) gen_loss 370.42\n",
            "iteration 584, epoch 1, batch 384/481,disc_loss 77.146, (real 82.383, fake 71.908 ) gen_loss 394.01\n",
            "iteration 585, epoch 1, batch 385/481,disc_loss 83.407, (real 86.324, fake 80.491 ) gen_loss 351.73\n",
            "iteration 586, epoch 1, batch 386/481,disc_loss 79.944, (real 84.046, fake 75.842 ) gen_loss 338.46\n",
            "iteration 587, epoch 1, batch 387/481,disc_loss 82.38, (real 86.7, fake 78.06 ) gen_loss 371.07\n",
            "iteration 588, epoch 1, batch 388/481,disc_loss 83.949, (real 88.432, fake 79.467 ) gen_loss 393.22\n",
            "iteration 589, epoch 1, batch 389/481,disc_loss 78.112, (real 82.714, fake 73.509 ) gen_loss 366.8\n",
            "iteration 590, epoch 1, batch 390/481,disc_loss 84.98, (real 88.558, fake 81.402 ) gen_loss 372.95\n",
            "iteration 591, epoch 1, batch 391/481,disc_loss 79.689, (real 84.015, fake 75.363 ) gen_loss 386.33\n",
            "iteration 592, epoch 1, batch 392/481,disc_loss 79.043, (real 82.526, fake 75.56 ) gen_loss 373.6\n",
            "iteration 593, epoch 1, batch 393/481,disc_loss 86.307, (real 90.509, fake 82.106 ) gen_loss 345.59\n",
            "iteration 594, epoch 1, batch 394/481,disc_loss 76.964, (real 80.988, fake 72.939 ) gen_loss 354.54\n",
            "iteration 595, epoch 1, batch 395/481,disc_loss 82.324, (real 87.009, fake 77.639 ) gen_loss 433.2\n",
            "iteration 596, epoch 1, batch 396/481,disc_loss 75.823, (real 79.708, fake 71.939 ) gen_loss 373.58\n",
            "iteration 597, epoch 1, batch 397/481,disc_loss 82.896, (real 88.86, fake 76.933 ) gen_loss 390.02\n",
            "iteration 598, epoch 1, batch 398/481,disc_loss 81.505, (real 85.999, fake 77.011 ) gen_loss 377.92\n",
            "iteration 599, epoch 1, batch 399/481,disc_loss 79.934, (real 85.333, fake 74.536 ) gen_loss 341.19\n",
            "iteration 600, epoch 1, batch 400/481,disc_loss 75.281, (real 80.157, fake 70.405 ) gen_loss 382.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 601, epoch 1, batch 401/481,disc_loss 79.434, (real 84.561, fake 74.307 ) gen_loss 377.52\n",
            "iteration 602, epoch 1, batch 402/481,disc_loss 83.344, (real 88.337, fake 78.35 ) gen_loss 405.02\n",
            "iteration 603, epoch 1, batch 403/481,disc_loss 82.798, (real 87.437, fake 78.158 ) gen_loss 431.02\n",
            "iteration 604, epoch 1, batch 404/481,disc_loss 82.388, (real 87.791, fake 76.985 ) gen_loss 390.67\n",
            "iteration 605, epoch 1, batch 405/481,disc_loss 79.564, (real 84.318, fake 74.809 ) gen_loss 397.57\n",
            "iteration 606, epoch 1, batch 406/481,disc_loss 78.93, (real 83.246, fake 74.614 ) gen_loss 393.12\n",
            "iteration 607, epoch 1, batch 407/481,disc_loss 80.816, (real 85.331, fake 76.302 ) gen_loss 366.89\n",
            "iteration 608, epoch 1, batch 408/481,disc_loss 80.172, (real 83.612, fake 76.733 ) gen_loss 379.01\n",
            "iteration 609, epoch 1, batch 409/481,disc_loss 81.431, (real 86.369, fake 76.493 ) gen_loss 397.59\n",
            "iteration 610, epoch 1, batch 410/481,disc_loss 84.223, (real 88.413, fake 80.034 ) gen_loss 425.03\n",
            "iteration 611, epoch 1, batch 411/481,disc_loss 81.313, (real 85.163, fake 77.463 ) gen_loss 390.58\n",
            "iteration 612, epoch 1, batch 412/481,disc_loss 85.646, (real 90.2, fake 81.091 ) gen_loss 464.7\n",
            "iteration 613, epoch 1, batch 413/481,disc_loss 77.836, (real 83.278, fake 72.394 ) gen_loss 397.12\n",
            "iteration 614, epoch 1, batch 414/481,disc_loss 79.782, (real 83.938, fake 75.626 ) gen_loss 425.02\n",
            "iteration 615, epoch 1, batch 415/481,disc_loss 80.151, (real 84.867, fake 75.435 ) gen_loss 404.33\n",
            "iteration 616, epoch 1, batch 416/481,disc_loss 79.433, (real 84.298, fake 74.569 ) gen_loss 396.91\n",
            "iteration 617, epoch 1, batch 417/481,disc_loss 80.788, (real 84.932, fake 76.644 ) gen_loss 369.2\n",
            "iteration 618, epoch 1, batch 418/481,disc_loss 78.305, (real 83.492, fake 73.119 ) gen_loss 424.07\n",
            "iteration 619, epoch 1, batch 419/481,disc_loss 80.316, (real 85.622, fake 75.01 ) gen_loss 359.08\n",
            "iteration 620, epoch 1, batch 420/481,disc_loss 79.452, (real 83.472, fake 75.431 ) gen_loss 416.42\n",
            "iteration 621, epoch 1, batch 421/481,disc_loss 80.244, (real 84.008, fake 76.48 ) gen_loss 342.81\n",
            "iteration 622, epoch 1, batch 422/481,disc_loss 83.211, (real 87.028, fake 79.393 ) gen_loss 390.35\n",
            "iteration 623, epoch 1, batch 423/481,disc_loss 78.817, (real 83.21, fake 74.423 ) gen_loss 353.13\n",
            "iteration 624, epoch 1, batch 424/481,disc_loss 79.154, (real 83.738, fake 74.569 ) gen_loss 412.94\n",
            "iteration 625, epoch 1, batch 425/481,disc_loss 84.609, (real 89.377, fake 79.841 ) gen_loss 404.35\n",
            "iteration 626, epoch 1, batch 426/481,disc_loss 80.412, (real 85.778, fake 75.046 ) gen_loss 384.06\n",
            "iteration 627, epoch 1, batch 427/481,disc_loss 78.683, (real 83.331, fake 74.035 ) gen_loss 375.15\n",
            "iteration 628, epoch 1, batch 428/481,disc_loss 81.936, (real 86.533, fake 77.339 ) gen_loss 411.41\n",
            "iteration 629, epoch 1, batch 429/481,disc_loss 78.857, (real 84.995, fake 72.719 ) gen_loss 358.47\n",
            "iteration 630, epoch 1, batch 430/481,disc_loss 84.45, (real 89.19, fake 79.709 ) gen_loss 381.13\n",
            "iteration 631, epoch 1, batch 431/481,disc_loss 82.602, (real 87.745, fake 77.46 ) gen_loss 371.03\n",
            "iteration 632, epoch 1, batch 432/481,disc_loss 77.971, (real 82.787, fake 73.154 ) gen_loss 376.83\n",
            "iteration 633, epoch 1, batch 433/481,disc_loss 83.568, (real 88.648, fake 78.487 ) gen_loss 427.64\n",
            "iteration 634, epoch 1, batch 434/481,disc_loss 80.439, (real 83.899, fake 76.979 ) gen_loss 371.95\n",
            "iteration 635, epoch 1, batch 435/481,disc_loss 80.778, (real 85.047, fake 76.51 ) gen_loss 350.65\n",
            "iteration 636, epoch 1, batch 436/481,disc_loss 80.426, (real 85.393, fake 75.459 ) gen_loss 390.56\n",
            "iteration 637, epoch 1, batch 437/481,disc_loss 83.158, (real 86.961, fake 79.354 ) gen_loss 373.43\n",
            "iteration 638, epoch 1, batch 438/481,disc_loss 79.798, (real 83.253, fake 76.343 ) gen_loss 355.93\n",
            "iteration 639, epoch 1, batch 439/481,disc_loss 80.335, (real 85.159, fake 75.511 ) gen_loss 362.76\n",
            "iteration 640, epoch 1, batch 440/481,disc_loss 80.661, (real 84.327, fake 76.996 ) gen_loss 389.32\n",
            "iteration 641, epoch 1, batch 441/481,disc_loss 80.605, (real 84.892, fake 76.317 ) gen_loss 370.48\n",
            "iteration 642, epoch 1, batch 442/481,disc_loss 84.102, (real 88.201, fake 80.004 ) gen_loss 377.0\n",
            "iteration 643, epoch 1, batch 443/481,disc_loss 78.102, (real 81.515, fake 74.689 ) gen_loss 358.69\n",
            "iteration 644, epoch 1, batch 444/481,disc_loss 79.962, (real 84.002, fake 75.922 ) gen_loss 396.6\n",
            "iteration 645, epoch 1, batch 445/481,disc_loss 80.553, (real 85.619, fake 75.487 ) gen_loss 383.69\n",
            "iteration 646, epoch 1, batch 446/481,disc_loss 78.065, (real 82.81, fake 73.319 ) gen_loss 398.45\n",
            "iteration 647, epoch 1, batch 447/481,disc_loss 78.47, (real 83.16, fake 73.781 ) gen_loss 365.32\n",
            "iteration 648, epoch 1, batch 448/481,disc_loss 82.037, (real 87.1, fake 76.975 ) gen_loss 367.16\n",
            "iteration 649, epoch 1, batch 449/481,disc_loss 81.369, (real 87.106, fake 75.631 ) gen_loss 357.35\n",
            "iteration 650, epoch 1, batch 450/481,disc_loss 84.231, (real 88.249, fake 80.212 ) gen_loss 401.35\n",
            "iteration 651, epoch 1, batch 451/481,disc_loss 76.105, (real 81.158, fake 71.052 ) gen_loss 419.72\n",
            "iteration 652, epoch 1, batch 452/481,disc_loss 77.931, (real 83.386, fake 72.476 ) gen_loss 444.2\n",
            "iteration 653, epoch 1, batch 453/481,disc_loss 82.049, (real 85.933, fake 78.165 ) gen_loss 398.15\n",
            "iteration 654, epoch 1, batch 454/481,disc_loss 80.717, (real 86.193, fake 75.241 ) gen_loss 454.79\n",
            "iteration 655, epoch 1, batch 455/481,disc_loss 80.192, (real 83.568, fake 76.816 ) gen_loss 396.28\n",
            "iteration 656, epoch 1, batch 456/481,disc_loss 76.414, (real 80.33, fake 72.497 ) gen_loss 372.2\n",
            "iteration 657, epoch 1, batch 457/481,disc_loss 81.816, (real 85.794, fake 77.837 ) gen_loss 424.44\n",
            "iteration 658, epoch 1, batch 458/481,disc_loss 80.814, (real 86.154, fake 75.474 ) gen_loss 360.55\n",
            "iteration 659, epoch 1, batch 459/481,disc_loss 79.539, (real 83.298, fake 75.78 ) gen_loss 391.86\n",
            "iteration 660, epoch 1, batch 460/481,disc_loss 79.408, (real 83.235, fake 75.581 ) gen_loss 446.84\n",
            "iteration 661, epoch 1, batch 461/481,disc_loss 79.731, (real 83.901, fake 75.561 ) gen_loss 377.2\n",
            "iteration 662, epoch 1, batch 462/481,disc_loss 86.606, (real 92.483, fake 80.729 ) gen_loss 399.48\n",
            "iteration 663, epoch 1, batch 463/481,disc_loss 83.732, (real 89.29, fake 78.174 ) gen_loss 349.95\n",
            "iteration 664, epoch 1, batch 464/481,disc_loss 82.17, (real 87.543, fake 76.798 ) gen_loss 380.48\n",
            "iteration 665, epoch 1, batch 465/481,disc_loss 72.072, (real 76.706, fake 67.437 ) gen_loss 379.88\n",
            "iteration 666, epoch 1, batch 466/481,disc_loss 81.225, (real 86.275, fake 76.175 ) gen_loss 379.22\n",
            "iteration 667, epoch 1, batch 467/481,disc_loss 77.445, (real 81.008, fake 73.881 ) gen_loss 338.69\n",
            "iteration 668, epoch 1, batch 468/481,disc_loss 85.769, (real 90.6, fake 80.937 ) gen_loss 399.09\n",
            "iteration 669, epoch 1, batch 469/481,disc_loss 77.058, (real 81.982, fake 72.134 ) gen_loss 380.74\n",
            "iteration 670, epoch 1, batch 470/481,disc_loss 79.734, (real 84.888, fake 74.58 ) gen_loss 486.76\n",
            "iteration 671, epoch 1, batch 471/481,disc_loss 78.705, (real 82.867, fake 74.543 ) gen_loss 409.64\n",
            "iteration 672, epoch 1, batch 472/481,disc_loss 85.313, (real 88.936, fake 81.69 ) gen_loss 410.81\n",
            "iteration 673, epoch 1, batch 473/481,disc_loss 75.474, (real 80.482, fake 70.465 ) gen_loss 405.66\n",
            "iteration 674, epoch 1, batch 474/481,disc_loss 81.541, (real 86.515, fake 76.567 ) gen_loss 415.58\n",
            "iteration 675, epoch 1, batch 475/481,disc_loss 79.71, (real 84.514, fake 74.906 ) gen_loss 410.69\n",
            "iteration 676, epoch 1, batch 476/481,disc_loss 79.088, (real 83.395, fake 74.781 ) gen_loss 435.02\n",
            "iteration 677, epoch 1, batch 477/481,disc_loss 78.682, (real 82.564, fake 74.801 ) gen_loss 403.9\n",
            "iteration 678, epoch 1, batch 478/481,disc_loss 80.729, (real 85.035, fake 76.423 ) gen_loss 432.38\n",
            "iteration 679, epoch 1, batch 479/481,disc_loss 76.682, (real 81.309, fake 72.055 ) gen_loss 422.63\n",
            "iteration 680, epoch 1, batch 480/481,disc_loss 80.382, (real 86.737, fake 74.027 ) gen_loss 401.9\n",
            "iteration 681, epoch 1, batch 481/481,disc_loss 77.311, (real 82.745, fake 71.876 ) gen_loss 350.11\n",
            "iteration 682, epoch 2, batch 1/481,disc_loss 80.542, (real 85.446, fake 75.638 ) gen_loss 355.65\n",
            "iteration 683, epoch 2, batch 2/481,disc_loss 84.877, (real 89.189, fake 80.565 ) gen_loss 362.59\n",
            "iteration 684, epoch 2, batch 3/481,disc_loss 79.595, (real 84.06, fake 75.13 ) gen_loss 357.98\n",
            "iteration 685, epoch 2, batch 4/481,disc_loss 77.144, (real 81.767, fake 72.521 ) gen_loss 334.49\n",
            "iteration 686, epoch 2, batch 5/481,disc_loss 76.588, (real 80.347, fake 72.83 ) gen_loss 396.88\n",
            "iteration 687, epoch 2, batch 6/481,disc_loss 77.753, (real 81.45, fake 74.056 ) gen_loss 370.48\n",
            "iteration 688, epoch 2, batch 7/481,disc_loss 78.134, (real 83.406, fake 72.861 ) gen_loss 367.73\n",
            "iteration 689, epoch 2, batch 8/481,disc_loss 79.477, (real 83.576, fake 75.378 ) gen_loss 392.18\n",
            "iteration 690, epoch 2, batch 9/481,disc_loss 78.531, (real 82.144, fake 74.918 ) gen_loss 398.48\n",
            "iteration 691, epoch 2, batch 10/481,disc_loss 81.975, (real 86.864, fake 77.087 ) gen_loss 365.01\n",
            "iteration 692, epoch 2, batch 11/481,disc_loss 85.131, (real 88.835, fake 81.427 ) gen_loss 359.57\n",
            "iteration 693, epoch 2, batch 12/481,disc_loss 80.026, (real 84.819, fake 75.233 ) gen_loss 382.05\n",
            "iteration 694, epoch 2, batch 13/481,disc_loss 80.56, (real 85.203, fake 75.918 ) gen_loss 370.48\n",
            "iteration 695, epoch 2, batch 14/481,disc_loss 78.232, (real 82.599, fake 73.866 ) gen_loss 380.96\n",
            "iteration 696, epoch 2, batch 15/481,disc_loss 83.123, (real 88.901, fake 77.345 ) gen_loss 362.59\n",
            "iteration 697, epoch 2, batch 16/481,disc_loss 84.285, (real 88.859, fake 79.71 ) gen_loss 395.23\n",
            "iteration 698, epoch 2, batch 17/481,disc_loss 82.671, (real 88.221, fake 77.121 ) gen_loss 447.71\n",
            "iteration 699, epoch 2, batch 18/481,disc_loss 77.92, (real 81.685, fake 74.155 ) gen_loss 487.4\n",
            "iteration 700, epoch 2, batch 19/481,disc_loss 83.272, (real 86.639, fake 79.905 ) gen_loss 425.57\n",
            "iteration 701, epoch 2, batch 20/481,disc_loss 81.086, (real 85.754, fake 76.418 ) gen_loss 459.78\n",
            "iteration 702, epoch 2, batch 21/481,disc_loss 82.753, (real 86.448, fake 79.057 ) gen_loss 399.13\n",
            "iteration 703, epoch 2, batch 22/481,disc_loss 82.122, (real 86.811, fake 77.433 ) gen_loss 463.79\n",
            "iteration 704, epoch 2, batch 23/481,disc_loss 82.491, (real 86.864, fake 78.118 ) gen_loss 383.5\n",
            "iteration 705, epoch 2, batch 24/481,disc_loss 87.767, (real 90.995, fake 84.54 ) gen_loss 418.71\n",
            "iteration 706, epoch 2, batch 25/481,disc_loss 83.964, (real 89.487, fake 78.441 ) gen_loss 458.24\n",
            "iteration 707, epoch 2, batch 26/481,disc_loss 81.417, (real 85.758, fake 77.076 ) gen_loss 415.41\n",
            "iteration 708, epoch 2, batch 27/481,disc_loss 80.129, (real 85.798, fake 74.461 ) gen_loss 441.85\n",
            "iteration 709, epoch 2, batch 28/481,disc_loss 80.362, (real 84.566, fake 76.158 ) gen_loss 413.12\n",
            "iteration 710, epoch 2, batch 29/481,disc_loss 81.896, (real 85.898, fake 77.895 ) gen_loss 447.27\n",
            "iteration 711, epoch 2, batch 30/481,disc_loss 81.545, (real 86.047, fake 77.043 ) gen_loss 373.91\n",
            "iteration 712, epoch 2, batch 31/481,disc_loss 85.59, (real 89.56, fake 81.619 ) gen_loss 394.49\n",
            "iteration 713, epoch 2, batch 32/481,disc_loss 80.554, (real 84.861, fake 76.247 ) gen_loss 371.48\n",
            "iteration 714, epoch 2, batch 33/481,disc_loss 77.398, (real 81.138, fake 73.657 ) gen_loss 426.57\n",
            "iteration 715, epoch 2, batch 34/481,disc_loss 79.05, (real 83.201, fake 74.899 ) gen_loss 397.4\n",
            "iteration 716, epoch 2, batch 35/481,disc_loss 84.748, (real 89.322, fake 80.173 ) gen_loss 405.06\n",
            "iteration 717, epoch 2, batch 36/481,disc_loss 75.967, (real 80.751, fake 71.183 ) gen_loss 387.7\n",
            "iteration 718, epoch 2, batch 37/481,disc_loss 78.236, (real 82.829, fake 73.642 ) gen_loss 384.76\n",
            "iteration 719, epoch 2, batch 38/481,disc_loss 81.422, (real 84.701, fake 78.142 ) gen_loss 364.81\n",
            "iteration 720, epoch 2, batch 39/481,disc_loss 78.871, (real 82.477, fake 75.265 ) gen_loss 377.75\n",
            "iteration 721, epoch 2, batch 40/481,disc_loss 77.172, (real 81.737, fake 72.606 ) gen_loss 412.95\n",
            "iteration 722, epoch 2, batch 41/481,disc_loss 85.034, (real 88.926, fake 81.141 ) gen_loss 389.07\n",
            "iteration 723, epoch 2, batch 42/481,disc_loss 79.945, (real 85.333, fake 74.557 ) gen_loss 400.31\n",
            "iteration 724, epoch 2, batch 43/481,disc_loss 83.583, (real 88.395, fake 78.772 ) gen_loss 374.71\n",
            "iteration 725, epoch 2, batch 44/481,disc_loss 79.809, (real 85.044, fake 74.574 ) gen_loss 407.83\n",
            "iteration 726, epoch 2, batch 45/481,disc_loss 83.925, (real 88.636, fake 79.214 ) gen_loss 440.87\n",
            "iteration 727, epoch 2, batch 46/481,disc_loss 80.789, (real 85.965, fake 75.612 ) gen_loss 376.28\n",
            "iteration 728, epoch 2, batch 47/481,disc_loss 82.589, (real 86.765, fake 78.414 ) gen_loss 416.96\n",
            "iteration 729, epoch 2, batch 48/481,disc_loss 74.563, (real 78.894, fake 70.233 ) gen_loss 446.24\n",
            "iteration 730, epoch 2, batch 49/481,disc_loss 79.227, (real 82.155, fake 76.3 ) gen_loss 439.42\n",
            "iteration 731, epoch 2, batch 50/481,disc_loss 76.85, (real 80.998, fake 72.702 ) gen_loss 420.5\n",
            "iteration 732, epoch 2, batch 51/481,disc_loss 83.12, (real 86.991, fake 79.248 ) gen_loss 364.55\n",
            "iteration 733, epoch 2, batch 52/481,disc_loss 83.011, (real 86.626, fake 79.397 ) gen_loss 380.26\n",
            "iteration 734, epoch 2, batch 53/481,disc_loss 79.946, (real 83.87, fake 76.022 ) gen_loss 398.42\n",
            "iteration 735, epoch 2, batch 54/481,disc_loss 84.71, (real 88.628, fake 80.793 ) gen_loss 468.99\n",
            "iteration 736, epoch 2, batch 55/481,disc_loss 80.774, (real 85.504, fake 76.045 ) gen_loss 389.14\n",
            "iteration 737, epoch 2, batch 56/481,disc_loss 79.892, (real 82.942, fake 76.842 ) gen_loss 364.68\n",
            "iteration 738, epoch 2, batch 57/481,disc_loss 79.784, (real 83.721, fake 75.847 ) gen_loss 388.47\n",
            "iteration 739, epoch 2, batch 58/481,disc_loss 80.92, (real 85.035, fake 76.806 ) gen_loss 392.51\n",
            "iteration 740, epoch 2, batch 59/481,disc_loss 80.594, (real 85.513, fake 75.675 ) gen_loss 415.76\n",
            "iteration 741, epoch 2, batch 60/481,disc_loss 77.35, (real 81.148, fake 73.552 ) gen_loss 414.68\n",
            "iteration 742, epoch 2, batch 61/481,disc_loss 77.613, (real 81.922, fake 73.304 ) gen_loss 385.34\n",
            "iteration 743, epoch 2, batch 62/481,disc_loss 77.274, (real 81.115, fake 73.433 ) gen_loss 387.69\n",
            "iteration 744, epoch 2, batch 63/481,disc_loss 75.279, (real 79.023, fake 71.535 ) gen_loss 365.28\n",
            "iteration 745, epoch 2, batch 64/481,disc_loss 83.058, (real 86.672, fake 79.445 ) gen_loss 452.4\n",
            "iteration 746, epoch 2, batch 65/481,disc_loss 81.744, (real 86.354, fake 77.133 ) gen_loss 446.65\n",
            "iteration 747, epoch 2, batch 66/481,disc_loss 82.838, (real 87.141, fake 78.536 ) gen_loss 354.98\n",
            "iteration 748, epoch 2, batch 67/481,disc_loss 76.108, (real 80.617, fake 71.599 ) gen_loss 442.13\n",
            "iteration 749, epoch 2, batch 68/481,disc_loss 85.719, (real 90.0, fake 81.438 ) gen_loss 387.37\n",
            "iteration 750, epoch 2, batch 69/481,disc_loss 80.388, (real 86.582, fake 74.194 ) gen_loss 404.76\n",
            "iteration 751, epoch 2, batch 70/481,disc_loss 84.883, (real 89.243, fake 80.523 ) gen_loss 434.76\n",
            "iteration 752, epoch 2, batch 71/481,disc_loss 81.084, (real 86.434, fake 75.734 ) gen_loss 424.05\n",
            "iteration 753, epoch 2, batch 72/481,disc_loss 81.572, (real 85.105, fake 78.039 ) gen_loss 439.83\n",
            "iteration 754, epoch 2, batch 73/481,disc_loss 78.678, (real 83.749, fake 73.607 ) gen_loss 418.19\n",
            "iteration 755, epoch 2, batch 74/481,disc_loss 75.945, (real 80.549, fake 71.34 ) gen_loss 399.37\n",
            "iteration 756, epoch 2, batch 75/481,disc_loss 84.975, (real 88.176, fake 81.773 ) gen_loss 372.58\n",
            "iteration 757, epoch 2, batch 76/481,disc_loss 81.949, (real 86.721, fake 77.178 ) gen_loss 402.78\n",
            "iteration 758, epoch 2, batch 77/481,disc_loss 80.536, (real 84.954, fake 76.117 ) gen_loss 365.61\n",
            "iteration 759, epoch 2, batch 78/481,disc_loss 79.885, (real 83.786, fake 75.983 ) gen_loss 406.61\n",
            "iteration 760, epoch 2, batch 79/481,disc_loss 82.376, (real 87.185, fake 77.567 ) gen_loss 374.41\n",
            "iteration 761, epoch 2, batch 80/481,disc_loss 82.275, (real 85.728, fake 78.822 ) gen_loss 381.41\n",
            "iteration 762, epoch 2, batch 81/481,disc_loss 76.616, (real 80.058, fake 73.173 ) gen_loss 402.42\n",
            "iteration 763, epoch 2, batch 82/481,disc_loss 80.827, (real 85.39, fake 76.263 ) gen_loss 375.09\n",
            "iteration 764, epoch 2, batch 83/481,disc_loss 80.29, (real 84.039, fake 76.54 ) gen_loss 378.77\n",
            "iteration 765, epoch 2, batch 84/481,disc_loss 83.742, (real 87.535, fake 79.948 ) gen_loss 386.87\n",
            "iteration 766, epoch 2, batch 85/481,disc_loss 82.615, (real 87.923, fake 77.307 ) gen_loss 410.46\n",
            "iteration 767, epoch 2, batch 86/481,disc_loss 76.519, (real 80.342, fake 72.695 ) gen_loss 386.34\n",
            "iteration 768, epoch 2, batch 87/481,disc_loss 80.055, (real 86.083, fake 74.028 ) gen_loss 395.01\n",
            "iteration 769, epoch 2, batch 88/481,disc_loss 79.075, (real 82.963, fake 75.187 ) gen_loss 438.64\n",
            "iteration 770, epoch 2, batch 89/481,disc_loss 74.742, (real 77.592, fake 71.891 ) gen_loss 406.75\n",
            "iteration 771, epoch 2, batch 90/481,disc_loss 84.937, (real 91.992, fake 77.882 ) gen_loss 454.37\n",
            "iteration 772, epoch 2, batch 91/481,disc_loss 79.107, (real 84.837, fake 73.377 ) gen_loss 454.9\n",
            "iteration 773, epoch 2, batch 92/481,disc_loss 79.506, (real 85.028, fake 73.983 ) gen_loss 408.83\n",
            "iteration 774, epoch 2, batch 93/481,disc_loss 87.058, (real 93.786, fake 80.331 ) gen_loss 398.05\n",
            "iteration 775, epoch 2, batch 94/481,disc_loss 80.804, (real 85.38, fake 76.229 ) gen_loss 369.94\n",
            "iteration 776, epoch 2, batch 95/481,disc_loss 85.974, (real 90.876, fake 81.071 ) gen_loss 388.83\n",
            "iteration 777, epoch 2, batch 96/481,disc_loss 77.639, (real 81.087, fake 74.192 ) gen_loss 425.81\n",
            "iteration 778, epoch 2, batch 97/481,disc_loss 79.146, (real 84.043, fake 74.249 ) gen_loss 471.26\n",
            "iteration 779, epoch 2, batch 98/481,disc_loss 80.35, (real 85.407, fake 75.293 ) gen_loss 494.01\n",
            "iteration 780, epoch 2, batch 99/481,disc_loss 78.488, (real 83.443, fake 73.533 ) gen_loss 428.86\n",
            "iteration 781, epoch 2, batch 100/481,disc_loss 83.059, (real 87.636, fake 78.481 ) gen_loss 407.78\n",
            "iteration 782, epoch 2, batch 101/481,disc_loss 78.418, (real 82.704, fake 74.132 ) gen_loss 412.6\n",
            "iteration 783, epoch 2, batch 102/481,disc_loss 82.014, (real 86.647, fake 77.38 ) gen_loss 395.6\n",
            "iteration 784, epoch 2, batch 103/481,disc_loss 77.292, (real 81.984, fake 72.599 ) gen_loss 393.85\n",
            "iteration 785, epoch 2, batch 104/481,disc_loss 80.099, (real 84.495, fake 75.704 ) gen_loss 367.47\n",
            "iteration 786, epoch 2, batch 105/481,disc_loss 78.572, (real 82.99, fake 74.153 ) gen_loss 377.74\n",
            "iteration 787, epoch 2, batch 106/481,disc_loss 75.471, (real 79.174, fake 71.769 ) gen_loss 382.75\n",
            "iteration 788, epoch 2, batch 107/481,disc_loss 78.664, (real 82.186, fake 75.141 ) gen_loss 402.05\n",
            "iteration 789, epoch 2, batch 108/481,disc_loss 75.403, (real 80.806, fake 70.0 ) gen_loss 389.16\n",
            "iteration 790, epoch 2, batch 109/481,disc_loss 75.367, (real 79.099, fake 71.635 ) gen_loss 373.05\n",
            "iteration 791, epoch 2, batch 110/481,disc_loss 84.949, (real 89.324, fake 80.574 ) gen_loss 484.17\n",
            "iteration 792, epoch 2, batch 111/481,disc_loss 78.492, (real 81.754, fake 75.229 ) gen_loss 421.57\n",
            "iteration 793, epoch 2, batch 112/481,disc_loss 81.936, (real 85.841, fake 78.032 ) gen_loss 402.12\n",
            "iteration 794, epoch 2, batch 113/481,disc_loss 80.945, (real 87.179, fake 74.712 ) gen_loss 399.91\n",
            "iteration 795, epoch 2, batch 114/481,disc_loss 72.994, (real 76.228, fake 69.761 ) gen_loss 440.54\n",
            "iteration 796, epoch 2, batch 115/481,disc_loss 85.187, (real 90.409, fake 79.966 ) gen_loss 505.24\n",
            "iteration 797, epoch 2, batch 116/481,disc_loss 83.819, (real 87.531, fake 80.108 ) gen_loss 419.74\n",
            "iteration 798, epoch 2, batch 117/481,disc_loss 81.014, (real 84.246, fake 77.782 ) gen_loss 405.47\n",
            "iteration 799, epoch 2, batch 118/481,disc_loss 86.053, (real 89.73, fake 82.376 ) gen_loss 480.64\n",
            "iteration 800, epoch 2, batch 119/481,disc_loss 81.589, (real 85.891, fake 77.288 ) gen_loss 451.81\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 801, epoch 2, batch 120/481,disc_loss 80.923, (real 84.099, fake 77.747 ) gen_loss 459.1\n",
            "iteration 802, epoch 2, batch 121/481,disc_loss 81.536, (real 85.798, fake 77.274 ) gen_loss 402.47\n",
            "iteration 803, epoch 2, batch 122/481,disc_loss 80.585, (real 85.341, fake 75.83 ) gen_loss 404.02\n",
            "iteration 804, epoch 2, batch 123/481,disc_loss 80.744, (real 86.494, fake 74.995 ) gen_loss 376.93\n",
            "iteration 805, epoch 2, batch 124/481,disc_loss 85.042, (real 88.738, fake 81.347 ) gen_loss 365.34\n",
            "iteration 806, epoch 2, batch 125/481,disc_loss 76.649, (real 80.809, fake 72.488 ) gen_loss 388.1\n",
            "iteration 807, epoch 2, batch 126/481,disc_loss 81.08, (real 85.073, fake 77.088 ) gen_loss 412.52\n",
            "iteration 808, epoch 2, batch 127/481,disc_loss 82.017, (real 85.34, fake 78.695 ) gen_loss 391.57\n",
            "iteration 809, epoch 2, batch 128/481,disc_loss 78.77, (real 83.825, fake 73.715 ) gen_loss 426.15\n",
            "iteration 810, epoch 2, batch 129/481,disc_loss 86.185, (real 90.5, fake 81.87 ) gen_loss 410.99\n",
            "iteration 811, epoch 2, batch 130/481,disc_loss 79.261, (real 83.327, fake 75.196 ) gen_loss 423.67\n",
            "iteration 812, epoch 2, batch 131/481,disc_loss 77.439, (real 81.185, fake 73.692 ) gen_loss 400.23\n",
            "iteration 813, epoch 2, batch 132/481,disc_loss 81.116, (real 85.414, fake 76.818 ) gen_loss 384.36\n",
            "iteration 814, epoch 2, batch 133/481,disc_loss 78.128, (real 82.35, fake 73.906 ) gen_loss 382.89\n",
            "iteration 815, epoch 2, batch 134/481,disc_loss 79.779, (real 85.022, fake 74.535 ) gen_loss 421.91\n",
            "iteration 816, epoch 2, batch 135/481,disc_loss 78.352, (real 82.915, fake 73.789 ) gen_loss 396.64\n",
            "iteration 817, epoch 2, batch 136/481,disc_loss 77.668, (real 81.104, fake 74.233 ) gen_loss 441.18\n",
            "iteration 818, epoch 2, batch 137/481,disc_loss 77.204, (real 80.967, fake 73.441 ) gen_loss 424.49\n",
            "iteration 819, epoch 2, batch 138/481,disc_loss 83.474, (real 87.61, fake 79.337 ) gen_loss 406.64\n",
            "iteration 820, epoch 2, batch 139/481,disc_loss 78.93, (real 82.543, fake 75.317 ) gen_loss 412.52\n",
            "iteration 821, epoch 2, batch 140/481,disc_loss 78.952, (real 83.416, fake 74.487 ) gen_loss 406.26\n",
            "iteration 822, epoch 2, batch 141/481,disc_loss 78.317, (real 82.988, fake 73.645 ) gen_loss 398.73\n",
            "iteration 823, epoch 2, batch 142/481,disc_loss 84.298, (real 88.526, fake 80.069 ) gen_loss 454.01\n",
            "iteration 824, epoch 2, batch 143/481,disc_loss 78.094, (real 82.479, fake 73.709 ) gen_loss 415.24\n",
            "iteration 825, epoch 2, batch 144/481,disc_loss 81.276, (real 86.525, fake 76.026 ) gen_loss 354.07\n",
            "iteration 826, epoch 2, batch 145/481,disc_loss 79.641, (real 83.608, fake 75.675 ) gen_loss 379.36\n",
            "iteration 827, epoch 2, batch 146/481,disc_loss 78.152, (real 82.367, fake 73.938 ) gen_loss 382.88\n",
            "iteration 828, epoch 2, batch 147/481,disc_loss 79.299, (real 83.749, fake 74.849 ) gen_loss 362.32\n",
            "iteration 829, epoch 2, batch 148/481,disc_loss 76.557, (real 79.644, fake 73.469 ) gen_loss 398.25\n",
            "iteration 830, epoch 2, batch 149/481,disc_loss 78.917, (real 82.954, fake 74.879 ) gen_loss 404.52\n",
            "iteration 831, epoch 2, batch 150/481,disc_loss 81.883, (real 85.354, fake 78.412 ) gen_loss 385.2\n",
            "iteration 832, epoch 2, batch 151/481,disc_loss 77.232, (real 82.234, fake 72.23 ) gen_loss 342.88\n",
            "iteration 833, epoch 2, batch 152/481,disc_loss 79.39, (real 82.71, fake 76.069 ) gen_loss 364.72\n",
            "iteration 834, epoch 2, batch 153/481,disc_loss 78.06, (real 81.614, fake 74.507 ) gen_loss 403.11\n",
            "iteration 835, epoch 2, batch 154/481,disc_loss 79.215, (real 83.33, fake 75.099 ) gen_loss 400.25\n",
            "iteration 836, epoch 2, batch 155/481,disc_loss 80.284, (real 84.289, fake 76.279 ) gen_loss 428.3\n",
            "iteration 837, epoch 2, batch 156/481,disc_loss 81.211, (real 84.963, fake 77.459 ) gen_loss 411.07\n",
            "iteration 838, epoch 2, batch 157/481,disc_loss 79.678, (real 82.676, fake 76.68 ) gen_loss 391.37\n",
            "iteration 839, epoch 2, batch 158/481,disc_loss 79.212, (real 82.39, fake 76.035 ) gen_loss 386.78\n",
            "iteration 840, epoch 2, batch 159/481,disc_loss 77.381, (real 82.188, fake 72.574 ) gen_loss 405.57\n",
            "iteration 841, epoch 2, batch 160/481,disc_loss 87.404, (real 91.441, fake 83.367 ) gen_loss 390.77\n",
            "iteration 842, epoch 2, batch 161/481,disc_loss 84.054, (real 87.9, fake 80.209 ) gen_loss 381.28\n",
            "iteration 843, epoch 2, batch 162/481,disc_loss 80.495, (real 84.084, fake 76.906 ) gen_loss 369.46\n",
            "iteration 844, epoch 2, batch 163/481,disc_loss 80.393, (real 83.77, fake 77.016 ) gen_loss 414.45\n",
            "iteration 845, epoch 2, batch 164/481,disc_loss 74.656, (real 78.941, fake 70.371 ) gen_loss 421.93\n",
            "iteration 846, epoch 2, batch 165/481,disc_loss 76.85, (real 80.848, fake 72.853 ) gen_loss 478.78\n",
            "iteration 847, epoch 2, batch 166/481,disc_loss 85.484, (real 89.272, fake 81.697 ) gen_loss 444.22\n",
            "iteration 848, epoch 2, batch 167/481,disc_loss 76.903, (real 81.914, fake 71.893 ) gen_loss 392.12\n",
            "iteration 849, epoch 2, batch 168/481,disc_loss 78.16, (real 81.174, fake 75.146 ) gen_loss 420.64\n",
            "iteration 850, epoch 2, batch 169/481,disc_loss 78.127, (real 82.517, fake 73.737 ) gen_loss 480.21\n",
            "iteration 851, epoch 2, batch 170/481,disc_loss 77.328, (real 81.887, fake 72.77 ) gen_loss 487.81\n",
            "iteration 852, epoch 2, batch 171/481,disc_loss 78.362, (real 83.402, fake 73.323 ) gen_loss 444.32\n",
            "iteration 853, epoch 2, batch 172/481,disc_loss 80.69, (real 85.967, fake 75.412 ) gen_loss 395.49\n",
            "iteration 854, epoch 2, batch 173/481,disc_loss 84.838, (real 88.546, fake 81.129 ) gen_loss 435.7\n",
            "iteration 855, epoch 2, batch 174/481,disc_loss 79.265, (real 83.739, fake 74.792 ) gen_loss 373.39\n",
            "iteration 856, epoch 2, batch 175/481,disc_loss 79.876, (real 83.363, fake 76.39 ) gen_loss 430.77\n",
            "iteration 857, epoch 2, batch 176/481,disc_loss 78.673, (real 83.971, fake 73.374 ) gen_loss 425.75\n",
            "iteration 858, epoch 2, batch 177/481,disc_loss 80.186, (real 84.36, fake 76.013 ) gen_loss 422.53\n",
            "iteration 859, epoch 2, batch 178/481,disc_loss 77.316, (real 82.439, fake 72.192 ) gen_loss 408.59\n",
            "iteration 860, epoch 2, batch 179/481,disc_loss 79.783, (real 84.224, fake 75.343 ) gen_loss 435.9\n",
            "iteration 861, epoch 2, batch 180/481,disc_loss 83.268, (real 88.121, fake 78.416 ) gen_loss 392.18\n",
            "iteration 862, epoch 2, batch 181/481,disc_loss 78.435, (real 83.689, fake 73.181 ) gen_loss 392.3\n",
            "iteration 863, epoch 2, batch 182/481,disc_loss 82.301, (real 87.343, fake 77.26 ) gen_loss 449.18\n",
            "iteration 864, epoch 2, batch 183/481,disc_loss 83.224, (real 87.522, fake 78.926 ) gen_loss 415.91\n",
            "iteration 865, epoch 2, batch 184/481,disc_loss 79.333, (real 82.917, fake 75.749 ) gen_loss 403.19\n",
            "iteration 866, epoch 2, batch 185/481,disc_loss 81.047, (real 84.697, fake 77.396 ) gen_loss 406.37\n",
            "iteration 867, epoch 2, batch 186/481,disc_loss 76.74, (real 80.201, fake 73.278 ) gen_loss 398.6\n",
            "iteration 868, epoch 2, batch 187/481,disc_loss 80.205, (real 84.083, fake 76.327 ) gen_loss 435.25\n",
            "iteration 869, epoch 2, batch 188/481,disc_loss 78.618, (real 82.9, fake 74.335 ) gen_loss 410.84\n",
            "iteration 870, epoch 2, batch 189/481,disc_loss 83.037, (real 86.664, fake 79.411 ) gen_loss 428.36\n",
            "iteration 871, epoch 2, batch 190/481,disc_loss 80.061, (real 84.456, fake 75.665 ) gen_loss 387.12\n",
            "iteration 872, epoch 2, batch 191/481,disc_loss 79.346, (real 83.8, fake 74.892 ) gen_loss 449.82\n",
            "iteration 873, epoch 2, batch 192/481,disc_loss 81.482, (real 85.632, fake 77.333 ) gen_loss 422.73\n",
            "iteration 874, epoch 2, batch 193/481,disc_loss 82.433, (real 86.282, fake 78.583 ) gen_loss 401.34\n",
            "iteration 875, epoch 2, batch 194/481,disc_loss 84.734, (real 88.709, fake 80.76 ) gen_loss 386.8\n",
            "iteration 876, epoch 2, batch 195/481,disc_loss 76.653, (real 80.953, fake 72.353 ) gen_loss 367.46\n",
            "iteration 877, epoch 2, batch 196/481,disc_loss 83.7, (real 87.917, fake 79.483 ) gen_loss 407.75\n",
            "iteration 878, epoch 2, batch 197/481,disc_loss 77.332, (real 80.456, fake 74.209 ) gen_loss 421.51\n",
            "iteration 879, epoch 2, batch 198/481,disc_loss 79.291, (real 84.96, fake 73.622 ) gen_loss 420.17\n",
            "iteration 880, epoch 2, batch 199/481,disc_loss 82.445, (real 86.648, fake 78.242 ) gen_loss 372.22\n",
            "iteration 881, epoch 2, batch 200/481,disc_loss 80.251, (real 85.584, fake 74.919 ) gen_loss 423.6\n",
            "iteration 882, epoch 2, batch 201/481,disc_loss 76.109, (real 79.83, fake 72.387 ) gen_loss 417.61\n",
            "iteration 883, epoch 2, batch 202/481,disc_loss 82.981, (real 87.28, fake 78.682 ) gen_loss 399.79\n",
            "iteration 884, epoch 2, batch 203/481,disc_loss 76.824, (real 80.879, fake 72.768 ) gen_loss 405.0\n",
            "iteration 885, epoch 2, batch 204/481,disc_loss 79.07, (real 83.078, fake 75.062 ) gen_loss 399.39\n",
            "iteration 886, epoch 2, batch 205/481,disc_loss 80.534, (real 84.733, fake 76.335 ) gen_loss 388.31\n",
            "iteration 887, epoch 2, batch 206/481,disc_loss 75.572, (real 79.147, fake 71.997 ) gen_loss 347.82\n",
            "iteration 888, epoch 2, batch 207/481,disc_loss 81.945, (real 87.198, fake 76.692 ) gen_loss 383.55\n",
            "iteration 889, epoch 2, batch 208/481,disc_loss 81.555, (real 89.069, fake 74.04 ) gen_loss 359.9\n",
            "iteration 890, epoch 2, batch 209/481,disc_loss 80.981, (real 85.455, fake 76.507 ) gen_loss 386.48\n",
            "iteration 891, epoch 2, batch 210/481,disc_loss 79.275, (real 84.151, fake 74.398 ) gen_loss 388.41\n",
            "iteration 892, epoch 2, batch 211/481,disc_loss 82.468, (real 88.106, fake 76.83 ) gen_loss 403.4\n",
            "iteration 893, epoch 2, batch 212/481,disc_loss 78.059, (real 83.579, fake 72.539 ) gen_loss 421.76\n",
            "iteration 894, epoch 2, batch 213/481,disc_loss 79.434, (real 83.522, fake 75.345 ) gen_loss 438.11\n",
            "iteration 895, epoch 2, batch 214/481,disc_loss 79.397, (real 82.815, fake 75.979 ) gen_loss 435.93\n",
            "iteration 896, epoch 2, batch 215/481,disc_loss 82.699, (real 87.505, fake 77.894 ) gen_loss 482.46\n",
            "iteration 897, epoch 2, batch 216/481,disc_loss 81.346, (real 85.268, fake 77.424 ) gen_loss 441.35\n",
            "iteration 898, epoch 2, batch 217/481,disc_loss 79.949, (real 83.825, fake 76.074 ) gen_loss 406.57\n",
            "iteration 899, epoch 2, batch 218/481,disc_loss 74.686, (real 79.301, fake 70.07 ) gen_loss 421.99\n",
            "iteration 900, epoch 2, batch 219/481,disc_loss 79.97, (real 83.864, fake 76.076 ) gen_loss 428.3\n",
            "iteration 901, epoch 2, batch 220/481,disc_loss 84.451, (real 87.049, fake 81.853 ) gen_loss 386.31\n",
            "iteration 902, epoch 2, batch 221/481,disc_loss 81.424, (real 84.782, fake 78.066 ) gen_loss 406.23\n",
            "iteration 903, epoch 2, batch 222/481,disc_loss 79.543, (real 84.059, fake 75.027 ) gen_loss 397.56\n",
            "iteration 904, epoch 2, batch 223/481,disc_loss 80.824, (real 84.189, fake 77.459 ) gen_loss 401.88\n",
            "iteration 905, epoch 2, batch 224/481,disc_loss 82.465, (real 88.031, fake 76.898 ) gen_loss 418.25\n",
            "iteration 906, epoch 2, batch 225/481,disc_loss 79.395, (real 83.105, fake 75.685 ) gen_loss 409.18\n",
            "iteration 907, epoch 2, batch 226/481,disc_loss 76.477, (real 81.21, fake 71.745 ) gen_loss 416.52\n",
            "iteration 908, epoch 2, batch 227/481,disc_loss 77.131, (real 81.732, fake 72.529 ) gen_loss 429.43\n",
            "iteration 909, epoch 2, batch 228/481,disc_loss 75.958, (real 81.183, fake 70.733 ) gen_loss 393.23\n",
            "iteration 910, epoch 2, batch 229/481,disc_loss 82.674, (real 87.831, fake 77.517 ) gen_loss 425.97\n",
            "iteration 911, epoch 2, batch 230/481,disc_loss 76.066, (real 80.556, fake 71.577 ) gen_loss 417.55\n",
            "iteration 912, epoch 2, batch 231/481,disc_loss 81.144, (real 85.46, fake 76.827 ) gen_loss 422.15\n",
            "iteration 913, epoch 2, batch 232/481,disc_loss 80.299, (real 84.58, fake 76.018 ) gen_loss 493.26\n",
            "iteration 914, epoch 2, batch 233/481,disc_loss 79.741, (real 83.157, fake 76.326 ) gen_loss 506.4\n",
            "iteration 915, epoch 2, batch 234/481,disc_loss 77.388, (real 81.666, fake 73.11 ) gen_loss 433.39\n",
            "iteration 916, epoch 2, batch 235/481,disc_loss 76.835, (real 81.174, fake 72.496 ) gen_loss 435.09\n",
            "iteration 917, epoch 2, batch 236/481,disc_loss 89.897, (real 94.502, fake 85.292 ) gen_loss 482.82\n",
            "iteration 918, epoch 2, batch 237/481,disc_loss 85.23, (real 89.914, fake 80.545 ) gen_loss 381.18\n",
            "iteration 919, epoch 2, batch 238/481,disc_loss 78.784, (real 82.104, fake 75.463 ) gen_loss 420.39\n",
            "iteration 920, epoch 2, batch 239/481,disc_loss 78.787, (real 82.338, fake 75.236 ) gen_loss 429.89\n",
            "iteration 921, epoch 2, batch 240/481,disc_loss 81.824, (real 86.15, fake 77.499 ) gen_loss 373.9\n",
            "iteration 922, epoch 2, batch 241/481,disc_loss 81.644, (real 86.943, fake 76.345 ) gen_loss 417.63\n",
            "iteration 923, epoch 2, batch 242/481,disc_loss 82.491, (real 85.49, fake 79.493 ) gen_loss 431.29\n",
            "iteration 924, epoch 2, batch 243/481,disc_loss 78.554, (real 81.919, fake 75.189 ) gen_loss 453.77\n",
            "iteration 925, epoch 2, batch 244/481,disc_loss 83.784, (real 87.728, fake 79.84 ) gen_loss 383.49\n",
            "iteration 926, epoch 2, batch 245/481,disc_loss 77.188, (real 80.867, fake 73.508 ) gen_loss 423.76\n",
            "iteration 927, epoch 2, batch 246/481,disc_loss 80.043, (real 84.747, fake 75.34 ) gen_loss 436.04\n",
            "iteration 928, epoch 2, batch 247/481,disc_loss 82.927, (real 87.079, fake 78.775 ) gen_loss 389.7\n",
            "iteration 929, epoch 2, batch 248/481,disc_loss 77.528, (real 82.533, fake 72.523 ) gen_loss 359.02\n",
            "iteration 930, epoch 2, batch 249/481,disc_loss 80.656, (real 85.631, fake 75.681 ) gen_loss 427.15\n",
            "iteration 931, epoch 2, batch 250/481,disc_loss 77.095, (real 81.047, fake 73.143 ) gen_loss 441.1\n",
            "iteration 932, epoch 2, batch 251/481,disc_loss 80.602, (real 84.607, fake 76.596 ) gen_loss 463.08\n",
            "iteration 933, epoch 2, batch 252/481,disc_loss 83.129, (real 86.737, fake 79.521 ) gen_loss 447.69\n",
            "iteration 934, epoch 2, batch 253/481,disc_loss 81.637, (real 85.339, fake 77.934 ) gen_loss 415.13\n",
            "iteration 935, epoch 2, batch 254/481,disc_loss 83.362, (real 85.746, fake 80.978 ) gen_loss 439.55\n",
            "iteration 936, epoch 2, batch 255/481,disc_loss 83.002, (real 85.399, fake 80.606 ) gen_loss 461.77\n",
            "iteration 937, epoch 2, batch 256/481,disc_loss 81.971, (real 84.344, fake 79.598 ) gen_loss 373.11\n",
            "iteration 938, epoch 2, batch 257/481,disc_loss 76.979, (real 80.543, fake 73.414 ) gen_loss 426.66\n",
            "iteration 939, epoch 2, batch 258/481,disc_loss 75.525, (real 78.395, fake 72.654 ) gen_loss 391.59\n",
            "iteration 940, epoch 2, batch 259/481,disc_loss 81.087, (real 86.836, fake 75.338 ) gen_loss 419.64\n",
            "iteration 941, epoch 2, batch 260/481,disc_loss 82.244, (real 87.366, fake 77.122 ) gen_loss 391.16\n",
            "iteration 942, epoch 2, batch 261/481,disc_loss 83.598, (real 88.403, fake 78.792 ) gen_loss 482.05\n",
            "iteration 943, epoch 2, batch 262/481,disc_loss 78.476, (real 81.7, fake 75.252 ) gen_loss 436.65\n",
            "iteration 944, epoch 2, batch 263/481,disc_loss 86.362, (real 91.117, fake 81.606 ) gen_loss 459.29\n",
            "iteration 945, epoch 2, batch 264/481,disc_loss 76.974, (real 81.32, fake 72.627 ) gen_loss 496.61\n",
            "iteration 946, epoch 2, batch 265/481,disc_loss 81.142, (real 85.141, fake 77.144 ) gen_loss 431.13\n",
            "iteration 947, epoch 2, batch 266/481,disc_loss 82.098, (real 86.291, fake 77.906 ) gen_loss 429.77\n",
            "iteration 948, epoch 2, batch 267/481,disc_loss 78.603, (real 82.814, fake 74.393 ) gen_loss 453.83\n",
            "iteration 949, epoch 2, batch 268/481,disc_loss 78.893, (real 81.105, fake 76.681 ) gen_loss 412.25\n",
            "iteration 950, epoch 2, batch 269/481,disc_loss 79.522, (real 82.601, fake 76.443 ) gen_loss 429.44\n",
            "iteration 951, epoch 2, batch 270/481,disc_loss 83.414, (real 87.12, fake 79.709 ) gen_loss 492.9\n",
            "iteration 952, epoch 2, batch 271/481,disc_loss 80.363, (real 84.504, fake 76.222 ) gen_loss 416.4\n",
            "iteration 953, epoch 2, batch 272/481,disc_loss 80.181, (real 83.24, fake 77.122 ) gen_loss 407.64\n",
            "iteration 954, epoch 2, batch 273/481,disc_loss 77.227, (real 80.792, fake 73.661 ) gen_loss 462.05\n",
            "iteration 955, epoch 2, batch 274/481,disc_loss 78.179, (real 82.332, fake 74.027 ) gen_loss 461.57\n",
            "iteration 956, epoch 2, batch 275/481,disc_loss 79.11, (real 84.856, fake 73.364 ) gen_loss 435.22\n",
            "iteration 957, epoch 2, batch 276/481,disc_loss 84.683, (real 89.447, fake 79.92 ) gen_loss 410.12\n",
            "iteration 958, epoch 2, batch 277/481,disc_loss 80.291, (real 85.718, fake 74.864 ) gen_loss 436.69\n",
            "iteration 959, epoch 2, batch 278/481,disc_loss 80.732, (real 85.025, fake 76.44 ) gen_loss 433.55\n",
            "iteration 960, epoch 2, batch 279/481,disc_loss 81.575, (real 85.018, fake 78.132 ) gen_loss 420.17\n",
            "iteration 961, epoch 2, batch 280/481,disc_loss 80.659, (real 84.674, fake 76.644 ) gen_loss 400.64\n",
            "iteration 962, epoch 2, batch 281/481,disc_loss 82.625, (real 86.885, fake 78.365 ) gen_loss 405.76\n",
            "iteration 963, epoch 2, batch 282/481,disc_loss 82.168, (real 86.815, fake 77.521 ) gen_loss 422.04\n",
            "iteration 964, epoch 2, batch 283/481,disc_loss 86.962, (real 90.464, fake 83.459 ) gen_loss 450.21\n",
            "iteration 965, epoch 2, batch 284/481,disc_loss 86.999, (real 91.858, fake 82.141 ) gen_loss 435.46\n",
            "iteration 966, epoch 2, batch 285/481,disc_loss 82.041, (real 86.16, fake 77.923 ) gen_loss 444.06\n",
            "iteration 967, epoch 2, batch 286/481,disc_loss 80.92, (real 83.48, fake 78.36 ) gen_loss 442.59\n",
            "iteration 968, epoch 2, batch 287/481,disc_loss 77.025, (real 81.922, fake 72.127 ) gen_loss 488.59\n",
            "iteration 969, epoch 2, batch 288/481,disc_loss 85.524, (real 88.232, fake 82.816 ) gen_loss 577.63\n",
            "iteration 970, epoch 2, batch 289/481,disc_loss 85.52, (real 90.399, fake 80.641 ) gen_loss 448.26\n",
            "iteration 971, epoch 2, batch 290/481,disc_loss 78.637, (real 83.58, fake 73.694 ) gen_loss 502.21\n",
            "iteration 972, epoch 2, batch 291/481,disc_loss 76.516, (real 80.911, fake 72.121 ) gen_loss 466.22\n",
            "iteration 973, epoch 2, batch 292/481,disc_loss 81.509, (real 86.429, fake 76.589 ) gen_loss 445.33\n",
            "iteration 974, epoch 2, batch 293/481,disc_loss 83.217, (real 88.265, fake 78.169 ) gen_loss 486.08\n",
            "iteration 975, epoch 2, batch 294/481,disc_loss 76.698, (real 81.451, fake 71.945 ) gen_loss 422.87\n",
            "iteration 976, epoch 2, batch 295/481,disc_loss 83.118, (real 85.914, fake 80.321 ) gen_loss 396.02\n",
            "iteration 977, epoch 2, batch 296/481,disc_loss 75.918, (real 80.418, fake 71.418 ) gen_loss 483.87\n",
            "iteration 978, epoch 2, batch 297/481,disc_loss 72.599, (real 76.616, fake 68.581 ) gen_loss 425.3\n",
            "iteration 979, epoch 2, batch 298/481,disc_loss 84.701, (real 89.442, fake 79.96 ) gen_loss 516.15\n",
            "iteration 980, epoch 2, batch 299/481,disc_loss 85.335, (real 88.253, fake 82.416 ) gen_loss 433.94\n",
            "iteration 981, epoch 2, batch 300/481,disc_loss 83.406, (real 88.761, fake 78.051 ) gen_loss 408.61\n",
            "iteration 982, epoch 2, batch 301/481,disc_loss 79.181, (real 83.532, fake 74.83 ) gen_loss 419.37\n",
            "iteration 983, epoch 2, batch 302/481,disc_loss 78.679, (real 82.383, fake 74.975 ) gen_loss 451.63\n",
            "iteration 984, epoch 2, batch 303/481,disc_loss 77.232, (real 82.174, fake 72.289 ) gen_loss 478.73\n",
            "iteration 985, epoch 2, batch 304/481,disc_loss 88.095, (real 92.47, fake 83.72 ) gen_loss 439.02\n",
            "iteration 986, epoch 2, batch 305/481,disc_loss 82.597, (real 86.732, fake 78.461 ) gen_loss 399.85\n",
            "iteration 987, epoch 2, batch 306/481,disc_loss 80.33, (real 84.049, fake 76.612 ) gen_loss 417.6\n",
            "iteration 988, epoch 2, batch 307/481,disc_loss 80.919, (real 84.807, fake 77.031 ) gen_loss 407.71\n",
            "iteration 989, epoch 2, batch 308/481,disc_loss 77.422, (real 81.831, fake 73.013 ) gen_loss 482.56\n",
            "iteration 990, epoch 2, batch 309/481,disc_loss 79.383, (real 83.591, fake 75.176 ) gen_loss 456.97\n",
            "iteration 991, epoch 2, batch 310/481,disc_loss 80.708, (real 83.922, fake 77.494 ) gen_loss 418.27\n",
            "iteration 992, epoch 2, batch 311/481,disc_loss 83.922, (real 87.064, fake 80.78 ) gen_loss 459.75\n",
            "iteration 993, epoch 2, batch 312/481,disc_loss 78.015, (real 81.063, fake 74.967 ) gen_loss 438.43\n",
            "iteration 994, epoch 2, batch 313/481,disc_loss 77.174, (real 82.017, fake 72.33 ) gen_loss 426.19\n",
            "iteration 995, epoch 2, batch 314/481,disc_loss 76.795, (real 80.84, fake 72.75 ) gen_loss 425.59\n",
            "iteration 996, epoch 2, batch 315/481,disc_loss 81.961, (real 85.972, fake 77.951 ) gen_loss 414.23\n",
            "iteration 997, epoch 2, batch 316/481,disc_loss 77.568, (real 80.611, fake 74.525 ) gen_loss 428.52\n",
            "iteration 998, epoch 2, batch 317/481,disc_loss 74.235, (real 79.052, fake 69.419 ) gen_loss 457.62\n",
            "iteration 999, epoch 2, batch 318/481,disc_loss 80.723, (real 85.627, fake 75.82 ) gen_loss 471.74\n",
            "iteration 1000, epoch 2, batch 319/481,disc_loss 79.066, (real 82.036, fake 76.096 ) gen_loss 469.13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1001, epoch 2, batch 320/481,disc_loss 79.649, (real 84.291, fake 75.006 ) gen_loss 446.46\n",
            "iteration 1002, epoch 2, batch 321/481,disc_loss 82.232, (real 86.276, fake 78.189 ) gen_loss 449.26\n",
            "iteration 1003, epoch 2, batch 322/481,disc_loss 82.469, (real 88.029, fake 76.909 ) gen_loss 421.46\n",
            "iteration 1004, epoch 2, batch 323/481,disc_loss 84.709, (real 88.92, fake 80.497 ) gen_loss 435.53\n",
            "iteration 1005, epoch 2, batch 324/481,disc_loss 78.074, (real 82.958, fake 73.191 ) gen_loss 439.35\n",
            "iteration 1006, epoch 2, batch 325/481,disc_loss 83.443, (real 87.689, fake 79.197 ) gen_loss 428.1\n",
            "iteration 1007, epoch 2, batch 326/481,disc_loss 80.404, (real 84.631, fake 76.177 ) gen_loss 442.86\n",
            "iteration 1008, epoch 2, batch 327/481,disc_loss 77.998, (real 83.399, fake 72.597 ) gen_loss 423.13\n",
            "iteration 1009, epoch 2, batch 328/481,disc_loss 81.236, (real 86.254, fake 76.218 ) gen_loss 457.93\n",
            "iteration 1010, epoch 2, batch 329/481,disc_loss 79.275, (real 84.045, fake 74.505 ) gen_loss 467.58\n",
            "iteration 1011, epoch 2, batch 330/481,disc_loss 84.683, (real 88.4, fake 80.967 ) gen_loss 485.15\n",
            "iteration 1012, epoch 2, batch 331/481,disc_loss 80.296, (real 83.91, fake 76.682 ) gen_loss 461.32\n",
            "iteration 1013, epoch 2, batch 332/481,disc_loss 83.703, (real 88.966, fake 78.439 ) gen_loss 512.26\n",
            "iteration 1014, epoch 2, batch 333/481,disc_loss 80.605, (real 83.654, fake 77.556 ) gen_loss 423.06\n",
            "iteration 1015, epoch 2, batch 334/481,disc_loss 78.971, (real 84.197, fake 73.745 ) gen_loss 442.62\n",
            "iteration 1016, epoch 2, batch 335/481,disc_loss 81.543, (real 85.914, fake 77.173 ) gen_loss 507.77\n",
            "iteration 1017, epoch 2, batch 336/481,disc_loss 80.386, (real 85.345, fake 75.427 ) gen_loss 457.39\n",
            "iteration 1018, epoch 2, batch 337/481,disc_loss 81.88, (real 86.309, fake 77.452 ) gen_loss 454.81\n",
            "iteration 1019, epoch 2, batch 338/481,disc_loss 77.36, (real 81.248, fake 73.471 ) gen_loss 447.65\n",
            "iteration 1020, epoch 2, batch 339/481,disc_loss 80.489, (real 84.345, fake 76.632 ) gen_loss 428.22\n",
            "iteration 1021, epoch 2, batch 340/481,disc_loss 78.515, (real 83.41, fake 73.621 ) gen_loss 412.21\n",
            "iteration 1022, epoch 2, batch 341/481,disc_loss 81.334, (real 85.116, fake 77.552 ) gen_loss 417.89\n",
            "iteration 1023, epoch 2, batch 342/481,disc_loss 78.727, (real 83.321, fake 74.133 ) gen_loss 442.49\n",
            "iteration 1024, epoch 2, batch 343/481,disc_loss 84.012, (real 88.299, fake 79.725 ) gen_loss 420.1\n",
            "iteration 1025, epoch 2, batch 344/481,disc_loss 79.226, (real 82.316, fake 76.136 ) gen_loss 482.79\n",
            "iteration 1026, epoch 2, batch 345/481,disc_loss 79.769, (real 84.41, fake 75.128 ) gen_loss 467.92\n",
            "iteration 1027, epoch 2, batch 346/481,disc_loss 76.525, (real 80.093, fake 72.958 ) gen_loss 437.7\n",
            "iteration 1028, epoch 2, batch 347/481,disc_loss 79.844, (real 84.297, fake 75.392 ) gen_loss 429.51\n",
            "iteration 1029, epoch 2, batch 348/481,disc_loss 81.652, (real 85.733, fake 77.571 ) gen_loss 385.75\n",
            "iteration 1030, epoch 2, batch 349/481,disc_loss 73.955, (real 78.738, fake 69.172 ) gen_loss 416.36\n",
            "iteration 1031, epoch 2, batch 350/481,disc_loss 84.019, (real 88.251, fake 79.787 ) gen_loss 444.42\n",
            "iteration 1032, epoch 2, batch 351/481,disc_loss 82.343, (real 87.308, fake 77.377 ) gen_loss 449.24\n",
            "iteration 1033, epoch 2, batch 352/481,disc_loss 84.707, (real 88.032, fake 81.383 ) gen_loss 436.58\n",
            "iteration 1034, epoch 2, batch 353/481,disc_loss 83.98, (real 87.514, fake 80.446 ) gen_loss 445.1\n",
            "iteration 1035, epoch 2, batch 354/481,disc_loss 82.155, (real 85.856, fake 78.454 ) gen_loss 504.99\n",
            "iteration 1036, epoch 2, batch 355/481,disc_loss 80.499, (real 84.612, fake 76.387 ) gen_loss 429.02\n",
            "iteration 1037, epoch 2, batch 356/481,disc_loss 83.01, (real 86.938, fake 79.081 ) gen_loss 383.56\n",
            "iteration 1038, epoch 2, batch 357/481,disc_loss 77.208, (real 81.224, fake 73.192 ) gen_loss 414.02\n",
            "iteration 1039, epoch 2, batch 358/481,disc_loss 79.01, (real 81.851, fake 76.168 ) gen_loss 450.84\n",
            "iteration 1040, epoch 2, batch 359/481,disc_loss 81.977, (real 84.632, fake 79.323 ) gen_loss 413.24\n",
            "iteration 1041, epoch 2, batch 360/481,disc_loss 80.374, (real 84.236, fake 76.511 ) gen_loss 471.67\n",
            "iteration 1042, epoch 2, batch 361/481,disc_loss 74.631, (real 78.831, fake 70.432 ) gen_loss 386.42\n",
            "iteration 1043, epoch 2, batch 362/481,disc_loss 83.418, (real 87.657, fake 79.179 ) gen_loss 413.14\n",
            "iteration 1044, epoch 2, batch 363/481,disc_loss 78.311, (real 82.822, fake 73.8 ) gen_loss 419.41\n",
            "iteration 1045, epoch 2, batch 364/481,disc_loss 80.076, (real 82.873, fake 77.28 ) gen_loss 430.78\n",
            "iteration 1046, epoch 2, batch 365/481,disc_loss 74.862, (real 78.829, fake 70.896 ) gen_loss 401.77\n",
            "iteration 1047, epoch 2, batch 366/481,disc_loss 78.118, (real 83.275, fake 72.961 ) gen_loss 456.28\n",
            "iteration 1048, epoch 2, batch 367/481,disc_loss 81.016, (real 84.697, fake 77.336 ) gen_loss 420.14\n",
            "iteration 1049, epoch 2, batch 368/481,disc_loss 79.873, (real 84.982, fake 74.764 ) gen_loss 399.46\n",
            "iteration 1050, epoch 2, batch 369/481,disc_loss 82.839, (real 87.353, fake 78.326 ) gen_loss 403.38\n",
            "iteration 1051, epoch 2, batch 370/481,disc_loss 79.911, (real 83.333, fake 76.489 ) gen_loss 441.26\n",
            "iteration 1052, epoch 2, batch 371/481,disc_loss 82.348, (real 85.774, fake 78.923 ) gen_loss 416.87\n",
            "iteration 1053, epoch 2, batch 372/481,disc_loss 77.518, (real 81.183, fake 73.853 ) gen_loss 378.46\n",
            "iteration 1054, epoch 2, batch 373/481,disc_loss 86.478, (real 90.622, fake 82.333 ) gen_loss 549.38\n",
            "iteration 1055, epoch 2, batch 374/481,disc_loss 81.181, (real 85.857, fake 76.505 ) gen_loss 464.19\n",
            "iteration 1056, epoch 2, batch 375/481,disc_loss 81.255, (real 84.67, fake 77.84 ) gen_loss 481.13\n",
            "iteration 1057, epoch 2, batch 376/481,disc_loss 81.487, (real 85.749, fake 77.225 ) gen_loss 458.45\n",
            "iteration 1058, epoch 2, batch 377/481,disc_loss 77.784, (real 82.409, fake 73.159 ) gen_loss 433.02\n",
            "iteration 1059, epoch 2, batch 378/481,disc_loss 77.384, (real 80.873, fake 73.896 ) gen_loss 483.21\n",
            "iteration 1060, epoch 2, batch 379/481,disc_loss 80.435, (real 84.513, fake 76.357 ) gen_loss 436.41\n",
            "iteration 1061, epoch 2, batch 380/481,disc_loss 80.904, (real 85.053, fake 76.755 ) gen_loss 415.39\n",
            "iteration 1062, epoch 2, batch 381/481,disc_loss 78.44, (real 81.641, fake 75.238 ) gen_loss 430.43\n",
            "iteration 1063, epoch 2, batch 382/481,disc_loss 77.649, (real 80.171, fake 75.128 ) gen_loss 382.08\n",
            "iteration 1064, epoch 2, batch 383/481,disc_loss 77.345, (real 80.975, fake 73.715 ) gen_loss 398.13\n",
            "iteration 1065, epoch 2, batch 384/481,disc_loss 77.738, (real 83.368, fake 72.109 ) gen_loss 421.2\n",
            "iteration 1066, epoch 2, batch 385/481,disc_loss 76.381, (real 81.157, fake 71.606 ) gen_loss 451.56\n",
            "iteration 1067, epoch 2, batch 386/481,disc_loss 76.278, (real 81.268, fake 71.287 ) gen_loss 483.19\n",
            "iteration 1068, epoch 2, batch 387/481,disc_loss 83.689, (real 88.545, fake 78.832 ) gen_loss 464.65\n",
            "iteration 1069, epoch 2, batch 388/481,disc_loss 80.541, (real 84.24, fake 76.841 ) gen_loss 454.98\n",
            "iteration 1070, epoch 2, batch 389/481,disc_loss 75.876, (real 79.816, fake 71.935 ) gen_loss 431.69\n",
            "iteration 1071, epoch 2, batch 390/481,disc_loss 87.302, (real 91.585, fake 83.02 ) gen_loss 441.99\n",
            "iteration 1072, epoch 2, batch 391/481,disc_loss 73.027, (real 76.978, fake 69.076 ) gen_loss 439.66\n",
            "iteration 1073, epoch 2, batch 392/481,disc_loss 77.81, (real 81.796, fake 73.824 ) gen_loss 401.43\n",
            "iteration 1074, epoch 2, batch 393/481,disc_loss 79.491, (real 83.014, fake 75.969 ) gen_loss 446.02\n",
            "iteration 1075, epoch 2, batch 394/481,disc_loss 83.592, (real 87.559, fake 79.626 ) gen_loss 450.53\n",
            "iteration 1076, epoch 2, batch 395/481,disc_loss 81.252, (real 85.212, fake 77.292 ) gen_loss 492.15\n",
            "iteration 1077, epoch 2, batch 396/481,disc_loss 82.021, (real 85.635, fake 78.406 ) gen_loss 441.38\n",
            "iteration 1078, epoch 2, batch 397/481,disc_loss 72.12, (real 76.217, fake 68.023 ) gen_loss 480.09\n",
            "iteration 1079, epoch 2, batch 398/481,disc_loss 81.786, (real 86.304, fake 77.269 ) gen_loss 473.98\n",
            "iteration 1080, epoch 2, batch 399/481,disc_loss 81.362, (real 85.662, fake 77.062 ) gen_loss 406.83\n",
            "iteration 1081, epoch 2, batch 400/481,disc_loss 77.775, (real 81.003, fake 74.548 ) gen_loss 404.38\n",
            "iteration 1082, epoch 2, batch 401/481,disc_loss 83.48, (real 86.609, fake 80.351 ) gen_loss 400.32\n",
            "iteration 1083, epoch 2, batch 402/481,disc_loss 78.301, (real 82.324, fake 74.278 ) gen_loss 397.02\n",
            "iteration 1084, epoch 2, batch 403/481,disc_loss 84.725, (real 87.494, fake 81.956 ) gen_loss 412.69\n",
            "iteration 1085, epoch 2, batch 404/481,disc_loss 78.627, (real 81.842, fake 75.412 ) gen_loss 390.02\n",
            "iteration 1086, epoch 2, batch 405/481,disc_loss 78.196, (real 81.018, fake 75.373 ) gen_loss 407.16\n",
            "iteration 1087, epoch 2, batch 406/481,disc_loss 82.961, (real 86.873, fake 79.049 ) gen_loss 400.73\n",
            "iteration 1088, epoch 2, batch 407/481,disc_loss 81.448, (real 85.684, fake 77.212 ) gen_loss 379.59\n",
            "iteration 1089, epoch 2, batch 408/481,disc_loss 78.57, (real 83.097, fake 74.042 ) gen_loss 430.9\n",
            "iteration 1090, epoch 2, batch 409/481,disc_loss 81.172, (real 85.184, fake 77.159 ) gen_loss 380.92\n",
            "iteration 1091, epoch 2, batch 410/481,disc_loss 79.256, (real 82.653, fake 75.859 ) gen_loss 476.55\n",
            "iteration 1092, epoch 2, batch 411/481,disc_loss 79.388, (real 83.545, fake 75.232 ) gen_loss 424.04\n",
            "iteration 1093, epoch 2, batch 412/481,disc_loss 81.469, (real 84.126, fake 78.812 ) gen_loss 437.44\n",
            "iteration 1094, epoch 2, batch 413/481,disc_loss 82.84, (real 86.854, fake 78.826 ) gen_loss 430.71\n",
            "iteration 1095, epoch 2, batch 414/481,disc_loss 80.134, (real 83.599, fake 76.669 ) gen_loss 401.49\n",
            "iteration 1096, epoch 2, batch 415/481,disc_loss 79.818, (real 84.209, fake 75.428 ) gen_loss 445.88\n",
            "iteration 1097, epoch 2, batch 416/481,disc_loss 78.174, (real 81.574, fake 74.774 ) gen_loss 439.32\n",
            "iteration 1098, epoch 2, batch 417/481,disc_loss 80.215, (real 84.13, fake 76.3 ) gen_loss 450.53\n",
            "iteration 1099, epoch 2, batch 418/481,disc_loss 80.632, (real 83.932, fake 77.332 ) gen_loss 422.62\n",
            "iteration 1100, epoch 2, batch 419/481,disc_loss 81.025, (real 85.432, fake 76.619 ) gen_loss 460.2\n",
            "iteration 1101, epoch 2, batch 420/481,disc_loss 80.557, (real 83.305, fake 77.809 ) gen_loss 477.09\n",
            "iteration 1102, epoch 2, batch 421/481,disc_loss 84.056, (real 88.709, fake 79.403 ) gen_loss 487.93\n",
            "iteration 1103, epoch 2, batch 422/481,disc_loss 80.4, (real 85.279, fake 75.522 ) gen_loss 513.73\n",
            "iteration 1104, epoch 2, batch 423/481,disc_loss 77.882, (real 81.643, fake 74.121 ) gen_loss 491.84\n",
            "iteration 1105, epoch 2, batch 424/481,disc_loss 81.659, (real 84.763, fake 78.555 ) gen_loss 436.65\n",
            "iteration 1106, epoch 2, batch 425/481,disc_loss 85.78, (real 90.195, fake 81.365 ) gen_loss 447.14\n",
            "iteration 1107, epoch 2, batch 426/481,disc_loss 79.066, (real 83.137, fake 74.994 ) gen_loss 405.07\n",
            "iteration 1108, epoch 2, batch 427/481,disc_loss 75.933, (real 79.092, fake 72.773 ) gen_loss 465.12\n",
            "iteration 1109, epoch 2, batch 428/481,disc_loss 80.721, (real 84.62, fake 76.822 ) gen_loss 449.55\n",
            "iteration 1110, epoch 2, batch 429/481,disc_loss 83.254, (real 86.629, fake 79.878 ) gen_loss 439.09\n",
            "iteration 1111, epoch 2, batch 430/481,disc_loss 82.048, (real 86.34, fake 77.755 ) gen_loss 395.37\n",
            "iteration 1112, epoch 2, batch 431/481,disc_loss 82.307, (real 87.295, fake 77.32 ) gen_loss 391.17\n",
            "iteration 1113, epoch 2, batch 432/481,disc_loss 78.697, (real 83.826, fake 73.568 ) gen_loss 451.44\n",
            "iteration 1114, epoch 2, batch 433/481,disc_loss 77.532, (real 81.174, fake 73.889 ) gen_loss 492.47\n",
            "iteration 1115, epoch 2, batch 434/481,disc_loss 78.771, (real 82.08, fake 75.462 ) gen_loss 484.37\n",
            "iteration 1116, epoch 2, batch 435/481,disc_loss 78.894, (real 83.567, fake 74.221 ) gen_loss 437.6\n",
            "iteration 1117, epoch 2, batch 436/481,disc_loss 80.23, (real 85.895, fake 74.564 ) gen_loss 487.97\n",
            "iteration 1118, epoch 2, batch 437/481,disc_loss 79.44, (real 84.441, fake 74.44 ) gen_loss 480.47\n",
            "iteration 1119, epoch 2, batch 438/481,disc_loss 75.44, (real 79.29, fake 71.589 ) gen_loss 453.63\n",
            "iteration 1120, epoch 2, batch 439/481,disc_loss 84.135, (real 87.816, fake 80.454 ) gen_loss 496.39\n",
            "iteration 1121, epoch 2, batch 440/481,disc_loss 82.45, (real 86.064, fake 78.836 ) gen_loss 459.21\n",
            "iteration 1122, epoch 2, batch 441/481,disc_loss 78.964, (real 82.686, fake 75.243 ) gen_loss 425.35\n",
            "iteration 1123, epoch 2, batch 442/481,disc_loss 79.577, (real 83.099, fake 76.056 ) gen_loss 451.7\n",
            "iteration 1124, epoch 2, batch 443/481,disc_loss 81.526, (real 84.97, fake 78.082 ) gen_loss 517.09\n",
            "iteration 1125, epoch 2, batch 444/481,disc_loss 75.239, (real 79.081, fake 71.398 ) gen_loss 437.85\n",
            "iteration 1126, epoch 2, batch 445/481,disc_loss 81.511, (real 85.716, fake 77.305 ) gen_loss 492.37\n",
            "iteration 1127, epoch 2, batch 446/481,disc_loss 85.372, (real 89.876, fake 80.867 ) gen_loss 543.36\n",
            "iteration 1128, epoch 2, batch 447/481,disc_loss 77.745, (real 80.906, fake 74.584 ) gen_loss 435.4\n",
            "iteration 1129, epoch 2, batch 448/481,disc_loss 88.122, (real 91.415, fake 84.829 ) gen_loss 630.13\n",
            "iteration 1130, epoch 2, batch 449/481,disc_loss 90.954, (real 90.059, fake 91.85 ) gen_loss 476.23\n",
            "iteration 1131, epoch 2, batch 450/481,disc_loss 90.008, (real 91.871, fake 88.145 ) gen_loss 622.36\n",
            "iteration 1132, epoch 2, batch 451/481,disc_loss 84.265, (real 87.36, fake 81.169 ) gen_loss 536.09\n",
            "iteration 1133, epoch 2, batch 452/481,disc_loss 82.507, (real 86.118, fake 78.897 ) gen_loss 473.87\n",
            "iteration 1134, epoch 2, batch 453/481,disc_loss 81.267, (real 84.8, fake 77.735 ) gen_loss 432.76\n",
            "iteration 1135, epoch 2, batch 454/481,disc_loss 79.44, (real 83.564, fake 75.316 ) gen_loss 416.44\n",
            "iteration 1136, epoch 2, batch 455/481,disc_loss 81.168, (real 85.172, fake 77.164 ) gen_loss 417.33\n",
            "iteration 1137, epoch 2, batch 456/481,disc_loss 86.144, (real 88.841, fake 83.446 ) gen_loss 419.31\n",
            "iteration 1138, epoch 2, batch 457/481,disc_loss 81.999, (real 86.345, fake 77.653 ) gen_loss 423.29\n",
            "iteration 1139, epoch 2, batch 458/481,disc_loss 80.869, (real 84.245, fake 77.492 ) gen_loss 412.96\n",
            "iteration 1140, epoch 2, batch 459/481,disc_loss 86.522, (real 91.418, fake 81.626 ) gen_loss 399.0\n",
            "iteration 1141, epoch 2, batch 460/481,disc_loss 79.489, (real 83.79, fake 75.189 ) gen_loss 473.27\n",
            "iteration 1142, epoch 2, batch 461/481,disc_loss 81.013, (real 84.05, fake 77.977 ) gen_loss 475.58\n",
            "iteration 1143, epoch 2, batch 462/481,disc_loss 83.684, (real 89.151, fake 78.216 ) gen_loss 449.53\n",
            "iteration 1144, epoch 2, batch 463/481,disc_loss 79.573, (real 84.714, fake 74.432 ) gen_loss 461.43\n",
            "iteration 1145, epoch 2, batch 464/481,disc_loss 81.042, (real 85.209, fake 76.875 ) gen_loss 442.12\n",
            "iteration 1146, epoch 2, batch 465/481,disc_loss 80.636, (real 84.374, fake 76.899 ) gen_loss 471.89\n",
            "iteration 1147, epoch 2, batch 466/481,disc_loss 78.356, (real 81.91, fake 74.802 ) gen_loss 460.91\n",
            "iteration 1148, epoch 2, batch 467/481,disc_loss 79.42, (real 84.361, fake 74.479 ) gen_loss 516.47\n",
            "iteration 1149, epoch 2, batch 468/481,disc_loss 79.551, (real 82.501, fake 76.6 ) gen_loss 511.62\n",
            "iteration 1150, epoch 2, batch 469/481,disc_loss 81.898, (real 85.908, fake 77.888 ) gen_loss 415.14\n",
            "iteration 1151, epoch 2, batch 470/481,disc_loss 81.687, (real 86.654, fake 76.72 ) gen_loss 489.36\n",
            "iteration 1152, epoch 2, batch 471/481,disc_loss 76.3, (real 81.29, fake 71.309 ) gen_loss 440.45\n",
            "iteration 1153, epoch 2, batch 472/481,disc_loss 78.661, (real 82.756, fake 74.567 ) gen_loss 478.25\n",
            "iteration 1154, epoch 2, batch 473/481,disc_loss 81.555, (real 84.835, fake 78.276 ) gen_loss 425.27\n",
            "iteration 1155, epoch 2, batch 474/481,disc_loss 79.529, (real 83.804, fake 75.254 ) gen_loss 416.55\n",
            "iteration 1156, epoch 2, batch 475/481,disc_loss 82.857, (real 86.444, fake 79.269 ) gen_loss 445.62\n",
            "iteration 1157, epoch 2, batch 476/481,disc_loss 81.383, (real 84.638, fake 78.127 ) gen_loss 453.93\n",
            "iteration 1158, epoch 2, batch 477/481,disc_loss 79.791, (real 83.096, fake 76.487 ) gen_loss 470.29\n",
            "iteration 1159, epoch 2, batch 478/481,disc_loss 78.787, (real 82.262, fake 75.313 ) gen_loss 423.12\n",
            "iteration 1160, epoch 2, batch 479/481,disc_loss 80.705, (real 85.535, fake 75.876 ) gen_loss 500.68\n",
            "iteration 1161, epoch 2, batch 480/481,disc_loss 78.771, (real 83.323, fake 74.218 ) gen_loss 476.68\n",
            "iteration 1162, epoch 2, batch 481/481,disc_loss 74.903, (real 77.79, fake 72.016 ) gen_loss 453.21\n",
            "iteration 1163, epoch 3, batch 1/481,disc_loss 83.334, (real 87.891, fake 78.776 ) gen_loss 571.59\n",
            "iteration 1164, epoch 3, batch 2/481,disc_loss 81.971, (real 84.732, fake 79.209 ) gen_loss 533.66\n",
            "iteration 1165, epoch 3, batch 3/481,disc_loss 82.195, (real 85.912, fake 78.478 ) gen_loss 504.57\n",
            "iteration 1166, epoch 3, batch 4/481,disc_loss 82.818, (real 85.655, fake 79.981 ) gen_loss 537.15\n",
            "iteration 1167, epoch 3, batch 5/481,disc_loss 79.28, (real 82.796, fake 75.764 ) gen_loss 479.22\n",
            "iteration 1168, epoch 3, batch 6/481,disc_loss 82.375, (real 85.176, fake 79.575 ) gen_loss 437.86\n",
            "iteration 1169, epoch 3, batch 7/481,disc_loss 83.495, (real 87.646, fake 79.343 ) gen_loss 375.53\n",
            "iteration 1170, epoch 3, batch 8/481,disc_loss 80.732, (real 85.471, fake 75.993 ) gen_loss 467.98\n",
            "iteration 1171, epoch 3, batch 9/481,disc_loss 81.073, (real 84.69, fake 77.455 ) gen_loss 450.16\n",
            "iteration 1172, epoch 3, batch 10/481,disc_loss 79.301, (real 82.969, fake 75.632 ) gen_loss 482.74\n",
            "iteration 1173, epoch 3, batch 11/481,disc_loss 76.09, (real 80.435, fake 71.744 ) gen_loss 470.17\n",
            "iteration 1174, epoch 3, batch 12/481,disc_loss 81.593, (real 85.259, fake 77.926 ) gen_loss 442.37\n",
            "iteration 1175, epoch 3, batch 13/481,disc_loss 82.161, (real 86.115, fake 78.207 ) gen_loss 467.22\n",
            "iteration 1176, epoch 3, batch 14/481,disc_loss 79.286, (real 82.596, fake 75.976 ) gen_loss 426.87\n",
            "iteration 1177, epoch 3, batch 15/481,disc_loss 85.691, (real 89.983, fake 81.399 ) gen_loss 481.22\n",
            "iteration 1178, epoch 3, batch 16/481,disc_loss 77.598, (real 81.727, fake 73.469 ) gen_loss 444.08\n",
            "iteration 1179, epoch 3, batch 17/481,disc_loss 79.928, (real 85.425, fake 74.431 ) gen_loss 471.5\n",
            "iteration 1180, epoch 3, batch 18/481,disc_loss 81.513, (real 85.281, fake 77.746 ) gen_loss 454.18\n",
            "iteration 1181, epoch 3, batch 19/481,disc_loss 80.865, (real 85.247, fake 76.482 ) gen_loss 453.77\n",
            "iteration 1182, epoch 3, batch 20/481,disc_loss 78.157, (real 81.882, fake 74.432 ) gen_loss 443.98\n",
            "iteration 1183, epoch 3, batch 21/481,disc_loss 77.251, (real 79.829, fake 74.673 ) gen_loss 442.09\n",
            "iteration 1184, epoch 3, batch 22/481,disc_loss 81.347, (real 84.138, fake 78.557 ) gen_loss 438.32\n",
            "iteration 1185, epoch 3, batch 23/481,disc_loss 77.883, (real 82.361, fake 73.404 ) gen_loss 459.0\n",
            "iteration 1186, epoch 3, batch 24/481,disc_loss 80.639, (real 83.783, fake 77.496 ) gen_loss 443.58\n",
            "iteration 1187, epoch 3, batch 25/481,disc_loss 79.322, (real 83.204, fake 75.44 ) gen_loss 471.5\n",
            "iteration 1188, epoch 3, batch 26/481,disc_loss 79.652, (real 82.739, fake 76.566 ) gen_loss 435.71\n",
            "iteration 1189, epoch 3, batch 27/481,disc_loss 81.896, (real 86.0, fake 77.792 ) gen_loss 482.5\n",
            "iteration 1190, epoch 3, batch 28/481,disc_loss 81.527, (real 85.593, fake 77.462 ) gen_loss 454.87\n",
            "iteration 1191, epoch 3, batch 29/481,disc_loss 80.786, (real 84.169, fake 77.402 ) gen_loss 624.43\n",
            "iteration 1192, epoch 3, batch 30/481,disc_loss 87.627, (real 90.795, fake 84.459 ) gen_loss 749.21\n",
            "iteration 1193, epoch 3, batch 31/481,disc_loss 78.414, (real 81.861, fake 74.967 ) gen_loss 460.66\n",
            "iteration 1194, epoch 3, batch 32/481,disc_loss 77.524, (real 79.729, fake 75.32 ) gen_loss 493.54\n",
            "iteration 1195, epoch 3, batch 33/481,disc_loss 79.326, (real 82.372, fake 76.279 ) gen_loss 490.04\n",
            "iteration 1196, epoch 3, batch 34/481,disc_loss 77.689, (real 80.912, fake 74.466 ) gen_loss 425.11\n",
            "iteration 1197, epoch 3, batch 35/481,disc_loss 78.28, (real 81.464, fake 75.096 ) gen_loss 432.4\n",
            "iteration 1198, epoch 3, batch 36/481,disc_loss 80.416, (real 83.891, fake 76.94 ) gen_loss 430.1\n",
            "iteration 1199, epoch 3, batch 37/481,disc_loss 79.193, (real 82.232, fake 76.154 ) gen_loss 441.32\n",
            "iteration 1200, epoch 3, batch 38/481,disc_loss 76.888, (real 81.039, fake 72.737 ) gen_loss 441.07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1201, epoch 3, batch 39/481,disc_loss 78.97, (real 83.884, fake 74.056 ) gen_loss 474.5\n",
            "iteration 1202, epoch 3, batch 40/481,disc_loss 80.173, (real 85.168, fake 75.179 ) gen_loss 429.78\n",
            "iteration 1203, epoch 3, batch 41/481,disc_loss 83.34, (real 86.446, fake 80.234 ) gen_loss 449.55\n",
            "iteration 1204, epoch 3, batch 42/481,disc_loss 81.65, (real 85.533, fake 77.767 ) gen_loss 457.41\n",
            "iteration 1205, epoch 3, batch 43/481,disc_loss 78.542, (real 80.89, fake 76.195 ) gen_loss 461.65\n",
            "iteration 1206, epoch 3, batch 44/481,disc_loss 76.726, (real 80.798, fake 72.654 ) gen_loss 445.29\n",
            "iteration 1207, epoch 3, batch 45/481,disc_loss 77.351, (real 81.603, fake 73.1 ) gen_loss 441.62\n",
            "iteration 1208, epoch 3, batch 46/481,disc_loss 83.065, (real 86.152, fake 79.979 ) gen_loss 449.13\n",
            "iteration 1209, epoch 3, batch 47/481,disc_loss 78.051, (real 81.926, fake 74.175 ) gen_loss 485.73\n",
            "iteration 1210, epoch 3, batch 48/481,disc_loss 80.532, (real 84.396, fake 76.668 ) gen_loss 476.72\n",
            "iteration 1211, epoch 3, batch 49/481,disc_loss 82.033, (real 87.383, fake 76.683 ) gen_loss 480.29\n",
            "iteration 1212, epoch 3, batch 50/481,disc_loss 79.287, (real 82.066, fake 76.508 ) gen_loss 448.25\n",
            "iteration 1213, epoch 3, batch 51/481,disc_loss 79.952, (real 83.299, fake 76.605 ) gen_loss 459.19\n",
            "iteration 1214, epoch 3, batch 52/481,disc_loss 80.476, (real 83.599, fake 77.353 ) gen_loss 440.3\n",
            "iteration 1215, epoch 3, batch 53/481,disc_loss 79.041, (real 82.278, fake 75.804 ) gen_loss 459.79\n",
            "iteration 1216, epoch 3, batch 54/481,disc_loss 77.962, (real 81.034, fake 74.889 ) gen_loss 530.39\n",
            "iteration 1217, epoch 3, batch 55/481,disc_loss 80.331, (real 83.778, fake 76.885 ) gen_loss 544.22\n",
            "iteration 1218, epoch 3, batch 56/481,disc_loss 88.639, (real 91.273, fake 86.004 ) gen_loss 455.68\n",
            "iteration 1219, epoch 3, batch 57/481,disc_loss 80.288, (real 84.416, fake 76.161 ) gen_loss 442.93\n",
            "iteration 1220, epoch 3, batch 58/481,disc_loss 78.397, (real 82.809, fake 73.985 ) gen_loss 475.09\n",
            "iteration 1221, epoch 3, batch 59/481,disc_loss 78.713, (real 81.738, fake 75.687 ) gen_loss 522.66\n",
            "iteration 1222, epoch 3, batch 60/481,disc_loss 80.743, (real 84.545, fake 76.941 ) gen_loss 495.28\n",
            "iteration 1223, epoch 3, batch 61/481,disc_loss 84.498, (real 87.856, fake 81.14 ) gen_loss 539.21\n",
            "iteration 1224, epoch 3, batch 62/481,disc_loss 78.285, (real 81.09, fake 75.48 ) gen_loss 435.59\n",
            "iteration 1225, epoch 3, batch 63/481,disc_loss 80.719, (real 83.681, fake 77.757 ) gen_loss 544.31\n",
            "iteration 1226, epoch 3, batch 64/481,disc_loss 78.155, (real 81.487, fake 74.822 ) gen_loss 457.31\n",
            "iteration 1227, epoch 3, batch 65/481,disc_loss 78.411, (real 82.734, fake 74.088 ) gen_loss 435.61\n",
            "iteration 1228, epoch 3, batch 66/481,disc_loss 83.197, (real 87.011, fake 79.382 ) gen_loss 437.12\n",
            "iteration 1229, epoch 3, batch 67/481,disc_loss 73.665, (real 77.498, fake 69.833 ) gen_loss 452.23\n",
            "iteration 1230, epoch 3, batch 68/481,disc_loss 77.41, (real 81.573, fake 73.246 ) gen_loss 552.48\n",
            "iteration 1231, epoch 3, batch 69/481,disc_loss 78.663, (real 82.202, fake 75.124 ) gen_loss 496.7\n",
            "iteration 1232, epoch 3, batch 70/481,disc_loss 78.826, (real 82.115, fake 75.537 ) gen_loss 480.62\n",
            "iteration 1233, epoch 3, batch 71/481,disc_loss 80.302, (real 83.424, fake 77.181 ) gen_loss 458.27\n",
            "iteration 1234, epoch 3, batch 72/481,disc_loss 82.737, (real 87.309, fake 78.166 ) gen_loss 539.74\n",
            "iteration 1235, epoch 3, batch 73/481,disc_loss 78.145, (real 81.521, fake 74.77 ) gen_loss 456.58\n",
            "iteration 1236, epoch 3, batch 74/481,disc_loss 79.03, (real 83.136, fake 74.924 ) gen_loss 489.99\n",
            "iteration 1237, epoch 3, batch 75/481,disc_loss 78.861, (real 82.255, fake 75.466 ) gen_loss 475.25\n",
            "iteration 1238, epoch 3, batch 76/481,disc_loss 75.315, (real 77.909, fake 72.72 ) gen_loss 544.09\n",
            "iteration 1239, epoch 3, batch 77/481,disc_loss 84.741, (real 88.355, fake 81.127 ) gen_loss 559.15\n",
            "iteration 1240, epoch 3, batch 78/481,disc_loss 78.831, (real 82.473, fake 75.189 ) gen_loss 531.7\n",
            "iteration 1241, epoch 3, batch 79/481,disc_loss 77.062, (real 80.624, fake 73.5 ) gen_loss 480.81\n",
            "iteration 1242, epoch 3, batch 80/481,disc_loss 81.314, (real 85.314, fake 77.314 ) gen_loss 457.68\n",
            "iteration 1243, epoch 3, batch 81/481,disc_loss 79.127, (real 83.071, fake 75.183 ) gen_loss 448.26\n",
            "iteration 1244, epoch 3, batch 82/481,disc_loss 78.305, (real 81.971, fake 74.639 ) gen_loss 498.68\n",
            "iteration 1245, epoch 3, batch 83/481,disc_loss 77.664, (real 80.801, fake 74.526 ) gen_loss 471.3\n",
            "iteration 1246, epoch 3, batch 84/481,disc_loss 79.624, (real 82.672, fake 76.576 ) gen_loss 471.77\n",
            "iteration 1247, epoch 3, batch 85/481,disc_loss 72.208, (real 75.266, fake 69.15 ) gen_loss 435.73\n",
            "iteration 1248, epoch 3, batch 86/481,disc_loss 80.162, (real 83.735, fake 76.589 ) gen_loss 466.41\n",
            "iteration 1249, epoch 3, batch 87/481,disc_loss 77.741, (real 81.092, fake 74.39 ) gen_loss 441.81\n",
            "iteration 1250, epoch 3, batch 88/481,disc_loss 78.792, (real 82.772, fake 74.811 ) gen_loss 405.92\n",
            "iteration 1251, epoch 3, batch 89/481,disc_loss 81.597, (real 85.513, fake 77.681 ) gen_loss 428.48\n",
            "iteration 1252, epoch 3, batch 90/481,disc_loss 76.614, (real 81.857, fake 71.372 ) gen_loss 506.04\n",
            "iteration 1253, epoch 3, batch 91/481,disc_loss 77.978, (real 82.438, fake 73.519 ) gen_loss 555.33\n",
            "iteration 1254, epoch 3, batch 92/481,disc_loss 85.966, (real 90.342, fake 81.59 ) gen_loss 545.62\n",
            "iteration 1255, epoch 3, batch 93/481,disc_loss 79.553, (real 83.642, fake 75.463 ) gen_loss 517.98\n",
            "iteration 1256, epoch 3, batch 94/481,disc_loss 80.347, (real 84.713, fake 75.982 ) gen_loss 472.73\n",
            "iteration 1257, epoch 3, batch 95/481,disc_loss 76.691, (real 80.061, fake 73.32 ) gen_loss 478.79\n",
            "iteration 1258, epoch 3, batch 96/481,disc_loss 79.709, (real 84.545, fake 74.872 ) gen_loss 463.43\n",
            "iteration 1259, epoch 3, batch 97/481,disc_loss 79.443, (real 83.898, fake 74.987 ) gen_loss 474.16\n",
            "iteration 1260, epoch 3, batch 98/481,disc_loss 78.89, (real 82.45, fake 75.331 ) gen_loss 395.82\n",
            "iteration 1261, epoch 3, batch 99/481,disc_loss 81.254, (real 85.816, fake 76.691 ) gen_loss 472.23\n",
            "iteration 1262, epoch 3, batch 100/481,disc_loss 78.516, (real 82.151, fake 74.881 ) gen_loss 481.2\n",
            "iteration 1263, epoch 3, batch 101/481,disc_loss 79.041, (real 82.394, fake 75.689 ) gen_loss 583.63\n",
            "iteration 1264, epoch 3, batch 102/481,disc_loss 79.737, (real 83.379, fake 76.095 ) gen_loss 513.99\n",
            "iteration 1265, epoch 3, batch 103/481,disc_loss 83.199, (real 86.653, fake 79.745 ) gen_loss 575.84\n",
            "iteration 1266, epoch 3, batch 104/481,disc_loss 83.362, (real 88.578, fake 78.146 ) gen_loss 457.31\n",
            "iteration 1267, epoch 3, batch 105/481,disc_loss 78.057, (real 82.06, fake 74.053 ) gen_loss 473.58\n",
            "iteration 1268, epoch 3, batch 106/481,disc_loss 79.105, (real 81.659, fake 76.55 ) gen_loss 431.85\n",
            "iteration 1269, epoch 3, batch 107/481,disc_loss 81.053, (real 83.926, fake 78.179 ) gen_loss 419.69\n",
            "iteration 1270, epoch 3, batch 108/481,disc_loss 74.467, (real 77.395, fake 71.539 ) gen_loss 438.04\n",
            "iteration 1271, epoch 3, batch 109/481,disc_loss 80.57, (real 85.56, fake 75.58 ) gen_loss 429.34\n",
            "iteration 1272, epoch 3, batch 110/481,disc_loss 78.971, (real 83.544, fake 74.397 ) gen_loss 446.19\n",
            "iteration 1273, epoch 3, batch 111/481,disc_loss 77.345, (real 80.376, fake 74.313 ) gen_loss 455.86\n",
            "iteration 1274, epoch 3, batch 112/481,disc_loss 80.685, (real 84.425, fake 76.944 ) gen_loss 473.62\n",
            "iteration 1275, epoch 3, batch 113/481,disc_loss 80.274, (real 84.31, fake 76.238 ) gen_loss 435.66\n",
            "iteration 1276, epoch 3, batch 114/481,disc_loss 75.812, (real 79.172, fake 72.453 ) gen_loss 461.32\n",
            "iteration 1277, epoch 3, batch 115/481,disc_loss 82.002, (real 85.566, fake 78.439 ) gen_loss 424.3\n",
            "iteration 1278, epoch 3, batch 116/481,disc_loss 77.631, (real 81.805, fake 73.458 ) gen_loss 474.83\n",
            "iteration 1279, epoch 3, batch 117/481,disc_loss 78.113, (real 81.022, fake 75.203 ) gen_loss 413.33\n",
            "iteration 1280, epoch 3, batch 118/481,disc_loss 76.266, (real 78.791, fake 73.741 ) gen_loss 457.37\n",
            "iteration 1281, epoch 3, batch 119/481,disc_loss 81.402, (real 86.155, fake 76.65 ) gen_loss 494.77\n",
            "iteration 1282, epoch 3, batch 120/481,disc_loss 85.283, (real 90.978, fake 79.587 ) gen_loss 441.92\n",
            "iteration 1283, epoch 3, batch 121/481,disc_loss 76.284, (real 79.279, fake 73.288 ) gen_loss 426.91\n",
            "iteration 1284, epoch 3, batch 122/481,disc_loss 79.407, (real 83.796, fake 75.018 ) gen_loss 408.94\n",
            "iteration 1285, epoch 3, batch 123/481,disc_loss 81.998, (real 85.551, fake 78.445 ) gen_loss 478.84\n",
            "iteration 1286, epoch 3, batch 124/481,disc_loss 79.225, (real 83.845, fake 74.605 ) gen_loss 425.99\n",
            "iteration 1287, epoch 3, batch 125/481,disc_loss 76.767, (real 80.773, fake 72.761 ) gen_loss 516.09\n",
            "iteration 1288, epoch 3, batch 126/481,disc_loss 79.886, (real 83.361, fake 76.412 ) gen_loss 487.19\n",
            "iteration 1289, epoch 3, batch 127/481,disc_loss 76.218, (real 79.858, fake 72.577 ) gen_loss 491.52\n",
            "iteration 1290, epoch 3, batch 128/481,disc_loss 83.846, (real 86.817, fake 80.875 ) gen_loss 482.82\n",
            "iteration 1291, epoch 3, batch 129/481,disc_loss 77.286, (real 80.593, fake 73.978 ) gen_loss 462.88\n",
            "iteration 1292, epoch 3, batch 130/481,disc_loss 82.101, (real 87.258, fake 76.944 ) gen_loss 537.96\n",
            "iteration 1293, epoch 3, batch 131/481,disc_loss 81.029, (real 84.288, fake 77.771 ) gen_loss 472.21\n",
            "iteration 1294, epoch 3, batch 132/481,disc_loss 80.073, (real 82.906, fake 77.24 ) gen_loss 441.56\n",
            "iteration 1295, epoch 3, batch 133/481,disc_loss 85.059, (real 87.931, fake 82.186 ) gen_loss 483.59\n",
            "iteration 1296, epoch 3, batch 134/481,disc_loss 77.175, (real 81.525, fake 72.825 ) gen_loss 450.48\n",
            "iteration 1297, epoch 3, batch 135/481,disc_loss 80.219, (real 83.473, fake 76.966 ) gen_loss 417.85\n",
            "iteration 1298, epoch 3, batch 136/481,disc_loss 83.294, (real 87.817, fake 78.77 ) gen_loss 537.57\n",
            "iteration 1299, epoch 3, batch 137/481,disc_loss 86.521, (real 89.598, fake 83.444 ) gen_loss 459.62\n",
            "iteration 1300, epoch 3, batch 138/481,disc_loss 80.583, (real 84.526, fake 76.639 ) gen_loss 512.3\n",
            "iteration 1301, epoch 3, batch 139/481,disc_loss 77.574, (real 81.013, fake 74.134 ) gen_loss 439.81\n",
            "iteration 1302, epoch 3, batch 140/481,disc_loss 81.513, (real 85.262, fake 77.765 ) gen_loss 441.96\n",
            "iteration 1303, epoch 3, batch 141/481,disc_loss 83.329, (real 86.958, fake 79.7 ) gen_loss 451.84\n",
            "iteration 1304, epoch 3, batch 142/481,disc_loss 84.13, (real 87.908, fake 80.353 ) gen_loss 403.49\n",
            "iteration 1305, epoch 3, batch 143/481,disc_loss 80.826, (real 85.478, fake 76.174 ) gen_loss 423.77\n",
            "iteration 1306, epoch 3, batch 144/481,disc_loss 81.979, (real 86.475, fake 77.483 ) gen_loss 450.09\n",
            "iteration 1307, epoch 3, batch 145/481,disc_loss 80.845, (real 84.813, fake 76.877 ) gen_loss 480.66\n",
            "iteration 1308, epoch 3, batch 146/481,disc_loss 76.478, (real 80.129, fake 72.827 ) gen_loss 443.13\n",
            "iteration 1309, epoch 3, batch 147/481,disc_loss 73.043, (real 76.824, fake 69.262 ) gen_loss 485.25\n",
            "iteration 1310, epoch 3, batch 148/481,disc_loss 85.245, (real 89.498, fake 80.991 ) gen_loss 488.29\n",
            "iteration 1311, epoch 3, batch 149/481,disc_loss 78.752, (real 82.009, fake 75.494 ) gen_loss 449.54\n",
            "iteration 1312, epoch 3, batch 150/481,disc_loss 79.574, (real 83.383, fake 75.766 ) gen_loss 454.49\n",
            "iteration 1313, epoch 3, batch 151/481,disc_loss 80.684, (real 84.283, fake 77.085 ) gen_loss 436.26\n",
            "iteration 1314, epoch 3, batch 152/481,disc_loss 73.415, (real 77.467, fake 69.364 ) gen_loss 502.32\n",
            "iteration 1315, epoch 3, batch 153/481,disc_loss 80.386, (real 82.193, fake 78.579 ) gen_loss 468.54\n",
            "iteration 1316, epoch 3, batch 154/481,disc_loss 74.005, (real 77.319, fake 70.691 ) gen_loss 500.06\n",
            "iteration 1317, epoch 3, batch 155/481,disc_loss 77.519, (real 80.954, fake 74.084 ) gen_loss 516.09\n",
            "iteration 1318, epoch 3, batch 156/481,disc_loss 80.441, (real 85.043, fake 75.839 ) gen_loss 577.02\n",
            "iteration 1319, epoch 3, batch 157/481,disc_loss 80.975, (real 83.94, fake 78.011 ) gen_loss 481.21\n",
            "iteration 1320, epoch 3, batch 158/481,disc_loss 78.05, (real 82.155, fake 73.945 ) gen_loss 436.39\n",
            "iteration 1321, epoch 3, batch 159/481,disc_loss 82.322, (real 84.929, fake 79.715 ) gen_loss 470.57\n",
            "iteration 1322, epoch 3, batch 160/481,disc_loss 81.271, (real 85.274, fake 77.267 ) gen_loss 435.31\n",
            "iteration 1323, epoch 3, batch 161/481,disc_loss 74.601, (real 77.564, fake 71.637 ) gen_loss 461.55\n",
            "iteration 1324, epoch 3, batch 162/481,disc_loss 79.185, (real 83.206, fake 75.165 ) gen_loss 429.35\n",
            "iteration 1325, epoch 3, batch 163/481,disc_loss 77.219, (real 81.009, fake 73.429 ) gen_loss 431.49\n",
            "iteration 1326, epoch 3, batch 164/481,disc_loss 80.77, (real 84.237, fake 77.303 ) gen_loss 424.02\n",
            "iteration 1327, epoch 3, batch 165/481,disc_loss 80.22, (real 83.841, fake 76.599 ) gen_loss 503.46\n",
            "iteration 1328, epoch 3, batch 166/481,disc_loss 76.604, (real 81.539, fake 71.668 ) gen_loss 502.95\n",
            "iteration 1329, epoch 3, batch 167/481,disc_loss 79.713, (real 82.898, fake 76.528 ) gen_loss 463.37\n",
            "iteration 1330, epoch 3, batch 168/481,disc_loss 77.981, (real 82.021, fake 73.942 ) gen_loss 478.52\n",
            "iteration 1331, epoch 3, batch 169/481,disc_loss 83.4, (real 87.534, fake 79.267 ) gen_loss 482.53\n",
            "iteration 1332, epoch 3, batch 170/481,disc_loss 80.611, (real 84.44, fake 76.781 ) gen_loss 604.08\n",
            "iteration 1333, epoch 3, batch 171/481,disc_loss 80.258, (real 84.55, fake 75.967 ) gen_loss 545.12\n",
            "iteration 1334, epoch 3, batch 172/481,disc_loss 79.889, (real 83.028, fake 76.751 ) gen_loss 574.66\n",
            "iteration 1335, epoch 3, batch 173/481,disc_loss 79.409, (real 82.528, fake 76.291 ) gen_loss 472.96\n",
            "iteration 1336, epoch 3, batch 174/481,disc_loss 80.717, (real 85.126, fake 76.307 ) gen_loss 480.51\n",
            "iteration 1337, epoch 3, batch 175/481,disc_loss 77.327, (real 79.899, fake 74.755 ) gen_loss 520.51\n",
            "iteration 1338, epoch 3, batch 176/481,disc_loss 77.794, (real 81.598, fake 73.989 ) gen_loss 507.85\n",
            "iteration 1339, epoch 3, batch 177/481,disc_loss 78.944, (real 82.496, fake 75.392 ) gen_loss 484.85\n",
            "iteration 1340, epoch 3, batch 178/481,disc_loss 77.619, (real 80.674, fake 74.564 ) gen_loss 460.58\n",
            "iteration 1341, epoch 3, batch 179/481,disc_loss 73.637, (real 77.882, fake 69.392 ) gen_loss 506.02\n",
            "iteration 1342, epoch 3, batch 180/481,disc_loss 74.339, (real 77.747, fake 70.93 ) gen_loss 506.57\n",
            "iteration 1343, epoch 3, batch 181/481,disc_loss 74.506, (real 78.012, fake 70.999 ) gen_loss 537.18\n",
            "iteration 1344, epoch 3, batch 182/481,disc_loss 80.784, (real 86.06, fake 75.508 ) gen_loss 498.24\n",
            "iteration 1345, epoch 3, batch 183/481,disc_loss 80.315, (real 83.523, fake 77.106 ) gen_loss 491.6\n",
            "iteration 1346, epoch 3, batch 184/481,disc_loss 83.688, (real 86.776, fake 80.599 ) gen_loss 467.81\n",
            "iteration 1347, epoch 3, batch 185/481,disc_loss 75.978, (real 78.957, fake 72.999 ) gen_loss 509.17\n",
            "iteration 1348, epoch 3, batch 186/481,disc_loss 81.311, (real 83.729, fake 78.892 ) gen_loss 512.84\n",
            "iteration 1349, epoch 3, batch 187/481,disc_loss 76.869, (real 80.252, fake 73.486 ) gen_loss 447.96\n",
            "iteration 1350, epoch 3, batch 188/481,disc_loss 81.935, (real 85.927, fake 77.944 ) gen_loss 568.87\n",
            "iteration 1351, epoch 3, batch 189/481,disc_loss 82.916, (real 86.457, fake 79.376 ) gen_loss 465.1\n",
            "iteration 1352, epoch 3, batch 190/481,disc_loss 78.374, (real 81.596, fake 75.153 ) gen_loss 459.82\n",
            "iteration 1353, epoch 3, batch 191/481,disc_loss 83.529, (real 88.29, fake 78.767 ) gen_loss 482.3\n",
            "iteration 1354, epoch 3, batch 192/481,disc_loss 82.112, (real 86.796, fake 77.428 ) gen_loss 427.51\n",
            "iteration 1355, epoch 3, batch 193/481,disc_loss 79.388, (real 82.901, fake 75.874 ) gen_loss 446.04\n",
            "iteration 1356, epoch 3, batch 194/481,disc_loss 81.292, (real 85.346, fake 77.238 ) gen_loss 466.81\n",
            "iteration 1357, epoch 3, batch 195/481,disc_loss 84.877, (real 89.55, fake 80.204 ) gen_loss 536.66\n",
            "iteration 1358, epoch 3, batch 196/481,disc_loss 83.458, (real 86.406, fake 80.511 ) gen_loss 508.88\n",
            "iteration 1359, epoch 3, batch 197/481,disc_loss 83.552, (real 86.69, fake 80.415 ) gen_loss 512.84\n",
            "iteration 1360, epoch 3, batch 198/481,disc_loss 77.503, (real 80.81, fake 74.195 ) gen_loss 528.64\n",
            "iteration 1361, epoch 3, batch 199/481,disc_loss 79.633, (real 83.006, fake 76.259 ) gen_loss 516.2\n",
            "iteration 1362, epoch 3, batch 200/481,disc_loss 79.773, (real 84.008, fake 75.537 ) gen_loss 523.38\n",
            "iteration 1363, epoch 3, batch 201/481,disc_loss 79.651, (real 83.348, fake 75.954 ) gen_loss 499.62\n",
            "iteration 1364, epoch 3, batch 202/481,disc_loss 71.025, (real 74.64, fake 67.41 ) gen_loss 509.99\n",
            "iteration 1365, epoch 3, batch 203/481,disc_loss 76.73, (real 78.907, fake 74.554 ) gen_loss 520.35\n",
            "iteration 1366, epoch 3, batch 204/481,disc_loss 80.335, (real 82.867, fake 77.803 ) gen_loss 479.11\n",
            "iteration 1367, epoch 3, batch 205/481,disc_loss 79.806, (real 82.938, fake 76.675 ) gen_loss 504.85\n",
            "iteration 1368, epoch 3, batch 206/481,disc_loss 79.244, (real 82.643, fake 75.846 ) gen_loss 485.77\n",
            "iteration 1369, epoch 3, batch 207/481,disc_loss 81.212, (real 85.636, fake 76.788 ) gen_loss 506.91\n",
            "iteration 1370, epoch 3, batch 208/481,disc_loss 80.845, (real 83.287, fake 78.403 ) gen_loss 549.42\n",
            "iteration 1371, epoch 3, batch 209/481,disc_loss 80.394, (real 82.828, fake 77.96 ) gen_loss 476.76\n",
            "iteration 1372, epoch 3, batch 210/481,disc_loss 81.41, (real 84.32, fake 78.499 ) gen_loss 461.87\n",
            "iteration 1373, epoch 3, batch 211/481,disc_loss 82.287, (real 87.685, fake 76.889 ) gen_loss 471.28\n",
            "iteration 1374, epoch 3, batch 212/481,disc_loss 80.822, (real 84.684, fake 76.96 ) gen_loss 452.4\n",
            "iteration 1375, epoch 3, batch 213/481,disc_loss 76.355, (real 79.621, fake 73.088 ) gen_loss 488.04\n",
            "iteration 1376, epoch 3, batch 214/481,disc_loss 76.946, (real 80.781, fake 73.111 ) gen_loss 479.61\n",
            "iteration 1377, epoch 3, batch 215/481,disc_loss 79.145, (real 82.584, fake 75.706 ) gen_loss 507.04\n",
            "iteration 1378, epoch 3, batch 216/481,disc_loss 78.417, (real 82.512, fake 74.322 ) gen_loss 499.03\n",
            "iteration 1379, epoch 3, batch 217/481,disc_loss 76.9, (real 80.862, fake 72.939 ) gen_loss 493.17\n",
            "iteration 1380, epoch 3, batch 218/481,disc_loss 78.537, (real 82.711, fake 74.363 ) gen_loss 443.31\n",
            "iteration 1381, epoch 3, batch 219/481,disc_loss 78.055, (real 80.939, fake 75.17 ) gen_loss 548.29\n",
            "iteration 1382, epoch 3, batch 220/481,disc_loss 76.76, (real 80.441, fake 73.079 ) gen_loss 499.76\n",
            "iteration 1383, epoch 3, batch 221/481,disc_loss 82.437, (real 85.014, fake 79.86 ) gen_loss 498.46\n",
            "iteration 1384, epoch 3, batch 222/481,disc_loss 80.576, (real 84.329, fake 76.823 ) gen_loss 465.67\n",
            "iteration 1385, epoch 3, batch 223/481,disc_loss 81.782, (real 84.374, fake 79.189 ) gen_loss 525.66\n",
            "iteration 1386, epoch 3, batch 224/481,disc_loss 79.632, (real 82.545, fake 76.72 ) gen_loss 472.51\n",
            "iteration 1387, epoch 3, batch 225/481,disc_loss 80.387, (real 82.632, fake 78.142 ) gen_loss 427.8\n",
            "iteration 1388, epoch 3, batch 226/481,disc_loss 80.5, (real 84.222, fake 76.778 ) gen_loss 469.97\n",
            "iteration 1389, epoch 3, batch 227/481,disc_loss 78.099, (real 81.867, fake 74.331 ) gen_loss 520.07\n",
            "iteration 1390, epoch 3, batch 228/481,disc_loss 83.466, (real 87.932, fake 79.0 ) gen_loss 521.11\n",
            "iteration 1391, epoch 3, batch 229/481,disc_loss 77.428, (real 81.359, fake 73.497 ) gen_loss 574.64\n",
            "iteration 1392, epoch 3, batch 230/481,disc_loss 81.693, (real 84.42, fake 78.966 ) gen_loss 543.1\n",
            "iteration 1393, epoch 3, batch 231/481,disc_loss 79.179, (real 82.565, fake 75.793 ) gen_loss 594.07\n",
            "iteration 1394, epoch 3, batch 232/481,disc_loss 85.792, (real 89.555, fake 82.029 ) gen_loss 439.87\n",
            "iteration 1395, epoch 3, batch 233/481,disc_loss 79.554, (real 83.13, fake 75.977 ) gen_loss 484.76\n",
            "iteration 1396, epoch 3, batch 234/481,disc_loss 82.897, (real 86.687, fake 79.108 ) gen_loss 562.29\n",
            "iteration 1397, epoch 3, batch 235/481,disc_loss 78.067, (real 81.904, fake 74.231 ) gen_loss 479.87\n",
            "iteration 1398, epoch 3, batch 236/481,disc_loss 80.725, (real 83.666, fake 77.784 ) gen_loss 512.82\n",
            "iteration 1399, epoch 3, batch 237/481,disc_loss 77.307, (real 81.708, fake 72.907 ) gen_loss 483.88\n",
            "iteration 1400, epoch 3, batch 238/481,disc_loss 78.388, (real 83.258, fake 73.519 ) gen_loss 534.65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1401, epoch 3, batch 239/481,disc_loss 75.67, (real 79.279, fake 72.06 ) gen_loss 500.06\n",
            "iteration 1402, epoch 3, batch 240/481,disc_loss 76.432, (real 79.957, fake 72.908 ) gen_loss 509.65\n",
            "iteration 1403, epoch 3, batch 241/481,disc_loss 83.859, (real 87.667, fake 80.05 ) gen_loss 519.58\n",
            "iteration 1404, epoch 3, batch 242/481,disc_loss 80.369, (real 84.089, fake 76.649 ) gen_loss 449.73\n",
            "iteration 1405, epoch 3, batch 243/481,disc_loss 84.491, (real 88.785, fake 80.197 ) gen_loss 494.89\n",
            "iteration 1406, epoch 3, batch 244/481,disc_loss 83.482, (real 87.681, fake 79.283 ) gen_loss 469.11\n",
            "iteration 1407, epoch 3, batch 245/481,disc_loss 83.372, (real 87.68, fake 79.064 ) gen_loss 502.24\n",
            "iteration 1408, epoch 3, batch 246/481,disc_loss 81.653, (real 84.525, fake 78.781 ) gen_loss 510.58\n",
            "iteration 1409, epoch 3, batch 247/481,disc_loss 81.198, (real 84.97, fake 77.426 ) gen_loss 470.36\n",
            "iteration 1410, epoch 3, batch 248/481,disc_loss 81.519, (real 85.571, fake 77.467 ) gen_loss 523.17\n",
            "iteration 1411, epoch 3, batch 249/481,disc_loss 82.824, (real 86.965, fake 78.683 ) gen_loss 485.95\n",
            "iteration 1412, epoch 3, batch 250/481,disc_loss 78.133, (real 82.068, fake 74.199 ) gen_loss 582.46\n",
            "iteration 1413, epoch 3, batch 251/481,disc_loss 79.724, (real 83.211, fake 76.237 ) gen_loss 545.51\n",
            "iteration 1414, epoch 3, batch 252/481,disc_loss 81.61, (real 84.911, fake 78.309 ) gen_loss 487.57\n",
            "iteration 1415, epoch 3, batch 253/481,disc_loss 78.971, (real 83.229, fake 74.713 ) gen_loss 482.87\n",
            "iteration 1416, epoch 3, batch 254/481,disc_loss 81.324, (real 83.885, fake 78.763 ) gen_loss 570.07\n",
            "iteration 1417, epoch 3, batch 255/481,disc_loss 83.872, (real 87.401, fake 80.343 ) gen_loss 545.76\n",
            "iteration 1418, epoch 3, batch 256/481,disc_loss 81.018, (real 84.082, fake 77.955 ) gen_loss 560.09\n",
            "iteration 1419, epoch 3, batch 257/481,disc_loss 87.024, (real 90.218, fake 83.831 ) gen_loss 450.26\n",
            "iteration 1420, epoch 3, batch 258/481,disc_loss 79.374, (real 82.123, fake 76.626 ) gen_loss 475.9\n",
            "iteration 1421, epoch 3, batch 259/481,disc_loss 73.958, (real 78.31, fake 69.606 ) gen_loss 444.15\n",
            "iteration 1422, epoch 3, batch 260/481,disc_loss 82.611, (real 85.828, fake 79.395 ) gen_loss 508.44\n",
            "iteration 1423, epoch 3, batch 261/481,disc_loss 79.348, (real 84.031, fake 74.666 ) gen_loss 517.98\n",
            "iteration 1424, epoch 3, batch 262/481,disc_loss 79.408, (real 84.209, fake 74.606 ) gen_loss 487.42\n",
            "iteration 1425, epoch 3, batch 263/481,disc_loss 81.655, (real 85.416, fake 77.895 ) gen_loss 463.07\n",
            "iteration 1426, epoch 3, batch 264/481,disc_loss 78.785, (real 81.993, fake 75.577 ) gen_loss 499.89\n",
            "iteration 1427, epoch 3, batch 265/481,disc_loss 81.666, (real 85.366, fake 77.966 ) gen_loss 508.99\n",
            "iteration 1428, epoch 3, batch 266/481,disc_loss 86.706, (real 90.27, fake 83.143 ) gen_loss 526.39\n",
            "iteration 1429, epoch 3, batch 267/481,disc_loss 78.322, (real 82.086, fake 74.558 ) gen_loss 467.37\n",
            "iteration 1430, epoch 3, batch 268/481,disc_loss 81.542, (real 85.687, fake 77.398 ) gen_loss 458.46\n",
            "iteration 1431, epoch 3, batch 269/481,disc_loss 80.945, (real 85.051, fake 76.838 ) gen_loss 557.45\n",
            "iteration 1432, epoch 3, batch 270/481,disc_loss 83.016, (real 85.491, fake 80.54 ) gen_loss 462.89\n",
            "iteration 1433, epoch 3, batch 271/481,disc_loss 84.114, (real 90.925, fake 77.303 ) gen_loss 461.41\n",
            "iteration 1434, epoch 3, batch 272/481,disc_loss 79.196, (real 83.045, fake 75.346 ) gen_loss 458.89\n",
            "iteration 1435, epoch 3, batch 273/481,disc_loss 82.138, (real 85.821, fake 78.455 ) gen_loss 434.06\n",
            "iteration 1436, epoch 3, batch 274/481,disc_loss 79.177, (real 83.538, fake 74.816 ) gen_loss 511.18\n",
            "iteration 1437, epoch 3, batch 275/481,disc_loss 79.948, (real 83.701, fake 76.194 ) gen_loss 516.09\n",
            "iteration 1438, epoch 3, batch 276/481,disc_loss 75.732, (real 79.2, fake 72.264 ) gen_loss 490.76\n",
            "iteration 1439, epoch 3, batch 277/481,disc_loss 76.954, (real 79.709, fake 74.198 ) gen_loss 511.85\n",
            "iteration 1440, epoch 3, batch 278/481,disc_loss 82.726, (real 85.604, fake 79.847 ) gen_loss 509.16\n",
            "iteration 1441, epoch 3, batch 279/481,disc_loss 83.326, (real 86.99, fake 79.662 ) gen_loss 499.94\n",
            "iteration 1442, epoch 3, batch 280/481,disc_loss 79.929, (real 82.446, fake 77.411 ) gen_loss 455.41\n",
            "iteration 1443, epoch 3, batch 281/481,disc_loss 82.369, (real 86.419, fake 78.319 ) gen_loss 533.64\n",
            "iteration 1444, epoch 3, batch 282/481,disc_loss 80.802, (real 84.692, fake 76.912 ) gen_loss 471.85\n",
            "iteration 1445, epoch 3, batch 283/481,disc_loss 77.657, (real 80.936, fake 74.377 ) gen_loss 496.35\n",
            "iteration 1446, epoch 3, batch 284/481,disc_loss 81.804, (real 84.742, fake 78.866 ) gen_loss 542.54\n",
            "iteration 1447, epoch 3, batch 285/481,disc_loss 74.576, (real 78.057, fake 71.094 ) gen_loss 612.91\n",
            "iteration 1448, epoch 3, batch 286/481,disc_loss 76.714, (real 81.048, fake 72.381 ) gen_loss 603.04\n",
            "iteration 1449, epoch 3, batch 287/481,disc_loss 79.985, (real 82.944, fake 77.025 ) gen_loss 592.65\n",
            "iteration 1450, epoch 3, batch 288/481,disc_loss 79.649, (real 82.023, fake 77.275 ) gen_loss 513.57\n",
            "iteration 1451, epoch 3, batch 289/481,disc_loss 80.863, (real 85.107, fake 76.619 ) gen_loss 561.66\n",
            "iteration 1452, epoch 3, batch 290/481,disc_loss 80.838, (real 84.363, fake 77.314 ) gen_loss 536.54\n",
            "iteration 1453, epoch 3, batch 291/481,disc_loss 81.944, (real 85.786, fake 78.101 ) gen_loss 544.59\n",
            "iteration 1454, epoch 3, batch 292/481,disc_loss 78.563, (real 81.759, fake 75.367 ) gen_loss 523.83\n",
            "iteration 1455, epoch 3, batch 293/481,disc_loss 79.752, (real 83.393, fake 76.11 ) gen_loss 550.11\n",
            "iteration 1456, epoch 3, batch 294/481,disc_loss 76.745, (real 79.053, fake 74.438 ) gen_loss 558.51\n",
            "iteration 1457, epoch 3, batch 295/481,disc_loss 81.873, (real 85.039, fake 78.707 ) gen_loss 505.71\n",
            "iteration 1458, epoch 3, batch 296/481,disc_loss 80.555, (real 84.674, fake 76.436 ) gen_loss 523.44\n",
            "iteration 1459, epoch 3, batch 297/481,disc_loss 82.844, (real 85.595, fake 80.094 ) gen_loss 502.93\n",
            "iteration 1460, epoch 3, batch 298/481,disc_loss 86.793, (real 90.235, fake 83.35 ) gen_loss 494.74\n",
            "iteration 1461, epoch 3, batch 299/481,disc_loss 78.397, (real 82.055, fake 74.739 ) gen_loss 505.26\n",
            "iteration 1462, epoch 3, batch 300/481,disc_loss 81.48, (real 84.757, fake 78.202 ) gen_loss 465.92\n",
            "iteration 1463, epoch 3, batch 301/481,disc_loss 79.57, (real 83.993, fake 75.148 ) gen_loss 542.37\n",
            "iteration 1464, epoch 3, batch 302/481,disc_loss 78.869, (real 82.492, fake 75.246 ) gen_loss 504.34\n",
            "iteration 1465, epoch 3, batch 303/481,disc_loss 81.317, (real 85.965, fake 76.669 ) gen_loss 498.25\n",
            "iteration 1466, epoch 3, batch 304/481,disc_loss 79.475, (real 83.388, fake 75.561 ) gen_loss 477.67\n",
            "iteration 1467, epoch 3, batch 305/481,disc_loss 74.962, (real 79.188, fake 70.736 ) gen_loss 509.83\n",
            "iteration 1468, epoch 3, batch 306/481,disc_loss 81.542, (real 83.628, fake 79.455 ) gen_loss 522.96\n",
            "iteration 1469, epoch 3, batch 307/481,disc_loss 78.045, (real 80.831, fake 75.259 ) gen_loss 577.5\n",
            "iteration 1470, epoch 3, batch 308/481,disc_loss 77.36, (real 79.854, fake 74.866 ) gen_loss 606.57\n",
            "iteration 1471, epoch 3, batch 309/481,disc_loss 80.721, (real 84.122, fake 77.32 ) gen_loss 463.08\n",
            "iteration 1472, epoch 3, batch 310/481,disc_loss 78.525, (real 81.748, fake 75.302 ) gen_loss 527.82\n",
            "iteration 1473, epoch 3, batch 311/481,disc_loss 81.732, (real 86.544, fake 76.919 ) gen_loss 522.36\n",
            "iteration 1474, epoch 3, batch 312/481,disc_loss 81.188, (real 85.047, fake 77.329 ) gen_loss 487.99\n",
            "iteration 1475, epoch 3, batch 313/481,disc_loss 80.786, (real 84.252, fake 77.32 ) gen_loss 559.01\n",
            "iteration 1476, epoch 3, batch 314/481,disc_loss 79.112, (real 83.72, fake 74.503 ) gen_loss 486.61\n",
            "iteration 1477, epoch 3, batch 315/481,disc_loss 77.753, (real 81.498, fake 74.007 ) gen_loss 495.86\n",
            "iteration 1478, epoch 3, batch 316/481,disc_loss 80.286, (real 83.374, fake 77.198 ) gen_loss 507.67\n",
            "iteration 1479, epoch 3, batch 317/481,disc_loss 82.297, (real 86.266, fake 78.328 ) gen_loss 673.7\n",
            "iteration 1480, epoch 3, batch 318/481,disc_loss 79.647, (real 82.771, fake 76.523 ) gen_loss 551.65\n",
            "iteration 1481, epoch 3, batch 319/481,disc_loss 83.93, (real 87.791, fake 80.07 ) gen_loss 569.95\n",
            "iteration 1482, epoch 3, batch 320/481,disc_loss 78.703, (real 81.962, fake 75.444 ) gen_loss 474.64\n",
            "iteration 1483, epoch 3, batch 321/481,disc_loss 80.317, (real 84.074, fake 76.561 ) gen_loss 497.33\n",
            "iteration 1484, epoch 3, batch 322/481,disc_loss 82.1, (real 85.717, fake 78.483 ) gen_loss 500.39\n",
            "iteration 1485, epoch 3, batch 323/481,disc_loss 80.591, (real 84.4, fake 76.782 ) gen_loss 478.35\n",
            "iteration 1486, epoch 3, batch 324/481,disc_loss 81.132, (real 84.966, fake 77.297 ) gen_loss 513.08\n",
            "iteration 1487, epoch 3, batch 325/481,disc_loss 78.211, (real 81.972, fake 74.45 ) gen_loss 462.83\n",
            "iteration 1488, epoch 3, batch 326/481,disc_loss 81.735, (real 85.9, fake 77.571 ) gen_loss 489.98\n",
            "iteration 1489, epoch 3, batch 327/481,disc_loss 80.568, (real 83.676, fake 77.459 ) gen_loss 489.35\n",
            "iteration 1490, epoch 3, batch 328/481,disc_loss 84.204, (real 88.637, fake 79.771 ) gen_loss 559.61\n",
            "iteration 1491, epoch 3, batch 329/481,disc_loss 77.63, (real 81.474, fake 73.786 ) gen_loss 495.63\n",
            "iteration 1492, epoch 3, batch 330/481,disc_loss 83.816, (real 86.513, fake 81.12 ) gen_loss 511.11\n",
            "iteration 1493, epoch 3, batch 331/481,disc_loss 82.002, (real 85.148, fake 78.856 ) gen_loss 494.37\n",
            "iteration 1494, epoch 3, batch 332/481,disc_loss 79.243, (real 83.111, fake 75.375 ) gen_loss 487.01\n",
            "iteration 1495, epoch 3, batch 333/481,disc_loss 81.679, (real 84.961, fake 78.396 ) gen_loss 506.32\n",
            "iteration 1496, epoch 3, batch 334/481,disc_loss 80.672, (real 84.572, fake 76.773 ) gen_loss 493.17\n",
            "iteration 1497, epoch 3, batch 335/481,disc_loss 88.172, (real 91.013, fake 85.33 ) gen_loss 487.96\n",
            "iteration 1498, epoch 3, batch 336/481,disc_loss 79.09, (real 82.852, fake 75.329 ) gen_loss 512.53\n",
            "iteration 1499, epoch 3, batch 337/481,disc_loss 83.213, (real 86.128, fake 80.298 ) gen_loss 553.2\n",
            "iteration 1500, epoch 3, batch 338/481,disc_loss 86.584, (real 88.609, fake 84.559 ) gen_loss 584.65\n",
            "iteration 1501, epoch 3, batch 339/481,disc_loss 81.999, (real 87.024, fake 76.973 ) gen_loss 772.06\n",
            "iteration 1502, epoch 3, batch 340/481,disc_loss 82.88, (real 87.181, fake 78.579 ) gen_loss 641.92\n",
            "iteration 1503, epoch 3, batch 341/481,disc_loss 81.344, (real 85.363, fake 77.326 ) gen_loss 490.03\n",
            "iteration 1504, epoch 3, batch 342/481,disc_loss 80.92, (real 85.432, fake 76.408 ) gen_loss 505.16\n",
            "iteration 1505, epoch 3, batch 343/481,disc_loss 80.216, (real 83.047, fake 77.385 ) gen_loss 506.58\n",
            "iteration 1506, epoch 3, batch 344/481,disc_loss 80.126, (real 85.026, fake 75.226 ) gen_loss 521.24\n",
            "iteration 1507, epoch 3, batch 345/481,disc_loss 78.979, (real 83.132, fake 74.827 ) gen_loss 497.75\n",
            "iteration 1508, epoch 3, batch 346/481,disc_loss 78.77, (real 83.818, fake 73.722 ) gen_loss 514.19\n",
            "iteration 1509, epoch 3, batch 347/481,disc_loss 80.096, (real 83.506, fake 76.687 ) gen_loss 496.61\n",
            "iteration 1510, epoch 3, batch 348/481,disc_loss 77.715, (real 81.563, fake 73.868 ) gen_loss 539.56\n",
            "iteration 1511, epoch 3, batch 349/481,disc_loss 77.227, (real 81.052, fake 73.402 ) gen_loss 496.87\n",
            "iteration 1512, epoch 3, batch 350/481,disc_loss 80.99, (real 83.694, fake 78.286 ) gen_loss 486.32\n",
            "iteration 1513, epoch 3, batch 351/481,disc_loss 78.737, (real 82.006, fake 75.467 ) gen_loss 495.39\n",
            "iteration 1514, epoch 3, batch 352/481,disc_loss 74.885, (real 79.527, fake 70.243 ) gen_loss 504.37\n",
            "iteration 1515, epoch 3, batch 353/481,disc_loss 77.708, (real 82.299, fake 73.117 ) gen_loss 491.29\n",
            "iteration 1516, epoch 3, batch 354/481,disc_loss 81.439, (real 85.213, fake 77.666 ) gen_loss 505.82\n",
            "iteration 1517, epoch 3, batch 355/481,disc_loss 82.047, (real 85.268, fake 78.826 ) gen_loss 591.63\n",
            "iteration 1518, epoch 3, batch 356/481,disc_loss 85.254, (real 88.347, fake 82.16 ) gen_loss 461.07\n",
            "iteration 1519, epoch 3, batch 357/481,disc_loss 83.602, (real 87.739, fake 79.464 ) gen_loss 461.76\n",
            "iteration 1520, epoch 3, batch 358/481,disc_loss 80.879, (real 84.232, fake 77.525 ) gen_loss 483.5\n",
            "iteration 1521, epoch 3, batch 359/481,disc_loss 80.811, (real 83.596, fake 78.025 ) gen_loss 523.09\n",
            "iteration 1522, epoch 3, batch 360/481,disc_loss 78.799, (real 82.597, fake 75.002 ) gen_loss 585.95\n",
            "iteration 1523, epoch 3, batch 361/481,disc_loss 76.831, (real 79.166, fake 74.496 ) gen_loss 492.58\n",
            "iteration 1524, epoch 3, batch 362/481,disc_loss 75.457, (real 78.648, fake 72.266 ) gen_loss 589.3\n",
            "iteration 1525, epoch 3, batch 363/481,disc_loss 76.889, (real 79.632, fake 74.147 ) gen_loss 606.09\n",
            "iteration 1526, epoch 3, batch 364/481,disc_loss 78.869, (real 82.471, fake 75.266 ) gen_loss 537.37\n",
            "iteration 1527, epoch 3, batch 365/481,disc_loss 82.013, (real 85.116, fake 78.91 ) gen_loss 460.28\n",
            "iteration 1528, epoch 3, batch 366/481,disc_loss 79.223, (real 83.401, fake 75.044 ) gen_loss 514.99\n",
            "iteration 1529, epoch 3, batch 367/481,disc_loss 78.876, (real 82.315, fake 75.436 ) gen_loss 513.96\n",
            "iteration 1530, epoch 3, batch 368/481,disc_loss 82.001, (real 83.725, fake 80.276 ) gen_loss 500.84\n",
            "iteration 1531, epoch 3, batch 369/481,disc_loss 80.271, (real 83.763, fake 76.778 ) gen_loss 474.32\n",
            "iteration 1532, epoch 3, batch 370/481,disc_loss 76.928, (real 79.712, fake 74.144 ) gen_loss 483.37\n",
            "iteration 1533, epoch 3, batch 371/481,disc_loss 80.466, (real 84.378, fake 76.553 ) gen_loss 505.71\n",
            "iteration 1534, epoch 3, batch 372/481,disc_loss 82.357, (real 85.758, fake 78.956 ) gen_loss 505.5\n",
            "iteration 1535, epoch 3, batch 373/481,disc_loss 77.772, (real 81.478, fake 74.066 ) gen_loss 491.42\n",
            "iteration 1536, epoch 3, batch 374/481,disc_loss 80.759, (real 84.239, fake 77.279 ) gen_loss 513.38\n",
            "iteration 1537, epoch 3, batch 375/481,disc_loss 75.911, (real 78.766, fake 73.056 ) gen_loss 552.84\n",
            "iteration 1538, epoch 3, batch 376/481,disc_loss 75.805, (real 78.871, fake 72.74 ) gen_loss 532.03\n",
            "iteration 1539, epoch 3, batch 377/481,disc_loss 79.262, (real 81.597, fake 76.926 ) gen_loss 518.85\n",
            "iteration 1540, epoch 3, batch 378/481,disc_loss 81.093, (real 84.156, fake 78.03 ) gen_loss 472.99\n",
            "iteration 1541, epoch 3, batch 379/481,disc_loss 81.968, (real 85.626, fake 78.311 ) gen_loss 467.17\n",
            "iteration 1542, epoch 3, batch 380/481,disc_loss 76.903, (real 80.662, fake 73.143 ) gen_loss 483.92\n",
            "iteration 1543, epoch 3, batch 381/481,disc_loss 82.736, (real 86.026, fake 79.445 ) gen_loss 504.93\n",
            "iteration 1544, epoch 3, batch 382/481,disc_loss 81.255, (real 84.816, fake 77.694 ) gen_loss 489.02\n",
            "iteration 1545, epoch 3, batch 383/481,disc_loss 84.949, (real 88.751, fake 81.146 ) gen_loss 477.1\n",
            "iteration 1546, epoch 3, batch 384/481,disc_loss 84.031, (real 87.236, fake 80.826 ) gen_loss 505.53\n",
            "iteration 1547, epoch 3, batch 385/481,disc_loss 80.924, (real 84.535, fake 77.312 ) gen_loss 546.98\n",
            "iteration 1548, epoch 3, batch 386/481,disc_loss 81.585, (real 86.173, fake 76.998 ) gen_loss 481.13\n",
            "iteration 1549, epoch 3, batch 387/481,disc_loss 80.75, (real 83.759, fake 77.742 ) gen_loss 542.69\n",
            "iteration 1550, epoch 3, batch 388/481,disc_loss 78.77, (real 82.024, fake 75.515 ) gen_loss 493.11\n",
            "iteration 1551, epoch 3, batch 389/481,disc_loss 79.531, (real 81.955, fake 77.107 ) gen_loss 510.88\n",
            "iteration 1552, epoch 3, batch 390/481,disc_loss 81.406, (real 84.321, fake 78.491 ) gen_loss 540.29\n",
            "iteration 1553, epoch 3, batch 391/481,disc_loss 84.522, (real 87.14, fake 81.903 ) gen_loss 484.21\n",
            "iteration 1554, epoch 3, batch 392/481,disc_loss 80.073, (real 84.118, fake 76.027 ) gen_loss 480.85\n",
            "iteration 1555, epoch 3, batch 393/481,disc_loss 75.737, (real 78.649, fake 72.826 ) gen_loss 446.17\n",
            "iteration 1556, epoch 3, batch 394/481,disc_loss 81.088, (real 84.263, fake 77.914 ) gen_loss 485.41\n",
            "iteration 1557, epoch 3, batch 395/481,disc_loss 79.162, (real 82.435, fake 75.889 ) gen_loss 477.34\n",
            "iteration 1558, epoch 3, batch 396/481,disc_loss 75.506, (real 78.687, fake 72.325 ) gen_loss 497.76\n",
            "iteration 1559, epoch 3, batch 397/481,disc_loss 75.77, (real 78.398, fake 73.142 ) gen_loss 496.85\n",
            "iteration 1560, epoch 3, batch 398/481,disc_loss 76.762, (real 79.128, fake 74.395 ) gen_loss 490.8\n",
            "iteration 1561, epoch 3, batch 399/481,disc_loss 81.211, (real 84.666, fake 77.756 ) gen_loss 484.87\n",
            "iteration 1562, epoch 3, batch 400/481,disc_loss 74.162, (real 77.396, fake 70.928 ) gen_loss 488.14\n",
            "iteration 1563, epoch 3, batch 401/481,disc_loss 76.933, (real 79.621, fake 74.245 ) gen_loss 491.13\n",
            "iteration 1564, epoch 3, batch 402/481,disc_loss 76.421, (real 79.445, fake 73.398 ) gen_loss 536.29\n",
            "iteration 1565, epoch 3, batch 403/481,disc_loss 80.298, (real 83.369, fake 77.227 ) gen_loss 492.88\n",
            "iteration 1566, epoch 3, batch 404/481,disc_loss 81.279, (real 85.88, fake 76.678 ) gen_loss 481.87\n",
            "iteration 1567, epoch 3, batch 405/481,disc_loss 81.021, (real 84.477, fake 77.565 ) gen_loss 488.06\n",
            "iteration 1568, epoch 3, batch 406/481,disc_loss 82.125, (real 84.188, fake 80.061 ) gen_loss 557.47\n",
            "iteration 1569, epoch 3, batch 407/481,disc_loss 77.385, (real 81.131, fake 73.638 ) gen_loss 500.25\n",
            "iteration 1570, epoch 3, batch 408/481,disc_loss 82.411, (real 86.395, fake 78.428 ) gen_loss 549.02\n",
            "iteration 1571, epoch 3, batch 409/481,disc_loss 82.656, (real 86.495, fake 78.817 ) gen_loss 444.02\n",
            "iteration 1572, epoch 3, batch 410/481,disc_loss 76.717, (real 80.259, fake 73.176 ) gen_loss 512.24\n",
            "iteration 1573, epoch 3, batch 411/481,disc_loss 79.685, (real 83.339, fake 76.032 ) gen_loss 466.9\n",
            "iteration 1574, epoch 3, batch 412/481,disc_loss 82.095, (real 84.829, fake 79.361 ) gen_loss 512.78\n",
            "iteration 1575, epoch 3, batch 413/481,disc_loss 84.666, (real 87.815, fake 81.518 ) gen_loss 507.18\n",
            "iteration 1576, epoch 3, batch 414/481,disc_loss 83.145, (real 85.634, fake 80.657 ) gen_loss 509.13\n",
            "iteration 1577, epoch 3, batch 415/481,disc_loss 79.209, (real 83.609, fake 74.808 ) gen_loss 562.54\n",
            "iteration 1578, epoch 3, batch 416/481,disc_loss 78.732, (real 82.036, fake 75.429 ) gen_loss 537.64\n",
            "iteration 1579, epoch 3, batch 417/481,disc_loss 79.67, (real 82.023, fake 77.317 ) gen_loss 532.7\n",
            "iteration 1580, epoch 3, batch 418/481,disc_loss 77.107, (real 80.698, fake 73.516 ) gen_loss 519.47\n",
            "iteration 1581, epoch 3, batch 419/481,disc_loss 79.757, (real 83.34, fake 76.175 ) gen_loss 515.69\n",
            "iteration 1582, epoch 3, batch 420/481,disc_loss 77.498, (real 82.345, fake 72.652 ) gen_loss 552.2\n",
            "iteration 1583, epoch 3, batch 421/481,disc_loss 76.736, (real 79.128, fake 74.344 ) gen_loss 489.97\n",
            "iteration 1584, epoch 3, batch 422/481,disc_loss 80.402, (real 85.095, fake 75.709 ) gen_loss 522.47\n",
            "iteration 1585, epoch 3, batch 423/481,disc_loss 79.179, (real 82.441, fake 75.916 ) gen_loss 569.54\n",
            "iteration 1586, epoch 3, batch 424/481,disc_loss 75.21, (real 77.727, fake 72.693 ) gen_loss 498.86\n",
            "iteration 1587, epoch 3, batch 425/481,disc_loss 79.268, (real 83.03, fake 75.506 ) gen_loss 509.74\n",
            "iteration 1588, epoch 3, batch 426/481,disc_loss 80.9, (real 83.6, fake 78.201 ) gen_loss 490.22\n",
            "iteration 1589, epoch 3, batch 427/481,disc_loss 85.148, (real 90.593, fake 79.703 ) gen_loss 501.06\n",
            "iteration 1590, epoch 3, batch 428/481,disc_loss 81.464, (real 85.154, fake 77.774 ) gen_loss 470.11\n",
            "iteration 1591, epoch 3, batch 429/481,disc_loss 79.525, (real 83.108, fake 75.943 ) gen_loss 467.25\n",
            "iteration 1592, epoch 3, batch 430/481,disc_loss 79.657, (real 82.942, fake 76.373 ) gen_loss 479.04\n",
            "iteration 1593, epoch 3, batch 431/481,disc_loss 83.312, (real 88.794, fake 77.831 ) gen_loss 510.45\n",
            "iteration 1594, epoch 3, batch 432/481,disc_loss 78.745, (real 81.775, fake 75.715 ) gen_loss 471.64\n",
            "iteration 1595, epoch 3, batch 433/481,disc_loss 76.769, (real 80.908, fake 72.629 ) gen_loss 528.79\n",
            "iteration 1596, epoch 3, batch 434/481,disc_loss 79.358, (real 83.286, fake 75.431 ) gen_loss 492.51\n",
            "iteration 1597, epoch 3, batch 435/481,disc_loss 84.699, (real 87.558, fake 81.841 ) gen_loss 531.97\n",
            "iteration 1598, epoch 3, batch 436/481,disc_loss 78.311, (real 81.684, fake 74.937 ) gen_loss 492.83\n",
            "iteration 1599, epoch 3, batch 437/481,disc_loss 81.982, (real 86.076, fake 77.888 ) gen_loss 521.74\n",
            "iteration 1600, epoch 3, batch 438/481,disc_loss 80.923, (real 85.963, fake 75.882 ) gen_loss 508.97\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1601, epoch 3, batch 439/481,disc_loss 77.275, (real 80.242, fake 74.309 ) gen_loss 477.28\n",
            "iteration 1602, epoch 3, batch 440/481,disc_loss 79.071, (real 82.382, fake 75.76 ) gen_loss 481.42\n",
            "iteration 1603, epoch 3, batch 441/481,disc_loss 81.382, (real 84.596, fake 78.168 ) gen_loss 494.48\n",
            "iteration 1604, epoch 3, batch 442/481,disc_loss 80.7, (real 84.054, fake 77.345 ) gen_loss 483.45\n",
            "iteration 1605, epoch 3, batch 443/481,disc_loss 82.015, (real 84.618, fake 79.412 ) gen_loss 503.36\n",
            "iteration 1606, epoch 3, batch 444/481,disc_loss 75.801, (real 79.176, fake 72.426 ) gen_loss 504.28\n",
            "iteration 1607, epoch 3, batch 445/481,disc_loss 80.696, (real 83.94, fake 77.451 ) gen_loss 502.84\n",
            "iteration 1608, epoch 3, batch 446/481,disc_loss 79.534, (real 83.616, fake 75.452 ) gen_loss 498.31\n",
            "iteration 1609, epoch 3, batch 447/481,disc_loss 83.225, (real 86.425, fake 80.024 ) gen_loss 442.99\n",
            "iteration 1610, epoch 3, batch 448/481,disc_loss 77.448, (real 80.751, fake 74.146 ) gen_loss 460.14\n",
            "iteration 1611, epoch 3, batch 449/481,disc_loss 77.311, (real 81.176, fake 73.447 ) gen_loss 486.48\n",
            "iteration 1612, epoch 3, batch 450/481,disc_loss 78.968, (real 83.233, fake 74.703 ) gen_loss 498.4\n",
            "iteration 1613, epoch 3, batch 451/481,disc_loss 80.15, (real 83.637, fake 76.662 ) gen_loss 509.67\n",
            "iteration 1614, epoch 3, batch 452/481,disc_loss 83.682, (real 87.528, fake 79.836 ) gen_loss 493.29\n",
            "iteration 1615, epoch 3, batch 453/481,disc_loss 79.843, (real 84.908, fake 74.779 ) gen_loss 481.81\n",
            "iteration 1616, epoch 3, batch 454/481,disc_loss 84.322, (real 89.721, fake 78.923 ) gen_loss 500.91\n",
            "iteration 1617, epoch 3, batch 455/481,disc_loss 80.338, (real 83.94, fake 76.737 ) gen_loss 514.63\n",
            "iteration 1618, epoch 3, batch 456/481,disc_loss 77.4, (real 80.931, fake 73.868 ) gen_loss 536.43\n",
            "iteration 1619, epoch 3, batch 457/481,disc_loss 78.005, (real 81.315, fake 74.694 ) gen_loss 489.18\n",
            "iteration 1620, epoch 3, batch 458/481,disc_loss 78.675, (real 83.197, fake 74.154 ) gen_loss 536.95\n",
            "iteration 1621, epoch 3, batch 459/481,disc_loss 82.022, (real 85.482, fake 78.561 ) gen_loss 556.7\n",
            "iteration 1622, epoch 3, batch 460/481,disc_loss 79.242, (real 82.789, fake 75.696 ) gen_loss 608.29\n",
            "iteration 1623, epoch 3, batch 461/481,disc_loss 78.834, (real 81.594, fake 76.073 ) gen_loss 522.94\n",
            "iteration 1624, epoch 3, batch 462/481,disc_loss 80.425, (real 84.474, fake 76.375 ) gen_loss 534.43\n",
            "iteration 1625, epoch 3, batch 463/481,disc_loss 79.059, (real 82.833, fake 75.285 ) gen_loss 494.63\n",
            "iteration 1626, epoch 3, batch 464/481,disc_loss 78.96, (real 82.195, fake 75.724 ) gen_loss 512.43\n",
            "iteration 1627, epoch 3, batch 465/481,disc_loss 81.889, (real 85.414, fake 78.364 ) gen_loss 521.41\n",
            "iteration 1628, epoch 3, batch 466/481,disc_loss 80.498, (real 82.435, fake 78.562 ) gen_loss 560.49\n",
            "iteration 1629, epoch 3, batch 467/481,disc_loss 75.189, (real 78.218, fake 72.16 ) gen_loss 547.72\n",
            "iteration 1630, epoch 3, batch 468/481,disc_loss 81.855, (real 84.868, fake 78.842 ) gen_loss 601.06\n",
            "iteration 1631, epoch 3, batch 469/481,disc_loss 79.487, (real 83.535, fake 75.439 ) gen_loss 504.43\n",
            "iteration 1632, epoch 3, batch 470/481,disc_loss 76.076, (real 79.605, fake 72.546 ) gen_loss 484.71\n",
            "iteration 1633, epoch 3, batch 471/481,disc_loss 80.226, (real 83.148, fake 77.304 ) gen_loss 481.65\n",
            "iteration 1634, epoch 3, batch 472/481,disc_loss 76.593, (real 80.557, fake 72.629 ) gen_loss 491.92\n",
            "iteration 1635, epoch 3, batch 473/481,disc_loss 77.373, (real 82.326, fake 72.42 ) gen_loss 497.39\n",
            "iteration 1636, epoch 3, batch 474/481,disc_loss 76.336, (real 80.214, fake 72.459 ) gen_loss 535.43\n",
            "iteration 1637, epoch 3, batch 475/481,disc_loss 82.921, (real 87.678, fake 78.163 ) gen_loss 682.12\n",
            "iteration 1638, epoch 3, batch 476/481,disc_loss 81.22, (real 85.721, fake 76.72 ) gen_loss 616.87\n",
            "iteration 1639, epoch 3, batch 477/481,disc_loss 81.241, (real 85.144, fake 77.337 ) gen_loss 620.92\n",
            "iteration 1640, epoch 3, batch 478/481,disc_loss 82.088, (real 86.906, fake 77.271 ) gen_loss 616.38\n",
            "iteration 1641, epoch 3, batch 479/481,disc_loss 79.588, (real 81.559, fake 77.617 ) gen_loss 521.27\n",
            "iteration 1642, epoch 3, batch 480/481,disc_loss 79.126, (real 83.279, fake 74.973 ) gen_loss 536.73\n",
            "iteration 1643, epoch 3, batch 481/481,disc_loss 82.833, (real 87.205, fake 78.461 ) gen_loss 504.95\n",
            "iteration 1644, epoch 4, batch 1/481,disc_loss 74.414, (real 77.535, fake 71.293 ) gen_loss 498.0\n",
            "iteration 1645, epoch 4, batch 2/481,disc_loss 80.106, (real 82.47, fake 77.742 ) gen_loss 503.64\n",
            "iteration 1646, epoch 4, batch 3/481,disc_loss 80.041, (real 82.808, fake 77.275 ) gen_loss 522.98\n",
            "iteration 1647, epoch 4, batch 4/481,disc_loss 79.098, (real 82.572, fake 75.623 ) gen_loss 479.84\n",
            "iteration 1648, epoch 4, batch 5/481,disc_loss 78.811, (real 82.437, fake 75.186 ) gen_loss 639.44\n",
            "iteration 1649, epoch 4, batch 6/481,disc_loss 78.656, (real 82.119, fake 75.193 ) gen_loss 526.9\n",
            "iteration 1650, epoch 4, batch 7/481,disc_loss 81.319, (real 84.415, fake 78.223 ) gen_loss 478.17\n",
            "iteration 1651, epoch 4, batch 8/481,disc_loss 83.882, (real 86.786, fake 80.979 ) gen_loss 518.1\n",
            "iteration 1652, epoch 4, batch 9/481,disc_loss 81.321, (real 83.785, fake 78.857 ) gen_loss 493.1\n",
            "iteration 1653, epoch 4, batch 10/481,disc_loss 79.806, (real 83.304, fake 76.308 ) gen_loss 539.38\n",
            "iteration 1654, epoch 4, batch 11/481,disc_loss 75.807, (real 79.259, fake 72.354 ) gen_loss 517.12\n",
            "iteration 1655, epoch 4, batch 12/481,disc_loss 85.689, (real 89.104, fake 82.275 ) gen_loss 489.66\n",
            "iteration 1656, epoch 4, batch 13/481,disc_loss 77.298, (real 80.102, fake 74.494 ) gen_loss 496.18\n",
            "iteration 1657, epoch 4, batch 14/481,disc_loss 78.581, (real 82.179, fake 74.984 ) gen_loss 483.6\n",
            "iteration 1658, epoch 4, batch 15/481,disc_loss 80.724, (real 83.693, fake 77.755 ) gen_loss 536.25\n",
            "iteration 1659, epoch 4, batch 16/481,disc_loss 80.094, (real 83.099, fake 77.089 ) gen_loss 531.66\n",
            "iteration 1660, epoch 4, batch 17/481,disc_loss 79.451, (real 82.802, fake 76.1 ) gen_loss 532.04\n",
            "iteration 1661, epoch 4, batch 18/481,disc_loss 81.611, (real 84.692, fake 78.529 ) gen_loss 515.09\n",
            "iteration 1662, epoch 4, batch 19/481,disc_loss 78.807, (real 81.817, fake 75.797 ) gen_loss 540.33\n",
            "iteration 1663, epoch 4, batch 20/481,disc_loss 76.81, (real 80.594, fake 73.026 ) gen_loss 525.96\n",
            "iteration 1664, epoch 4, batch 21/481,disc_loss 76.665, (real 79.074, fake 74.256 ) gen_loss 589.23\n",
            "iteration 1665, epoch 4, batch 22/481,disc_loss 84.307, (real 87.636, fake 80.978 ) gen_loss 766.08\n",
            "iteration 1666, epoch 4, batch 23/481,disc_loss 80.691, (real 84.388, fake 76.995 ) gen_loss 845.06\n",
            "iteration 1667, epoch 4, batch 24/481,disc_loss 85.24, (real 85.766, fake 84.714 ) gen_loss 782.09\n",
            "iteration 1668, epoch 4, batch 25/481,disc_loss 81.936, (real 85.347, fake 78.525 ) gen_loss 564.7\n",
            "iteration 1669, epoch 4, batch 26/481,disc_loss 79.679, (real 82.472, fake 76.886 ) gen_loss 600.53\n",
            "iteration 1670, epoch 4, batch 27/481,disc_loss 84.351, (real 88.011, fake 80.69 ) gen_loss 566.88\n",
            "iteration 1671, epoch 4, batch 28/481,disc_loss 80.776, (real 83.568, fake 77.984 ) gen_loss 518.84\n",
            "iteration 1672, epoch 4, batch 29/481,disc_loss 76.588, (real 80.373, fake 72.803 ) gen_loss 506.11\n",
            "iteration 1673, epoch 4, batch 30/481,disc_loss 80.501, (real 84.501, fake 76.5 ) gen_loss 523.15\n",
            "iteration 1674, epoch 4, batch 31/481,disc_loss 80.736, (real 83.602, fake 77.87 ) gen_loss 519.12\n",
            "iteration 1675, epoch 4, batch 32/481,disc_loss 77.967, (real 80.958, fake 74.976 ) gen_loss 561.97\n",
            "iteration 1676, epoch 4, batch 33/481,disc_loss 79.796, (real 82.275, fake 77.317 ) gen_loss 559.37\n",
            "iteration 1677, epoch 4, batch 34/481,disc_loss 77.083, (real 80.619, fake 73.547 ) gen_loss 547.86\n",
            "iteration 1678, epoch 4, batch 35/481,disc_loss 78.064, (real 80.9, fake 75.229 ) gen_loss 591.18\n",
            "iteration 1679, epoch 4, batch 36/481,disc_loss 84.056, (real 87.039, fake 81.074 ) gen_loss 585.67\n",
            "iteration 1680, epoch 4, batch 37/481,disc_loss 80.214, (real 84.22, fake 76.208 ) gen_loss 517.92\n",
            "iteration 1681, epoch 4, batch 38/481,disc_loss 77.171, (real 81.784, fake 72.559 ) gen_loss 538.97\n",
            "iteration 1682, epoch 4, batch 39/481,disc_loss 78.876, (real 83.538, fake 74.214 ) gen_loss 537.32\n",
            "iteration 1683, epoch 4, batch 40/481,disc_loss 76.068, (real 78.212, fake 73.924 ) gen_loss 487.12\n",
            "iteration 1684, epoch 4, batch 41/481,disc_loss 78.639, (real 81.207, fake 76.071 ) gen_loss 486.49\n",
            "iteration 1685, epoch 4, batch 42/481,disc_loss 83.348, (real 84.463, fake 82.234 ) gen_loss 541.62\n",
            "iteration 1686, epoch 4, batch 43/481,disc_loss 79.777, (real 82.637, fake 76.917 ) gen_loss 484.36\n",
            "iteration 1687, epoch 4, batch 44/481,disc_loss 74.394, (real 78.545, fake 70.244 ) gen_loss 519.78\n",
            "iteration 1688, epoch 4, batch 45/481,disc_loss 80.073, (real 82.707, fake 77.44 ) gen_loss 532.42\n",
            "iteration 1689, epoch 4, batch 46/481,disc_loss 81.976, (real 85.651, fake 78.3 ) gen_loss 518.07\n",
            "iteration 1690, epoch 4, batch 47/481,disc_loss 80.194, (real 82.742, fake 77.646 ) gen_loss 475.25\n",
            "iteration 1691, epoch 4, batch 48/481,disc_loss 79.76, (real 82.848, fake 76.672 ) gen_loss 539.7\n",
            "iteration 1692, epoch 4, batch 49/481,disc_loss 76.253, (real 79.163, fake 73.343 ) gen_loss 512.49\n",
            "iteration 1693, epoch 4, batch 50/481,disc_loss 78.392, (real 81.718, fake 75.067 ) gen_loss 546.89\n",
            "iteration 1694, epoch 4, batch 51/481,disc_loss 75.398, (real 78.866, fake 71.93 ) gen_loss 491.94\n",
            "iteration 1695, epoch 4, batch 52/481,disc_loss 77.223, (real 80.049, fake 74.397 ) gen_loss 537.25\n",
            "iteration 1696, epoch 4, batch 53/481,disc_loss 80.289, (real 84.233, fake 76.346 ) gen_loss 709.55\n",
            "iteration 1697, epoch 4, batch 54/481,disc_loss 82.933, (real 85.547, fake 80.319 ) gen_loss 543.64\n",
            "iteration 1698, epoch 4, batch 55/481,disc_loss 77.157, (real 80.398, fake 73.917 ) gen_loss 519.32\n",
            "iteration 1699, epoch 4, batch 56/481,disc_loss 84.069, (real 87.716, fake 80.422 ) gen_loss 537.39\n",
            "iteration 1700, epoch 4, batch 57/481,disc_loss 84.502, (real 88.953, fake 80.051 ) gen_loss 556.46\n",
            "iteration 1701, epoch 4, batch 58/481,disc_loss 83.761, (real 86.509, fake 81.014 ) gen_loss 539.52\n",
            "iteration 1702, epoch 4, batch 59/481,disc_loss 81.342, (real 84.46, fake 78.223 ) gen_loss 517.73\n",
            "iteration 1703, epoch 4, batch 60/481,disc_loss 82.226, (real 86.09, fake 78.362 ) gen_loss 471.64\n",
            "iteration 1704, epoch 4, batch 61/481,disc_loss 78.379, (real 82.752, fake 74.006 ) gen_loss 499.3\n",
            "iteration 1705, epoch 4, batch 62/481,disc_loss 79.946, (real 83.708, fake 76.184 ) gen_loss 538.38\n",
            "iteration 1706, epoch 4, batch 63/481,disc_loss 81.333, (real 86.187, fake 76.478 ) gen_loss 597.38\n",
            "iteration 1707, epoch 4, batch 64/481,disc_loss 78.199, (real 80.822, fake 75.577 ) gen_loss 512.84\n",
            "iteration 1708, epoch 4, batch 65/481,disc_loss 83.331, (real 87.461, fake 79.201 ) gen_loss 525.14\n",
            "iteration 1709, epoch 4, batch 66/481,disc_loss 79.019, (real 82.503, fake 75.536 ) gen_loss 442.79\n",
            "iteration 1710, epoch 4, batch 67/481,disc_loss 75.621, (real 78.95, fake 72.292 ) gen_loss 497.64\n",
            "iteration 1711, epoch 4, batch 68/481,disc_loss 80.764, (real 83.571, fake 77.957 ) gen_loss 556.22\n",
            "iteration 1712, epoch 4, batch 69/481,disc_loss 81.135, (real 84.166, fake 78.104 ) gen_loss 594.31\n",
            "iteration 1713, epoch 4, batch 70/481,disc_loss 81.491, (real 84.88, fake 78.102 ) gen_loss 582.66\n",
            "iteration 1714, epoch 4, batch 71/481,disc_loss 78.864, (real 81.564, fake 76.165 ) gen_loss 559.43\n",
            "iteration 1715, epoch 4, batch 72/481,disc_loss 81.146, (real 83.891, fake 78.402 ) gen_loss 622.19\n",
            "iteration 1716, epoch 4, batch 73/481,disc_loss 82.062, (real 85.492, fake 78.631 ) gen_loss 510.28\n",
            "iteration 1717, epoch 4, batch 74/481,disc_loss 80.643, (real 85.612, fake 75.675 ) gen_loss 441.56\n",
            "iteration 1718, epoch 4, batch 75/481,disc_loss 80.129, (real 83.666, fake 76.591 ) gen_loss 493.38\n",
            "iteration 1719, epoch 4, batch 76/481,disc_loss 77.272, (real 81.129, fake 73.414 ) gen_loss 510.49\n",
            "iteration 1720, epoch 4, batch 77/481,disc_loss 76.86, (real 80.323, fake 73.397 ) gen_loss 495.12\n",
            "iteration 1721, epoch 4, batch 78/481,disc_loss 80.826, (real 84.067, fake 77.585 ) gen_loss 522.71\n",
            "iteration 1722, epoch 4, batch 79/481,disc_loss 81.615, (real 86.28, fake 76.95 ) gen_loss 485.59\n",
            "iteration 1723, epoch 4, batch 80/481,disc_loss 80.525, (real 83.396, fake 77.653 ) gen_loss 519.74\n",
            "iteration 1724, epoch 4, batch 81/481,disc_loss 75.998, (real 79.069, fake 72.928 ) gen_loss 523.18\n",
            "iteration 1725, epoch 4, batch 82/481,disc_loss 79.713, (real 83.944, fake 75.483 ) gen_loss 494.19\n",
            "iteration 1726, epoch 4, batch 83/481,disc_loss 75.876, (real 81.129, fake 70.623 ) gen_loss 542.88\n",
            "iteration 1727, epoch 4, batch 84/481,disc_loss 81.096, (real 85.045, fake 77.147 ) gen_loss 572.86\n",
            "iteration 1728, epoch 4, batch 85/481,disc_loss 82.768, (real 86.101, fake 79.435 ) gen_loss 538.08\n",
            "iteration 1729, epoch 4, batch 86/481,disc_loss 81.476, (real 84.148, fake 78.804 ) gen_loss 561.07\n",
            "iteration 1730, epoch 4, batch 87/481,disc_loss 80.122, (real 84.074, fake 76.17 ) gen_loss 536.83\n",
            "iteration 1731, epoch 4, batch 88/481,disc_loss 80.993, (real 84.327, fake 77.659 ) gen_loss 579.51\n",
            "iteration 1732, epoch 4, batch 89/481,disc_loss 76.599, (real 79.534, fake 73.664 ) gen_loss 549.0\n",
            "iteration 1733, epoch 4, batch 90/481,disc_loss 75.661, (real 78.468, fake 72.854 ) gen_loss 507.3\n",
            "iteration 1734, epoch 4, batch 91/481,disc_loss 75.844, (real 79.589, fake 72.1 ) gen_loss 553.68\n",
            "iteration 1735, epoch 4, batch 92/481,disc_loss 81.279, (real 84.466, fake 78.093 ) gen_loss 493.76\n",
            "iteration 1736, epoch 4, batch 93/481,disc_loss 79.537, (real 82.896, fake 76.179 ) gen_loss 463.97\n",
            "iteration 1737, epoch 4, batch 94/481,disc_loss 79.259, (real 82.1, fake 76.419 ) gen_loss 446.95\n",
            "iteration 1738, epoch 4, batch 95/481,disc_loss 78.218, (real 81.216, fake 75.22 ) gen_loss 499.84\n",
            "iteration 1739, epoch 4, batch 96/481,disc_loss 80.506, (real 84.196, fake 76.816 ) gen_loss 493.29\n",
            "iteration 1740, epoch 4, batch 97/481,disc_loss 80.59, (real 84.513, fake 76.666 ) gen_loss 498.45\n",
            "iteration 1741, epoch 4, batch 98/481,disc_loss 79.903, (real 83.5, fake 76.307 ) gen_loss 635.22\n",
            "iteration 1742, epoch 4, batch 99/481,disc_loss 78.573, (real 81.663, fake 75.483 ) gen_loss 583.46\n",
            "iteration 1743, epoch 4, batch 100/481,disc_loss 81.877, (real 84.397, fake 79.357 ) gen_loss 563.65\n",
            "iteration 1744, epoch 4, batch 101/481,disc_loss 78.222, (real 79.987, fake 76.458 ) gen_loss 496.8\n",
            "iteration 1745, epoch 4, batch 102/481,disc_loss 81.231, (real 84.469, fake 77.994 ) gen_loss 484.72\n",
            "iteration 1746, epoch 4, batch 103/481,disc_loss 77.359, (real 79.081, fake 75.637 ) gen_loss 468.99\n",
            "iteration 1747, epoch 4, batch 104/481,disc_loss 80.395, (real 83.639, fake 77.151 ) gen_loss 550.59\n",
            "iteration 1748, epoch 4, batch 105/481,disc_loss 76.011, (real 78.69, fake 73.333 ) gen_loss 541.66\n",
            "iteration 1749, epoch 4, batch 106/481,disc_loss 82.295, (real 85.442, fake 79.149 ) gen_loss 510.89\n",
            "iteration 1750, epoch 4, batch 107/481,disc_loss 81.496, (real 84.437, fake 78.555 ) gen_loss 496.25\n",
            "iteration 1751, epoch 4, batch 108/481,disc_loss 76.909, (real 80.508, fake 73.311 ) gen_loss 490.83\n",
            "iteration 1752, epoch 4, batch 109/481,disc_loss 81.292, (real 84.74, fake 77.844 ) gen_loss 472.2\n",
            "iteration 1753, epoch 4, batch 110/481,disc_loss 81.591, (real 84.385, fake 78.798 ) gen_loss 466.98\n",
            "iteration 1754, epoch 4, batch 111/481,disc_loss 80.13, (real 85.082, fake 75.179 ) gen_loss 478.99\n",
            "iteration 1755, epoch 4, batch 112/481,disc_loss 72.836, (real 76.818, fake 68.853 ) gen_loss 499.55\n",
            "iteration 1756, epoch 4, batch 113/481,disc_loss 80.495, (real 83.901, fake 77.089 ) gen_loss 483.82\n",
            "iteration 1757, epoch 4, batch 114/481,disc_loss 78.496, (real 81.558, fake 75.434 ) gen_loss 529.47\n",
            "iteration 1758, epoch 4, batch 115/481,disc_loss 75.726, (real 79.318, fake 72.135 ) gen_loss 510.2\n",
            "iteration 1759, epoch 4, batch 116/481,disc_loss 80.618, (real 82.568, fake 78.668 ) gen_loss 513.5\n",
            "iteration 1760, epoch 4, batch 117/481,disc_loss 79.274, (real 82.269, fake 76.28 ) gen_loss 529.04\n",
            "iteration 1761, epoch 4, batch 118/481,disc_loss 80.921, (real 84.485, fake 77.356 ) gen_loss 520.67\n",
            "iteration 1762, epoch 4, batch 119/481,disc_loss 78.221, (real 81.583, fake 74.86 ) gen_loss 529.32\n",
            "iteration 1763, epoch 4, batch 120/481,disc_loss 79.815, (real 83.413, fake 76.218 ) gen_loss 478.97\n",
            "iteration 1764, epoch 4, batch 121/481,disc_loss 80.427, (real 82.907, fake 77.947 ) gen_loss 503.31\n",
            "iteration 1765, epoch 4, batch 122/481,disc_loss 82.049, (real 83.973, fake 80.126 ) gen_loss 510.27\n",
            "iteration 1766, epoch 4, batch 123/481,disc_loss 84.716, (real 86.382, fake 83.05 ) gen_loss 501.29\n",
            "iteration 1767, epoch 4, batch 124/481,disc_loss 78.991, (real 82.86, fake 75.122 ) gen_loss 519.65\n",
            "iteration 1768, epoch 4, batch 125/481,disc_loss 80.112, (real 83.025, fake 77.199 ) gen_loss 528.45\n",
            "iteration 1769, epoch 4, batch 126/481,disc_loss 77.947, (real 80.92, fake 74.974 ) gen_loss 501.0\n",
            "iteration 1770, epoch 4, batch 127/481,disc_loss 78.22, (real 81.899, fake 74.542 ) gen_loss 513.37\n",
            "iteration 1771, epoch 4, batch 128/481,disc_loss 79.924, (real 83.576, fake 76.272 ) gen_loss 538.98\n",
            "iteration 1772, epoch 4, batch 129/481,disc_loss 80.477, (real 83.543, fake 77.411 ) gen_loss 545.32\n",
            "iteration 1773, epoch 4, batch 130/481,disc_loss 82.938, (real 86.231, fake 79.645 ) gen_loss 492.05\n",
            "iteration 1774, epoch 4, batch 131/481,disc_loss 77.681, (real 80.413, fake 74.949 ) gen_loss 541.27\n",
            "iteration 1775, epoch 4, batch 132/481,disc_loss 78.797, (real 81.66, fake 75.935 ) gen_loss 571.2\n",
            "iteration 1776, epoch 4, batch 133/481,disc_loss 78.315, (real 82.491, fake 74.138 ) gen_loss 495.01\n",
            "iteration 1777, epoch 4, batch 134/481,disc_loss 76.781, (real 79.864, fake 73.698 ) gen_loss 491.81\n",
            "iteration 1778, epoch 4, batch 135/481,disc_loss 81.835, (real 85.839, fake 77.831 ) gen_loss 504.65\n",
            "iteration 1779, epoch 4, batch 136/481,disc_loss 79.393, (real 83.471, fake 75.315 ) gen_loss 585.05\n",
            "iteration 1780, epoch 4, batch 137/481,disc_loss 76.32, (real 79.108, fake 73.531 ) gen_loss 587.9\n",
            "iteration 1781, epoch 4, batch 138/481,disc_loss 80.068, (real 82.7, fake 77.436 ) gen_loss 532.15\n",
            "iteration 1782, epoch 4, batch 139/481,disc_loss 80.614, (real 83.419, fake 77.808 ) gen_loss 569.18\n",
            "iteration 1783, epoch 4, batch 140/481,disc_loss 87.99, (real 92.403, fake 83.577 ) gen_loss 560.36\n",
            "iteration 1784, epoch 4, batch 141/481,disc_loss 81.308, (real 83.797, fake 78.819 ) gen_loss 501.7\n",
            "iteration 1785, epoch 4, batch 142/481,disc_loss 76.058, (real 80.14, fake 71.976 ) gen_loss 543.97\n",
            "iteration 1786, epoch 4, batch 143/481,disc_loss 82.067, (real 85.37, fake 78.764 ) gen_loss 509.55\n",
            "iteration 1787, epoch 4, batch 144/481,disc_loss 80.67, (real 84.961, fake 76.38 ) gen_loss 543.17\n",
            "iteration 1788, epoch 4, batch 145/481,disc_loss 81.549, (real 85.641, fake 77.456 ) gen_loss 550.47\n",
            "iteration 1789, epoch 4, batch 146/481,disc_loss 81.681, (real 85.528, fake 77.834 ) gen_loss 614.4\n",
            "iteration 1790, epoch 4, batch 147/481,disc_loss 76.927, (real 79.646, fake 74.207 ) gen_loss 526.58\n",
            "iteration 1791, epoch 4, batch 148/481,disc_loss 79.116, (real 83.291, fake 74.941 ) gen_loss 539.94\n",
            "iteration 1792, epoch 4, batch 149/481,disc_loss 78.136, (real 81.261, fake 75.011 ) gen_loss 522.18\n",
            "iteration 1793, epoch 4, batch 150/481,disc_loss 76.182, (real 79.685, fake 72.68 ) gen_loss 546.23\n",
            "iteration 1794, epoch 4, batch 151/481,disc_loss 75.927, (real 79.192, fake 72.662 ) gen_loss 518.05\n",
            "iteration 1795, epoch 4, batch 152/481,disc_loss 82.575, (real 85.16, fake 79.99 ) gen_loss 522.19\n",
            "iteration 1796, epoch 4, batch 153/481,disc_loss 78.926, (real 81.658, fake 76.194 ) gen_loss 520.85\n",
            "iteration 1797, epoch 4, batch 154/481,disc_loss 75.867, (real 79.738, fake 71.997 ) gen_loss 475.97\n",
            "iteration 1798, epoch 4, batch 155/481,disc_loss 80.634, (real 84.053, fake 77.216 ) gen_loss 528.56\n",
            "iteration 1799, epoch 4, batch 156/481,disc_loss 79.369, (real 83.29, fake 75.447 ) gen_loss 558.66\n",
            "iteration 1800, epoch 4, batch 157/481,disc_loss 78.216, (real 80.948, fake 75.483 ) gen_loss 514.76\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1801, epoch 4, batch 158/481,disc_loss 78.493, (real 81.479, fake 75.506 ) gen_loss 465.06\n",
            "iteration 1802, epoch 4, batch 159/481,disc_loss 83.919, (real 88.623, fake 79.215 ) gen_loss 534.87\n",
            "iteration 1803, epoch 4, batch 160/481,disc_loss 77.996, (real 81.199, fake 74.792 ) gen_loss 476.97\n",
            "iteration 1804, epoch 4, batch 161/481,disc_loss 79.239, (real 82.581, fake 75.897 ) gen_loss 481.34\n",
            "iteration 1805, epoch 4, batch 162/481,disc_loss 77.944, (real 81.446, fake 74.441 ) gen_loss 487.67\n",
            "iteration 1806, epoch 4, batch 163/481,disc_loss 79.153, (real 82.075, fake 76.231 ) gen_loss 567.69\n",
            "iteration 1807, epoch 4, batch 164/481,disc_loss 82.014, (real 86.141, fake 77.888 ) gen_loss 691.67\n",
            "iteration 1808, epoch 4, batch 165/481,disc_loss 81.472, (real 81.825, fake 81.118 ) gen_loss 582.05\n",
            "iteration 1809, epoch 4, batch 166/481,disc_loss 78.594, (real 81.955, fake 75.234 ) gen_loss 575.03\n",
            "iteration 1810, epoch 4, batch 167/481,disc_loss 79.445, (real 82.526, fake 76.363 ) gen_loss 527.23\n",
            "iteration 1811, epoch 4, batch 168/481,disc_loss 82.865, (real 86.569, fake 79.161 ) gen_loss 532.18\n",
            "iteration 1812, epoch 4, batch 169/481,disc_loss 73.54, (real 77.174, fake 69.906 ) gen_loss 506.51\n",
            "iteration 1813, epoch 4, batch 170/481,disc_loss 80.621, (real 84.121, fake 77.121 ) gen_loss 549.16\n",
            "iteration 1814, epoch 4, batch 171/481,disc_loss 73.346, (real 77.039, fake 69.653 ) gen_loss 539.55\n",
            "iteration 1815, epoch 4, batch 172/481,disc_loss 75.746, (real 78.599, fake 72.893 ) gen_loss 535.94\n",
            "iteration 1816, epoch 4, batch 173/481,disc_loss 79.061, (real 81.606, fake 76.516 ) gen_loss 491.67\n",
            "iteration 1817, epoch 4, batch 174/481,disc_loss 82.769, (real 86.313, fake 79.224 ) gen_loss 551.7\n",
            "iteration 1818, epoch 4, batch 175/481,disc_loss 85.111, (real 87.673, fake 82.55 ) gen_loss 510.96\n",
            "iteration 1819, epoch 4, batch 176/481,disc_loss 79.46, (real 83.066, fake 75.854 ) gen_loss 502.52\n",
            "iteration 1820, epoch 4, batch 177/481,disc_loss 81.911, (real 84.357, fake 79.465 ) gen_loss 523.93\n",
            "iteration 1821, epoch 4, batch 178/481,disc_loss 78.643, (real 80.732, fake 76.553 ) gen_loss 526.15\n",
            "iteration 1822, epoch 4, batch 179/481,disc_loss 76.559, (real 79.493, fake 73.625 ) gen_loss 540.33\n",
            "iteration 1823, epoch 4, batch 180/481,disc_loss 81.492, (real 84.384, fake 78.601 ) gen_loss 510.84\n",
            "iteration 1824, epoch 4, batch 181/481,disc_loss 83.768, (real 88.817, fake 78.718 ) gen_loss 491.95\n",
            "iteration 1825, epoch 4, batch 182/481,disc_loss 81.331, (real 84.618, fake 78.044 ) gen_loss 549.63\n",
            "iteration 1826, epoch 4, batch 183/481,disc_loss 78.25, (real 81.318, fake 75.182 ) gen_loss 516.93\n",
            "iteration 1827, epoch 4, batch 184/481,disc_loss 81.4, (real 84.753, fake 78.047 ) gen_loss 513.81\n",
            "iteration 1828, epoch 4, batch 185/481,disc_loss 82.087, (real 85.918, fake 78.256 ) gen_loss 520.13\n",
            "iteration 1829, epoch 4, batch 186/481,disc_loss 78.821, (real 82.338, fake 75.304 ) gen_loss 565.18\n",
            "iteration 1830, epoch 4, batch 187/481,disc_loss 84.493, (real 87.225, fake 81.761 ) gen_loss 507.15\n",
            "iteration 1831, epoch 4, batch 188/481,disc_loss 78.626, (real 82.432, fake 74.82 ) gen_loss 463.55\n",
            "iteration 1832, epoch 4, batch 189/481,disc_loss 79.466, (real 82.477, fake 76.454 ) gen_loss 522.02\n",
            "iteration 1833, epoch 4, batch 190/481,disc_loss 80.051, (real 83.005, fake 77.097 ) gen_loss 495.88\n",
            "iteration 1834, epoch 4, batch 191/481,disc_loss 81.674, (real 84.726, fake 78.621 ) gen_loss 574.45\n",
            "iteration 1835, epoch 4, batch 192/481,disc_loss 80.151, (real 83.97, fake 76.331 ) gen_loss 479.68\n",
            "iteration 1836, epoch 4, batch 193/481,disc_loss 76.575, (real 80.07, fake 73.079 ) gen_loss 524.07\n",
            "iteration 1837, epoch 4, batch 194/481,disc_loss 81.347, (real 84.479, fake 78.215 ) gen_loss 517.82\n",
            "iteration 1838, epoch 4, batch 195/481,disc_loss 82.972, (real 86.126, fake 79.818 ) gen_loss 556.14\n",
            "iteration 1839, epoch 4, batch 196/481,disc_loss 81.944, (real 84.597, fake 79.29 ) gen_loss 530.45\n",
            "iteration 1840, epoch 4, batch 197/481,disc_loss 71.614, (real 74.729, fake 68.498 ) gen_loss 588.36\n",
            "iteration 1841, epoch 4, batch 198/481,disc_loss 85.73, (real 89.161, fake 82.299 ) gen_loss 539.21\n",
            "iteration 1842, epoch 4, batch 199/481,disc_loss 78.376, (real 82.987, fake 73.764 ) gen_loss 513.84\n",
            "iteration 1843, epoch 4, batch 200/481,disc_loss 78.634, (real 81.047, fake 76.22 ) gen_loss 489.85\n",
            "iteration 1844, epoch 4, batch 201/481,disc_loss 80.417, (real 83.29, fake 77.543 ) gen_loss 529.38\n",
            "iteration 1845, epoch 4, batch 202/481,disc_loss 81.838, (real 84.981, fake 78.694 ) gen_loss 490.13\n",
            "iteration 1846, epoch 4, batch 203/481,disc_loss 74.157, (real 76.861, fake 71.453 ) gen_loss 591.6\n",
            "iteration 1847, epoch 4, batch 204/481,disc_loss 71.969, (real 74.747, fake 69.191 ) gen_loss 571.4\n",
            "iteration 1848, epoch 4, batch 205/481,disc_loss 79.97, (real 83.015, fake 76.924 ) gen_loss 593.52\n",
            "iteration 1849, epoch 4, batch 206/481,disc_loss 77.087, (real 81.381, fake 72.794 ) gen_loss 562.14\n",
            "iteration 1850, epoch 4, batch 207/481,disc_loss 82.992, (real 85.551, fake 80.433 ) gen_loss 595.34\n",
            "iteration 1851, epoch 4, batch 208/481,disc_loss 80.866, (real 83.175, fake 78.556 ) gen_loss 533.59\n",
            "iteration 1852, epoch 4, batch 209/481,disc_loss 78.581, (real 81.776, fake 75.386 ) gen_loss 519.67\n",
            "iteration 1853, epoch 4, batch 210/481,disc_loss 80.12, (real 84.397, fake 75.843 ) gen_loss 632.01\n",
            "iteration 1854, epoch 4, batch 211/481,disc_loss 78.376, (real 81.99, fake 74.761 ) gen_loss 620.99\n",
            "iteration 1855, epoch 4, batch 212/481,disc_loss 81.374, (real 83.844, fake 78.904 ) gen_loss 524.65\n",
            "iteration 1856, epoch 4, batch 213/481,disc_loss 84.284, (real 87.866, fake 80.703 ) gen_loss 538.16\n",
            "iteration 1857, epoch 4, batch 214/481,disc_loss 76.3, (real 79.897, fake 72.702 ) gen_loss 502.25\n",
            "iteration 1858, epoch 4, batch 215/481,disc_loss 81.642, (real 85.595, fake 77.688 ) gen_loss 545.82\n",
            "iteration 1859, epoch 4, batch 216/481,disc_loss 78.029, (real 81.484, fake 74.573 ) gen_loss 526.02\n",
            "iteration 1860, epoch 4, batch 217/481,disc_loss 77.87, (real 80.894, fake 74.847 ) gen_loss 511.0\n",
            "iteration 1861, epoch 4, batch 218/481,disc_loss 79.405, (real 82.935, fake 75.876 ) gen_loss 523.69\n",
            "iteration 1862, epoch 4, batch 219/481,disc_loss 78.991, (real 82.927, fake 75.056 ) gen_loss 508.83\n",
            "iteration 1863, epoch 4, batch 220/481,disc_loss 76.786, (real 79.232, fake 74.34 ) gen_loss 496.18\n",
            "iteration 1864, epoch 4, batch 221/481,disc_loss 78.891, (real 82.013, fake 75.769 ) gen_loss 555.01\n",
            "iteration 1865, epoch 4, batch 222/481,disc_loss 86.68, (real 88.253, fake 85.108 ) gen_loss 603.68\n",
            "iteration 1866, epoch 4, batch 223/481,disc_loss 78.337, (real 81.949, fake 74.724 ) gen_loss 659.89\n",
            "iteration 1867, epoch 4, batch 224/481,disc_loss 78.86, (real 82.04, fake 75.68 ) gen_loss 488.12\n",
            "iteration 1868, epoch 4, batch 225/481,disc_loss 78.385, (real 81.667, fake 75.104 ) gen_loss 576.53\n",
            "iteration 1869, epoch 4, batch 226/481,disc_loss 82.469, (real 86.504, fake 78.435 ) gen_loss 606.39\n",
            "iteration 1870, epoch 4, batch 227/481,disc_loss 81.84, (real 84.642, fake 79.037 ) gen_loss 521.33\n",
            "iteration 1871, epoch 4, batch 228/481,disc_loss 80.258, (real 83.873, fake 76.642 ) gen_loss 509.44\n",
            "iteration 1872, epoch 4, batch 229/481,disc_loss 78.653, (real 81.044, fake 76.262 ) gen_loss 507.82\n",
            "iteration 1873, epoch 4, batch 230/481,disc_loss 77.62, (real 80.303, fake 74.937 ) gen_loss 540.91\n",
            "iteration 1874, epoch 4, batch 231/481,disc_loss 79.677, (real 83.087, fake 76.266 ) gen_loss 521.49\n",
            "iteration 1875, epoch 4, batch 232/481,disc_loss 84.61, (real 87.923, fake 81.297 ) gen_loss 456.62\n",
            "iteration 1876, epoch 4, batch 233/481,disc_loss 80.392, (real 83.243, fake 77.541 ) gen_loss 514.32\n",
            "iteration 1877, epoch 4, batch 234/481,disc_loss 78.817, (real 81.31, fake 76.323 ) gen_loss 532.01\n",
            "iteration 1878, epoch 4, batch 235/481,disc_loss 80.892, (real 84.217, fake 77.567 ) gen_loss 514.55\n",
            "iteration 1879, epoch 4, batch 236/481,disc_loss 76.457, (real 79.271, fake 73.644 ) gen_loss 513.64\n",
            "iteration 1880, epoch 4, batch 237/481,disc_loss 79.117, (real 82.397, fake 75.837 ) gen_loss 532.78\n",
            "iteration 1881, epoch 4, batch 238/481,disc_loss 73.595, (real 76.619, fake 70.572 ) gen_loss 546.42\n",
            "iteration 1882, epoch 4, batch 239/481,disc_loss 74.456, (real 78.716, fake 70.195 ) gen_loss 543.83\n",
            "iteration 1883, epoch 4, batch 240/481,disc_loss 82.368, (real 84.836, fake 79.9 ) gen_loss 619.67\n",
            "iteration 1884, epoch 4, batch 241/481,disc_loss 80.905, (real 83.504, fake 78.306 ) gen_loss 543.07\n",
            "iteration 1885, epoch 4, batch 242/481,disc_loss 78.332, (real 80.982, fake 75.682 ) gen_loss 546.01\n",
            "iteration 1886, epoch 4, batch 243/481,disc_loss 82.387, (real 85.437, fake 79.338 ) gen_loss 522.41\n",
            "iteration 1887, epoch 4, batch 244/481,disc_loss 80.531, (real 83.541, fake 77.52 ) gen_loss 474.97\n",
            "iteration 1888, epoch 4, batch 245/481,disc_loss 80.536, (real 83.782, fake 77.289 ) gen_loss 492.38\n",
            "iteration 1889, epoch 4, batch 246/481,disc_loss 72.839, (real 75.769, fake 69.909 ) gen_loss 571.33\n",
            "iteration 1890, epoch 4, batch 247/481,disc_loss 86.857, (real 89.61, fake 84.104 ) gen_loss 555.52\n",
            "iteration 1891, epoch 4, batch 248/481,disc_loss 82.337, (real 85.504, fake 79.17 ) gen_loss 478.97\n",
            "iteration 1892, epoch 4, batch 249/481,disc_loss 74.924, (real 78.39, fake 71.458 ) gen_loss 545.66\n",
            "iteration 1893, epoch 4, batch 250/481,disc_loss 74.079, (real 77.062, fake 71.096 ) gen_loss 536.7\n",
            "iteration 1894, epoch 4, batch 251/481,disc_loss 82.424, (real 86.203, fake 78.644 ) gen_loss 585.51\n",
            "iteration 1895, epoch 4, batch 252/481,disc_loss 77.385, (real 80.839, fake 73.931 ) gen_loss 536.02\n",
            "iteration 1896, epoch 4, batch 253/481,disc_loss 80.772, (real 84.565, fake 76.979 ) gen_loss 547.81\n",
            "iteration 1897, epoch 4, batch 254/481,disc_loss 78.831, (real 81.983, fake 75.678 ) gen_loss 546.37\n",
            "iteration 1898, epoch 4, batch 255/481,disc_loss 78.883, (real 82.703, fake 75.063 ) gen_loss 549.75\n",
            "iteration 1899, epoch 4, batch 256/481,disc_loss 80.861, (real 83.736, fake 77.986 ) gen_loss 549.18\n",
            "iteration 1900, epoch 4, batch 257/481,disc_loss 81.493, (real 83.531, fake 79.455 ) gen_loss 521.25\n",
            "iteration 1901, epoch 4, batch 258/481,disc_loss 80.808, (real 83.129, fake 78.487 ) gen_loss 573.21\n",
            "iteration 1902, epoch 4, batch 259/481,disc_loss 77.546, (real 81.514, fake 73.579 ) gen_loss 625.99\n",
            "iteration 1903, epoch 4, batch 260/481,disc_loss 81.81, (real 86.422, fake 77.197 ) gen_loss 565.06\n",
            "iteration 1904, epoch 4, batch 261/481,disc_loss 80.096, (real 83.05, fake 77.143 ) gen_loss 602.66\n",
            "iteration 1905, epoch 4, batch 262/481,disc_loss 77.167, (real 79.266, fake 75.067 ) gen_loss 642.32\n",
            "iteration 1906, epoch 4, batch 263/481,disc_loss 81.339, (real 82.333, fake 80.344 ) gen_loss 547.96\n",
            "iteration 1907, epoch 4, batch 264/481,disc_loss 83.992, (real 87.289, fake 80.695 ) gen_loss 517.05\n",
            "iteration 1908, epoch 4, batch 265/481,disc_loss 81.341, (real 84.485, fake 78.196 ) gen_loss 502.01\n",
            "iteration 1909, epoch 4, batch 266/481,disc_loss 79.418, (real 83.004, fake 75.832 ) gen_loss 543.03\n",
            "iteration 1910, epoch 4, batch 267/481,disc_loss 81.629, (real 83.672, fake 79.585 ) gen_loss 587.22\n",
            "iteration 1911, epoch 4, batch 268/481,disc_loss 80.214, (real 82.9, fake 77.528 ) gen_loss 471.92\n",
            "iteration 1912, epoch 4, batch 269/481,disc_loss 76.303, (real 80.459, fake 72.147 ) gen_loss 459.4\n",
            "iteration 1913, epoch 4, batch 270/481,disc_loss 83.675, (real 86.678, fake 80.672 ) gen_loss 521.37\n",
            "iteration 1914, epoch 4, batch 271/481,disc_loss 77.001, (real 80.085, fake 73.916 ) gen_loss 539.24\n",
            "iteration 1915, epoch 4, batch 272/481,disc_loss 77.445, (real 79.045, fake 75.844 ) gen_loss 574.39\n",
            "iteration 1916, epoch 4, batch 273/481,disc_loss 80.128, (real 82.998, fake 77.257 ) gen_loss 580.52\n",
            "iteration 1917, epoch 4, batch 274/481,disc_loss 83.711, (real 87.364, fake 80.058 ) gen_loss 561.01\n",
            "iteration 1918, epoch 4, batch 275/481,disc_loss 75.381, (real 78.006, fake 72.757 ) gen_loss 516.59\n",
            "iteration 1919, epoch 4, batch 276/481,disc_loss 74.044, (real 77.772, fake 70.316 ) gen_loss 539.09\n",
            "iteration 1920, epoch 4, batch 277/481,disc_loss 78.132, (real 81.652, fake 74.611 ) gen_loss 574.14\n",
            "iteration 1921, epoch 4, batch 278/481,disc_loss 79.389, (real 82.47, fake 76.308 ) gen_loss 574.71\n",
            "iteration 1922, epoch 4, batch 279/481,disc_loss 79.559, (real 82.912, fake 76.206 ) gen_loss 528.43\n",
            "iteration 1923, epoch 4, batch 280/481,disc_loss 84.58, (real 89.339, fake 79.822 ) gen_loss 590.84\n",
            "iteration 1924, epoch 4, batch 281/481,disc_loss 82.179, (real 85.114, fake 79.244 ) gen_loss 527.94\n",
            "iteration 1925, epoch 4, batch 282/481,disc_loss 79.259, (real 83.209, fake 75.309 ) gen_loss 531.86\n",
            "iteration 1926, epoch 4, batch 283/481,disc_loss 80.718, (real 84.158, fake 77.279 ) gen_loss 528.5\n",
            "iteration 1927, epoch 4, batch 284/481,disc_loss 80.57, (real 83.135, fake 78.005 ) gen_loss 521.85\n",
            "iteration 1928, epoch 4, batch 285/481,disc_loss 78.85, (real 82.485, fake 75.214 ) gen_loss 555.71\n",
            "iteration 1929, epoch 4, batch 286/481,disc_loss 80.311, (real 82.922, fake 77.699 ) gen_loss 586.71\n",
            "iteration 1930, epoch 4, batch 287/481,disc_loss 82.204, (real 84.477, fake 79.931 ) gen_loss 570.37\n",
            "iteration 1931, epoch 4, batch 288/481,disc_loss 78.464, (real 80.014, fake 76.914 ) gen_loss 534.32\n",
            "iteration 1932, epoch 4, batch 289/481,disc_loss 80.85, (real 84.49, fake 77.211 ) gen_loss 579.83\n",
            "iteration 1933, epoch 4, batch 290/481,disc_loss 80.368, (real 82.401, fake 78.335 ) gen_loss 511.06\n",
            "iteration 1934, epoch 4, batch 291/481,disc_loss 79.172, (real 81.37, fake 76.974 ) gen_loss 539.92\n",
            "iteration 1935, epoch 4, batch 292/481,disc_loss 81.464, (real 85.232, fake 77.696 ) gen_loss 507.25\n",
            "iteration 1936, epoch 4, batch 293/481,disc_loss 84.131, (real 87.353, fake 80.909 ) gen_loss 506.09\n",
            "iteration 1937, epoch 4, batch 294/481,disc_loss 82.102, (real 85.227, fake 78.978 ) gen_loss 500.43\n",
            "iteration 1938, epoch 4, batch 295/481,disc_loss 82.623, (real 86.395, fake 78.851 ) gen_loss 529.15\n",
            "iteration 1939, epoch 4, batch 296/481,disc_loss 77.073, (real 80.512, fake 73.634 ) gen_loss 578.76\n",
            "iteration 1940, epoch 4, batch 297/481,disc_loss 77.274, (real 80.368, fake 74.18 ) gen_loss 590.61\n",
            "iteration 1941, epoch 4, batch 298/481,disc_loss 83.759, (real 87.02, fake 80.498 ) gen_loss 610.27\n",
            "iteration 1942, epoch 4, batch 299/481,disc_loss 79.554, (real 82.156, fake 76.952 ) gen_loss 528.18\n",
            "iteration 1943, epoch 4, batch 300/481,disc_loss 78.842, (real 81.306, fake 76.378 ) gen_loss 541.46\n",
            "iteration 1944, epoch 4, batch 301/481,disc_loss 78.34, (real 81.14, fake 75.541 ) gen_loss 544.71\n",
            "iteration 1945, epoch 4, batch 302/481,disc_loss 76.523, (real 81.094, fake 71.951 ) gen_loss 584.14\n",
            "iteration 1946, epoch 4, batch 303/481,disc_loss 79.161, (real 83.579, fake 74.743 ) gen_loss 580.24\n",
            "iteration 1947, epoch 4, batch 304/481,disc_loss 80.48, (real 83.632, fake 77.329 ) gen_loss 563.37\n",
            "iteration 1948, epoch 4, batch 305/481,disc_loss 83.378, (real 87.545, fake 79.211 ) gen_loss 544.89\n",
            "iteration 1949, epoch 4, batch 306/481,disc_loss 80.433, (real 83.562, fake 77.303 ) gen_loss 560.2\n",
            "iteration 1950, epoch 4, batch 307/481,disc_loss 81.035, (real 83.518, fake 78.552 ) gen_loss 588.49\n",
            "iteration 1951, epoch 4, batch 308/481,disc_loss 79.011, (real 82.364, fake 75.658 ) gen_loss 614.26\n",
            "iteration 1952, epoch 4, batch 309/481,disc_loss 81.026, (real 83.649, fake 78.403 ) gen_loss 572.06\n",
            "iteration 1953, epoch 4, batch 310/481,disc_loss 78.468, (real 82.888, fake 74.047 ) gen_loss 566.95\n",
            "iteration 1954, epoch 4, batch 311/481,disc_loss 79.457, (real 82.32, fake 76.594 ) gen_loss 588.97\n",
            "iteration 1955, epoch 4, batch 312/481,disc_loss 81.099, (real 84.132, fake 78.066 ) gen_loss 527.8\n",
            "iteration 1956, epoch 4, batch 313/481,disc_loss 79.569, (real 82.89, fake 76.248 ) gen_loss 541.07\n",
            "iteration 1957, epoch 4, batch 314/481,disc_loss 79.965, (real 83.729, fake 76.2 ) gen_loss 517.98\n",
            "iteration 1958, epoch 4, batch 315/481,disc_loss 84.073, (real 87.275, fake 80.872 ) gen_loss 504.58\n",
            "iteration 1959, epoch 4, batch 316/481,disc_loss 78.633, (real 82.282, fake 74.984 ) gen_loss 513.84\n",
            "iteration 1960, epoch 4, batch 317/481,disc_loss 83.454, (real 86.261, fake 80.647 ) gen_loss 680.85\n",
            "iteration 1961, epoch 4, batch 318/481,disc_loss 84.821, (real 86.743, fake 82.9 ) gen_loss 595.27\n",
            "iteration 1962, epoch 4, batch 319/481,disc_loss 85.754, (real 88.917, fake 82.592 ) gen_loss 602.43\n",
            "iteration 1963, epoch 4, batch 320/481,disc_loss 82.652, (real 85.214, fake 80.091 ) gen_loss 479.19\n",
            "iteration 1964, epoch 4, batch 321/481,disc_loss 83.638, (real 87.578, fake 79.698 ) gen_loss 544.13\n",
            "iteration 1965, epoch 4, batch 322/481,disc_loss 81.827, (real 84.902, fake 78.752 ) gen_loss 571.71\n",
            "iteration 1966, epoch 4, batch 323/481,disc_loss 78.624, (real 80.877, fake 76.372 ) gen_loss 588.85\n",
            "iteration 1967, epoch 4, batch 324/481,disc_loss 81.311, (real 84.588, fake 78.035 ) gen_loss 500.91\n",
            "iteration 1968, epoch 4, batch 325/481,disc_loss 80.923, (real 83.749, fake 78.097 ) gen_loss 511.29\n",
            "iteration 1969, epoch 4, batch 326/481,disc_loss 76.138, (real 79.999, fake 72.276 ) gen_loss 516.48\n",
            "iteration 1970, epoch 4, batch 327/481,disc_loss 81.716, (real 85.45, fake 77.982 ) gen_loss 585.82\n",
            "iteration 1971, epoch 4, batch 328/481,disc_loss 75.567, (real 78.886, fake 72.247 ) gen_loss 552.81\n",
            "iteration 1972, epoch 4, batch 329/481,disc_loss 81.499, (real 84.399, fake 78.6 ) gen_loss 544.14\n",
            "iteration 1973, epoch 4, batch 330/481,disc_loss 80.192, (real 83.231, fake 77.154 ) gen_loss 530.99\n",
            "iteration 1974, epoch 4, batch 331/481,disc_loss 78.839, (real 82.188, fake 75.49 ) gen_loss 504.82\n",
            "iteration 1975, epoch 4, batch 332/481,disc_loss 77.127, (real 80.408, fake 73.847 ) gen_loss 483.36\n",
            "iteration 1976, epoch 4, batch 333/481,disc_loss 79.974, (real 82.28, fake 77.669 ) gen_loss 519.81\n",
            "iteration 1977, epoch 4, batch 334/481,disc_loss 88.622, (real 89.486, fake 87.757 ) gen_loss 748.42\n",
            "iteration 1978, epoch 4, batch 335/481,disc_loss 81.673, (real 84.322, fake 79.025 ) gen_loss 1129.8\n",
            "iteration 1979, epoch 4, batch 336/481,disc_loss 81.963, (real 84.224, fake 79.702 ) gen_loss 608.05\n",
            "iteration 1980, epoch 4, batch 337/481,disc_loss 82.461, (real 84.908, fake 80.014 ) gen_loss 566.31\n",
            "iteration 1981, epoch 4, batch 338/481,disc_loss 80.539, (real 83.204, fake 77.875 ) gen_loss 559.0\n",
            "iteration 1982, epoch 4, batch 339/481,disc_loss 80.028, (real 83.015, fake 77.04 ) gen_loss 510.15\n",
            "iteration 1983, epoch 4, batch 340/481,disc_loss 82.47, (real 85.919, fake 79.02 ) gen_loss 551.83\n",
            "iteration 1984, epoch 4, batch 341/481,disc_loss 81.438, (real 84.172, fake 78.705 ) gen_loss 590.54\n",
            "iteration 1985, epoch 4, batch 342/481,disc_loss 76.138, (real 79.287, fake 72.989 ) gen_loss 575.0\n",
            "iteration 1986, epoch 4, batch 343/481,disc_loss 77.366, (real 79.963, fake 74.768 ) gen_loss 574.86\n",
            "iteration 1987, epoch 4, batch 344/481,disc_loss 81.204, (real 85.113, fake 77.296 ) gen_loss 592.74\n",
            "iteration 1988, epoch 4, batch 345/481,disc_loss 78.354, (real 80.002, fake 76.706 ) gen_loss 568.38\n",
            "iteration 1989, epoch 4, batch 346/481,disc_loss 81.825, (real 84.344, fake 79.307 ) gen_loss 539.02\n",
            "iteration 1990, epoch 4, batch 347/481,disc_loss 78.606, (real 82.003, fake 75.21 ) gen_loss 479.74\n",
            "iteration 1991, epoch 4, batch 348/481,disc_loss 80.194, (real 84.244, fake 76.143 ) gen_loss 513.65\n",
            "iteration 1992, epoch 4, batch 349/481,disc_loss 87.302, (real 90.427, fake 84.177 ) gen_loss 534.94\n",
            "iteration 1993, epoch 4, batch 350/481,disc_loss 78.75, (real 82.035, fake 75.465 ) gen_loss 535.92\n",
            "iteration 1994, epoch 4, batch 351/481,disc_loss 78.326, (real 81.077, fake 75.576 ) gen_loss 551.46\n",
            "iteration 1995, epoch 4, batch 352/481,disc_loss 80.94, (real 84.219, fake 77.661 ) gen_loss 542.73\n",
            "iteration 1996, epoch 4, batch 353/481,disc_loss 81.629, (real 83.59, fake 79.668 ) gen_loss 560.5\n",
            "iteration 1997, epoch 4, batch 354/481,disc_loss 81.414, (real 85.4, fake 77.429 ) gen_loss 530.86\n",
            "iteration 1998, epoch 4, batch 355/481,disc_loss 78.062, (real 81.335, fake 74.789 ) gen_loss 565.06\n",
            "iteration 1999, epoch 4, batch 356/481,disc_loss 78.75, (real 81.981, fake 75.52 ) gen_loss 571.83\n",
            "iteration 2000, epoch 4, batch 357/481,disc_loss 79.892, (real 82.783, fake 77.002 ) gen_loss 560.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 2001, epoch 4, batch 358/481,disc_loss 79.594, (real 82.91, fake 76.278 ) gen_loss 665.76\n",
            "iteration 2002, epoch 4, batch 359/481,disc_loss 75.273, (real 78.112, fake 72.433 ) gen_loss 590.12\n",
            "iteration 2003, epoch 4, batch 360/481,disc_loss 79.667, (real 83.44, fake 75.894 ) gen_loss 589.3\n",
            "iteration 2004, epoch 4, batch 361/481,disc_loss 79.854, (real 82.823, fake 76.884 ) gen_loss 647.6\n",
            "iteration 2005, epoch 4, batch 362/481,disc_loss 86.261, (real 90.982, fake 81.54 ) gen_loss 572.74\n",
            "iteration 2006, epoch 4, batch 363/481,disc_loss 76.738, (real 80.097, fake 73.378 ) gen_loss 529.43\n",
            "iteration 2007, epoch 4, batch 364/481,disc_loss 86.088, (real 89.098, fake 83.079 ) gen_loss 500.87\n",
            "iteration 2008, epoch 4, batch 365/481,disc_loss 81.821, (real 83.961, fake 79.682 ) gen_loss 524.86\n",
            "iteration 2009, epoch 4, batch 366/481,disc_loss 79.643, (real 82.23, fake 77.055 ) gen_loss 580.6\n",
            "iteration 2010, epoch 4, batch 367/481,disc_loss 76.18, (real 79.614, fake 72.747 ) gen_loss 581.84\n",
            "iteration 2011, epoch 4, batch 368/481,disc_loss 79.191, (real 83.398, fake 74.983 ) gen_loss 550.34\n",
            "iteration 2012, epoch 4, batch 369/481,disc_loss 78.757, (real 82.645, fake 74.869 ) gen_loss 592.25\n",
            "iteration 2013, epoch 4, batch 370/481,disc_loss 80.076, (real 84.063, fake 76.089 ) gen_loss 532.68\n",
            "iteration 2014, epoch 4, batch 371/481,disc_loss 82.231, (real 84.308, fake 80.153 ) gen_loss 597.38\n",
            "iteration 2015, epoch 4, batch 372/481,disc_loss 77.759, (real 80.559, fake 74.959 ) gen_loss 597.3\n",
            "iteration 2016, epoch 4, batch 373/481,disc_loss 80.712, (real 83.685, fake 77.739 ) gen_loss 565.3\n",
            "iteration 2017, epoch 4, batch 374/481,disc_loss 80.231, (real 83.271, fake 77.192 ) gen_loss 593.38\n",
            "iteration 2018, epoch 4, batch 375/481,disc_loss 78.647, (real 82.704, fake 74.591 ) gen_loss 550.18\n",
            "iteration 2019, epoch 4, batch 376/481,disc_loss 77.34, (real 80.742, fake 73.938 ) gen_loss 574.73\n",
            "iteration 2020, epoch 4, batch 377/481,disc_loss 77.187, (real 79.397, fake 74.976 ) gen_loss 548.31\n",
            "iteration 2021, epoch 4, batch 378/481,disc_loss 81.692, (real 84.406, fake 78.979 ) gen_loss 579.79\n",
            "iteration 2022, epoch 4, batch 379/481,disc_loss 79.56, (real 82.978, fake 76.142 ) gen_loss 560.29\n",
            "iteration 2023, epoch 4, batch 380/481,disc_loss 78.728, (real 81.655, fake 75.801 ) gen_loss 524.58\n",
            "iteration 2024, epoch 4, batch 381/481,disc_loss 83.596, (real 86.569, fake 80.623 ) gen_loss 576.01\n",
            "iteration 2025, epoch 4, batch 382/481,disc_loss 76.763, (real 79.342, fake 74.184 ) gen_loss 564.06\n",
            "iteration 2026, epoch 4, batch 383/481,disc_loss 78.963, (real 81.964, fake 75.963 ) gen_loss 615.9\n",
            "iteration 2027, epoch 4, batch 384/481,disc_loss 85.264, (real 88.641, fake 81.886 ) gen_loss 627.25\n",
            "iteration 2028, epoch 4, batch 385/481,disc_loss 78.873, (real 80.64, fake 77.106 ) gen_loss 544.05\n",
            "iteration 2029, epoch 4, batch 386/481,disc_loss 83.199, (real 87.027, fake 79.371 ) gen_loss 522.39\n",
            "iteration 2030, epoch 4, batch 387/481,disc_loss 82.081, (real 85.414, fake 78.748 ) gen_loss 473.75\n",
            "iteration 2031, epoch 4, batch 388/481,disc_loss 77.719, (real 81.4, fake 74.038 ) gen_loss 485.45\n",
            "iteration 2032, epoch 4, batch 389/481,disc_loss 81.57, (real 83.829, fake 79.311 ) gen_loss 714.62\n",
            "iteration 2033, epoch 4, batch 390/481,disc_loss 79.769, (real 81.841, fake 77.696 ) gen_loss 563.23\n",
            "iteration 2034, epoch 4, batch 391/481,disc_loss 82.704, (real 85.59, fake 79.818 ) gen_loss 495.41\n",
            "iteration 2035, epoch 4, batch 392/481,disc_loss 83.457, (real 86.292, fake 80.622 ) gen_loss 512.33\n",
            "iteration 2036, epoch 4, batch 393/481,disc_loss 83.822, (real 88.662, fake 78.983 ) gen_loss 528.09\n",
            "iteration 2037, epoch 4, batch 394/481,disc_loss 78.305, (real 81.971, fake 74.64 ) gen_loss 601.14\n",
            "iteration 2038, epoch 4, batch 395/481,disc_loss 77.673, (real 81.775, fake 73.571 ) gen_loss 582.65\n",
            "iteration 2039, epoch 4, batch 396/481,disc_loss 73.274, (real 76.501, fake 70.047 ) gen_loss 524.41\n",
            "iteration 2040, epoch 4, batch 397/481,disc_loss 76.807, (real 80.99, fake 72.625 ) gen_loss 508.95\n",
            "iteration 2041, epoch 4, batch 398/481,disc_loss 78.284, (real 81.235, fake 75.332 ) gen_loss 561.29\n",
            "iteration 2042, epoch 4, batch 399/481,disc_loss 83.3, (real 87.813, fake 78.788 ) gen_loss 569.63\n",
            "iteration 2043, epoch 4, batch 400/481,disc_loss 80.246, (real 82.727, fake 77.766 ) gen_loss 577.74\n",
            "iteration 2044, epoch 4, batch 401/481,disc_loss 81.336, (real 84.84, fake 77.833 ) gen_loss 582.89\n",
            "iteration 2045, epoch 4, batch 402/481,disc_loss 76.756, (real 79.722, fake 73.79 ) gen_loss 545.95\n",
            "iteration 2046, epoch 4, batch 403/481,disc_loss 77.807, (real 80.46, fake 75.154 ) gen_loss 566.42\n",
            "iteration 2047, epoch 4, batch 404/481,disc_loss 75.364, (real 78.536, fake 72.191 ) gen_loss 591.45\n",
            "iteration 2048, epoch 4, batch 405/481,disc_loss 76.528, (real 78.352, fake 74.704 ) gen_loss 622.07\n",
            "iteration 2049, epoch 4, batch 406/481,disc_loss 85.364, (real 86.904, fake 83.824 ) gen_loss 714.42\n",
            "iteration 2050, epoch 4, batch 407/481,disc_loss 83.453, (real 86.134, fake 80.772 ) gen_loss 583.08\n",
            "iteration 2051, epoch 4, batch 408/481,disc_loss 81.207, (real 84.851, fake 77.562 ) gen_loss 556.65\n",
            "iteration 2052, epoch 4, batch 409/481,disc_loss 80.275, (real 83.727, fake 76.823 ) gen_loss 590.92\n",
            "iteration 2053, epoch 4, batch 410/481,disc_loss 79.748, (real 83.248, fake 76.248 ) gen_loss 584.81\n",
            "iteration 2054, epoch 4, batch 411/481,disc_loss 83.268, (real 88.164, fake 78.372 ) gen_loss 525.88\n",
            "iteration 2055, epoch 4, batch 412/481,disc_loss 79.02, (real 83.289, fake 74.75 ) gen_loss 551.85\n",
            "iteration 2056, epoch 4, batch 413/481,disc_loss 77.72, (real 81.32, fake 74.12 ) gen_loss 553.35\n",
            "iteration 2057, epoch 4, batch 414/481,disc_loss 81.305, (real 84.716, fake 77.894 ) gen_loss 577.93\n",
            "iteration 2058, epoch 4, batch 415/481,disc_loss 74.391, (real 78.624, fake 70.158 ) gen_loss 499.17\n",
            "iteration 2059, epoch 4, batch 416/481,disc_loss 80.982, (real 84.173, fake 77.79 ) gen_loss 565.25\n",
            "iteration 2060, epoch 4, batch 417/481,disc_loss 78.165, (real 81.096, fake 75.234 ) gen_loss 503.87\n",
            "iteration 2061, epoch 4, batch 418/481,disc_loss 80.891, (real 83.088, fake 78.694 ) gen_loss 512.54\n",
            "iteration 2062, epoch 4, batch 419/481,disc_loss 81.319, (real 85.07, fake 77.568 ) gen_loss 520.98\n",
            "iteration 2063, epoch 4, batch 420/481,disc_loss 78.317, (real 81.687, fake 74.947 ) gen_loss 517.93\n",
            "iteration 2064, epoch 4, batch 421/481,disc_loss 84.305, (real 87.75, fake 80.859 ) gen_loss 569.59\n",
            "iteration 2065, epoch 4, batch 422/481,disc_loss 77.896, (real 80.804, fake 74.988 ) gen_loss 554.3\n",
            "iteration 2066, epoch 4, batch 423/481,disc_loss 79.601, (real 83.897, fake 75.305 ) gen_loss 583.76\n",
            "iteration 2067, epoch 4, batch 424/481,disc_loss 78.156, (real 81.959, fake 74.353 ) gen_loss 571.24\n",
            "iteration 2068, epoch 4, batch 425/481,disc_loss 80.439, (real 83.838, fake 77.041 ) gen_loss 556.77\n",
            "iteration 2069, epoch 4, batch 426/481,disc_loss 81.679, (real 85.675, fake 77.682 ) gen_loss 582.11\n",
            "iteration 2070, epoch 4, batch 427/481,disc_loss 85.711, (real 88.094, fake 83.329 ) gen_loss 608.81\n",
            "iteration 2071, epoch 4, batch 428/481,disc_loss 77.52, (real 81.309, fake 73.73 ) gen_loss 571.35\n",
            "iteration 2072, epoch 4, batch 429/481,disc_loss 77.791, (real 81.806, fake 73.776 ) gen_loss 568.79\n",
            "iteration 2073, epoch 4, batch 430/481,disc_loss 76.878, (real 80.186, fake 73.57 ) gen_loss 550.76\n",
            "iteration 2074, epoch 4, batch 431/481,disc_loss 80.181, (real 82.89, fake 77.472 ) gen_loss 502.91\n",
            "iteration 2075, epoch 4, batch 432/481,disc_loss 79.107, (real 81.26, fake 76.954 ) gen_loss 577.52\n",
            "iteration 2076, epoch 4, batch 433/481,disc_loss 79.488, (real 81.848, fake 77.128 ) gen_loss 507.89\n",
            "iteration 2077, epoch 4, batch 434/481,disc_loss 85.36, (real 89.266, fake 81.454 ) gen_loss 555.94\n",
            "iteration 2078, epoch 4, batch 435/481,disc_loss 88.359, (real 90.483, fake 86.236 ) gen_loss 544.75\n",
            "iteration 2079, epoch 4, batch 436/481,disc_loss 80.999, (real 82.87, fake 79.129 ) gen_loss 616.59\n",
            "iteration 2080, epoch 4, batch 437/481,disc_loss 75.885, (real 78.321, fake 73.449 ) gen_loss 579.19\n",
            "iteration 2081, epoch 4, batch 438/481,disc_loss 79.746, (real 82.346, fake 77.146 ) gen_loss 575.41\n",
            "iteration 2082, epoch 4, batch 439/481,disc_loss 77.558, (real 79.656, fake 75.461 ) gen_loss 586.59\n",
            "iteration 2083, epoch 4, batch 440/481,disc_loss 77.873, (real 80.828, fake 74.918 ) gen_loss 507.37\n",
            "iteration 2084, epoch 4, batch 441/481,disc_loss 77.546, (real 80.696, fake 74.396 ) gen_loss 578.71\n",
            "iteration 2085, epoch 4, batch 442/481,disc_loss 80.709, (real 84.09, fake 77.328 ) gen_loss 559.23\n",
            "iteration 2086, epoch 4, batch 443/481,disc_loss 73.746, (real 75.878, fake 71.614 ) gen_loss 538.04\n",
            "iteration 2087, epoch 4, batch 444/481,disc_loss 72.362, (real 74.802, fake 69.921 ) gen_loss 557.03\n",
            "iteration 2088, epoch 4, batch 445/481,disc_loss 75.232, (real 77.488, fake 72.977 ) gen_loss 548.16\n",
            "iteration 2089, epoch 4, batch 446/481,disc_loss 85.687, (real 90.454, fake 80.92 ) gen_loss 602.3\n",
            "iteration 2090, epoch 4, batch 447/481,disc_loss 75.739, (real 77.911, fake 73.567 ) gen_loss 491.01\n",
            "iteration 2091, epoch 4, batch 448/481,disc_loss 79.604, (real 82.07, fake 77.137 ) gen_loss 530.0\n",
            "iteration 2092, epoch 4, batch 449/481,disc_loss 77.694, (real 80.455, fake 74.933 ) gen_loss 508.07\n",
            "iteration 2093, epoch 4, batch 450/481,disc_loss 79.739, (real 83.382, fake 76.096 ) gen_loss 495.02\n",
            "iteration 2094, epoch 4, batch 451/481,disc_loss 79.597, (real 82.056, fake 77.137 ) gen_loss 506.47\n",
            "iteration 2095, epoch 4, batch 452/481,disc_loss 75.747, (real 78.697, fake 72.796 ) gen_loss 508.52\n",
            "iteration 2096, epoch 4, batch 453/481,disc_loss 77.882, (real 81.205, fake 74.558 ) gen_loss 529.36\n",
            "iteration 2097, epoch 4, batch 454/481,disc_loss 79.59, (real 83.891, fake 75.29 ) gen_loss 617.87\n",
            "iteration 2098, epoch 4, batch 455/481,disc_loss 78.344, (real 81.586, fake 75.101 ) gen_loss 548.0\n",
            "iteration 2099, epoch 4, batch 456/481,disc_loss 80.469, (real 83.58, fake 77.358 ) gen_loss 509.22\n",
            "iteration 2100, epoch 4, batch 457/481,disc_loss 81.428, (real 84.172, fake 78.683 ) gen_loss 590.91\n",
            "iteration 2101, epoch 4, batch 458/481,disc_loss 82.634, (real 86.108, fake 79.161 ) gen_loss 539.78\n",
            "iteration 2102, epoch 4, batch 459/481,disc_loss 77.67, (real 81.482, fake 73.857 ) gen_loss 537.89\n",
            "iteration 2103, epoch 4, batch 460/481,disc_loss 79.708, (real 81.451, fake 77.965 ) gen_loss 535.04\n",
            "iteration 2104, epoch 4, batch 461/481,disc_loss 78.882, (real 81.491, fake 76.274 ) gen_loss 535.67\n",
            "iteration 2105, epoch 4, batch 462/481,disc_loss 76.992, (real 80.707, fake 73.277 ) gen_loss 519.24\n",
            "iteration 2106, epoch 4, batch 463/481,disc_loss 73.3, (real 77.331, fake 69.269 ) gen_loss 541.73\n",
            "iteration 2107, epoch 4, batch 464/481,disc_loss 79.66, (real 83.37, fake 75.95 ) gen_loss 535.89\n",
            "iteration 2108, epoch 4, batch 465/481,disc_loss 81.636, (real 84.422, fake 78.85 ) gen_loss 551.39\n",
            "iteration 2109, epoch 4, batch 466/481,disc_loss 80.822, (real 84.289, fake 77.355 ) gen_loss 512.92\n",
            "iteration 2110, epoch 4, batch 467/481,disc_loss 78.041, (real 80.333, fake 75.749 ) gen_loss 530.55\n",
            "iteration 2111, epoch 4, batch 468/481,disc_loss 73.721, (real 76.433, fake 71.009 ) gen_loss 556.83\n",
            "iteration 2112, epoch 4, batch 469/481,disc_loss 77.013, (real 79.464, fake 74.562 ) gen_loss 543.85\n",
            "iteration 2113, epoch 4, batch 470/481,disc_loss 81.752, (real 84.79, fake 78.714 ) gen_loss 508.16\n",
            "iteration 2114, epoch 4, batch 471/481,disc_loss 82.79, (real 85.576, fake 80.003 ) gen_loss 542.44\n",
            "iteration 2115, epoch 4, batch 472/481,disc_loss 81.927, (real 85.563, fake 78.292 ) gen_loss 574.49\n",
            "iteration 2116, epoch 4, batch 473/481,disc_loss 81.931, (real 84.798, fake 79.065 ) gen_loss 631.23\n",
            "iteration 2117, epoch 4, batch 474/481,disc_loss 79.861, (real 82.502, fake 77.219 ) gen_loss 551.4\n",
            "iteration 2118, epoch 4, batch 475/481,disc_loss 80.348, (real 83.908, fake 76.789 ) gen_loss 645.47\n",
            "iteration 2119, epoch 4, batch 476/481,disc_loss 78.601, (real 81.435, fake 75.767 ) gen_loss 589.67\n",
            "iteration 2120, epoch 4, batch 477/481,disc_loss 74.713, (real 78.538, fake 70.888 ) gen_loss 621.62\n",
            "iteration 2121, epoch 4, batch 478/481,disc_loss 81.923, (real 84.028, fake 79.818 ) gen_loss 635.99\n",
            "iteration 2122, epoch 4, batch 479/481,disc_loss 78.91, (real 82.173, fake 75.648 ) gen_loss 508.09\n",
            "iteration 2123, epoch 4, batch 480/481,disc_loss 80.455, (real 81.812, fake 79.097 ) gen_loss 571.57\n",
            "iteration 2124, epoch 4, batch 481/481,disc_loss 76.724, (real 79.583, fake 73.864 ) gen_loss 621.42\n",
            "iteration 2125, epoch 5, batch 1/481,disc_loss 81.81, (real 82.584, fake 81.037 ) gen_loss 558.35\n",
            "iteration 2126, epoch 5, batch 2/481,disc_loss 82.055, (real 87.58, fake 76.531 ) gen_loss 539.71\n",
            "iteration 2127, epoch 5, batch 3/481,disc_loss 77.971, (real 82.413, fake 73.529 ) gen_loss 541.84\n",
            "iteration 2128, epoch 5, batch 4/481,disc_loss 81.754, (real 85.631, fake 77.877 ) gen_loss 525.65\n",
            "iteration 2129, epoch 5, batch 5/481,disc_loss 86.338, (real 89.4, fake 83.276 ) gen_loss 582.55\n",
            "iteration 2130, epoch 5, batch 6/481,disc_loss 80.132, (real 82.594, fake 77.67 ) gen_loss 588.0\n",
            "iteration 2131, epoch 5, batch 7/481,disc_loss 78.493, (real 80.967, fake 76.02 ) gen_loss 633.38\n",
            "iteration 2132, epoch 5, batch 8/481,disc_loss 79.67, (real 82.759, fake 76.582 ) gen_loss 578.34\n",
            "iteration 2133, epoch 5, batch 9/481,disc_loss 75.611, (real 78.244, fake 72.978 ) gen_loss 577.86\n",
            "iteration 2134, epoch 5, batch 10/481,disc_loss 72.675, (real 74.468, fake 70.883 ) gen_loss 561.48\n",
            "iteration 2135, epoch 5, batch 11/481,disc_loss 76.554, (real 79.784, fake 73.325 ) gen_loss 575.51\n",
            "iteration 2136, epoch 5, batch 12/481,disc_loss 78.504, (real 80.709, fake 76.298 ) gen_loss 567.84\n",
            "iteration 2137, epoch 5, batch 13/481,disc_loss 75.726, (real 78.117, fake 73.336 ) gen_loss 566.24\n",
            "iteration 2138, epoch 5, batch 14/481,disc_loss 78.475, (real 81.242, fake 75.707 ) gen_loss 587.33\n",
            "iteration 2139, epoch 5, batch 15/481,disc_loss 80.101, (real 83.8, fake 76.402 ) gen_loss 614.07\n",
            "iteration 2140, epoch 5, batch 16/481,disc_loss 82.491, (real 86.263, fake 78.719 ) gen_loss 585.68\n",
            "iteration 2141, epoch 5, batch 17/481,disc_loss 80.318, (real 83.058, fake 77.579 ) gen_loss 558.79\n",
            "iteration 2142, epoch 5, batch 18/481,disc_loss 79.28, (real 81.201, fake 77.359 ) gen_loss 539.16\n",
            "iteration 2143, epoch 5, batch 19/481,disc_loss 80.163, (real 82.321, fake 78.004 ) gen_loss 573.45\n",
            "iteration 2144, epoch 5, batch 20/481,disc_loss 78.938, (real 82.43, fake 75.447 ) gen_loss 568.91\n",
            "iteration 2145, epoch 5, batch 21/481,disc_loss 80.644, (real 83.445, fake 77.843 ) gen_loss 555.97\n",
            "iteration 2146, epoch 5, batch 22/481,disc_loss 81.736, (real 85.736, fake 77.736 ) gen_loss 594.71\n",
            "iteration 2147, epoch 5, batch 23/481,disc_loss 82.363, (real 85.582, fake 79.145 ) gen_loss 547.26\n",
            "iteration 2148, epoch 5, batch 24/481,disc_loss 81.511, (real 83.845, fake 79.177 ) gen_loss 520.96\n",
            "iteration 2149, epoch 5, batch 25/481,disc_loss 77.447, (real 80.362, fake 74.533 ) gen_loss 507.67\n",
            "iteration 2150, epoch 5, batch 26/481,disc_loss 74.895, (real 78.715, fake 71.074 ) gen_loss 585.7\n",
            "iteration 2151, epoch 5, batch 27/481,disc_loss 81.279, (real 84.305, fake 78.253 ) gen_loss 583.27\n",
            "iteration 2152, epoch 5, batch 28/481,disc_loss 75.183, (real 78.629, fake 71.737 ) gen_loss 567.55\n",
            "iteration 2153, epoch 5, batch 29/481,disc_loss 75.051, (real 78.408, fake 71.693 ) gen_loss 579.26\n",
            "iteration 2154, epoch 5, batch 30/481,disc_loss 75.674, (real 79.339, fake 72.01 ) gen_loss 541.39\n",
            "iteration 2155, epoch 5, batch 31/481,disc_loss 74.717, (real 78.1, fake 71.335 ) gen_loss 563.09\n",
            "iteration 2156, epoch 5, batch 32/481,disc_loss 79.411, (real 83.324, fake 75.498 ) gen_loss 571.76\n",
            "iteration 2157, epoch 5, batch 33/481,disc_loss 79.394, (real 82.452, fake 76.335 ) gen_loss 561.44\n",
            "iteration 2158, epoch 5, batch 34/481,disc_loss 80.661, (real 83.228, fake 78.094 ) gen_loss 476.49\n",
            "iteration 2159, epoch 5, batch 35/481,disc_loss 80.688, (real 83.031, fake 78.345 ) gen_loss 544.9\n",
            "iteration 2160, epoch 5, batch 36/481,disc_loss 80.26, (real 82.548, fake 77.973 ) gen_loss 484.72\n",
            "iteration 2161, epoch 5, batch 37/481,disc_loss 77.69, (real 79.857, fake 75.523 ) gen_loss 564.34\n",
            "iteration 2162, epoch 5, batch 38/481,disc_loss 80.135, (real 83.637, fake 76.633 ) gen_loss 500.46\n",
            "iteration 2163, epoch 5, batch 39/481,disc_loss 78.405, (real 81.801, fake 75.01 ) gen_loss 528.43\n",
            "iteration 2164, epoch 5, batch 40/481,disc_loss 74.62, (real 77.604, fake 71.636 ) gen_loss 498.12\n",
            "iteration 2165, epoch 5, batch 41/481,disc_loss 76.47, (real 78.914, fake 74.026 ) gen_loss 518.94\n",
            "iteration 2166, epoch 5, batch 42/481,disc_loss 84.334, (real 88.531, fake 80.137 ) gen_loss 603.35\n",
            "iteration 2167, epoch 5, batch 43/481,disc_loss 79.071, (real 82.124, fake 76.019 ) gen_loss 544.6\n",
            "iteration 2168, epoch 5, batch 44/481,disc_loss 77.445, (real 80.25, fake 74.64 ) gen_loss 548.42\n",
            "iteration 2169, epoch 5, batch 45/481,disc_loss 79.166, (real 80.493, fake 77.838 ) gen_loss 553.28\n",
            "iteration 2170, epoch 5, batch 46/481,disc_loss 78.633, (real 80.397, fake 76.87 ) gen_loss 610.03\n",
            "iteration 2171, epoch 5, batch 47/481,disc_loss 79.619, (real 83.475, fake 75.762 ) gen_loss 508.35\n",
            "iteration 2172, epoch 5, batch 48/481,disc_loss 76.825, (real 79.979, fake 73.671 ) gen_loss 513.38\n",
            "iteration 2173, epoch 5, batch 49/481,disc_loss 79.517, (real 82.896, fake 76.139 ) gen_loss 606.96\n",
            "iteration 2174, epoch 5, batch 50/481,disc_loss 79.778, (real 82.82, fake 76.735 ) gen_loss 584.05\n",
            "iteration 2175, epoch 5, batch 51/481,disc_loss 75.679, (real 77.905, fake 73.453 ) gen_loss 553.86\n",
            "iteration 2176, epoch 5, batch 52/481,disc_loss 77.265, (real 80.394, fake 74.137 ) gen_loss 539.63\n",
            "iteration 2177, epoch 5, batch 53/481,disc_loss 78.074, (real 80.044, fake 76.105 ) gen_loss 501.02\n",
            "iteration 2178, epoch 5, batch 54/481,disc_loss 74.178, (real 76.542, fake 71.813 ) gen_loss 580.73\n",
            "iteration 2179, epoch 5, batch 55/481,disc_loss 79.934, (real 84.089, fake 75.779 ) gen_loss 559.47\n",
            "iteration 2180, epoch 5, batch 56/481,disc_loss 80.857, (real 84.361, fake 77.352 ) gen_loss 567.72\n",
            "iteration 2181, epoch 5, batch 57/481,disc_loss 78.07, (real 80.865, fake 75.275 ) gen_loss 544.96\n",
            "iteration 2182, epoch 5, batch 58/481,disc_loss 78.945, (real 81.582, fake 76.307 ) gen_loss 595.73\n",
            "iteration 2183, epoch 5, batch 59/481,disc_loss 78.631, (real 80.774, fake 76.489 ) gen_loss 594.9\n",
            "iteration 2184, epoch 5, batch 60/481,disc_loss 82.569, (real 85.909, fake 79.23 ) gen_loss 569.32\n",
            "iteration 2185, epoch 5, batch 61/481,disc_loss 72.769, (real 75.04, fake 70.499 ) gen_loss 551.49\n",
            "iteration 2186, epoch 5, batch 62/481,disc_loss 78.663, (real 81.455, fake 75.872 ) gen_loss 600.81\n",
            "iteration 2187, epoch 5, batch 63/481,disc_loss 78.365, (real 81.385, fake 75.346 ) gen_loss 630.91\n",
            "iteration 2188, epoch 5, batch 64/481,disc_loss 78.958, (real 81.907, fake 76.008 ) gen_loss 537.42\n",
            "iteration 2189, epoch 5, batch 65/481,disc_loss 72.307, (real 74.381, fake 70.233 ) gen_loss 578.78\n",
            "iteration 2190, epoch 5, batch 66/481,disc_loss 81.708, (real 84.912, fake 78.503 ) gen_loss 640.87\n",
            "iteration 2191, epoch 5, batch 67/481,disc_loss 81.215, (real 83.564, fake 78.865 ) gen_loss 572.27\n",
            "iteration 2192, epoch 5, batch 68/481,disc_loss 84.555, (real 85.379, fake 83.73 ) gen_loss 551.12\n",
            "iteration 2193, epoch 5, batch 69/481,disc_loss 82.165, (real 85.541, fake 78.788 ) gen_loss 556.21\n",
            "iteration 2194, epoch 5, batch 70/481,disc_loss 79.179, (real 82.011, fake 76.347 ) gen_loss 506.04\n",
            "iteration 2195, epoch 5, batch 71/481,disc_loss 85.429, (real 89.708, fake 81.149 ) gen_loss 527.12\n",
            "iteration 2196, epoch 5, batch 72/481,disc_loss 82.034, (real 84.5, fake 79.569 ) gen_loss 580.12\n",
            "iteration 2197, epoch 5, batch 73/481,disc_loss 81.552, (real 84.742, fake 78.361 ) gen_loss 522.65\n",
            "iteration 2198, epoch 5, batch 74/481,disc_loss 82.69, (real 85.388, fake 79.993 ) gen_loss 543.39\n",
            "iteration 2199, epoch 5, batch 75/481,disc_loss 77.924, (real 80.491, fake 75.356 ) gen_loss 546.98\n",
            "iteration 2200, epoch 5, batch 76/481,disc_loss 81.364, (real 84.524, fake 78.205 ) gen_loss 549.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 2201, epoch 5, batch 77/481,disc_loss 79.237, (real 81.101, fake 77.373 ) gen_loss 599.34\n",
            "iteration 2202, epoch 5, batch 78/481,disc_loss 83.029, (real 84.647, fake 81.412 ) gen_loss 560.45\n",
            "iteration 2203, epoch 5, batch 79/481,disc_loss 80.963, (real 84.411, fake 77.515 ) gen_loss 600.3\n",
            "iteration 2204, epoch 5, batch 80/481,disc_loss 77.662, (real 80.132, fake 75.193 ) gen_loss 532.17\n",
            "iteration 2205, epoch 5, batch 81/481,disc_loss 81.061, (real 84.747, fake 77.375 ) gen_loss 564.24\n",
            "iteration 2206, epoch 5, batch 82/481,disc_loss 80.751, (real 84.885, fake 76.616 ) gen_loss 606.0\n",
            "iteration 2207, epoch 5, batch 83/481,disc_loss 82.086, (real 84.979, fake 79.193 ) gen_loss 601.56\n",
            "iteration 2208, epoch 5, batch 84/481,disc_loss 82.39, (real 85.029, fake 79.75 ) gen_loss 572.49\n",
            "iteration 2209, epoch 5, batch 85/481,disc_loss 79.435, (real 82.867, fake 76.002 ) gen_loss 603.0\n",
            "iteration 2210, epoch 5, batch 86/481,disc_loss 82.322, (real 85.847, fake 78.797 ) gen_loss 595.73\n",
            "iteration 2211, epoch 5, batch 87/481,disc_loss 79.025, (real 80.814, fake 77.236 ) gen_loss 540.98\n",
            "iteration 2212, epoch 5, batch 88/481,disc_loss 81.65, (real 85.719, fake 77.581 ) gen_loss 519.96\n",
            "iteration 2213, epoch 5, batch 89/481,disc_loss 83.006, (real 85.782, fake 80.23 ) gen_loss 546.51\n",
            "iteration 2214, epoch 5, batch 90/481,disc_loss 80.625, (real 83.909, fake 77.342 ) gen_loss 514.43\n",
            "iteration 2215, epoch 5, batch 91/481,disc_loss 80.978, (real 83.857, fake 78.099 ) gen_loss 554.26\n",
            "iteration 2216, epoch 5, batch 92/481,disc_loss 75.47, (real 78.384, fake 72.557 ) gen_loss 546.45\n",
            "iteration 2217, epoch 5, batch 93/481,disc_loss 80.927, (real 84.109, fake 77.745 ) gen_loss 657.41\n",
            "iteration 2218, epoch 5, batch 94/481,disc_loss 78.754, (real 81.31, fake 76.197 ) gen_loss 532.77\n",
            "iteration 2219, epoch 5, batch 95/481,disc_loss 81.003, (real 83.289, fake 78.718 ) gen_loss 531.76\n",
            "iteration 2220, epoch 5, batch 96/481,disc_loss 78.476, (real 82.194, fake 74.758 ) gen_loss 513.96\n",
            "iteration 2221, epoch 5, batch 97/481,disc_loss 72.538, (real 74.99, fake 70.086 ) gen_loss 571.25\n",
            "iteration 2222, epoch 5, batch 98/481,disc_loss 75.607, (real 78.432, fake 72.783 ) gen_loss 562.24\n",
            "iteration 2223, epoch 5, batch 99/481,disc_loss 77.028, (real 79.807, fake 74.249 ) gen_loss 491.38\n",
            "iteration 2224, epoch 5, batch 100/481,disc_loss 80.222, (real 83.312, fake 77.132 ) gen_loss 573.91\n",
            "iteration 2225, epoch 5, batch 101/481,disc_loss 80.996, (real 83.578, fake 78.413 ) gen_loss 507.5\n",
            "iteration 2226, epoch 5, batch 102/481,disc_loss 76.187, (real 80.128, fake 72.247 ) gen_loss 574.67\n",
            "iteration 2227, epoch 5, batch 103/481,disc_loss 81.823, (real 85.081, fake 78.565 ) gen_loss 667.01\n",
            "iteration 2228, epoch 5, batch 104/481,disc_loss 80.519, (real 84.083, fake 76.956 ) gen_loss 606.11\n",
            "iteration 2229, epoch 5, batch 105/481,disc_loss 80.643, (real 83.892, fake 77.393 ) gen_loss 585.92\n",
            "iteration 2230, epoch 5, batch 106/481,disc_loss 76.483, (real 79.714, fake 73.253 ) gen_loss 589.37\n",
            "iteration 2231, epoch 5, batch 107/481,disc_loss 77.915, (real 81.373, fake 74.457 ) gen_loss 588.61\n",
            "iteration 2232, epoch 5, batch 108/481,disc_loss 81.91, (real 84.171, fake 79.649 ) gen_loss 593.96\n",
            "iteration 2233, epoch 5, batch 109/481,disc_loss 79.869, (real 82.77, fake 76.968 ) gen_loss 608.05\n",
            "iteration 2234, epoch 5, batch 110/481,disc_loss 78.948, (real 81.643, fake 76.252 ) gen_loss 536.62\n",
            "iteration 2235, epoch 5, batch 111/481,disc_loss 79.548, (real 83.058, fake 76.037 ) gen_loss 582.61\n",
            "iteration 2236, epoch 5, batch 112/481,disc_loss 76.705, (real 79.339, fake 74.072 ) gen_loss 627.23\n",
            "iteration 2237, epoch 5, batch 113/481,disc_loss 78.784, (real 82.465, fake 75.103 ) gen_loss 583.8\n",
            "iteration 2238, epoch 5, batch 114/481,disc_loss 79.388, (real 83.195, fake 75.58 ) gen_loss 548.9\n",
            "iteration 2239, epoch 5, batch 115/481,disc_loss 79.589, (real 83.08, fake 76.099 ) gen_loss 547.38\n",
            "iteration 2240, epoch 5, batch 116/481,disc_loss 84.525, (real 88.354, fake 80.695 ) gen_loss 575.19\n",
            "iteration 2241, epoch 5, batch 117/481,disc_loss 78.5, (real 83.266, fake 73.734 ) gen_loss 551.64\n",
            "iteration 2242, epoch 5, batch 118/481,disc_loss 83.56, (real 86.821, fake 80.299 ) gen_loss 497.66\n",
            "iteration 2243, epoch 5, batch 119/481,disc_loss 77.045, (real 78.566, fake 75.524 ) gen_loss 623.01\n",
            "iteration 2244, epoch 5, batch 120/481,disc_loss 79.913, (real 83.385, fake 76.442 ) gen_loss 717.31\n",
            "iteration 2245, epoch 5, batch 121/481,disc_loss 80.824, (real 83.788, fake 77.861 ) gen_loss 596.9\n",
            "iteration 2246, epoch 5, batch 122/481,disc_loss 80.43, (real 82.809, fake 78.051 ) gen_loss 593.12\n",
            "iteration 2247, epoch 5, batch 123/481,disc_loss 74.577, (real 77.394, fake 71.761 ) gen_loss 571.52\n",
            "iteration 2248, epoch 5, batch 124/481,disc_loss 79.306, (real 81.626, fake 76.985 ) gen_loss 509.74\n",
            "iteration 2249, epoch 5, batch 125/481,disc_loss 78.914, (real 81.659, fake 76.169 ) gen_loss 560.19\n",
            "iteration 2250, epoch 5, batch 126/481,disc_loss 77.7, (real 81.408, fake 73.993 ) gen_loss 537.52\n",
            "iteration 2251, epoch 5, batch 127/481,disc_loss 81.613, (real 84.921, fake 78.305 ) gen_loss 634.97\n",
            "iteration 2252, epoch 5, batch 128/481,disc_loss 81.32, (real 82.765, fake 79.876 ) gen_loss 617.13\n",
            "iteration 2253, epoch 5, batch 129/481,disc_loss 79.09, (real 82.051, fake 76.129 ) gen_loss 565.38\n",
            "iteration 2254, epoch 5, batch 130/481,disc_loss 76.473, (real 79.452, fake 73.493 ) gen_loss 584.75\n",
            "iteration 2255, epoch 5, batch 131/481,disc_loss 77.937, (real 80.229, fake 75.645 ) gen_loss 569.11\n",
            "iteration 2256, epoch 5, batch 132/481,disc_loss 79.027, (real 81.899, fake 76.155 ) gen_loss 536.38\n",
            "iteration 2257, epoch 5, batch 133/481,disc_loss 74.342, (real 77.521, fake 71.163 ) gen_loss 569.2\n",
            "iteration 2258, epoch 5, batch 134/481,disc_loss 80.601, (real 83.339, fake 77.863 ) gen_loss 512.91\n",
            "iteration 2259, epoch 5, batch 135/481,disc_loss 79.034, (real 82.512, fake 75.556 ) gen_loss 514.72\n",
            "iteration 2260, epoch 5, batch 136/481,disc_loss 79.066, (real 81.743, fake 76.389 ) gen_loss 539.44\n",
            "iteration 2261, epoch 5, batch 137/481,disc_loss 79.346, (real 82.604, fake 76.089 ) gen_loss 548.07\n",
            "iteration 2262, epoch 5, batch 138/481,disc_loss 78.686, (real 81.448, fake 75.924 ) gen_loss 528.35\n",
            "iteration 2263, epoch 5, batch 139/481,disc_loss 75.633, (real 78.736, fake 72.53 ) gen_loss 521.35\n",
            "iteration 2264, epoch 5, batch 140/481,disc_loss 82.916, (real 86.673, fake 79.159 ) gen_loss 658.5\n",
            "iteration 2265, epoch 5, batch 141/481,disc_loss 78.619, (real 81.101, fake 76.137 ) gen_loss 520.88\n",
            "iteration 2266, epoch 5, batch 142/481,disc_loss 81.264, (real 83.893, fake 78.636 ) gen_loss 537.63\n",
            "iteration 2267, epoch 5, batch 143/481,disc_loss 78.751, (real 81.896, fake 75.607 ) gen_loss 534.09\n",
            "iteration 2268, epoch 5, batch 144/481,disc_loss 79.874, (real 83.528, fake 76.22 ) gen_loss 596.01\n",
            "iteration 2269, epoch 5, batch 145/481,disc_loss 76.529, (real 79.503, fake 73.555 ) gen_loss 532.89\n",
            "iteration 2270, epoch 5, batch 146/481,disc_loss 81.139, (real 83.883, fake 78.394 ) gen_loss 636.32\n",
            "iteration 2271, epoch 5, batch 147/481,disc_loss 81.546, (real 82.959, fake 80.134 ) gen_loss 523.16\n",
            "iteration 2272, epoch 5, batch 148/481,disc_loss 82.245, (real 85.78, fake 78.71 ) gen_loss 515.36\n",
            "iteration 2273, epoch 5, batch 149/481,disc_loss 76.942, (real 80.24, fake 73.643 ) gen_loss 551.4\n",
            "iteration 2274, epoch 5, batch 150/481,disc_loss 79.727, (real 83.812, fake 75.643 ) gen_loss 574.06\n",
            "iteration 2275, epoch 5, batch 151/481,disc_loss 76.103, (real 79.197, fake 73.008 ) gen_loss 569.27\n",
            "iteration 2276, epoch 5, batch 152/481,disc_loss 80.254, (real 83.926, fake 76.581 ) gen_loss 593.86\n",
            "iteration 2277, epoch 5, batch 153/481,disc_loss 83.723, (real 87.792, fake 79.655 ) gen_loss 563.07\n",
            "iteration 2278, epoch 5, batch 154/481,disc_loss 78.823, (real 81.321, fake 76.325 ) gen_loss 513.12\n",
            "iteration 2279, epoch 5, batch 155/481,disc_loss 80.33, (real 83.416, fake 77.244 ) gen_loss 524.71\n",
            "iteration 2280, epoch 5, batch 156/481,disc_loss 77.523, (real 79.73, fake 75.316 ) gen_loss 539.73\n",
            "iteration 2281, epoch 5, batch 157/481,disc_loss 81.868, (real 84.669, fake 79.067 ) gen_loss 560.39\n",
            "iteration 2282, epoch 5, batch 158/481,disc_loss 79.745, (real 82.378, fake 77.113 ) gen_loss 514.88\n",
            "iteration 2283, epoch 5, batch 159/481,disc_loss 81.803, (real 85.212, fake 78.395 ) gen_loss 541.47\n",
            "iteration 2284, epoch 5, batch 160/481,disc_loss 82.211, (real 85.279, fake 79.144 ) gen_loss 565.16\n",
            "iteration 2285, epoch 5, batch 161/481,disc_loss 75.631, (real 79.255, fake 72.007 ) gen_loss 597.88\n",
            "iteration 2286, epoch 5, batch 162/481,disc_loss 76.958, (real 79.595, fake 74.321 ) gen_loss 563.19\n",
            "iteration 2287, epoch 5, batch 163/481,disc_loss 75.531, (real 77.446, fake 73.616 ) gen_loss 569.43\n",
            "iteration 2288, epoch 5, batch 164/481,disc_loss 80.466, (real 83.087, fake 77.846 ) gen_loss 560.72\n",
            "iteration 2289, epoch 5, batch 165/481,disc_loss 77.307, (real 80.577, fake 74.037 ) gen_loss 591.24\n",
            "iteration 2290, epoch 5, batch 166/481,disc_loss 77.544, (real 80.951, fake 74.138 ) gen_loss 549.49\n",
            "iteration 2291, epoch 5, batch 167/481,disc_loss 80.416, (real 84.187, fake 76.646 ) gen_loss 567.39\n",
            "iteration 2292, epoch 5, batch 168/481,disc_loss 80.222, (real 83.052, fake 77.393 ) gen_loss 576.51\n",
            "iteration 2293, epoch 5, batch 169/481,disc_loss 79.897, (real 83.006, fake 76.789 ) gen_loss 520.74\n",
            "iteration 2294, epoch 5, batch 170/481,disc_loss 76.999, (real 79.49, fake 74.509 ) gen_loss 558.56\n",
            "iteration 2295, epoch 5, batch 171/481,disc_loss 81.689, (real 84.826, fake 78.552 ) gen_loss 619.57\n",
            "iteration 2296, epoch 5, batch 172/481,disc_loss 74.976, (real 78.127, fake 71.825 ) gen_loss 575.34\n",
            "iteration 2297, epoch 5, batch 173/481,disc_loss 78.395, (real 80.816, fake 75.974 ) gen_loss 555.15\n",
            "iteration 2298, epoch 5, batch 174/481,disc_loss 78.198, (real 81.575, fake 74.821 ) gen_loss 564.46\n",
            "iteration 2299, epoch 5, batch 175/481,disc_loss 74.03, (real 76.883, fake 71.176 ) gen_loss 535.44\n",
            "iteration 2300, epoch 5, batch 176/481,disc_loss 80.47, (real 83.222, fake 77.718 ) gen_loss 599.27\n",
            "iteration 2301, epoch 5, batch 177/481,disc_loss 79.275, (real 82.183, fake 76.366 ) gen_loss 591.95\n",
            "iteration 2302, epoch 5, batch 178/481,disc_loss 82.552, (real 85.394, fake 79.709 ) gen_loss 589.17\n",
            "iteration 2303, epoch 5, batch 179/481,disc_loss 81.316, (real 83.401, fake 79.232 ) gen_loss 644.42\n",
            "iteration 2304, epoch 5, batch 180/481,disc_loss 76.312, (real 80.267, fake 72.357 ) gen_loss 613.91\n",
            "iteration 2305, epoch 5, batch 181/481,disc_loss 77.377, (real 81.453, fake 73.301 ) gen_loss 601.35\n",
            "iteration 2306, epoch 5, batch 182/481,disc_loss 76.706, (real 79.289, fake 74.123 ) gen_loss 584.02\n",
            "iteration 2307, epoch 5, batch 183/481,disc_loss 84.563, (real 88.278, fake 80.848 ) gen_loss 551.99\n",
            "iteration 2308, epoch 5, batch 184/481,disc_loss 81.267, (real 84.318, fake 78.216 ) gen_loss 568.58\n",
            "iteration 2309, epoch 5, batch 185/481,disc_loss 81.33, (real 85.673, fake 76.986 ) gen_loss 589.25\n",
            "iteration 2310, epoch 5, batch 186/481,disc_loss 81.136, (real 84.148, fake 78.124 ) gen_loss 595.45\n",
            "iteration 2311, epoch 5, batch 187/481,disc_loss 82.46, (real 86.219, fake 78.7 ) gen_loss 585.75\n",
            "iteration 2312, epoch 5, batch 188/481,disc_loss 79.509, (real 82.423, fake 76.595 ) gen_loss 656.31\n",
            "iteration 2313, epoch 5, batch 189/481,disc_loss 76.524, (real 79.54, fake 73.508 ) gen_loss 654.28\n",
            "iteration 2314, epoch 5, batch 190/481,disc_loss 76.19, (real 78.44, fake 73.939 ) gen_loss 671.03\n",
            "iteration 2315, epoch 5, batch 191/481,disc_loss 78.372, (real 80.991, fake 75.753 ) gen_loss 553.61\n",
            "iteration 2316, epoch 5, batch 192/481,disc_loss 79.108, (real 82.961, fake 75.254 ) gen_loss 662.13\n",
            "iteration 2317, epoch 5, batch 193/481,disc_loss 80.008, (real 83.247, fake 76.77 ) gen_loss 542.38\n",
            "iteration 2318, epoch 5, batch 194/481,disc_loss 80.939, (real 83.755, fake 78.124 ) gen_loss 556.19\n",
            "iteration 2319, epoch 5, batch 195/481,disc_loss 79.453, (real 83.014, fake 75.893 ) gen_loss 586.13\n",
            "iteration 2320, epoch 5, batch 196/481,disc_loss 79.219, (real 81.491, fake 76.947 ) gen_loss 571.67\n",
            "iteration 2321, epoch 5, batch 197/481,disc_loss 75.572, (real 79.473, fake 71.672 ) gen_loss 540.69\n",
            "iteration 2322, epoch 5, batch 198/481,disc_loss 77.618, (real 80.947, fake 74.289 ) gen_loss 598.68\n",
            "iteration 2323, epoch 5, batch 199/481,disc_loss 74.643, (real 76.544, fake 72.741 ) gen_loss 652.19\n",
            "iteration 2324, epoch 5, batch 200/481,disc_loss 81.129, (real 84.524, fake 77.734 ) gen_loss 704.06\n",
            "iteration 2325, epoch 5, batch 201/481,disc_loss 81.388, (real 84.906, fake 77.869 ) gen_loss 875.76\n",
            "iteration 2326, epoch 5, batch 202/481,disc_loss 85.87, (real 84.919, fake 86.821 ) gen_loss 784.2\n",
            "iteration 2327, epoch 5, batch 203/481,disc_loss 79.819, (real 80.954, fake 78.685 ) gen_loss 767.06\n",
            "iteration 2328, epoch 5, batch 204/481,disc_loss 84.08, (real 88.052, fake 80.108 ) gen_loss 645.31\n",
            "iteration 2329, epoch 5, batch 205/481,disc_loss 79.271, (real 81.859, fake 76.683 ) gen_loss 665.35\n",
            "iteration 2330, epoch 5, batch 206/481,disc_loss 77.812, (real 80.647, fake 74.977 ) gen_loss 641.37\n",
            "iteration 2331, epoch 5, batch 207/481,disc_loss 78.96, (real 81.448, fake 76.472 ) gen_loss 581.59\n",
            "iteration 2332, epoch 5, batch 208/481,disc_loss 83.986, (real 86.891, fake 81.081 ) gen_loss 604.25\n",
            "iteration 2333, epoch 5, batch 209/481,disc_loss 76.035, (real 79.417, fake 72.652 ) gen_loss 549.64\n",
            "iteration 2334, epoch 5, batch 210/481,disc_loss 81.815, (real 86.569, fake 77.061 ) gen_loss 608.86\n",
            "iteration 2335, epoch 5, batch 211/481,disc_loss 79.807, (real 83.187, fake 76.428 ) gen_loss 642.22\n",
            "iteration 2336, epoch 5, batch 212/481,disc_loss 83.224, (real 86.99, fake 79.457 ) gen_loss 607.19\n",
            "iteration 2337, epoch 5, batch 213/481,disc_loss 77.503, (real 79.717, fake 75.29 ) gen_loss 588.95\n",
            "iteration 2338, epoch 5, batch 214/481,disc_loss 81.577, (real 85.215, fake 77.938 ) gen_loss 553.9\n",
            "iteration 2339, epoch 5, batch 215/481,disc_loss 83.536, (real 86.052, fake 81.02 ) gen_loss 584.22\n",
            "iteration 2340, epoch 5, batch 216/481,disc_loss 74.366, (real 76.941, fake 71.791 ) gen_loss 593.07\n",
            "iteration 2341, epoch 5, batch 217/481,disc_loss 80.517, (real 83.902, fake 77.132 ) gen_loss 582.9\n",
            "iteration 2342, epoch 5, batch 218/481,disc_loss 80.575, (real 84.014, fake 77.137 ) gen_loss 541.44\n",
            "iteration 2343, epoch 5, batch 219/481,disc_loss 80.406, (real 82.701, fake 78.11 ) gen_loss 601.1\n",
            "iteration 2344, epoch 5, batch 220/481,disc_loss 81.378, (real 85.207, fake 77.55 ) gen_loss 629.73\n",
            "iteration 2345, epoch 5, batch 221/481,disc_loss 80.627, (real 85.444, fake 75.809 ) gen_loss 564.64\n",
            "iteration 2346, epoch 5, batch 222/481,disc_loss 81.246, (real 84.794, fake 77.698 ) gen_loss 573.07\n",
            "iteration 2347, epoch 5, batch 223/481,disc_loss 74.654, (real 77.761, fake 71.547 ) gen_loss 593.38\n",
            "iteration 2348, epoch 5, batch 224/481,disc_loss 75.247, (real 78.907, fake 71.587 ) gen_loss 595.82\n",
            "iteration 2349, epoch 5, batch 225/481,disc_loss 78.742, (real 82.1, fake 75.384 ) gen_loss 574.45\n",
            "iteration 2350, epoch 5, batch 226/481,disc_loss 77.186, (real 79.458, fake 74.913 ) gen_loss 590.42\n",
            "iteration 2351, epoch 5, batch 227/481,disc_loss 77.117, (real 79.751, fake 74.482 ) gen_loss 549.66\n",
            "iteration 2352, epoch 5, batch 228/481,disc_loss 75.278, (real 77.951, fake 72.605 ) gen_loss 613.58\n",
            "iteration 2353, epoch 5, batch 229/481,disc_loss 81.453, (real 83.904, fake 79.001 ) gen_loss 593.68\n",
            "iteration 2354, epoch 5, batch 230/481,disc_loss 81.717, (real 85.042, fake 78.393 ) gen_loss 614.59\n",
            "iteration 2355, epoch 5, batch 231/481,disc_loss 76.887, (real 80.053, fake 73.72 ) gen_loss 580.22\n",
            "iteration 2356, epoch 5, batch 232/481,disc_loss 79.718, (real 82.587, fake 76.85 ) gen_loss 614.09\n",
            "iteration 2357, epoch 5, batch 233/481,disc_loss 78.027, (real 81.292, fake 74.761 ) gen_loss 644.04\n",
            "iteration 2358, epoch 5, batch 234/481,disc_loss 84.973, (real 87.779, fake 82.167 ) gen_loss 720.67\n",
            "iteration 2359, epoch 5, batch 235/481,disc_loss 79.126, (real 82.012, fake 76.24 ) gen_loss 574.45\n",
            "iteration 2360, epoch 5, batch 236/481,disc_loss 77.359, (real 81.228, fake 73.491 ) gen_loss 636.87\n",
            "iteration 2361, epoch 5, batch 237/481,disc_loss 74.898, (real 78.137, fake 71.66 ) gen_loss 680.77\n",
            "iteration 2362, epoch 5, batch 238/481,disc_loss 79.162, (real 82.97, fake 75.353 ) gen_loss 600.26\n",
            "iteration 2363, epoch 5, batch 239/481,disc_loss 82.396, (real 85.963, fake 78.828 ) gen_loss 638.61\n",
            "iteration 2364, epoch 5, batch 240/481,disc_loss 82.879, (real 85.194, fake 80.565 ) gen_loss 588.87\n",
            "iteration 2365, epoch 5, batch 241/481,disc_loss 77.508, (real 80.559, fake 74.457 ) gen_loss 564.05\n",
            "iteration 2366, epoch 5, batch 242/481,disc_loss 83.117, (real 86.079, fake 80.155 ) gen_loss 583.17\n",
            "iteration 2367, epoch 5, batch 243/481,disc_loss 79.552, (real 81.479, fake 77.626 ) gen_loss 567.63\n",
            "iteration 2368, epoch 5, batch 244/481,disc_loss 78.63, (real 82.015, fake 75.246 ) gen_loss 643.0\n",
            "iteration 2369, epoch 5, batch 245/481,disc_loss 78.813, (real 81.919, fake 75.707 ) gen_loss 638.91\n",
            "iteration 2370, epoch 5, batch 246/481,disc_loss 82.72, (real 85.064, fake 80.376 ) gen_loss 659.88\n",
            "iteration 2371, epoch 5, batch 247/481,disc_loss 86.173, (real 86.182, fake 86.163 ) gen_loss 657.47\n",
            "iteration 2372, epoch 5, batch 248/481,disc_loss 82.259, (real 82.801, fake 81.716 ) gen_loss 657.61\n",
            "iteration 2373, epoch 5, batch 249/481,disc_loss 78.546, (real 80.902, fake 76.19 ) gen_loss 693.97\n",
            "iteration 2374, epoch 5, batch 250/481,disc_loss 80.851, (real 84.203, fake 77.498 ) gen_loss 686.51\n",
            "iteration 2375, epoch 5, batch 251/481,disc_loss 84.876, (real 88.725, fake 81.028 ) gen_loss 713.26\n",
            "iteration 2376, epoch 5, batch 252/481,disc_loss 75.52, (real 78.752, fake 72.288 ) gen_loss 614.18\n",
            "iteration 2377, epoch 5, batch 253/481,disc_loss 80.281, (real 82.663, fake 77.9 ) gen_loss 561.29\n",
            "iteration 2378, epoch 5, batch 254/481,disc_loss 81.198, (real 85.202, fake 77.194 ) gen_loss 576.25\n",
            "iteration 2379, epoch 5, batch 255/481,disc_loss 79.688, (real 82.697, fake 76.68 ) gen_loss 591.15\n",
            "iteration 2380, epoch 5, batch 256/481,disc_loss 79.455, (real 81.687, fake 77.223 ) gen_loss 662.54\n",
            "iteration 2381, epoch 5, batch 257/481,disc_loss 78.783, (real 81.095, fake 76.47 ) gen_loss 558.95\n",
            "iteration 2382, epoch 5, batch 258/481,disc_loss 80.301, (real 83.17, fake 77.431 ) gen_loss 572.92\n",
            "iteration 2383, epoch 5, batch 259/481,disc_loss 74.666, (real 77.07, fake 72.263 ) gen_loss 576.5\n",
            "iteration 2384, epoch 5, batch 260/481,disc_loss 77.977, (real 81.101, fake 74.853 ) gen_loss 640.62\n",
            "iteration 2385, epoch 5, batch 261/481,disc_loss 78.226, (real 81.278, fake 75.174 ) gen_loss 635.31\n",
            "iteration 2386, epoch 5, batch 262/481,disc_loss 79.752, (real 83.312, fake 76.191 ) gen_loss 656.31\n",
            "iteration 2387, epoch 5, batch 263/481,disc_loss 87.52, (real 91.285, fake 83.754 ) gen_loss 675.44\n",
            "iteration 2388, epoch 5, batch 264/481,disc_loss 81.386, (real 84.55, fake 78.221 ) gen_loss 590.47\n",
            "iteration 2389, epoch 5, batch 265/481,disc_loss 81.259, (real 83.647, fake 78.871 ) gen_loss 643.9\n",
            "iteration 2390, epoch 5, batch 266/481,disc_loss 81.91, (real 84.765, fake 79.055 ) gen_loss 618.9\n",
            "iteration 2391, epoch 5, batch 267/481,disc_loss 81.083, (real 84.972, fake 77.194 ) gen_loss 623.08\n",
            "iteration 2392, epoch 5, batch 268/481,disc_loss 77.917, (real 81.686, fake 74.148 ) gen_loss 521.23\n",
            "iteration 2393, epoch 5, batch 269/481,disc_loss 84.577, (real 87.862, fake 81.291 ) gen_loss 544.19\n",
            "iteration 2394, epoch 5, batch 270/481,disc_loss 80.593, (real 83.846, fake 77.34 ) gen_loss 550.93\n",
            "iteration 2395, epoch 5, batch 271/481,disc_loss 78.323, (real 81.457, fake 75.189 ) gen_loss 528.09\n",
            "iteration 2396, epoch 5, batch 272/481,disc_loss 83.688, (real 86.426, fake 80.949 ) gen_loss 613.61\n",
            "iteration 2397, epoch 5, batch 273/481,disc_loss 79.051, (real 81.577, fake 76.525 ) gen_loss 610.64\n",
            "iteration 2398, epoch 5, batch 274/481,disc_loss 82.41, (real 85.148, fake 79.673 ) gen_loss 604.27\n",
            "iteration 2399, epoch 5, batch 275/481,disc_loss 78.496, (real 81.691, fake 75.301 ) gen_loss 605.41\n",
            "iteration 2400, epoch 5, batch 276/481,disc_loss 79.969, (real 82.897, fake 77.042 ) gen_loss 564.23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 2401, epoch 5, batch 277/481,disc_loss 76.885, (real 79.137, fake 74.634 ) gen_loss 638.95\n",
            "iteration 2402, epoch 5, batch 278/481,disc_loss 78.978, (real 81.335, fake 76.622 ) gen_loss 631.0\n",
            "iteration 2403, epoch 5, batch 279/481,disc_loss 80.981, (real 83.972, fake 77.991 ) gen_loss 553.46\n",
            "iteration 2404, epoch 5, batch 280/481,disc_loss 80.443, (real 83.68, fake 77.206 ) gen_loss 630.45\n",
            "iteration 2405, epoch 5, batch 281/481,disc_loss 75.243, (real 80.015, fake 70.47 ) gen_loss 597.7\n",
            "iteration 2406, epoch 5, batch 282/481,disc_loss 77.914, (real 81.623, fake 74.204 ) gen_loss 615.37\n",
            "iteration 2407, epoch 5, batch 283/481,disc_loss 81.659, (real 85.497, fake 77.82 ) gen_loss 575.12\n",
            "iteration 2408, epoch 5, batch 284/481,disc_loss 79.336, (real 81.675, fake 76.997 ) gen_loss 536.85\n",
            "iteration 2409, epoch 5, batch 285/481,disc_loss 77.714, (real 81.525, fake 73.903 ) gen_loss 598.34\n",
            "iteration 2410, epoch 5, batch 286/481,disc_loss 78.031, (real 80.977, fake 75.084 ) gen_loss 594.52\n",
            "iteration 2411, epoch 5, batch 287/481,disc_loss 78.937, (real 81.143, fake 76.731 ) gen_loss 571.69\n",
            "iteration 2412, epoch 5, batch 288/481,disc_loss 82.314, (real 85.64, fake 78.987 ) gen_loss 585.99\n",
            "iteration 2413, epoch 5, batch 289/481,disc_loss 82.563, (real 86.162, fake 78.964 ) gen_loss 565.25\n",
            "iteration 2414, epoch 5, batch 290/481,disc_loss 74.757, (real 77.844, fake 71.67 ) gen_loss 577.49\n",
            "iteration 2415, epoch 5, batch 291/481,disc_loss 80.333, (real 83.272, fake 77.394 ) gen_loss 577.65\n",
            "iteration 2416, epoch 5, batch 292/481,disc_loss 80.426, (real 84.835, fake 76.018 ) gen_loss 498.99\n",
            "iteration 2417, epoch 5, batch 293/481,disc_loss 80.752, (real 84.994, fake 76.51 ) gen_loss 538.17\n",
            "iteration 2418, epoch 5, batch 294/481,disc_loss 82.728, (real 85.887, fake 79.57 ) gen_loss 593.71\n",
            "iteration 2419, epoch 5, batch 295/481,disc_loss 83.957, (real 85.962, fake 81.951 ) gen_loss 786.35\n",
            "iteration 2420, epoch 5, batch 296/481,disc_loss 85.052, (real 85.184, fake 84.921 ) gen_loss 612.52\n",
            "iteration 2421, epoch 5, batch 297/481,disc_loss 78.777, (real 79.634, fake 77.92 ) gen_loss 598.8\n",
            "iteration 2422, epoch 5, batch 298/481,disc_loss 82.35, (real 85.249, fake 79.451 ) gen_loss 596.41\n",
            "iteration 2423, epoch 5, batch 299/481,disc_loss 77.634, (real 79.875, fake 75.392 ) gen_loss 612.0\n",
            "iteration 2424, epoch 5, batch 300/481,disc_loss 81.78, (real 84.549, fake 79.012 ) gen_loss 575.09\n",
            "iteration 2425, epoch 5, batch 301/481,disc_loss 80.18, (real 82.986, fake 77.373 ) gen_loss 555.08\n",
            "iteration 2426, epoch 5, batch 302/481,disc_loss 75.986, (real 78.983, fake 72.989 ) gen_loss 599.78\n",
            "iteration 2427, epoch 5, batch 303/481,disc_loss 76.245, (real 78.931, fake 73.559 ) gen_loss 658.47\n",
            "iteration 2428, epoch 5, batch 304/481,disc_loss 83.657, (real 88.081, fake 79.234 ) gen_loss 601.41\n",
            "iteration 2429, epoch 5, batch 305/481,disc_loss 79.113, (real 81.558, fake 76.668 ) gen_loss 581.62\n",
            "iteration 2430, epoch 5, batch 306/481,disc_loss 79.168, (real 82.032, fake 76.305 ) gen_loss 606.86\n",
            "iteration 2431, epoch 5, batch 307/481,disc_loss 82.161, (real 84.177, fake 80.145 ) gen_loss 567.76\n",
            "iteration 2432, epoch 5, batch 308/481,disc_loss 75.962, (real 78.572, fake 73.352 ) gen_loss 545.77\n",
            "iteration 2433, epoch 5, batch 309/481,disc_loss 78.087, (real 81.288, fake 74.885 ) gen_loss 567.75\n",
            "iteration 2434, epoch 5, batch 310/481,disc_loss 81.672, (real 84.298, fake 79.046 ) gen_loss 644.58\n",
            "iteration 2435, epoch 5, batch 311/481,disc_loss 79.237, (real 81.955, fake 76.518 ) gen_loss 535.17\n",
            "iteration 2436, epoch 5, batch 312/481,disc_loss 77.178, (real 79.727, fake 74.629 ) gen_loss 554.43\n",
            "iteration 2437, epoch 5, batch 313/481,disc_loss 78.585, (real 81.761, fake 75.409 ) gen_loss 552.51\n",
            "iteration 2438, epoch 5, batch 314/481,disc_loss 76.547, (real 80.139, fake 72.955 ) gen_loss 642.27\n",
            "iteration 2439, epoch 5, batch 315/481,disc_loss 79.486, (real 83.117, fake 75.856 ) gen_loss 581.92\n",
            "iteration 2440, epoch 5, batch 316/481,disc_loss 77.203, (real 79.583, fake 74.822 ) gen_loss 599.43\n",
            "iteration 2441, epoch 5, batch 317/481,disc_loss 79.572, (real 81.912, fake 77.232 ) gen_loss 559.79\n",
            "iteration 2442, epoch 5, batch 318/481,disc_loss 75.189, (real 77.145, fake 73.233 ) gen_loss 558.31\n",
            "iteration 2443, epoch 5, batch 319/481,disc_loss 75.043, (real 77.692, fake 72.393 ) gen_loss 576.46\n",
            "iteration 2444, epoch 5, batch 320/481,disc_loss 76.904, (real 79.771, fake 74.038 ) gen_loss 656.79\n",
            "iteration 2445, epoch 5, batch 321/481,disc_loss 86.268, (real 89.205, fake 83.331 ) gen_loss 629.13\n",
            "iteration 2446, epoch 5, batch 322/481,disc_loss 75.959, (real 79.196, fake 72.722 ) gen_loss 542.76\n",
            "iteration 2447, epoch 5, batch 323/481,disc_loss 79.986, (real 82.481, fake 77.49 ) gen_loss 531.33\n",
            "iteration 2448, epoch 5, batch 324/481,disc_loss 78.818, (real 81.627, fake 76.008 ) gen_loss 644.5\n",
            "iteration 2449, epoch 5, batch 325/481,disc_loss 80.435, (real 84.391, fake 76.478 ) gen_loss 661.79\n",
            "iteration 2450, epoch 5, batch 326/481,disc_loss 79.815, (real 82.463, fake 77.168 ) gen_loss 598.71\n",
            "iteration 2451, epoch 5, batch 327/481,disc_loss 78.828, (real 81.78, fake 75.875 ) gen_loss 605.08\n",
            "iteration 2452, epoch 5, batch 328/481,disc_loss 81.447, (real 84.871, fake 78.022 ) gen_loss 613.11\n",
            "iteration 2453, epoch 5, batch 329/481,disc_loss 80.522, (real 82.944, fake 78.099 ) gen_loss 580.45\n",
            "iteration 2454, epoch 5, batch 330/481,disc_loss 79.195, (real 83.465, fake 74.925 ) gen_loss 620.51\n",
            "iteration 2455, epoch 5, batch 331/481,disc_loss 77.012, (real 78.987, fake 75.037 ) gen_loss 681.5\n",
            "iteration 2456, epoch 5, batch 332/481,disc_loss 82.205, (real 85.805, fake 78.606 ) gen_loss 760.05\n",
            "iteration 2457, epoch 5, batch 333/481,disc_loss 81.668, (real 84.237, fake 79.099 ) gen_loss 717.18\n",
            "iteration 2458, epoch 5, batch 334/481,disc_loss 81.86, (real 78.914, fake 84.806 ) gen_loss 663.68\n",
            "iteration 2459, epoch 5, batch 335/481,disc_loss 80.526, (real 80.481, fake 80.571 ) gen_loss 673.44\n",
            "iteration 2460, epoch 5, batch 336/481,disc_loss 81.526, (real 84.289, fake 78.763 ) gen_loss 586.02\n",
            "iteration 2461, epoch 5, batch 337/481,disc_loss 82.626, (real 85.974, fake 79.277 ) gen_loss 580.62\n",
            "iteration 2462, epoch 5, batch 338/481,disc_loss 81.426, (real 84.48, fake 78.372 ) gen_loss 626.89\n",
            "iteration 2463, epoch 5, batch 339/481,disc_loss 81.12, (real 85.468, fake 76.772 ) gen_loss 573.04\n",
            "iteration 2464, epoch 5, batch 340/481,disc_loss 80.642, (real 83.201, fake 78.082 ) gen_loss 541.41\n",
            "iteration 2465, epoch 5, batch 341/481,disc_loss 77.871, (real 80.481, fake 75.261 ) gen_loss 556.86\n",
            "iteration 2466, epoch 5, batch 342/481,disc_loss 81.116, (real 84.062, fake 78.169 ) gen_loss 538.62\n",
            "iteration 2467, epoch 5, batch 343/481,disc_loss 82.025, (real 85.75, fake 78.301 ) gen_loss 555.48\n",
            "iteration 2468, epoch 5, batch 344/481,disc_loss 82.089, (real 85.255, fake 78.923 ) gen_loss 608.02\n",
            "iteration 2469, epoch 5, batch 345/481,disc_loss 78.92, (real 82.217, fake 75.623 ) gen_loss 549.01\n",
            "iteration 2470, epoch 5, batch 346/481,disc_loss 82.488, (real 85.027, fake 79.949 ) gen_loss 514.0\n",
            "iteration 2471, epoch 5, batch 347/481,disc_loss 75.533, (real 78.892, fake 72.174 ) gen_loss 567.58\n",
            "iteration 2472, epoch 5, batch 348/481,disc_loss 79.57, (real 82.525, fake 76.614 ) gen_loss 570.43\n",
            "iteration 2473, epoch 5, batch 349/481,disc_loss 79.466, (real 82.51, fake 76.421 ) gen_loss 684.87\n",
            "iteration 2474, epoch 5, batch 350/481,disc_loss 79.345, (real 82.52, fake 76.17 ) gen_loss 692.66\n",
            "iteration 2475, epoch 5, batch 351/481,disc_loss 79.381, (real 82.376, fake 76.387 ) gen_loss 655.86\n",
            "iteration 2476, epoch 5, batch 352/481,disc_loss 77.35, (real 81.028, fake 73.673 ) gen_loss 617.12\n",
            "iteration 2477, epoch 5, batch 353/481,disc_loss 81.091, (real 84.967, fake 77.215 ) gen_loss 592.53\n",
            "iteration 2478, epoch 5, batch 354/481,disc_loss 78.555, (real 81.244, fake 75.867 ) gen_loss 554.35\n",
            "iteration 2479, epoch 5, batch 355/481,disc_loss 82.042, (real 84.254, fake 79.83 ) gen_loss 561.33\n",
            "iteration 2480, epoch 5, batch 356/481,disc_loss 78.314, (real 81.723, fake 74.906 ) gen_loss 623.49\n",
            "iteration 2481, epoch 5, batch 357/481,disc_loss 80.372, (real 83.37, fake 77.374 ) gen_loss 567.24\n",
            "iteration 2482, epoch 5, batch 358/481,disc_loss 81.294, (real 83.853, fake 78.735 ) gen_loss 616.22\n",
            "iteration 2483, epoch 5, batch 359/481,disc_loss 81.58, (real 84.264, fake 78.896 ) gen_loss 583.06\n",
            "iteration 2484, epoch 5, batch 360/481,disc_loss 74.994, (real 76.747, fake 73.241 ) gen_loss 631.84\n",
            "iteration 2485, epoch 5, batch 361/481,disc_loss 77.487, (real 79.685, fake 75.288 ) gen_loss 612.4\n",
            "iteration 2486, epoch 5, batch 362/481,disc_loss 75.785, (real 78.48, fake 73.089 ) gen_loss 626.91\n",
            "iteration 2487, epoch 5, batch 363/481,disc_loss 80.924, (real 83.679, fake 78.168 ) gen_loss 636.82\n",
            "iteration 2488, epoch 5, batch 364/481,disc_loss 79.499, (real 82.673, fake 76.325 ) gen_loss 734.79\n",
            "iteration 2489, epoch 5, batch 365/481,disc_loss 79.013, (real 82.622, fake 75.403 ) gen_loss 623.36\n",
            "iteration 2490, epoch 5, batch 366/481,disc_loss 80.117, (real 82.684, fake 77.55 ) gen_loss 567.98\n",
            "iteration 2491, epoch 5, batch 367/481,disc_loss 80.083, (real 83.029, fake 77.137 ) gen_loss 579.07\n",
            "iteration 2492, epoch 5, batch 368/481,disc_loss 77.805, (real 79.212, fake 76.398 ) gen_loss 585.29\n",
            "iteration 2493, epoch 5, batch 369/481,disc_loss 76.982, (real 82.181, fake 71.783 ) gen_loss 595.46\n",
            "iteration 2494, epoch 5, batch 370/481,disc_loss 77.96, (real 80.759, fake 75.161 ) gen_loss 651.16\n",
            "iteration 2495, epoch 5, batch 371/481,disc_loss 74.792, (real 78.059, fake 71.525 ) gen_loss 610.58\n",
            "iteration 2496, epoch 5, batch 372/481,disc_loss 81.011, (real 84.267, fake 77.755 ) gen_loss 621.64\n",
            "iteration 2497, epoch 5, batch 373/481,disc_loss 81.011, (real 83.639, fake 78.383 ) gen_loss 603.45\n",
            "iteration 2498, epoch 5, batch 374/481,disc_loss 77.755, (real 79.946, fake 75.564 ) gen_loss 677.9\n",
            "iteration 2499, epoch 5, batch 375/481,disc_loss 81.159, (real 83.382, fake 78.935 ) gen_loss 625.45\n",
            "iteration 2500, epoch 5, batch 376/481,disc_loss 80.501, (real 83.338, fake 77.663 ) gen_loss 591.47\n",
            "iteration 2501, epoch 5, batch 377/481,disc_loss 78.482, (real 82.055, fake 74.909 ) gen_loss 629.62\n",
            "iteration 2502, epoch 5, batch 378/481,disc_loss 81.44, (real 83.292, fake 79.587 ) gen_loss 663.35\n",
            "iteration 2503, epoch 5, batch 379/481,disc_loss 80.272, (real 83.671, fake 76.873 ) gen_loss 637.19\n",
            "iteration 2504, epoch 5, batch 380/481,disc_loss 84.755, (real 86.819, fake 82.692 ) gen_loss 600.28\n",
            "iteration 2505, epoch 5, batch 381/481,disc_loss 81.304, (real 83.281, fake 79.328 ) gen_loss 611.61\n",
            "iteration 2506, epoch 5, batch 382/481,disc_loss 82.669, (real 86.414, fake 78.924 ) gen_loss 574.71\n",
            "iteration 2507, epoch 5, batch 383/481,disc_loss 79.715, (real 81.985, fake 77.445 ) gen_loss 721.14\n",
            "iteration 2508, epoch 5, batch 384/481,disc_loss 78.479, (real 80.636, fake 76.322 ) gen_loss 583.61\n",
            "iteration 2509, epoch 5, batch 385/481,disc_loss 77.997, (real 81.538, fake 74.457 ) gen_loss 598.26\n",
            "iteration 2510, epoch 5, batch 386/481,disc_loss 80.301, (real 82.957, fake 77.645 ) gen_loss 560.18\n",
            "iteration 2511, epoch 5, batch 387/481,disc_loss 81.39, (real 83.504, fake 79.276 ) gen_loss 555.07\n",
            "iteration 2512, epoch 5, batch 388/481,disc_loss 80.77, (real 84.534, fake 77.007 ) gen_loss 597.35\n",
            "iteration 2513, epoch 5, batch 389/481,disc_loss 78.552, (real 81.369, fake 75.736 ) gen_loss 636.98\n",
            "iteration 2514, epoch 5, batch 390/481,disc_loss 82.593, (real 86.023, fake 79.163 ) gen_loss 653.81\n",
            "iteration 2515, epoch 5, batch 391/481,disc_loss 78.326, (real 81.911, fake 74.74 ) gen_loss 646.5\n",
            "iteration 2516, epoch 5, batch 392/481,disc_loss 76.546, (real 79.36, fake 73.733 ) gen_loss 571.28\n",
            "iteration 2517, epoch 5, batch 393/481,disc_loss 84.359, (real 87.558, fake 81.16 ) gen_loss 582.14\n",
            "iteration 2518, epoch 5, batch 394/481,disc_loss 79.019, (real 81.656, fake 76.383 ) gen_loss 584.88\n",
            "iteration 2519, epoch 5, batch 395/481,disc_loss 77.082, (real 80.175, fake 73.989 ) gen_loss 644.72\n",
            "iteration 2520, epoch 5, batch 396/481,disc_loss 77.164, (real 79.491, fake 74.836 ) gen_loss 721.68\n",
            "iteration 2521, epoch 5, batch 397/481,disc_loss 77.386, (real 80.404, fake 74.368 ) gen_loss 603.56\n",
            "iteration 2522, epoch 5, batch 398/481,disc_loss 83.923, (real 87.313, fake 80.533 ) gen_loss 625.94\n",
            "iteration 2523, epoch 5, batch 399/481,disc_loss 78.041, (real 80.252, fake 75.83 ) gen_loss 580.44\n",
            "iteration 2524, epoch 5, batch 400/481,disc_loss 76.496, (real 79.828, fake 73.164 ) gen_loss 594.46\n",
            "iteration 2525, epoch 5, batch 401/481,disc_loss 82.194, (real 85.386, fake 79.001 ) gen_loss 650.76\n",
            "iteration 2526, epoch 5, batch 402/481,disc_loss 77.889, (real 81.176, fake 74.603 ) gen_loss 634.32\n",
            "iteration 2527, epoch 5, batch 403/481,disc_loss 81.91, (real 84.231, fake 79.589 ) gen_loss 638.46\n",
            "iteration 2528, epoch 5, batch 404/481,disc_loss 75.169, (real 77.625, fake 72.712 ) gen_loss 578.85\n",
            "iteration 2529, epoch 5, batch 405/481,disc_loss 80.637, (real 84.717, fake 76.557 ) gen_loss 656.64\n",
            "iteration 2530, epoch 5, batch 406/481,disc_loss 77.267, (real 80.615, fake 73.919 ) gen_loss 565.22\n",
            "iteration 2531, epoch 5, batch 407/481,disc_loss 78.643, (real 82.457, fake 74.829 ) gen_loss 589.11\n",
            "iteration 2532, epoch 5, batch 408/481,disc_loss 79.255, (real 81.977, fake 76.534 ) gen_loss 581.99\n",
            "iteration 2533, epoch 5, batch 409/481,disc_loss 82.299, (real 84.236, fake 80.361 ) gen_loss 611.66\n",
            "iteration 2534, epoch 5, batch 410/481,disc_loss 79.642, (real 82.845, fake 76.438 ) gen_loss 553.35\n",
            "iteration 2535, epoch 5, batch 411/481,disc_loss 77.143, (real 79.689, fake 74.598 ) gen_loss 585.36\n",
            "iteration 2536, epoch 5, batch 412/481,disc_loss 82.778, (real 85.558, fake 79.999 ) gen_loss 572.16\n",
            "iteration 2537, epoch 5, batch 413/481,disc_loss 79.888, (real 83.411, fake 76.365 ) gen_loss 603.91\n",
            "iteration 2538, epoch 5, batch 414/481,disc_loss 78.123, (real 80.498, fake 75.748 ) gen_loss 536.97\n",
            "iteration 2539, epoch 5, batch 415/481,disc_loss 79.932, (real 84.053, fake 75.81 ) gen_loss 636.98\n",
            "iteration 2540, epoch 5, batch 416/481,disc_loss 78.83, (real 82.338, fake 75.323 ) gen_loss 581.54\n",
            "iteration 2541, epoch 5, batch 417/481,disc_loss 79.65, (real 82.38, fake 76.92 ) gen_loss 621.26\n",
            "iteration 2542, epoch 5, batch 418/481,disc_loss 75.091, (real 77.565, fake 72.617 ) gen_loss 599.37\n",
            "iteration 2543, epoch 5, batch 419/481,disc_loss 77.762, (real 80.182, fake 75.342 ) gen_loss 593.36\n",
            "iteration 2544, epoch 5, batch 420/481,disc_loss 79.819, (real 82.916, fake 76.723 ) gen_loss 536.86\n",
            "iteration 2545, epoch 5, batch 421/481,disc_loss 80.21, (real 83.166, fake 77.253 ) gen_loss 602.11\n",
            "iteration 2546, epoch 5, batch 422/481,disc_loss 79.89, (real 82.944, fake 76.835 ) gen_loss 614.1\n",
            "iteration 2547, epoch 5, batch 423/481,disc_loss 77.734, (real 79.997, fake 75.472 ) gen_loss 636.43\n",
            "iteration 2548, epoch 5, batch 424/481,disc_loss 81.861, (real 85.961, fake 77.761 ) gen_loss 632.0\n",
            "iteration 2549, epoch 5, batch 425/481,disc_loss 79.017, (real 82.496, fake 75.539 ) gen_loss 612.91\n",
            "iteration 2550, epoch 5, batch 426/481,disc_loss 78.452, (real 81.302, fake 75.603 ) gen_loss 604.42\n",
            "iteration 2551, epoch 5, batch 427/481,disc_loss 83.389, (real 86.451, fake 80.327 ) gen_loss 561.31\n",
            "iteration 2552, epoch 5, batch 428/481,disc_loss 80.354, (real 83.303, fake 77.406 ) gen_loss 603.92\n",
            "iteration 2553, epoch 5, batch 429/481,disc_loss 76.896, (real 79.294, fake 74.498 ) gen_loss 625.8\n",
            "iteration 2554, epoch 5, batch 430/481,disc_loss 81.313, (real 83.031, fake 79.595 ) gen_loss 599.67\n",
            "iteration 2555, epoch 5, batch 431/481,disc_loss 79.942, (real 83.384, fake 76.5 ) gen_loss 603.21\n",
            "iteration 2556, epoch 5, batch 432/481,disc_loss 78.082, (real 81.616, fake 74.548 ) gen_loss 555.88\n",
            "iteration 2557, epoch 5, batch 433/481,disc_loss 76.1, (real 78.921, fake 73.279 ) gen_loss 624.06\n",
            "iteration 2558, epoch 5, batch 434/481,disc_loss 76.53, (real 78.439, fake 74.621 ) gen_loss 598.63\n",
            "iteration 2559, epoch 5, batch 435/481,disc_loss 83.147, (real 86.714, fake 79.579 ) gen_loss 593.45\n",
            "iteration 2560, epoch 5, batch 436/481,disc_loss 81.521, (real 86.989, fake 76.053 ) gen_loss 568.41\n",
            "iteration 2561, epoch 5, batch 437/481,disc_loss 76.795, (real 79.74, fake 73.849 ) gen_loss 582.19\n",
            "iteration 2562, epoch 5, batch 438/481,disc_loss 77.869, (real 81.521, fake 74.217 ) gen_loss 624.64\n",
            "iteration 2563, epoch 5, batch 439/481,disc_loss 79.946, (real 82.295, fake 77.597 ) gen_loss 585.18\n",
            "iteration 2564, epoch 5, batch 440/481,disc_loss 79.232, (real 81.508, fake 76.955 ) gen_loss 586.74\n",
            "iteration 2565, epoch 5, batch 441/481,disc_loss 81.7, (real 84.714, fake 78.686 ) gen_loss 561.97\n",
            "iteration 2566, epoch 5, batch 442/481,disc_loss 78.818, (real 81.522, fake 76.114 ) gen_loss 573.46\n",
            "iteration 2567, epoch 5, batch 443/481,disc_loss 84.883, (real 88.072, fake 81.695 ) gen_loss 572.47\n",
            "iteration 2568, epoch 5, batch 444/481,disc_loss 78.519, (real 80.856, fake 76.183 ) gen_loss 521.86\n",
            "iteration 2569, epoch 5, batch 445/481,disc_loss 80.994, (real 83.673, fake 78.315 ) gen_loss 607.8\n",
            "iteration 2570, epoch 5, batch 446/481,disc_loss 77.859, (real 80.38, fake 75.339 ) gen_loss 621.75\n",
            "iteration 2571, epoch 5, batch 447/481,disc_loss 81.113, (real 83.816, fake 78.409 ) gen_loss 599.53\n",
            "iteration 2572, epoch 5, batch 448/481,disc_loss 76.509, (real 78.984, fake 74.034 ) gen_loss 538.25\n",
            "iteration 2573, epoch 5, batch 449/481,disc_loss 82.193, (real 86.037, fake 78.35 ) gen_loss 629.45\n",
            "iteration 2574, epoch 5, batch 450/481,disc_loss 77.649, (real 79.596, fake 75.702 ) gen_loss 645.05\n",
            "iteration 2575, epoch 5, batch 451/481,disc_loss 78.685, (real 81.944, fake 75.426 ) gen_loss 564.42\n",
            "iteration 2576, epoch 5, batch 452/481,disc_loss 74.962, (real 77.929, fake 71.996 ) gen_loss 603.48\n",
            "iteration 2577, epoch 5, batch 453/481,disc_loss 82.961, (real 84.814, fake 81.108 ) gen_loss 611.29\n",
            "iteration 2578, epoch 5, batch 454/481,disc_loss 81.986, (real 84.051, fake 79.922 ) gen_loss 547.77\n",
            "iteration 2579, epoch 5, batch 455/481,disc_loss 77.32, (real 81.53, fake 73.111 ) gen_loss 637.41\n",
            "iteration 2580, epoch 5, batch 456/481,disc_loss 77.192, (real 80.39, fake 73.993 ) gen_loss 639.3\n",
            "iteration 2581, epoch 5, batch 457/481,disc_loss 80.432, (real 84.044, fake 76.82 ) gen_loss 578.38\n",
            "iteration 2582, epoch 5, batch 458/481,disc_loss 79.466, (real 81.593, fake 77.338 ) gen_loss 579.69\n",
            "iteration 2583, epoch 5, batch 459/481,disc_loss 78.88, (real 81.94, fake 75.821 ) gen_loss 554.71\n",
            "iteration 2584, epoch 5, batch 460/481,disc_loss 78.773, (real 82.893, fake 74.653 ) gen_loss 587.84\n",
            "iteration 2585, epoch 5, batch 461/481,disc_loss 75.859, (real 79.084, fake 72.635 ) gen_loss 583.03\n",
            "iteration 2586, epoch 5, batch 462/481,disc_loss 80.798, (real 83.921, fake 77.675 ) gen_loss 626.26\n",
            "iteration 2587, epoch 5, batch 463/481,disc_loss 84.253, (real 86.648, fake 81.858 ) gen_loss 605.65\n",
            "iteration 2588, epoch 5, batch 464/481,disc_loss 81.573, (real 84.216, fake 78.931 ) gen_loss 669.6\n",
            "iteration 2589, epoch 5, batch 465/481,disc_loss 79.979, (real 82.095, fake 77.863 ) gen_loss 626.94\n",
            "iteration 2590, epoch 5, batch 466/481,disc_loss 76.905, (real 80.37, fake 73.44 ) gen_loss 511.77\n",
            "iteration 2591, epoch 5, batch 467/481,disc_loss 79.142, (real 82.072, fake 76.211 ) gen_loss 605.43\n",
            "iteration 2592, epoch 5, batch 468/481,disc_loss 79.93, (real 82.683, fake 77.178 ) gen_loss 601.33\n",
            "iteration 2593, epoch 5, batch 469/481,disc_loss 80.534, (real 83.791, fake 77.276 ) gen_loss 590.55\n",
            "iteration 2594, epoch 5, batch 470/481,disc_loss 78.598, (real 82.159, fake 75.038 ) gen_loss 589.4\n",
            "iteration 2595, epoch 5, batch 471/481,disc_loss 75.982, (real 79.023, fake 72.942 ) gen_loss 612.26\n",
            "iteration 2596, epoch 5, batch 472/481,disc_loss 80.398, (real 84.52, fake 76.276 ) gen_loss 603.04\n",
            "iteration 2597, epoch 5, batch 473/481,disc_loss 79.355, (real 81.582, fake 77.129 ) gen_loss 559.28\n",
            "iteration 2598, epoch 5, batch 474/481,disc_loss 78.266, (real 81.217, fake 75.314 ) gen_loss 573.44\n",
            "iteration 2599, epoch 5, batch 475/481,disc_loss 79.994, (real 82.539, fake 77.449 ) gen_loss 585.35\n",
            "iteration 2600, epoch 5, batch 476/481,disc_loss 83.928, (real 84.754, fake 83.103 ) gen_loss 577.43\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 2601, epoch 5, batch 477/481,disc_loss 87.385, (real 86.477, fake 88.293 ) gen_loss 604.0\n",
            "iteration 2602, epoch 5, batch 478/481,disc_loss 79.112, (real 80.446, fake 77.777 ) gen_loss 627.75\n",
            "iteration 2603, epoch 5, batch 479/481,disc_loss 83.341, (real 85.401, fake 81.282 ) gen_loss 516.01\n",
            "iteration 2604, epoch 5, batch 480/481,disc_loss 81.741, (real 84.602, fake 78.88 ) gen_loss 627.21\n",
            "iteration 2605, epoch 5, batch 481/481,disc_loss 81.464, (real 85.771, fake 77.156 ) gen_loss 579.14\n",
            "iteration 2606, epoch 6, batch 1/481,disc_loss 77.646, (real 80.284, fake 75.009 ) gen_loss 497.88\n",
            "iteration 2607, epoch 6, batch 2/481,disc_loss 80.025, (real 83.162, fake 76.888 ) gen_loss 588.92\n",
            "iteration 2608, epoch 6, batch 3/481,disc_loss 76.773, (real 79.155, fake 74.391 ) gen_loss 571.91\n",
            "iteration 2609, epoch 6, batch 4/481,disc_loss 81.608, (real 84.968, fake 78.248 ) gen_loss 634.36\n",
            "iteration 2610, epoch 6, batch 5/481,disc_loss 77.942, (real 80.552, fake 75.332 ) gen_loss 629.49\n",
            "iteration 2611, epoch 6, batch 6/481,disc_loss 78.786, (real 81.812, fake 75.761 ) gen_loss 566.31\n",
            "iteration 2612, epoch 6, batch 7/481,disc_loss 78.91, (real 82.554, fake 75.266 ) gen_loss 556.13\n",
            "iteration 2613, epoch 6, batch 8/481,disc_loss 79.07, (real 81.534, fake 76.605 ) gen_loss 543.35\n",
            "iteration 2614, epoch 6, batch 9/481,disc_loss 77.059, (real 79.833, fake 74.284 ) gen_loss 590.44\n",
            "iteration 2615, epoch 6, batch 10/481,disc_loss 79.386, (real 80.823, fake 77.948 ) gen_loss 577.61\n",
            "iteration 2616, epoch 6, batch 11/481,disc_loss 79.557, (real 82.064, fake 77.05 ) gen_loss 563.87\n",
            "iteration 2617, epoch 6, batch 12/481,disc_loss 76.339, (real 78.999, fake 73.68 ) gen_loss 561.15\n",
            "iteration 2618, epoch 6, batch 13/481,disc_loss 74.385, (real 77.693, fake 71.076 ) gen_loss 551.94\n",
            "iteration 2619, epoch 6, batch 14/481,disc_loss 77.257, (real 79.596, fake 74.917 ) gen_loss 564.54\n",
            "iteration 2620, epoch 6, batch 15/481,disc_loss 75.364, (real 78.914, fake 71.813 ) gen_loss 604.27\n",
            "iteration 2621, epoch 6, batch 16/481,disc_loss 78.836, (real 81.866, fake 75.806 ) gen_loss 542.82\n",
            "iteration 2622, epoch 6, batch 17/481,disc_loss 79.048, (real 82.174, fake 75.922 ) gen_loss 543.31\n",
            "iteration 2623, epoch 6, batch 18/481,disc_loss 76.751, (real 79.551, fake 73.951 ) gen_loss 589.88\n",
            "iteration 2624, epoch 6, batch 19/481,disc_loss 78.897, (real 81.139, fake 76.655 ) gen_loss 591.11\n",
            "iteration 2625, epoch 6, batch 20/481,disc_loss 77.572, (real 80.054, fake 75.091 ) gen_loss 650.61\n",
            "iteration 2626, epoch 6, batch 21/481,disc_loss 79.453, (real 82.757, fake 76.149 ) gen_loss 621.53\n",
            "iteration 2627, epoch 6, batch 22/481,disc_loss 78.5, (real 80.54, fake 76.459 ) gen_loss 595.79\n",
            "iteration 2628, epoch 6, batch 23/481,disc_loss 81.834, (real 86.158, fake 77.51 ) gen_loss 625.94\n",
            "iteration 2629, epoch 6, batch 24/481,disc_loss 75.123, (real 78.214, fake 72.032 ) gen_loss 617.17\n",
            "iteration 2630, epoch 6, batch 25/481,disc_loss 73.121, (real 75.648, fake 70.594 ) gen_loss 593.7\n",
            "iteration 2631, epoch 6, batch 26/481,disc_loss 81.325, (real 84.869, fake 77.782 ) gen_loss 573.48\n",
            "iteration 2632, epoch 6, batch 27/481,disc_loss 79.561, (real 82.71, fake 76.412 ) gen_loss 582.38\n",
            "iteration 2633, epoch 6, batch 28/481,disc_loss 81.513, (real 83.822, fake 79.204 ) gen_loss 550.91\n",
            "iteration 2634, epoch 6, batch 29/481,disc_loss 80.069, (real 82.69, fake 77.449 ) gen_loss 549.13\n",
            "iteration 2635, epoch 6, batch 30/481,disc_loss 82.963, (real 85.19, fake 80.737 ) gen_loss 573.12\n",
            "iteration 2636, epoch 6, batch 31/481,disc_loss 84.847, (real 87.586, fake 82.108 ) gen_loss 601.42\n",
            "iteration 2637, epoch 6, batch 32/481,disc_loss 76.675, (real 80.524, fake 72.826 ) gen_loss 606.08\n",
            "iteration 2638, epoch 6, batch 33/481,disc_loss 80.125, (real 82.735, fake 77.515 ) gen_loss 576.79\n",
            "iteration 2639, epoch 6, batch 34/481,disc_loss 81.716, (real 84.815, fake 78.617 ) gen_loss 564.1\n",
            "iteration 2640, epoch 6, batch 35/481,disc_loss 79.386, (real 82.006, fake 76.765 ) gen_loss 619.13\n",
            "iteration 2641, epoch 6, batch 36/481,disc_loss 75.155, (real 78.281, fake 72.03 ) gen_loss 557.19\n",
            "iteration 2642, epoch 6, batch 37/481,disc_loss 80.964, (real 84.329, fake 77.599 ) gen_loss 640.41\n",
            "iteration 2643, epoch 6, batch 38/481,disc_loss 79.029, (real 83.121, fake 74.937 ) gen_loss 616.31\n",
            "iteration 2644, epoch 6, batch 39/481,disc_loss 79.502, (real 85.218, fake 73.786 ) gen_loss 607.24\n",
            "iteration 2645, epoch 6, batch 40/481,disc_loss 84.582, (real 86.673, fake 82.491 ) gen_loss 611.98\n",
            "iteration 2646, epoch 6, batch 41/481,disc_loss 78.671, (real 81.818, fake 75.524 ) gen_loss 556.65\n",
            "iteration 2647, epoch 6, batch 42/481,disc_loss 79.192, (real 82.356, fake 76.028 ) gen_loss 574.65\n",
            "iteration 2648, epoch 6, batch 43/481,disc_loss 77.814, (real 80.522, fake 75.107 ) gen_loss 527.14\n",
            "iteration 2649, epoch 6, batch 44/481,disc_loss 79.671, (real 82.818, fake 76.524 ) gen_loss 530.78\n",
            "iteration 2650, epoch 6, batch 45/481,disc_loss 78.617, (real 80.407, fake 76.827 ) gen_loss 559.16\n",
            "iteration 2651, epoch 6, batch 46/481,disc_loss 79.06, (real 81.204, fake 76.915 ) gen_loss 617.1\n",
            "iteration 2652, epoch 6, batch 47/481,disc_loss 76.304, (real 79.257, fake 73.35 ) gen_loss 762.02\n",
            "iteration 2653, epoch 6, batch 48/481,disc_loss 79.927, (real 82.719, fake 77.135 ) gen_loss 618.58\n",
            "iteration 2654, epoch 6, batch 49/481,disc_loss 84.312, (real 85.678, fake 82.947 ) gen_loss 665.05\n",
            "iteration 2655, epoch 6, batch 50/481,disc_loss 87.048, (real 89.475, fake 84.62 ) gen_loss 566.17\n",
            "iteration 2656, epoch 6, batch 51/481,disc_loss 79.27, (real 81.957, fake 76.583 ) gen_loss 581.31\n",
            "iteration 2657, epoch 6, batch 52/481,disc_loss 80.763, (real 83.428, fake 78.097 ) gen_loss 592.73\n",
            "iteration 2658, epoch 6, batch 53/481,disc_loss 76.588, (real 78.422, fake 74.754 ) gen_loss 565.07\n",
            "iteration 2659, epoch 6, batch 54/481,disc_loss 82.653, (real 85.354, fake 79.952 ) gen_loss 568.49\n",
            "iteration 2660, epoch 6, batch 55/481,disc_loss 81.483, (real 85.726, fake 77.239 ) gen_loss 605.84\n",
            "iteration 2661, epoch 6, batch 56/481,disc_loss 78.824, (real 81.678, fake 75.969 ) gen_loss 575.34\n",
            "iteration 2662, epoch 6, batch 57/481,disc_loss 77.849, (real 80.544, fake 75.154 ) gen_loss 591.71\n",
            "iteration 2663, epoch 6, batch 58/481,disc_loss 79.226, (real 82.576, fake 75.877 ) gen_loss 592.24\n",
            "iteration 2664, epoch 6, batch 59/481,disc_loss 80.612, (real 83.605, fake 77.619 ) gen_loss 616.8\n",
            "iteration 2665, epoch 6, batch 60/481,disc_loss 81.229, (real 83.423, fake 79.034 ) gen_loss 627.43\n",
            "iteration 2666, epoch 6, batch 61/481,disc_loss 82.652, (real 85.075, fake 80.229 ) gen_loss 644.15\n",
            "iteration 2667, epoch 6, batch 62/481,disc_loss 79.768, (real 82.699, fake 76.836 ) gen_loss 661.93\n",
            "iteration 2668, epoch 6, batch 63/481,disc_loss 83.749, (real 87.645, fake 79.854 ) gen_loss 616.75\n",
            "iteration 2669, epoch 6, batch 64/481,disc_loss 81.257, (real 82.941, fake 79.572 ) gen_loss 594.5\n",
            "iteration 2670, epoch 6, batch 65/481,disc_loss 74.178, (real 76.718, fake 71.638 ) gen_loss 564.19\n",
            "iteration 2671, epoch 6, batch 66/481,disc_loss 79.749, (real 82.256, fake 77.242 ) gen_loss 605.57\n",
            "iteration 2672, epoch 6, batch 67/481,disc_loss 74.653, (real 77.06, fake 72.245 ) gen_loss 570.42\n",
            "iteration 2673, epoch 6, batch 68/481,disc_loss 79.971, (real 83.085, fake 76.856 ) gen_loss 566.15\n",
            "iteration 2674, epoch 6, batch 69/481,disc_loss 78.55, (real 80.901, fake 76.199 ) gen_loss 583.25\n",
            "iteration 2675, epoch 6, batch 70/481,disc_loss 78.668, (real 81.601, fake 75.736 ) gen_loss 596.26\n",
            "iteration 2676, epoch 6, batch 71/481,disc_loss 75.986, (real 79.704, fake 72.268 ) gen_loss 626.26\n",
            "iteration 2677, epoch 6, batch 72/481,disc_loss 75.585, (real 78.455, fake 72.716 ) gen_loss 581.3\n",
            "iteration 2678, epoch 6, batch 73/481,disc_loss 76.722, (real 79.048, fake 74.395 ) gen_loss 571.92\n",
            "iteration 2679, epoch 6, batch 74/481,disc_loss 81.087, (real 83.93, fake 78.245 ) gen_loss 596.28\n",
            "iteration 2680, epoch 6, batch 75/481,disc_loss 77.874, (real 80.774, fake 74.974 ) gen_loss 615.78\n",
            "iteration 2681, epoch 6, batch 76/481,disc_loss 80.52, (real 82.646, fake 78.393 ) gen_loss 601.92\n",
            "iteration 2682, epoch 6, batch 77/481,disc_loss 82.439, (real 83.035, fake 81.843 ) gen_loss 549.3\n",
            "iteration 2683, epoch 6, batch 78/481,disc_loss 83.366, (real 84.119, fake 82.612 ) gen_loss 525.92\n",
            "iteration 2684, epoch 6, batch 79/481,disc_loss 76.275, (real 78.364, fake 74.186 ) gen_loss 574.37\n",
            "iteration 2685, epoch 6, batch 80/481,disc_loss 83.4, (real 86.768, fake 80.032 ) gen_loss 543.84\n",
            "iteration 2686, epoch 6, batch 81/481,disc_loss 79.614, (real 82.146, fake 77.081 ) gen_loss 540.22\n",
            "iteration 2687, epoch 6, batch 82/481,disc_loss 78.903, (real 81.88, fake 75.927 ) gen_loss 590.86\n",
            "iteration 2688, epoch 6, batch 83/481,disc_loss 78.476, (real 81.521, fake 75.431 ) gen_loss 564.73\n",
            "iteration 2689, epoch 6, batch 84/481,disc_loss 74.428, (real 77.146, fake 71.709 ) gen_loss 585.7\n",
            "iteration 2690, epoch 6, batch 85/481,disc_loss 80.605, (real 83.619, fake 77.592 ) gen_loss 569.67\n",
            "iteration 2691, epoch 6, batch 86/481,disc_loss 73.783, (real 77.432, fake 70.134 ) gen_loss 575.13\n",
            "iteration 2692, epoch 6, batch 87/481,disc_loss 77.31, (real 80.037, fake 74.583 ) gen_loss 611.66\n",
            "iteration 2693, epoch 6, batch 88/481,disc_loss 83.169, (real 84.973, fake 81.364 ) gen_loss 608.51\n",
            "iteration 2694, epoch 6, batch 89/481,disc_loss 81.32, (real 83.907, fake 78.733 ) gen_loss 650.86\n",
            "iteration 2695, epoch 6, batch 90/481,disc_loss 78.53, (real 81.549, fake 75.512 ) gen_loss 550.19\n",
            "iteration 2696, epoch 6, batch 91/481,disc_loss 84.749, (real 88.181, fake 81.316 ) gen_loss 569.41\n",
            "iteration 2697, epoch 6, batch 92/481,disc_loss 78.114, (real 80.292, fake 75.935 ) gen_loss 566.81\n",
            "iteration 2698, epoch 6, batch 93/481,disc_loss 80.496, (real 82.972, fake 78.021 ) gen_loss 536.51\n",
            "iteration 2699, epoch 6, batch 94/481,disc_loss 80.029, (real 82.728, fake 77.33 ) gen_loss 569.28\n",
            "iteration 2700, epoch 6, batch 95/481,disc_loss 78.941, (real 81.889, fake 75.993 ) gen_loss 550.94\n",
            "iteration 2701, epoch 6, batch 96/481,disc_loss 76.471, (real 79.761, fake 73.18 ) gen_loss 576.66\n",
            "iteration 2702, epoch 6, batch 97/481,disc_loss 74.262, (real 76.621, fake 71.903 ) gen_loss 557.68\n",
            "iteration 2703, epoch 6, batch 98/481,disc_loss 77.282, (real 80.501, fake 74.063 ) gen_loss 575.67\n",
            "iteration 2704, epoch 6, batch 99/481,disc_loss 79.946, (real 83.465, fake 76.426 ) gen_loss 565.42\n",
            "iteration 2705, epoch 6, batch 100/481,disc_loss 79.7, (real 82.445, fake 76.956 ) gen_loss 556.22\n",
            "iteration 2706, epoch 6, batch 101/481,disc_loss 79.976, (real 82.925, fake 77.026 ) gen_loss 655.88\n",
            "iteration 2707, epoch 6, batch 102/481,disc_loss 80.603, (real 83.248, fake 77.959 ) gen_loss 651.35\n",
            "iteration 2708, epoch 6, batch 103/481,disc_loss 76.317, (real 79.086, fake 73.547 ) gen_loss 597.71\n",
            "iteration 2709, epoch 6, batch 104/481,disc_loss 78.482, (real 81.252, fake 75.712 ) gen_loss 586.03\n",
            "iteration 2710, epoch 6, batch 105/481,disc_loss 82.129, (real 84.505, fake 79.753 ) gen_loss 609.17\n",
            "iteration 2711, epoch 6, batch 106/481,disc_loss 87.151, (real 90.798, fake 83.504 ) gen_loss 622.92\n",
            "iteration 2712, epoch 6, batch 107/481,disc_loss 78.644, (real 82.204, fake 75.084 ) gen_loss 582.45\n",
            "iteration 2713, epoch 6, batch 108/481,disc_loss 77.223, (real 80.716, fake 73.731 ) gen_loss 558.14\n",
            "iteration 2714, epoch 6, batch 109/481,disc_loss 82.449, (real 86.502, fake 78.396 ) gen_loss 624.45\n",
            "iteration 2715, epoch 6, batch 110/481,disc_loss 83.434, (real 86.236, fake 80.632 ) gen_loss 631.28\n",
            "iteration 2716, epoch 6, batch 111/481,disc_loss 79.842, (real 82.187, fake 77.496 ) gen_loss 608.15\n",
            "iteration 2717, epoch 6, batch 112/481,disc_loss 77.843, (real 80.356, fake 75.329 ) gen_loss 603.87\n",
            "iteration 2718, epoch 6, batch 113/481,disc_loss 77.615, (real 79.745, fake 75.485 ) gen_loss 619.35\n",
            "iteration 2719, epoch 6, batch 114/481,disc_loss 83.693, (real 86.554, fake 80.833 ) gen_loss 600.54\n",
            "iteration 2720, epoch 6, batch 115/481,disc_loss 83.438, (real 84.181, fake 82.696 ) gen_loss 531.77\n",
            "iteration 2721, epoch 6, batch 116/481,disc_loss 81.097, (real 81.791, fake 80.402 ) gen_loss 598.55\n",
            "iteration 2722, epoch 6, batch 117/481,disc_loss 83.574, (real 85.017, fake 82.131 ) gen_loss 567.17\n",
            "iteration 2723, epoch 6, batch 118/481,disc_loss 73.925, (real 76.501, fake 71.348 ) gen_loss 489.98\n",
            "iteration 2724, epoch 6, batch 119/481,disc_loss 79.277, (real 81.64, fake 76.913 ) gen_loss 502.54\n",
            "iteration 2725, epoch 6, batch 120/481,disc_loss 83.497, (real 86.796, fake 80.199 ) gen_loss 539.72\n",
            "iteration 2726, epoch 6, batch 121/481,disc_loss 79.948, (real 81.943, fake 77.954 ) gen_loss 504.08\n",
            "iteration 2727, epoch 6, batch 122/481,disc_loss 78.035, (real 80.485, fake 75.585 ) gen_loss 539.66\n",
            "iteration 2728, epoch 6, batch 123/481,disc_loss 77.781, (real 81.088, fake 74.474 ) gen_loss 546.92\n",
            "iteration 2729, epoch 6, batch 124/481,disc_loss 80.712, (real 84.771, fake 76.653 ) gen_loss 585.52\n",
            "iteration 2730, epoch 6, batch 125/481,disc_loss 77.111, (real 79.943, fake 74.28 ) gen_loss 661.44\n",
            "iteration 2731, epoch 6, batch 126/481,disc_loss 74.557, (real 78.184, fake 70.931 ) gen_loss 550.22\n",
            "iteration 2732, epoch 6, batch 127/481,disc_loss 78.308, (real 80.686, fake 75.93 ) gen_loss 543.96\n",
            "iteration 2733, epoch 6, batch 128/481,disc_loss 84.747, (real 87.273, fake 82.221 ) gen_loss 533.85\n",
            "iteration 2734, epoch 6, batch 129/481,disc_loss 78.277, (real 81.229, fake 75.325 ) gen_loss 549.63\n",
            "iteration 2735, epoch 6, batch 130/481,disc_loss 81.792, (real 85.04, fake 78.544 ) gen_loss 579.78\n",
            "iteration 2736, epoch 6, batch 131/481,disc_loss 79.287, (real 82.4, fake 76.173 ) gen_loss 527.35\n",
            "iteration 2737, epoch 6, batch 132/481,disc_loss 76.672, (real 79.296, fake 74.048 ) gen_loss 542.23\n",
            "iteration 2738, epoch 6, batch 133/481,disc_loss 81.792, (real 87.382, fake 76.202 ) gen_loss 592.35\n",
            "iteration 2739, epoch 6, batch 134/481,disc_loss 81.375, (real 85.229, fake 77.521 ) gen_loss 549.18\n",
            "iteration 2740, epoch 6, batch 135/481,disc_loss 78.862, (real 82.124, fake 75.6 ) gen_loss 544.5\n",
            "iteration 2741, epoch 6, batch 136/481,disc_loss 81.214, (real 84.056, fake 78.372 ) gen_loss 583.56\n",
            "iteration 2742, epoch 6, batch 137/481,disc_loss 77.586, (real 81.698, fake 73.474 ) gen_loss 528.37\n",
            "iteration 2743, epoch 6, batch 138/481,disc_loss 78.849, (real 81.734, fake 75.963 ) gen_loss 610.26\n",
            "iteration 2744, epoch 6, batch 139/481,disc_loss 75.238, (real 77.542, fake 72.933 ) gen_loss 535.26\n",
            "iteration 2745, epoch 6, batch 140/481,disc_loss 77.581, (real 79.951, fake 75.211 ) gen_loss 595.4\n",
            "iteration 2746, epoch 6, batch 141/481,disc_loss 83.372, (real 86.367, fake 80.378 ) gen_loss 556.73\n",
            "iteration 2747, epoch 6, batch 142/481,disc_loss 80.547, (real 83.938, fake 77.156 ) gen_loss 535.16\n",
            "iteration 2748, epoch 6, batch 143/481,disc_loss 83.236, (real 86.277, fake 80.196 ) gen_loss 542.02\n",
            "iteration 2749, epoch 6, batch 144/481,disc_loss 80.788, (real 83.923, fake 77.653 ) gen_loss 542.87\n",
            "iteration 2750, epoch 6, batch 145/481,disc_loss 80.967, (real 84.21, fake 77.725 ) gen_loss 567.1\n",
            "iteration 2751, epoch 6, batch 146/481,disc_loss 76.392, (real 79.212, fake 73.572 ) gen_loss 533.26\n",
            "iteration 2752, epoch 6, batch 147/481,disc_loss 78.553, (real 81.86, fake 75.245 ) gen_loss 557.96\n",
            "iteration 2753, epoch 6, batch 148/481,disc_loss 74.121, (real 77.276, fake 70.966 ) gen_loss 582.09\n",
            "iteration 2754, epoch 6, batch 149/481,disc_loss 82.965, (real 86.541, fake 79.39 ) gen_loss 586.87\n",
            "iteration 2755, epoch 6, batch 150/481,disc_loss 77.796, (real 80.553, fake 75.039 ) gen_loss 560.74\n",
            "iteration 2756, epoch 6, batch 151/481,disc_loss 77.567, (real 80.63, fake 74.505 ) gen_loss 529.96\n",
            "iteration 2757, epoch 6, batch 152/481,disc_loss 78.256, (real 81.14, fake 75.372 ) gen_loss 565.64\n",
            "iteration 2758, epoch 6, batch 153/481,disc_loss 77.472, (real 80.448, fake 74.496 ) gen_loss 562.64\n",
            "iteration 2759, epoch 6, batch 154/481,disc_loss 78.786, (real 81.598, fake 75.975 ) gen_loss 644.21\n",
            "iteration 2760, epoch 6, batch 155/481,disc_loss 75.767, (real 78.347, fake 73.188 ) gen_loss 542.29\n",
            "iteration 2761, epoch 6, batch 156/481,disc_loss 77.86, (real 82.013, fake 73.708 ) gen_loss 572.21\n",
            "iteration 2762, epoch 6, batch 157/481,disc_loss 87.087, (real 89.995, fake 84.18 ) gen_loss 573.77\n",
            "iteration 2763, epoch 6, batch 158/481,disc_loss 74.454, (real 77.126, fake 71.782 ) gen_loss 636.78\n",
            "iteration 2764, epoch 6, batch 159/481,disc_loss 76.853, (real 80.008, fake 73.698 ) gen_loss 583.01\n",
            "iteration 2765, epoch 6, batch 160/481,disc_loss 80.471, (real 83.025, fake 77.917 ) gen_loss 545.56\n",
            "iteration 2766, epoch 6, batch 161/481,disc_loss 84.218, (real 86.753, fake 81.684 ) gen_loss 532.32\n",
            "iteration 2767, epoch 6, batch 162/481,disc_loss 79.158, (real 81.394, fake 76.922 ) gen_loss 515.51\n",
            "iteration 2768, epoch 6, batch 163/481,disc_loss 78.365, (real 80.851, fake 75.878 ) gen_loss 532.53\n",
            "iteration 2769, epoch 6, batch 164/481,disc_loss 76.749, (real 79.717, fake 73.782 ) gen_loss 606.5\n",
            "iteration 2770, epoch 6, batch 165/481,disc_loss 78.453, (real 81.742, fake 75.164 ) gen_loss 590.14\n",
            "iteration 2771, epoch 6, batch 166/481,disc_loss 77.184, (real 79.233, fake 75.136 ) gen_loss 637.42\n",
            "iteration 2772, epoch 6, batch 167/481,disc_loss 80.391, (real 83.449, fake 77.332 ) gen_loss 631.14\n",
            "iteration 2773, epoch 6, batch 168/481,disc_loss 82.993, (real 86.02, fake 79.967 ) gen_loss 573.37\n",
            "iteration 2774, epoch 6, batch 169/481,disc_loss 79.374, (real 82.485, fake 76.263 ) gen_loss 521.7\n",
            "iteration 2775, epoch 6, batch 170/481,disc_loss 77.696, (real 80.244, fake 75.147 ) gen_loss 560.83\n",
            "iteration 2776, epoch 6, batch 171/481,disc_loss 79.906, (real 83.191, fake 76.622 ) gen_loss 552.66\n",
            "iteration 2777, epoch 6, batch 172/481,disc_loss 73.851, (real 77.968, fake 69.733 ) gen_loss 616.03\n",
            "iteration 2778, epoch 6, batch 173/481,disc_loss 79.246, (real 82.202, fake 76.29 ) gen_loss 569.42\n",
            "iteration 2779, epoch 6, batch 174/481,disc_loss 83.563, (real 88.417, fake 78.71 ) gen_loss 576.48\n",
            "iteration 2780, epoch 6, batch 175/481,disc_loss 82.34, (real 86.104, fake 78.576 ) gen_loss 551.37\n",
            "iteration 2781, epoch 6, batch 176/481,disc_loss 75.561, (real 78.056, fake 73.066 ) gen_loss 628.33\n",
            "iteration 2782, epoch 6, batch 177/481,disc_loss 83.207, (real 85.06, fake 81.355 ) gen_loss 603.26\n",
            "iteration 2783, epoch 6, batch 178/481,disc_loss 76.311, (real 79.023, fake 73.6 ) gen_loss 670.69\n",
            "iteration 2784, epoch 6, batch 179/481,disc_loss 82.269, (real 86.123, fake 78.415 ) gen_loss 648.05\n",
            "iteration 2785, epoch 6, batch 180/481,disc_loss 82.09, (real 84.613, fake 79.568 ) gen_loss 615.36\n",
            "iteration 2786, epoch 6, batch 181/481,disc_loss 78.454, (real 82.137, fake 74.77 ) gen_loss 540.98\n",
            "iteration 2787, epoch 6, batch 182/481,disc_loss 81.519, (real 84.866, fake 78.172 ) gen_loss 638.05\n",
            "iteration 2788, epoch 6, batch 183/481,disc_loss 75.223, (real 77.29, fake 73.156 ) gen_loss 566.02\n",
            "iteration 2789, epoch 6, batch 184/481,disc_loss 86.452, (real 89.235, fake 83.668 ) gen_loss 653.27\n",
            "iteration 2790, epoch 6, batch 185/481,disc_loss 75.007, (real 78.485, fake 71.53 ) gen_loss 509.74\n",
            "iteration 2791, epoch 6, batch 186/481,disc_loss 80.523, (real 83.731, fake 77.315 ) gen_loss 591.35\n",
            "iteration 2792, epoch 6, batch 187/481,disc_loss 77.376, (real 80.003, fake 74.749 ) gen_loss 594.45\n",
            "iteration 2793, epoch 6, batch 188/481,disc_loss 76.504, (real 78.848, fake 74.16 ) gen_loss 551.94\n",
            "iteration 2794, epoch 6, batch 189/481,disc_loss 77.155, (real 78.806, fake 75.504 ) gen_loss 578.13\n",
            "iteration 2795, epoch 6, batch 190/481,disc_loss 76.19, (real 78.739, fake 73.641 ) gen_loss 543.34\n",
            "iteration 2796, epoch 6, batch 191/481,disc_loss 76.432, (real 78.976, fake 73.889 ) gen_loss 613.23\n",
            "iteration 2797, epoch 6, batch 192/481,disc_loss 77.201, (real 79.11, fake 75.293 ) gen_loss 617.91\n",
            "iteration 2798, epoch 6, batch 193/481,disc_loss 78.595, (real 81.575, fake 75.614 ) gen_loss 597.13\n",
            "iteration 2799, epoch 6, batch 194/481,disc_loss 80.398, (real 83.376, fake 77.419 ) gen_loss 563.04\n",
            "iteration 2800, epoch 6, batch 195/481,disc_loss 77.624, (real 79.373, fake 75.874 ) gen_loss 611.64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 2801, epoch 6, batch 196/481,disc_loss 77.558, (real 79.718, fake 75.398 ) gen_loss 595.14\n",
            "iteration 2802, epoch 6, batch 197/481,disc_loss 74.201, (real 77.715, fake 70.687 ) gen_loss 572.25\n",
            "iteration 2803, epoch 6, batch 198/481,disc_loss 81.188, (real 83.356, fake 79.019 ) gen_loss 576.88\n",
            "iteration 2804, epoch 6, batch 199/481,disc_loss 80.86, (real 83.872, fake 77.848 ) gen_loss 582.04\n",
            "iteration 2805, epoch 6, batch 200/481,disc_loss 78.14, (real 81.23, fake 75.05 ) gen_loss 607.07\n",
            "iteration 2806, epoch 6, batch 201/481,disc_loss 76.829, (real 79.782, fake 73.876 ) gen_loss 587.76\n",
            "iteration 2807, epoch 6, batch 202/481,disc_loss 77.765, (real 80.7, fake 74.83 ) gen_loss 551.16\n",
            "iteration 2808, epoch 6, batch 203/481,disc_loss 78.945, (real 81.258, fake 76.633 ) gen_loss 559.22\n",
            "iteration 2809, epoch 6, batch 204/481,disc_loss 73.892, (real 77.501, fake 70.284 ) gen_loss 621.85\n",
            "iteration 2810, epoch 6, batch 205/481,disc_loss 79.741, (real 82.564, fake 76.917 ) gen_loss 576.03\n",
            "iteration 2811, epoch 6, batch 206/481,disc_loss 78.6, (real 82.132, fake 75.068 ) gen_loss 607.6\n",
            "iteration 2812, epoch 6, batch 207/481,disc_loss 75.594, (real 77.895, fake 73.292 ) gen_loss 584.04\n",
            "iteration 2813, epoch 6, batch 208/481,disc_loss 79.201, (real 82.253, fake 76.149 ) gen_loss 611.61\n",
            "iteration 2814, epoch 6, batch 209/481,disc_loss 78.814, (real 81.172, fake 76.455 ) gen_loss 568.63\n",
            "iteration 2815, epoch 6, batch 210/481,disc_loss 79.226, (real 80.863, fake 77.589 ) gen_loss 618.41\n",
            "iteration 2816, epoch 6, batch 211/481,disc_loss 81.187, (real 84.505, fake 77.87 ) gen_loss 602.0\n",
            "iteration 2817, epoch 6, batch 212/481,disc_loss 82.474, (real 84.668, fake 80.28 ) gen_loss 656.94\n",
            "iteration 2818, epoch 6, batch 213/481,disc_loss 78.82, (real 82.587, fake 75.052 ) gen_loss 634.76\n",
            "iteration 2819, epoch 6, batch 214/481,disc_loss 76.18, (real 79.542, fake 72.817 ) gen_loss 587.01\n",
            "iteration 2820, epoch 6, batch 215/481,disc_loss 80.991, (real 83.935, fake 78.048 ) gen_loss 575.15\n",
            "iteration 2821, epoch 6, batch 216/481,disc_loss 77.126, (real 79.924, fake 74.328 ) gen_loss 582.87\n",
            "iteration 2822, epoch 6, batch 217/481,disc_loss 74.027, (real 77.486, fake 70.567 ) gen_loss 627.97\n",
            "iteration 2823, epoch 6, batch 218/481,disc_loss 78.392, (real 80.888, fake 75.896 ) gen_loss 558.85\n",
            "iteration 2824, epoch 6, batch 219/481,disc_loss 80.894, (real 84.392, fake 77.395 ) gen_loss 664.81\n",
            "iteration 2825, epoch 6, batch 220/481,disc_loss 77.636, (real 80.596, fake 74.677 ) gen_loss 574.52\n",
            "iteration 2826, epoch 6, batch 221/481,disc_loss 79.506, (real 82.277, fake 76.736 ) gen_loss 551.08\n",
            "iteration 2827, epoch 6, batch 222/481,disc_loss 80.316, (real 82.958, fake 77.674 ) gen_loss 606.69\n",
            "iteration 2828, epoch 6, batch 223/481,disc_loss 77.562, (real 79.666, fake 75.459 ) gen_loss 591.33\n",
            "iteration 2829, epoch 6, batch 224/481,disc_loss 73.968, (real 77.252, fake 70.684 ) gen_loss 583.09\n",
            "iteration 2830, epoch 6, batch 225/481,disc_loss 78.614, (real 80.945, fake 76.283 ) gen_loss 621.15\n",
            "iteration 2831, epoch 6, batch 226/481,disc_loss 78.555, (real 80.297, fake 76.814 ) gen_loss 557.2\n",
            "iteration 2832, epoch 6, batch 227/481,disc_loss 79.012, (real 81.647, fake 76.378 ) gen_loss 611.08\n",
            "iteration 2833, epoch 6, batch 228/481,disc_loss 82.429, (real 85.584, fake 79.273 ) gen_loss 566.79\n",
            "iteration 2834, epoch 6, batch 229/481,disc_loss 77.559, (real 81.38, fake 73.738 ) gen_loss 626.59\n",
            "iteration 2835, epoch 6, batch 230/481,disc_loss 81.445, (real 84.876, fake 78.013 ) gen_loss 624.59\n",
            "iteration 2836, epoch 6, batch 231/481,disc_loss 75.111, (real 77.536, fake 72.686 ) gen_loss 545.98\n",
            "iteration 2837, epoch 6, batch 232/481,disc_loss 81.48, (real 84.557, fake 78.402 ) gen_loss 577.59\n",
            "iteration 2838, epoch 6, batch 233/481,disc_loss 82.493, (real 86.154, fake 78.833 ) gen_loss 514.66\n",
            "iteration 2839, epoch 6, batch 234/481,disc_loss 83.029, (real 85.999, fake 80.059 ) gen_loss 553.98\n",
            "iteration 2840, epoch 6, batch 235/481,disc_loss 78.691, (real 81.202, fake 76.18 ) gen_loss 576.69\n",
            "iteration 2841, epoch 6, batch 236/481,disc_loss 77.875, (real 79.696, fake 76.054 ) gen_loss 569.18\n",
            "iteration 2842, epoch 6, batch 237/481,disc_loss 81.465, (real 83.566, fake 79.363 ) gen_loss 531.47\n",
            "iteration 2843, epoch 6, batch 238/481,disc_loss 71.377, (real 75.118, fake 67.636 ) gen_loss 497.38\n",
            "iteration 2844, epoch 6, batch 239/481,disc_loss 80.887, (real 84.563, fake 77.211 ) gen_loss 582.94\n",
            "iteration 2845, epoch 6, batch 240/481,disc_loss 82.831, (real 85.985, fake 79.677 ) gen_loss 581.32\n",
            "iteration 2846, epoch 6, batch 241/481,disc_loss 77.236, (real 79.333, fake 75.14 ) gen_loss 632.38\n",
            "iteration 2847, epoch 6, batch 242/481,disc_loss 78.985, (real 81.803, fake 76.167 ) gen_loss 610.58\n",
            "iteration 2848, epoch 6, batch 243/481,disc_loss 76.024, (real 79.149, fake 72.899 ) gen_loss 588.35\n",
            "iteration 2849, epoch 6, batch 244/481,disc_loss 80.299, (real 84.016, fake 76.582 ) gen_loss 567.53\n",
            "iteration 2850, epoch 6, batch 245/481,disc_loss 77.884, (real 80.316, fake 75.451 ) gen_loss 509.13\n",
            "iteration 2851, epoch 6, batch 246/481,disc_loss 82.734, (real 85.432, fake 80.036 ) gen_loss 532.74\n",
            "iteration 2852, epoch 6, batch 247/481,disc_loss 77.543, (real 79.153, fake 75.933 ) gen_loss 618.76\n",
            "iteration 2853, epoch 6, batch 248/481,disc_loss 81.168, (real 84.117, fake 78.219 ) gen_loss 599.11\n",
            "iteration 2854, epoch 6, batch 249/481,disc_loss 79.318, (real 82.737, fake 75.9 ) gen_loss 557.9\n",
            "iteration 2855, epoch 6, batch 250/481,disc_loss 79.258, (real 81.977, fake 76.539 ) gen_loss 591.97\n",
            "iteration 2856, epoch 6, batch 251/481,disc_loss 79.112, (real 82.794, fake 75.43 ) gen_loss 653.4\n",
            "iteration 2857, epoch 6, batch 252/481,disc_loss 80.731, (real 83.492, fake 77.97 ) gen_loss 564.55\n",
            "iteration 2858, epoch 6, batch 253/481,disc_loss 81.614, (real 84.298, fake 78.931 ) gen_loss 600.74\n",
            "iteration 2859, epoch 6, batch 254/481,disc_loss 86.481, (real 89.647, fake 83.314 ) gen_loss 505.03\n",
            "iteration 2860, epoch 6, batch 255/481,disc_loss 81.026, (real 83.429, fake 78.623 ) gen_loss 584.69\n",
            "iteration 2861, epoch 6, batch 256/481,disc_loss 81.586, (real 84.078, fake 79.095 ) gen_loss 587.99\n",
            "iteration 2862, epoch 6, batch 257/481,disc_loss 78.313, (real 80.618, fake 76.008 ) gen_loss 577.21\n",
            "iteration 2863, epoch 6, batch 258/481,disc_loss 77.246, (real 79.989, fake 74.503 ) gen_loss 574.01\n",
            "iteration 2864, epoch 6, batch 259/481,disc_loss 80.294, (real 81.925, fake 78.663 ) gen_loss 605.73\n",
            "iteration 2865, epoch 6, batch 260/481,disc_loss 79.526, (real 81.817, fake 77.236 ) gen_loss 530.2\n",
            "iteration 2866, epoch 6, batch 261/481,disc_loss 85.231, (real 87.525, fake 82.937 ) gen_loss 585.52\n",
            "iteration 2867, epoch 6, batch 262/481,disc_loss 78.042, (real 80.86, fake 75.225 ) gen_loss 554.57\n",
            "iteration 2868, epoch 6, batch 263/481,disc_loss 78.625, (real 81.627, fake 75.623 ) gen_loss 611.37\n",
            "iteration 2869, epoch 6, batch 264/481,disc_loss 76.783, (real 79.446, fake 74.119 ) gen_loss 578.02\n",
            "iteration 2870, epoch 6, batch 265/481,disc_loss 80.022, (real 81.98, fake 78.065 ) gen_loss 573.63\n",
            "iteration 2871, epoch 6, batch 266/481,disc_loss 81.201, (real 83.613, fake 78.788 ) gen_loss 610.51\n",
            "iteration 2872, epoch 6, batch 267/481,disc_loss 79.704, (real 81.774, fake 77.633 ) gen_loss 546.25\n",
            "iteration 2873, epoch 6, batch 268/481,disc_loss 73.206, (real 76.446, fake 69.966 ) gen_loss 601.21\n",
            "iteration 2874, epoch 6, batch 269/481,disc_loss 80.422, (real 83.562, fake 77.283 ) gen_loss 585.66\n",
            "iteration 2875, epoch 6, batch 270/481,disc_loss 79.154, (real 81.613, fake 76.694 ) gen_loss 572.05\n",
            "iteration 2876, epoch 6, batch 271/481,disc_loss 79.319, (real 82.455, fake 76.183 ) gen_loss 591.4\n",
            "iteration 2877, epoch 6, batch 272/481,disc_loss 76.702, (real 79.139, fake 74.265 ) gen_loss 619.42\n",
            "iteration 2878, epoch 6, batch 273/481,disc_loss 80.235, (real 82.793, fake 77.677 ) gen_loss 534.15\n",
            "iteration 2879, epoch 6, batch 274/481,disc_loss 78.82, (real 81.468, fake 76.173 ) gen_loss 578.32\n",
            "iteration 2880, epoch 6, batch 275/481,disc_loss 77.715, (real 82.223, fake 73.207 ) gen_loss 553.5\n",
            "iteration 2881, epoch 6, batch 276/481,disc_loss 80.321, (real 83.823, fake 76.819 ) gen_loss 582.77\n",
            "iteration 2882, epoch 6, batch 277/481,disc_loss 81.813, (real 84.716, fake 78.911 ) gen_loss 543.18\n",
            "iteration 2883, epoch 6, batch 278/481,disc_loss 80.616, (real 83.811, fake 77.421 ) gen_loss 571.25\n",
            "iteration 2884, epoch 6, batch 279/481,disc_loss 80.324, (real 83.861, fake 76.787 ) gen_loss 580.4\n",
            "iteration 2885, epoch 6, batch 280/481,disc_loss 74.992, (real 78.474, fake 71.511 ) gen_loss 652.99\n",
            "iteration 2886, epoch 6, batch 281/481,disc_loss 75.605, (real 78.016, fake 73.193 ) gen_loss 640.14\n",
            "iteration 2887, epoch 6, batch 282/481,disc_loss 78.014, (real 80.866, fake 75.163 ) gen_loss 572.89\n",
            "iteration 2888, epoch 6, batch 283/481,disc_loss 76.219, (real 78.857, fake 73.582 ) gen_loss 556.8\n",
            "iteration 2889, epoch 6, batch 284/481,disc_loss 74.89, (real 76.973, fake 72.808 ) gen_loss 592.5\n",
            "iteration 2890, epoch 6, batch 285/481,disc_loss 77.272, (real 79.798, fake 74.747 ) gen_loss 622.28\n",
            "iteration 2891, epoch 6, batch 286/481,disc_loss 75.264, (real 77.672, fake 72.855 ) gen_loss 546.99\n",
            "iteration 2892, epoch 6, batch 287/481,disc_loss 75.808, (real 78.103, fake 73.513 ) gen_loss 557.72\n",
            "iteration 2893, epoch 6, batch 288/481,disc_loss 84.724, (real 88.175, fake 81.274 ) gen_loss 529.51\n",
            "iteration 2894, epoch 6, batch 289/481,disc_loss 79.278, (real 81.979, fake 76.578 ) gen_loss 553.21\n",
            "iteration 2895, epoch 6, batch 290/481,disc_loss 77.396, (real 80.004, fake 74.788 ) gen_loss 576.64\n",
            "iteration 2896, epoch 6, batch 291/481,disc_loss 77.881, (real 80.926, fake 74.835 ) gen_loss 532.91\n",
            "iteration 2897, epoch 6, batch 292/481,disc_loss 77.485, (real 79.758, fake 75.212 ) gen_loss 637.84\n",
            "iteration 2898, epoch 6, batch 293/481,disc_loss 79.195, (real 80.674, fake 77.715 ) gen_loss 591.45\n",
            "iteration 2899, epoch 6, batch 294/481,disc_loss 80.233, (real 81.737, fake 78.729 ) gen_loss 567.03\n",
            "iteration 2900, epoch 6, batch 295/481,disc_loss 82.486, (real 85.359, fake 79.613 ) gen_loss 587.13\n",
            "iteration 2901, epoch 6, batch 296/481,disc_loss 81.374, (real 84.625, fake 78.123 ) gen_loss 526.67\n",
            "iteration 2902, epoch 6, batch 297/481,disc_loss 81.298, (real 83.702, fake 78.894 ) gen_loss 598.3\n",
            "iteration 2903, epoch 6, batch 298/481,disc_loss 78.425, (real 81.365, fake 75.485 ) gen_loss 558.73\n",
            "iteration 2904, epoch 6, batch 299/481,disc_loss 82.086, (real 84.455, fake 79.717 ) gen_loss 552.27\n",
            "iteration 2905, epoch 6, batch 300/481,disc_loss 80.277, (real 83.666, fake 76.889 ) gen_loss 597.24\n",
            "iteration 2906, epoch 6, batch 301/481,disc_loss 79.543, (real 82.528, fake 76.557 ) gen_loss 604.8\n",
            "iteration 2907, epoch 6, batch 302/481,disc_loss 76.147, (real 78.692, fake 73.601 ) gen_loss 601.18\n",
            "iteration 2908, epoch 6, batch 303/481,disc_loss 78.631, (real 81.466, fake 75.796 ) gen_loss 595.42\n",
            "iteration 2909, epoch 6, batch 304/481,disc_loss 74.782, (real 78.242, fake 71.322 ) gen_loss 650.63\n",
            "iteration 2910, epoch 6, batch 305/481,disc_loss 79.839, (real 83.283, fake 76.395 ) gen_loss 661.15\n",
            "iteration 2911, epoch 6, batch 306/481,disc_loss 81.446, (real 85.094, fake 77.797 ) gen_loss 599.47\n",
            "iteration 2912, epoch 6, batch 307/481,disc_loss 78.888, (real 81.51, fake 76.267 ) gen_loss 590.26\n",
            "iteration 2913, epoch 6, batch 308/481,disc_loss 77.407, (real 79.637, fake 75.176 ) gen_loss 637.4\n",
            "iteration 2914, epoch 6, batch 309/481,disc_loss 76.465, (real 79.758, fake 73.172 ) gen_loss 653.93\n",
            "iteration 2915, epoch 6, batch 310/481,disc_loss 79.2, (real 82.049, fake 76.35 ) gen_loss 601.26\n",
            "iteration 2916, epoch 6, batch 311/481,disc_loss 75.039, (real 78.758, fake 71.32 ) gen_loss 648.64\n",
            "iteration 2917, epoch 6, batch 312/481,disc_loss 79.887, (real 82.213, fake 77.56 ) gen_loss 619.82\n",
            "iteration 2918, epoch 6, batch 313/481,disc_loss 80.008, (real 83.499, fake 76.517 ) gen_loss 583.35\n",
            "iteration 2919, epoch 6, batch 314/481,disc_loss 75.306, (real 77.486, fake 73.126 ) gen_loss 585.27\n",
            "iteration 2920, epoch 6, batch 315/481,disc_loss 81.648, (real 83.495, fake 79.8 ) gen_loss 576.68\n",
            "iteration 2921, epoch 6, batch 316/481,disc_loss 80.581, (real 83.514, fake 77.649 ) gen_loss 621.07\n",
            "iteration 2922, epoch 6, batch 317/481,disc_loss 81.337, (real 85.79, fake 76.884 ) gen_loss 536.87\n",
            "iteration 2923, epoch 6, batch 318/481,disc_loss 82.196, (real 85.555, fake 78.838 ) gen_loss 557.91\n",
            "iteration 2924, epoch 6, batch 319/481,disc_loss 79.938, (real 82.952, fake 76.925 ) gen_loss 593.02\n",
            "iteration 2925, epoch 6, batch 320/481,disc_loss 76.887, (real 80.21, fake 73.564 ) gen_loss 658.31\n",
            "iteration 2926, epoch 6, batch 321/481,disc_loss 75.609, (real 78.368, fake 72.85 ) gen_loss 588.41\n",
            "iteration 2927, epoch 6, batch 322/481,disc_loss 77.708, (real 80.894, fake 74.523 ) gen_loss 575.52\n",
            "iteration 2928, epoch 6, batch 323/481,disc_loss 79.317, (real 82.521, fake 76.114 ) gen_loss 624.66\n",
            "iteration 2929, epoch 6, batch 324/481,disc_loss 79.875, (real 82.966, fake 76.784 ) gen_loss 604.11\n",
            "iteration 2930, epoch 6, batch 325/481,disc_loss 84.043, (real 86.799, fake 81.288 ) gen_loss 637.04\n",
            "iteration 2931, epoch 6, batch 326/481,disc_loss 81.416, (real 85.244, fake 77.589 ) gen_loss 605.45\n",
            "iteration 2932, epoch 6, batch 327/481,disc_loss 79.692, (real 82.004, fake 77.38 ) gen_loss 560.64\n",
            "iteration 2933, epoch 6, batch 328/481,disc_loss 79.89, (real 82.851, fake 76.929 ) gen_loss 529.46\n",
            "iteration 2934, epoch 6, batch 329/481,disc_loss 81.722, (real 85.397, fake 78.047 ) gen_loss 551.05\n",
            "iteration 2935, epoch 6, batch 330/481,disc_loss 80.087, (real 83.444, fake 76.731 ) gen_loss 581.66\n",
            "iteration 2936, epoch 6, batch 331/481,disc_loss 77.995, (real 80.895, fake 75.095 ) gen_loss 559.45\n",
            "iteration 2937, epoch 6, batch 332/481,disc_loss 78.5, (real 81.4, fake 75.6 ) gen_loss 551.12\n",
            "iteration 2938, epoch 6, batch 333/481,disc_loss 74.203, (real 76.422, fake 71.984 ) gen_loss 534.58\n",
            "iteration 2939, epoch 6, batch 334/481,disc_loss 80.748, (real 84.983, fake 76.514 ) gen_loss 588.09\n",
            "iteration 2940, epoch 6, batch 335/481,disc_loss 75.325, (real 79.516, fake 71.134 ) gen_loss 579.94\n",
            "iteration 2941, epoch 6, batch 336/481,disc_loss 80.089, (real 83.253, fake 76.926 ) gen_loss 620.91\n",
            "iteration 2942, epoch 6, batch 337/481,disc_loss 78.056, (real 80.922, fake 75.189 ) gen_loss 688.83\n",
            "iteration 2943, epoch 6, batch 338/481,disc_loss 79.363, (real 82.393, fake 76.333 ) gen_loss 642.78\n",
            "iteration 2944, epoch 6, batch 339/481,disc_loss 81.949, (real 84.767, fake 79.131 ) gen_loss 637.75\n",
            "iteration 2945, epoch 6, batch 340/481,disc_loss 77.046, (real 80.216, fake 73.876 ) gen_loss 705.4\n",
            "iteration 2946, epoch 6, batch 341/481,disc_loss 74.426, (real 78.412, fake 70.44 ) gen_loss 550.01\n",
            "iteration 2947, epoch 6, batch 342/481,disc_loss 79.425, (real 82.213, fake 76.638 ) gen_loss 590.58\n",
            "iteration 2948, epoch 6, batch 343/481,disc_loss 77.593, (real 79.852, fake 75.334 ) gen_loss 617.71\n",
            "iteration 2949, epoch 6, batch 344/481,disc_loss 77.604, (real 80.83, fake 74.377 ) gen_loss 582.78\n",
            "iteration 2950, epoch 6, batch 345/481,disc_loss 77.537, (real 80.325, fake 74.749 ) gen_loss 600.16\n",
            "iteration 2951, epoch 6, batch 346/481,disc_loss 79.028, (real 81.47, fake 76.586 ) gen_loss 570.0\n",
            "iteration 2952, epoch 6, batch 347/481,disc_loss 74.475, (real 78.284, fake 70.667 ) gen_loss 610.19\n",
            "iteration 2953, epoch 6, batch 348/481,disc_loss 82.084, (real 84.603, fake 79.565 ) gen_loss 592.7\n",
            "iteration 2954, epoch 6, batch 349/481,disc_loss 79.121, (real 81.781, fake 76.461 ) gen_loss 574.98\n",
            "iteration 2955, epoch 6, batch 350/481,disc_loss 75.802, (real 78.335, fake 73.269 ) gen_loss 588.18\n",
            "iteration 2956, epoch 6, batch 351/481,disc_loss 79.758, (real 82.443, fake 77.072 ) gen_loss 676.12\n",
            "iteration 2957, epoch 6, batch 352/481,disc_loss 83.542, (real 86.956, fake 80.128 ) gen_loss 591.67\n",
            "iteration 2958, epoch 6, batch 353/481,disc_loss 76.49, (real 80.437, fake 72.544 ) gen_loss 606.79\n",
            "iteration 2959, epoch 6, batch 354/481,disc_loss 81.117, (real 85.521, fake 76.712 ) gen_loss 573.25\n",
            "iteration 2960, epoch 6, batch 355/481,disc_loss 78.679, (real 81.267, fake 76.091 ) gen_loss 576.8\n",
            "iteration 2961, epoch 6, batch 356/481,disc_loss 78.498, (real 82.708, fake 74.288 ) gen_loss 575.98\n",
            "iteration 2962, epoch 6, batch 357/481,disc_loss 78.036, (real 81.571, fake 74.501 ) gen_loss 544.42\n",
            "iteration 2963, epoch 6, batch 358/481,disc_loss 76.065, (real 79.192, fake 72.938 ) gen_loss 651.81\n",
            "iteration 2964, epoch 6, batch 359/481,disc_loss 79.851, (real 83.561, fake 76.141 ) gen_loss 628.32\n",
            "iteration 2965, epoch 6, batch 360/481,disc_loss 81.852, (real 84.895, fake 78.809 ) gen_loss 663.63\n",
            "iteration 2966, epoch 6, batch 361/481,disc_loss 77.51, (real 79.68, fake 75.34 ) gen_loss 630.14\n",
            "iteration 2967, epoch 6, batch 362/481,disc_loss 76.929, (real 79.527, fake 74.33 ) gen_loss 630.65\n",
            "iteration 2968, epoch 6, batch 363/481,disc_loss 82.237, (real 85.681, fake 78.793 ) gen_loss 634.28\n",
            "iteration 2969, epoch 6, batch 364/481,disc_loss 81.733, (real 85.186, fake 78.279 ) gen_loss 580.61\n",
            "iteration 2970, epoch 6, batch 365/481,disc_loss 77.573, (real 80.579, fake 74.568 ) gen_loss 616.15\n",
            "iteration 2971, epoch 6, batch 366/481,disc_loss 81.967, (real 84.535, fake 79.4 ) gen_loss 591.55\n",
            "iteration 2972, epoch 6, batch 367/481,disc_loss 77.527, (real 80.135, fake 74.919 ) gen_loss 625.51\n",
            "iteration 2973, epoch 6, batch 368/481,disc_loss 83.694, (real 87.676, fake 79.713 ) gen_loss 627.63\n",
            "iteration 2974, epoch 6, batch 369/481,disc_loss 78.935, (real 81.302, fake 76.567 ) gen_loss 665.73\n",
            "iteration 2975, epoch 6, batch 370/481,disc_loss 76.144, (real 77.858, fake 74.43 ) gen_loss 585.15\n",
            "iteration 2976, epoch 6, batch 371/481,disc_loss 79.685, (real 83.215, fake 76.155 ) gen_loss 643.51\n",
            "iteration 2977, epoch 6, batch 372/481,disc_loss 81.24, (real 83.587, fake 78.894 ) gen_loss 562.47\n",
            "iteration 2978, epoch 6, batch 373/481,disc_loss 80.503, (real 82.079, fake 78.927 ) gen_loss 545.3\n",
            "iteration 2979, epoch 6, batch 374/481,disc_loss 80.4, (real 81.587, fake 79.213 ) gen_loss 582.44\n",
            "iteration 2980, epoch 6, batch 375/481,disc_loss 80.476, (real 83.388, fake 77.563 ) gen_loss 580.82\n",
            "iteration 2981, epoch 6, batch 376/481,disc_loss 75.727, (real 78.853, fake 72.601 ) gen_loss 630.22\n",
            "iteration 2982, epoch 6, batch 377/481,disc_loss 75.807, (real 78.622, fake 72.992 ) gen_loss 596.97\n",
            "iteration 2983, epoch 6, batch 378/481,disc_loss 74.638, (real 77.347, fake 71.93 ) gen_loss 581.62\n",
            "iteration 2984, epoch 6, batch 379/481,disc_loss 83.007, (real 85.845, fake 80.168 ) gen_loss 653.4\n",
            "iteration 2985, epoch 6, batch 380/481,disc_loss 81.222, (real 83.014, fake 79.43 ) gen_loss 585.35\n",
            "iteration 2986, epoch 6, batch 381/481,disc_loss 77.451, (real 78.888, fake 76.015 ) gen_loss 565.24\n",
            "iteration 2987, epoch 6, batch 382/481,disc_loss 77.506, (real 80.692, fake 74.32 ) gen_loss 639.68\n",
            "iteration 2988, epoch 6, batch 383/481,disc_loss 83.646, (real 86.739, fake 80.553 ) gen_loss 604.64\n",
            "iteration 2989, epoch 6, batch 384/481,disc_loss 76.914, (real 80.615, fake 73.212 ) gen_loss 554.05\n",
            "iteration 2990, epoch 6, batch 385/481,disc_loss 77.108, (real 79.729, fake 74.487 ) gen_loss 548.93\n",
            "iteration 2991, epoch 6, batch 386/481,disc_loss 80.758, (real 86.023, fake 75.492 ) gen_loss 565.41\n",
            "iteration 2992, epoch 6, batch 387/481,disc_loss 80.705, (real 83.645, fake 77.765 ) gen_loss 563.05\n",
            "iteration 2993, epoch 6, batch 388/481,disc_loss 79.333, (real 80.792, fake 77.874 ) gen_loss 552.23\n",
            "iteration 2994, epoch 6, batch 389/481,disc_loss 77.103, (real 79.767, fake 74.439 ) gen_loss 589.91\n",
            "iteration 2995, epoch 6, batch 390/481,disc_loss 81.334, (real 85.001, fake 77.666 ) gen_loss 548.49\n",
            "iteration 2996, epoch 6, batch 391/481,disc_loss 80.929, (real 83.524, fake 78.335 ) gen_loss 622.05\n",
            "iteration 2997, epoch 6, batch 392/481,disc_loss 80.498, (real 83.749, fake 77.246 ) gen_loss 599.12\n",
            "iteration 2998, epoch 6, batch 393/481,disc_loss 77.761, (real 80.178, fake 75.343 ) gen_loss 544.48\n",
            "iteration 2999, epoch 6, batch 394/481,disc_loss 82.325, (real 85.726, fake 78.924 ) gen_loss 606.55\n",
            "iteration 3000, epoch 6, batch 395/481,disc_loss 80.255, (real 83.301, fake 77.209 ) gen_loss 537.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 3001, epoch 6, batch 396/481,disc_loss 77.056, (real 79.902, fake 74.21 ) gen_loss 573.63\n",
            "iteration 3002, epoch 6, batch 397/481,disc_loss 77.476, (real 80.145, fake 74.807 ) gen_loss 597.16\n",
            "iteration 3003, epoch 6, batch 398/481,disc_loss 77.046, (real 79.463, fake 74.629 ) gen_loss 554.91\n",
            "iteration 3004, epoch 6, batch 399/481,disc_loss 78.807, (real 80.825, fake 76.789 ) gen_loss 598.45\n",
            "iteration 3005, epoch 6, batch 400/481,disc_loss 78.407, (real 81.112, fake 75.702 ) gen_loss 558.49\n",
            "iteration 3006, epoch 6, batch 401/481,disc_loss 77.682, (real 80.639, fake 74.724 ) gen_loss 622.95\n",
            "iteration 3007, epoch 6, batch 402/481,disc_loss 85.403, (real 88.588, fake 82.218 ) gen_loss 630.66\n",
            "iteration 3008, epoch 6, batch 403/481,disc_loss 82.718, (real 87.023, fake 78.414 ) gen_loss 627.69\n",
            "iteration 3009, epoch 6, batch 404/481,disc_loss 81.558, (real 84.369, fake 78.747 ) gen_loss 584.81\n",
            "iteration 3010, epoch 6, batch 405/481,disc_loss 78.674, (real 80.78, fake 76.568 ) gen_loss 541.58\n",
            "iteration 3011, epoch 6, batch 406/481,disc_loss 77.851, (real 80.698, fake 75.005 ) gen_loss 556.01\n",
            "iteration 3012, epoch 6, batch 407/481,disc_loss 77.249, (real 80.423, fake 74.076 ) gen_loss 564.12\n",
            "iteration 3013, epoch 6, batch 408/481,disc_loss 75.191, (real 78.796, fake 71.587 ) gen_loss 639.89\n",
            "iteration 3014, epoch 6, batch 409/481,disc_loss 77.976, (real 80.535, fake 75.417 ) gen_loss 614.56\n",
            "iteration 3015, epoch 6, batch 410/481,disc_loss 80.295, (real 83.37, fake 77.221 ) gen_loss 650.25\n",
            "iteration 3016, epoch 6, batch 411/481,disc_loss 83.905, (real 86.82, fake 80.989 ) gen_loss 602.69\n",
            "iteration 3017, epoch 6, batch 412/481,disc_loss 78.07, (real 80.502, fake 75.637 ) gen_loss 669.59\n",
            "iteration 3018, epoch 6, batch 413/481,disc_loss 74.316, (real 76.557, fake 72.076 ) gen_loss 603.12\n",
            "iteration 3019, epoch 6, batch 414/481,disc_loss 81.006, (real 83.684, fake 78.329 ) gen_loss 581.52\n",
            "iteration 3020, epoch 6, batch 415/481,disc_loss 77.809, (real 79.97, fake 75.648 ) gen_loss 650.93\n",
            "iteration 3021, epoch 6, batch 416/481,disc_loss 74.156, (real 77.252, fake 71.06 ) gen_loss 614.52\n",
            "iteration 3022, epoch 6, batch 417/481,disc_loss 75.981, (real 79.823, fake 72.14 ) gen_loss 596.44\n",
            "iteration 3023, epoch 6, batch 418/481,disc_loss 77.911, (real 80.965, fake 74.858 ) gen_loss 565.59\n",
            "iteration 3024, epoch 6, batch 419/481,disc_loss 78.529, (real 81.569, fake 75.49 ) gen_loss 652.81\n",
            "iteration 3025, epoch 6, batch 420/481,disc_loss 75.757, (real 78.968, fake 72.546 ) gen_loss 617.44\n",
            "iteration 3026, epoch 6, batch 421/481,disc_loss 79.609, (real 82.709, fake 76.51 ) gen_loss 628.58\n",
            "iteration 3027, epoch 6, batch 422/481,disc_loss 78.718, (real 81.935, fake 75.501 ) gen_loss 599.41\n",
            "iteration 3028, epoch 6, batch 423/481,disc_loss 80.897, (real 84.142, fake 77.653 ) gen_loss 641.17\n",
            "iteration 3029, epoch 6, batch 424/481,disc_loss 81.149, (real 85.689, fake 76.608 ) gen_loss 704.61\n",
            "iteration 3030, epoch 6, batch 425/481,disc_loss 74.4, (real 77.009, fake 71.791 ) gen_loss 651.49\n",
            "iteration 3031, epoch 6, batch 426/481,disc_loss 77.742, (real 80.957, fake 74.526 ) gen_loss 701.3\n",
            "iteration 3032, epoch 6, batch 427/481,disc_loss 81.179, (real 84.015, fake 78.344 ) gen_loss 506.42\n",
            "iteration 3033, epoch 6, batch 428/481,disc_loss 84.61, (real 87.926, fake 81.294 ) gen_loss 556.25\n",
            "iteration 3034, epoch 6, batch 429/481,disc_loss 80.7, (real 85.134, fake 76.266 ) gen_loss 536.57\n",
            "iteration 3035, epoch 6, batch 430/481,disc_loss 76.485, (real 80.009, fake 72.962 ) gen_loss 618.04\n",
            "iteration 3036, epoch 6, batch 431/481,disc_loss 77.864, (real 82.088, fake 73.639 ) gen_loss 555.37\n",
            "iteration 3037, epoch 6, batch 432/481,disc_loss 78.581, (real 80.975, fake 76.188 ) gen_loss 587.2\n",
            "iteration 3038, epoch 6, batch 433/481,disc_loss 84.617, (real 88.054, fake 81.179 ) gen_loss 666.06\n",
            "iteration 3039, epoch 6, batch 434/481,disc_loss 77.434, (real 80.295, fake 74.574 ) gen_loss 584.65\n",
            "iteration 3040, epoch 6, batch 435/481,disc_loss 76.099, (real 79.305, fake 72.893 ) gen_loss 744.29\n",
            "iteration 3041, epoch 6, batch 436/481,disc_loss 78.301, (real 81.52, fake 75.081 ) gen_loss 648.67\n",
            "iteration 3042, epoch 6, batch 437/481,disc_loss 77.918, (real 81.495, fake 74.341 ) gen_loss 748.92\n",
            "iteration 3043, epoch 6, batch 438/481,disc_loss 79.893, (real 82.323, fake 77.463 ) gen_loss 609.08\n",
            "iteration 3044, epoch 6, batch 439/481,disc_loss 78.432, (real 81.641, fake 75.223 ) gen_loss 590.6\n",
            "iteration 3045, epoch 6, batch 440/481,disc_loss 78.834, (real 82.225, fake 75.442 ) gen_loss 572.65\n",
            "iteration 3046, epoch 6, batch 441/481,disc_loss 80.934, (real 83.375, fake 78.492 ) gen_loss 642.43\n",
            "iteration 3047, epoch 6, batch 442/481,disc_loss 77.363, (real 79.297, fake 75.43 ) gen_loss 624.4\n",
            "iteration 3048, epoch 6, batch 443/481,disc_loss 74.646, (real 77.484, fake 71.807 ) gen_loss 558.56\n",
            "iteration 3049, epoch 6, batch 444/481,disc_loss 80.826, (real 84.526, fake 77.127 ) gen_loss 627.6\n",
            "iteration 3050, epoch 6, batch 445/481,disc_loss 81.697, (real 85.379, fake 78.016 ) gen_loss 610.67\n",
            "iteration 3051, epoch 6, batch 446/481,disc_loss 78.876, (real 82.367, fake 75.386 ) gen_loss 573.66\n",
            "iteration 3052, epoch 6, batch 447/481,disc_loss 78.772, (real 80.529, fake 77.014 ) gen_loss 625.68\n",
            "iteration 3053, epoch 6, batch 448/481,disc_loss 79.241, (real 82.11, fake 76.372 ) gen_loss 740.94\n",
            "iteration 3054, epoch 6, batch 449/481,disc_loss 83.636, (real 86.95, fake 80.322 ) gen_loss 641.79\n",
            "iteration 3055, epoch 6, batch 450/481,disc_loss 79.773, (real 83.48, fake 76.066 ) gen_loss 629.19\n",
            "iteration 3056, epoch 6, batch 451/481,disc_loss 78.215, (real 80.567, fake 75.863 ) gen_loss 605.24\n",
            "iteration 3057, epoch 6, batch 452/481,disc_loss 82.329, (real 85.314, fake 79.343 ) gen_loss 629.4\n",
            "iteration 3058, epoch 6, batch 453/481,disc_loss 78.773, (real 81.621, fake 75.926 ) gen_loss 570.04\n",
            "iteration 3059, epoch 6, batch 454/481,disc_loss 79.967, (real 83.877, fake 76.057 ) gen_loss 632.33\n",
            "iteration 3060, epoch 6, batch 455/481,disc_loss 75.011, (real 78.564, fake 71.458 ) gen_loss 603.15\n",
            "iteration 3061, epoch 6, batch 456/481,disc_loss 75.852, (real 79.313, fake 72.392 ) gen_loss 688.9\n",
            "iteration 3062, epoch 6, batch 457/481,disc_loss 78.044, (real 81.017, fake 75.071 ) gen_loss 644.4\n",
            "iteration 3063, epoch 6, batch 458/481,disc_loss 84.713, (real 85.053, fake 84.374 ) gen_loss 677.53\n",
            "iteration 3064, epoch 6, batch 459/481,disc_loss 79.981, (real 81.067, fake 78.895 ) gen_loss 658.99\n",
            "iteration 3065, epoch 6, batch 460/481,disc_loss 80.766, (real 82.829, fake 78.703 ) gen_loss 661.27\n",
            "iteration 3066, epoch 6, batch 461/481,disc_loss 78.531, (real 81.615, fake 75.447 ) gen_loss 620.23\n",
            "iteration 3067, epoch 6, batch 462/481,disc_loss 82.265, (real 85.142, fake 79.388 ) gen_loss 618.87\n",
            "iteration 3068, epoch 6, batch 463/481,disc_loss 77.457, (real 80.534, fake 74.379 ) gen_loss 642.13\n",
            "iteration 3069, epoch 6, batch 464/481,disc_loss 78.779, (real 80.938, fake 76.621 ) gen_loss 616.17\n",
            "iteration 3070, epoch 6, batch 465/481,disc_loss 81.363, (real 83.976, fake 78.751 ) gen_loss 599.87\n",
            "iteration 3071, epoch 6, batch 466/481,disc_loss 79.068, (real 81.326, fake 76.809 ) gen_loss 624.57\n",
            "iteration 3072, epoch 6, batch 467/481,disc_loss 82.865, (real 86.003, fake 79.728 ) gen_loss 605.78\n",
            "iteration 3073, epoch 6, batch 468/481,disc_loss 78.704, (real 81.852, fake 75.556 ) gen_loss 576.81\n",
            "iteration 3074, epoch 6, batch 469/481,disc_loss 78.961, (real 82.564, fake 75.357 ) gen_loss 559.75\n",
            "iteration 3075, epoch 6, batch 470/481,disc_loss 78.004, (real 81.248, fake 74.76 ) gen_loss 606.48\n",
            "iteration 3076, epoch 6, batch 471/481,disc_loss 77.233, (real 79.869, fake 74.598 ) gen_loss 580.41\n",
            "iteration 3077, epoch 6, batch 472/481,disc_loss 76.45, (real 79.955, fake 72.945 ) gen_loss 648.4\n",
            "iteration 3078, epoch 6, batch 473/481,disc_loss 76.871, (real 79.7, fake 74.042 ) gen_loss 581.47\n",
            "iteration 3079, epoch 6, batch 474/481,disc_loss 82.166, (real 85.541, fake 78.79 ) gen_loss 623.88\n",
            "iteration 3080, epoch 6, batch 475/481,disc_loss 76.578, (real 79.003, fake 74.152 ) gen_loss 657.02\n",
            "iteration 3081, epoch 6, batch 476/481,disc_loss 82.055, (real 85.287, fake 78.822 ) gen_loss 625.51\n",
            "iteration 3082, epoch 6, batch 477/481,disc_loss 80.173, (real 81.853, fake 78.493 ) gen_loss 636.45\n",
            "iteration 3083, epoch 6, batch 478/481,disc_loss 80.925, (real 83.86, fake 77.989 ) gen_loss 634.61\n",
            "iteration 3084, epoch 6, batch 479/481,disc_loss 75.284, (real 77.51, fake 73.058 ) gen_loss 578.67\n",
            "iteration 3085, epoch 6, batch 480/481,disc_loss 78.261, (real 81.314, fake 75.209 ) gen_loss 614.86\n",
            "iteration 3086, epoch 6, batch 481/481,disc_loss 73.44, (real 75.871, fake 71.01 ) gen_loss 596.67\n",
            "iteration 3087, epoch 7, batch 1/481,disc_loss 81.637, (real 85.419, fake 77.854 ) gen_loss 610.5\n",
            "iteration 3088, epoch 7, batch 2/481,disc_loss 80.242, (real 83.37, fake 77.113 ) gen_loss 620.17\n",
            "iteration 3089, epoch 7, batch 3/481,disc_loss 71.988, (real 74.387, fake 69.59 ) gen_loss 594.01\n",
            "iteration 3090, epoch 7, batch 4/481,disc_loss 79.933, (real 82.235, fake 77.63 ) gen_loss 566.02\n",
            "iteration 3091, epoch 7, batch 5/481,disc_loss 76.238, (real 79.472, fake 73.005 ) gen_loss 568.94\n",
            "iteration 3092, epoch 7, batch 6/481,disc_loss 80.555, (real 83.714, fake 77.397 ) gen_loss 567.43\n",
            "iteration 3093, epoch 7, batch 7/481,disc_loss 78.194, (real 80.559, fake 75.829 ) gen_loss 606.85\n",
            "iteration 3094, epoch 7, batch 8/481,disc_loss 77.195, (real 80.352, fake 74.039 ) gen_loss 700.66\n",
            "iteration 3095, epoch 7, batch 9/481,disc_loss 76.744, (real 79.936, fake 73.552 ) gen_loss 627.45\n",
            "iteration 3096, epoch 7, batch 10/481,disc_loss 78.96, (real 81.436, fake 76.483 ) gen_loss 596.55\n",
            "iteration 3097, epoch 7, batch 11/481,disc_loss 78.004, (real 80.227, fake 75.782 ) gen_loss 656.02\n",
            "iteration 3098, epoch 7, batch 12/481,disc_loss 74.399, (real 77.343, fake 71.454 ) gen_loss 630.65\n",
            "iteration 3099, epoch 7, batch 13/481,disc_loss 77.37, (real 81.326, fake 73.413 ) gen_loss 642.13\n",
            "iteration 3100, epoch 7, batch 14/481,disc_loss 82.184, (real 84.944, fake 79.425 ) gen_loss 572.64\n",
            "iteration 3101, epoch 7, batch 15/481,disc_loss 76.733, (real 78.793, fake 74.673 ) gen_loss 611.33\n",
            "iteration 3102, epoch 7, batch 16/481,disc_loss 82.108, (real 84.667, fake 79.549 ) gen_loss 641.6\n",
            "iteration 3103, epoch 7, batch 17/481,disc_loss 80.096, (real 83.401, fake 76.791 ) gen_loss 623.93\n",
            "iteration 3104, epoch 7, batch 18/481,disc_loss 75.991, (real 78.846, fake 73.137 ) gen_loss 643.42\n",
            "iteration 3105, epoch 7, batch 19/481,disc_loss 82.892, (real 85.587, fake 80.196 ) gen_loss 595.19\n",
            "iteration 3106, epoch 7, batch 20/481,disc_loss 76.63, (real 79.2, fake 74.06 ) gen_loss 584.38\n",
            "iteration 3107, epoch 7, batch 21/481,disc_loss 78.493, (real 81.452, fake 75.534 ) gen_loss 578.96\n",
            "iteration 3108, epoch 7, batch 22/481,disc_loss 80.506, (real 84.139, fake 76.873 ) gen_loss 612.69\n",
            "iteration 3109, epoch 7, batch 23/481,disc_loss 78.804, (real 81.403, fake 76.206 ) gen_loss 612.72\n",
            "iteration 3110, epoch 7, batch 24/481,disc_loss 78.175, (real 80.307, fake 76.043 ) gen_loss 599.64\n",
            "iteration 3111, epoch 7, batch 25/481,disc_loss 77.083, (real 80.034, fake 74.132 ) gen_loss 550.86\n",
            "iteration 3112, epoch 7, batch 26/481,disc_loss 76.141, (real 78.511, fake 73.77 ) gen_loss 611.45\n",
            "iteration 3113, epoch 7, batch 27/481,disc_loss 80.936, (real 83.48, fake 78.392 ) gen_loss 572.67\n",
            "iteration 3114, epoch 7, batch 28/481,disc_loss 79.84, (real 81.789, fake 77.891 ) gen_loss 535.52\n",
            "iteration 3115, epoch 7, batch 29/481,disc_loss 80.133, (real 82.457, fake 77.81 ) gen_loss 577.66\n",
            "iteration 3116, epoch 7, batch 30/481,disc_loss 77.533, (real 80.294, fake 74.773 ) gen_loss 634.46\n",
            "iteration 3117, epoch 7, batch 31/481,disc_loss 79.64, (real 82.334, fake 76.947 ) gen_loss 609.53\n",
            "iteration 3118, epoch 7, batch 32/481,disc_loss 79.062, (real 81.799, fake 76.324 ) gen_loss 597.11\n",
            "iteration 3119, epoch 7, batch 33/481,disc_loss 81.769, (real 85.384, fake 78.153 ) gen_loss 614.23\n",
            "iteration 3120, epoch 7, batch 34/481,disc_loss 78.581, (real 81.252, fake 75.91 ) gen_loss 586.25\n",
            "iteration 3121, epoch 7, batch 35/481,disc_loss 77.427, (real 80.736, fake 74.118 ) gen_loss 593.9\n",
            "iteration 3122, epoch 7, batch 36/481,disc_loss 83.432, (real 87.276, fake 79.587 ) gen_loss 664.63\n",
            "iteration 3123, epoch 7, batch 37/481,disc_loss 83.857, (real 86.452, fake 81.262 ) gen_loss 614.08\n",
            "iteration 3124, epoch 7, batch 38/481,disc_loss 78.589, (real 80.659, fake 76.518 ) gen_loss 622.85\n",
            "iteration 3125, epoch 7, batch 39/481,disc_loss 77.29, (real 79.096, fake 75.485 ) gen_loss 614.54\n",
            "iteration 3126, epoch 7, batch 40/481,disc_loss 78.588, (real 80.861, fake 76.315 ) gen_loss 595.76\n",
            "iteration 3127, epoch 7, batch 41/481,disc_loss 79.288, (real 81.74, fake 76.835 ) gen_loss 616.69\n",
            "iteration 3128, epoch 7, batch 42/481,disc_loss 80.588, (real 83.189, fake 77.986 ) gen_loss 597.89\n",
            "iteration 3129, epoch 7, batch 43/481,disc_loss 80.276, (real 83.86, fake 76.693 ) gen_loss 583.88\n",
            "iteration 3130, epoch 7, batch 44/481,disc_loss 75.211, (real 77.33, fake 73.092 ) gen_loss 636.02\n",
            "iteration 3131, epoch 7, batch 45/481,disc_loss 77.538, (real 80.299, fake 74.776 ) gen_loss 588.79\n",
            "iteration 3132, epoch 7, batch 46/481,disc_loss 77.55, (real 79.331, fake 75.769 ) gen_loss 655.59\n",
            "iteration 3133, epoch 7, batch 47/481,disc_loss 81.567, (real 83.359, fake 79.776 ) gen_loss 712.78\n",
            "iteration 3134, epoch 7, batch 48/481,disc_loss 78.181, (real 81.763, fake 74.598 ) gen_loss 596.59\n",
            "iteration 3135, epoch 7, batch 49/481,disc_loss 78.728, (real 82.034, fake 75.423 ) gen_loss 624.35\n",
            "iteration 3136, epoch 7, batch 50/481,disc_loss 76.849, (real 80.109, fake 73.589 ) gen_loss 629.93\n",
            "iteration 3137, epoch 7, batch 51/481,disc_loss 80.14, (real 83.202, fake 77.079 ) gen_loss 599.18\n",
            "iteration 3138, epoch 7, batch 52/481,disc_loss 77.918, (real 81.844, fake 73.992 ) gen_loss 663.6\n",
            "iteration 3139, epoch 7, batch 53/481,disc_loss 78.832, (real 80.704, fake 76.959 ) gen_loss 673.69\n",
            "iteration 3140, epoch 7, batch 54/481,disc_loss 76.636, (real 79.01, fake 74.262 ) gen_loss 707.09\n",
            "iteration 3141, epoch 7, batch 55/481,disc_loss 75.704, (real 78.452, fake 72.956 ) gen_loss 642.12\n",
            "iteration 3142, epoch 7, batch 56/481,disc_loss 81.284, (real 83.858, fake 78.711 ) gen_loss 551.28\n",
            "iteration 3143, epoch 7, batch 57/481,disc_loss 78.168, (real 80.258, fake 76.077 ) gen_loss 551.92\n",
            "iteration 3144, epoch 7, batch 58/481,disc_loss 77.572, (real 79.769, fake 75.375 ) gen_loss 568.26\n",
            "iteration 3145, epoch 7, batch 59/481,disc_loss 78.177, (real 80.61, fake 75.744 ) gen_loss 648.73\n",
            "iteration 3146, epoch 7, batch 60/481,disc_loss 78.219, (real 80.646, fake 75.791 ) gen_loss 592.25\n",
            "iteration 3147, epoch 7, batch 61/481,disc_loss 79.408, (real 82.098, fake 76.717 ) gen_loss 587.61\n",
            "iteration 3148, epoch 7, batch 62/481,disc_loss 77.351, (real 81.14, fake 73.563 ) gen_loss 593.63\n",
            "iteration 3149, epoch 7, batch 63/481,disc_loss 79.127, (real 80.363, fake 77.89 ) gen_loss 586.78\n",
            "iteration 3150, epoch 7, batch 64/481,disc_loss 81.331, (real 84.147, fake 78.515 ) gen_loss 629.77\n",
            "iteration 3151, epoch 7, batch 65/481,disc_loss 78.276, (real 81.324, fake 75.228 ) gen_loss 619.77\n",
            "iteration 3152, epoch 7, batch 66/481,disc_loss 80.585, (real 83.152, fake 78.018 ) gen_loss 534.33\n",
            "iteration 3153, epoch 7, batch 67/481,disc_loss 75.837, (real 79.349, fake 72.326 ) gen_loss 562.86\n",
            "iteration 3154, epoch 7, batch 68/481,disc_loss 79.58, (real 81.713, fake 77.447 ) gen_loss 608.02\n",
            "iteration 3155, epoch 7, batch 69/481,disc_loss 78.335, (real 80.852, fake 75.819 ) gen_loss 600.19\n",
            "iteration 3156, epoch 7, batch 70/481,disc_loss 81.998, (real 85.018, fake 78.977 ) gen_loss 613.9\n",
            "iteration 3157, epoch 7, batch 71/481,disc_loss 79.393, (real 81.67, fake 77.116 ) gen_loss 588.43\n",
            "iteration 3158, epoch 7, batch 72/481,disc_loss 83.155, (real 85.724, fake 80.585 ) gen_loss 657.27\n",
            "iteration 3159, epoch 7, batch 73/481,disc_loss 77.657, (real 79.321, fake 75.992 ) gen_loss 592.4\n",
            "iteration 3160, epoch 7, batch 74/481,disc_loss 83.084, (real 85.665, fake 80.504 ) gen_loss 601.82\n",
            "iteration 3161, epoch 7, batch 75/481,disc_loss 74.825, (real 76.547, fake 73.104 ) gen_loss 593.5\n",
            "iteration 3162, epoch 7, batch 76/481,disc_loss 76.585, (real 79.844, fake 73.326 ) gen_loss 576.61\n",
            "iteration 3163, epoch 7, batch 77/481,disc_loss 82.147, (real 86.462, fake 77.833 ) gen_loss 594.98\n",
            "iteration 3164, epoch 7, batch 78/481,disc_loss 80.366, (real 83.668, fake 77.064 ) gen_loss 609.76\n",
            "iteration 3165, epoch 7, batch 79/481,disc_loss 79.161, (real 82.225, fake 76.097 ) gen_loss 628.02\n",
            "iteration 3166, epoch 7, batch 80/481,disc_loss 80.583, (real 84.217, fake 76.95 ) gen_loss 701.36\n",
            "iteration 3167, epoch 7, batch 81/481,disc_loss 77.525, (real 79.906, fake 75.144 ) gen_loss 582.41\n",
            "iteration 3168, epoch 7, batch 82/481,disc_loss 75.267, (real 78.964, fake 71.57 ) gen_loss 621.76\n",
            "iteration 3169, epoch 7, batch 83/481,disc_loss 81.054, (real 83.804, fake 78.304 ) gen_loss 610.89\n",
            "iteration 3170, epoch 7, batch 84/481,disc_loss 81.226, (real 84.244, fake 78.208 ) gen_loss 684.14\n",
            "iteration 3171, epoch 7, batch 85/481,disc_loss 76.373, (real 79.18, fake 73.567 ) gen_loss 606.29\n",
            "iteration 3172, epoch 7, batch 86/481,disc_loss 76.453, (real 78.955, fake 73.95 ) gen_loss 624.66\n",
            "iteration 3173, epoch 7, batch 87/481,disc_loss 76.815, (real 80.171, fake 73.459 ) gen_loss 606.7\n",
            "iteration 3174, epoch 7, batch 88/481,disc_loss 78.338, (real 81.491, fake 75.186 ) gen_loss 625.99\n",
            "iteration 3175, epoch 7, batch 89/481,disc_loss 78.007, (real 81.389, fake 74.626 ) gen_loss 574.78\n",
            "iteration 3176, epoch 7, batch 90/481,disc_loss 76.449, (real 79.551, fake 73.347 ) gen_loss 594.14\n",
            "iteration 3177, epoch 7, batch 91/481,disc_loss 80.29, (real 83.672, fake 76.907 ) gen_loss 620.48\n",
            "iteration 3178, epoch 7, batch 92/481,disc_loss 78.108, (real 81.47, fake 74.745 ) gen_loss 614.74\n",
            "iteration 3179, epoch 7, batch 93/481,disc_loss 82.442, (real 83.176, fake 81.708 ) gen_loss 622.85\n",
            "iteration 3180, epoch 7, batch 94/481,disc_loss 77.944, (real 79.462, fake 76.426 ) gen_loss 544.12\n",
            "iteration 3181, epoch 7, batch 95/481,disc_loss 74.128, (real 76.559, fake 71.698 ) gen_loss 577.84\n",
            "iteration 3182, epoch 7, batch 96/481,disc_loss 78.336, (real 80.871, fake 75.801 ) gen_loss 601.14\n",
            "iteration 3183, epoch 7, batch 97/481,disc_loss 77.561, (real 79.971, fake 75.15 ) gen_loss 572.37\n",
            "iteration 3184, epoch 7, batch 98/481,disc_loss 80.124, (real 83.534, fake 76.714 ) gen_loss 544.85\n",
            "iteration 3185, epoch 7, batch 99/481,disc_loss 75.264, (real 78.403, fake 72.125 ) gen_loss 561.55\n",
            "iteration 3186, epoch 7, batch 100/481,disc_loss 80.037, (real 82.36, fake 77.714 ) gen_loss 592.04\n",
            "iteration 3187, epoch 7, batch 101/481,disc_loss 76.542, (real 78.777, fake 74.307 ) gen_loss 548.05\n",
            "iteration 3188, epoch 7, batch 102/481,disc_loss 82.287, (real 84.377, fake 80.197 ) gen_loss 562.96\n",
            "iteration 3189, epoch 7, batch 103/481,disc_loss 75.257, (real 78.105, fake 72.408 ) gen_loss 530.46\n",
            "iteration 3190, epoch 7, batch 104/481,disc_loss 76.693, (real 80.513, fake 72.873 ) gen_loss 591.25\n",
            "iteration 3191, epoch 7, batch 105/481,disc_loss 80.621, (real 83.265, fake 77.978 ) gen_loss 534.28\n",
            "iteration 3192, epoch 7, batch 106/481,disc_loss 76.63, (real 79.764, fake 73.497 ) gen_loss 617.29\n",
            "iteration 3193, epoch 7, batch 107/481,disc_loss 79.978, (real 83.101, fake 76.856 ) gen_loss 570.48\n",
            "iteration 3194, epoch 7, batch 108/481,disc_loss 78.783, (real 81.912, fake 75.653 ) gen_loss 566.48\n",
            "iteration 3195, epoch 7, batch 109/481,disc_loss 78.996, (real 81.553, fake 76.439 ) gen_loss 606.96\n",
            "iteration 3196, epoch 7, batch 110/481,disc_loss 80.49, (real 83.233, fake 77.746 ) gen_loss 594.93\n",
            "iteration 3197, epoch 7, batch 111/481,disc_loss 78.65, (real 80.971, fake 76.328 ) gen_loss 634.49\n",
            "iteration 3198, epoch 7, batch 112/481,disc_loss 79.198, (real 82.578, fake 75.819 ) gen_loss 575.36\n",
            "iteration 3199, epoch 7, batch 113/481,disc_loss 78.507, (real 81.897, fake 75.117 ) gen_loss 576.58\n",
            "iteration 3200, epoch 7, batch 114/481,disc_loss 78.439, (real 82.147, fake 74.73 ) gen_loss 608.59\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 3201, epoch 7, batch 115/481,disc_loss 81.47, (real 85.644, fake 77.296 ) gen_loss 662.1\n",
            "iteration 3202, epoch 7, batch 116/481,disc_loss 79.513, (real 82.882, fake 76.145 ) gen_loss 640.71\n",
            "iteration 3203, epoch 7, batch 117/481,disc_loss 83.056, (real 84.836, fake 81.277 ) gen_loss 535.2\n",
            "iteration 3204, epoch 7, batch 118/481,disc_loss 80.35, (real 83.535, fake 77.164 ) gen_loss 540.25\n",
            "iteration 3205, epoch 7, batch 119/481,disc_loss 72.983, (real 75.097, fake 70.868 ) gen_loss 628.31\n",
            "iteration 3206, epoch 7, batch 120/481,disc_loss 73.61, (real 77.023, fake 70.197 ) gen_loss 705.58\n",
            "iteration 3207, epoch 7, batch 121/481,disc_loss 77.273, (real 80.688, fake 73.859 ) gen_loss 651.29\n",
            "iteration 3208, epoch 7, batch 122/481,disc_loss 75.829, (real 79.098, fake 72.56 ) gen_loss 675.36\n",
            "iteration 3209, epoch 7, batch 123/481,disc_loss 77.188, (real 79.241, fake 75.134 ) gen_loss 580.8\n",
            "iteration 3210, epoch 7, batch 124/481,disc_loss 78.241, (real 81.649, fake 74.834 ) gen_loss 665.74\n",
            "iteration 3211, epoch 7, batch 125/481,disc_loss 79.254, (real 82.508, fake 76.0 ) gen_loss 604.84\n",
            "iteration 3212, epoch 7, batch 126/481,disc_loss 80.373, (real 82.936, fake 77.81 ) gen_loss 581.12\n",
            "iteration 3213, epoch 7, batch 127/481,disc_loss 81.083, (real 83.846, fake 78.319 ) gen_loss 627.26\n",
            "iteration 3214, epoch 7, batch 128/481,disc_loss 78.356, (real 81.038, fake 75.675 ) gen_loss 593.43\n",
            "iteration 3215, epoch 7, batch 129/481,disc_loss 79.186, (real 81.926, fake 76.447 ) gen_loss 575.18\n",
            "iteration 3216, epoch 7, batch 130/481,disc_loss 80.186, (real 83.395, fake 76.976 ) gen_loss 592.04\n",
            "iteration 3217, epoch 7, batch 131/481,disc_loss 75.493, (real 77.295, fake 73.691 ) gen_loss 591.29\n",
            "iteration 3218, epoch 7, batch 132/481,disc_loss 82.139, (real 84.913, fake 79.365 ) gen_loss 619.26\n",
            "iteration 3219, epoch 7, batch 133/481,disc_loss 77.596, (real 80.371, fake 74.822 ) gen_loss 710.16\n",
            "iteration 3220, epoch 7, batch 134/481,disc_loss 76.631, (real 79.834, fake 73.427 ) gen_loss 725.61\n",
            "iteration 3221, epoch 7, batch 135/481,disc_loss 75.298, (real 78.589, fake 72.007 ) gen_loss 572.45\n",
            "iteration 3222, epoch 7, batch 136/481,disc_loss 81.45, (real 84.289, fake 78.612 ) gen_loss 582.29\n",
            "iteration 3223, epoch 7, batch 137/481,disc_loss 76.898, (real 78.416, fake 75.38 ) gen_loss 634.94\n",
            "iteration 3224, epoch 7, batch 138/481,disc_loss 76.023, (real 77.271, fake 74.775 ) gen_loss 551.4\n",
            "iteration 3225, epoch 7, batch 139/481,disc_loss 81.559, (real 84.407, fake 78.711 ) gen_loss 584.63\n",
            "iteration 3226, epoch 7, batch 140/481,disc_loss 77.906, (real 81.02, fake 74.791 ) gen_loss 524.22\n",
            "iteration 3227, epoch 7, batch 141/481,disc_loss 74.903, (real 77.709, fake 72.096 ) gen_loss 638.36\n",
            "iteration 3228, epoch 7, batch 142/481,disc_loss 79.352, (real 82.745, fake 75.958 ) gen_loss 532.46\n",
            "iteration 3229, epoch 7, batch 143/481,disc_loss 78.867, (real 80.647, fake 77.086 ) gen_loss 602.2\n",
            "iteration 3230, epoch 7, batch 144/481,disc_loss 79.246, (real 81.969, fake 76.522 ) gen_loss 557.03\n",
            "iteration 3231, epoch 7, batch 145/481,disc_loss 78.635, (real 81.983, fake 75.287 ) gen_loss 612.77\n",
            "iteration 3232, epoch 7, batch 146/481,disc_loss 79.774, (real 82.876, fake 76.673 ) gen_loss 577.21\n",
            "iteration 3233, epoch 7, batch 147/481,disc_loss 77.733, (real 80.223, fake 75.243 ) gen_loss 567.78\n",
            "iteration 3234, epoch 7, batch 148/481,disc_loss 79.122, (real 82.824, fake 75.419 ) gen_loss 585.84\n",
            "iteration 3235, epoch 7, batch 149/481,disc_loss 79.346, (real 81.92, fake 76.771 ) gen_loss 623.96\n",
            "iteration 3236, epoch 7, batch 150/481,disc_loss 78.143, (real 80.495, fake 75.79 ) gen_loss 593.22\n",
            "iteration 3237, epoch 7, batch 151/481,disc_loss 75.951, (real 78.66, fake 73.243 ) gen_loss 641.01\n",
            "iteration 3238, epoch 7, batch 152/481,disc_loss 77.483, (real 80.973, fake 73.994 ) gen_loss 663.77\n",
            "iteration 3239, epoch 7, batch 153/481,disc_loss 78.487, (real 81.264, fake 75.709 ) gen_loss 628.62\n",
            "iteration 3240, epoch 7, batch 154/481,disc_loss 81.528, (real 84.696, fake 78.36 ) gen_loss 595.67\n",
            "iteration 3241, epoch 7, batch 155/481,disc_loss 82.901, (real 86.506, fake 79.295 ) gen_loss 558.46\n",
            "iteration 3242, epoch 7, batch 156/481,disc_loss 76.576, (real 80.231, fake 72.92 ) gen_loss 549.73\n",
            "iteration 3243, epoch 7, batch 157/481,disc_loss 81.904, (real 84.495, fake 79.313 ) gen_loss 591.49\n",
            "iteration 3244, epoch 7, batch 158/481,disc_loss 82.77, (real 84.511, fake 81.029 ) gen_loss 542.77\n",
            "iteration 3245, epoch 7, batch 159/481,disc_loss 81.351, (real 83.416, fake 79.287 ) gen_loss 584.51\n",
            "iteration 3246, epoch 7, batch 160/481,disc_loss 75.677, (real 79.127, fake 72.228 ) gen_loss 550.36\n",
            "iteration 3247, epoch 7, batch 161/481,disc_loss 78.433, (real 81.627, fake 75.238 ) gen_loss 603.2\n",
            "iteration 3248, epoch 7, batch 162/481,disc_loss 84.463, (real 87.079, fake 81.847 ) gen_loss 664.47\n",
            "iteration 3249, epoch 7, batch 163/481,disc_loss 76.883, (real 79.828, fake 73.939 ) gen_loss 590.26\n",
            "iteration 3250, epoch 7, batch 164/481,disc_loss 78.339, (real 81.796, fake 74.882 ) gen_loss 583.99\n",
            "iteration 3251, epoch 7, batch 165/481,disc_loss 78.305, (real 80.868, fake 75.742 ) gen_loss 635.28\n",
            "iteration 3252, epoch 7, batch 166/481,disc_loss 76.992, (real 80.603, fake 73.382 ) gen_loss 710.04\n",
            "iteration 3253, epoch 7, batch 167/481,disc_loss 81.838, (real 83.349, fake 80.328 ) gen_loss 579.25\n",
            "iteration 3254, epoch 7, batch 168/481,disc_loss 81.507, (real 83.628, fake 79.386 ) gen_loss 683.0\n",
            "iteration 3255, epoch 7, batch 169/481,disc_loss 77.734, (real 80.142, fake 75.327 ) gen_loss 569.25\n",
            "iteration 3256, epoch 7, batch 170/481,disc_loss 75.113, (real 77.844, fake 72.383 ) gen_loss 656.57\n",
            "iteration 3257, epoch 7, batch 171/481,disc_loss 81.833, (real 85.539, fake 78.126 ) gen_loss 617.69\n",
            "iteration 3258, epoch 7, batch 172/481,disc_loss 78.483, (real 81.078, fake 75.888 ) gen_loss 575.9\n",
            "iteration 3259, epoch 7, batch 173/481,disc_loss 77.936, (real 80.09, fake 75.781 ) gen_loss 644.53\n",
            "iteration 3260, epoch 7, batch 174/481,disc_loss 81.449, (real 84.348, fake 78.55 ) gen_loss 576.05\n",
            "iteration 3261, epoch 7, batch 175/481,disc_loss 80.984, (real 83.572, fake 78.397 ) gen_loss 655.82\n",
            "iteration 3262, epoch 7, batch 176/481,disc_loss 78.464, (real 81.384, fake 75.545 ) gen_loss 610.48\n",
            "iteration 3263, epoch 7, batch 177/481,disc_loss 78.97, (real 79.707, fake 78.233 ) gen_loss 586.3\n",
            "iteration 3264, epoch 7, batch 178/481,disc_loss 83.751, (real 87.074, fake 80.427 ) gen_loss 576.98\n",
            "iteration 3265, epoch 7, batch 179/481,disc_loss 79.284, (real 81.593, fake 76.974 ) gen_loss 664.35\n",
            "iteration 3266, epoch 7, batch 180/481,disc_loss 84.211, (real 87.623, fake 80.799 ) gen_loss 710.89\n",
            "iteration 3267, epoch 7, batch 181/481,disc_loss 84.57, (real 87.823, fake 81.317 ) gen_loss 630.07\n",
            "iteration 3268, epoch 7, batch 182/481,disc_loss 81.538, (real 82.782, fake 80.294 ) gen_loss 590.4\n",
            "iteration 3269, epoch 7, batch 183/481,disc_loss 80.674, (real 83.291, fake 78.058 ) gen_loss 561.79\n",
            "iteration 3270, epoch 7, batch 184/481,disc_loss 79.588, (real 81.915, fake 77.261 ) gen_loss 596.23\n",
            "iteration 3271, epoch 7, batch 185/481,disc_loss 78.096, (real 81.169, fake 75.022 ) gen_loss 696.2\n",
            "iteration 3272, epoch 7, batch 186/481,disc_loss 77.04, (real 80.21, fake 73.869 ) gen_loss 555.51\n",
            "iteration 3273, epoch 7, batch 187/481,disc_loss 74.931, (real 77.357, fake 72.506 ) gen_loss 604.06\n",
            "iteration 3274, epoch 7, batch 188/481,disc_loss 75.551, (real 78.653, fake 72.449 ) gen_loss 613.02\n",
            "iteration 3275, epoch 7, batch 189/481,disc_loss 80.997, (real 84.489, fake 77.505 ) gen_loss 685.9\n",
            "iteration 3276, epoch 7, batch 190/481,disc_loss 78.646, (real 82.046, fake 75.246 ) gen_loss 658.75\n",
            "iteration 3277, epoch 7, batch 191/481,disc_loss 78.526, (real 80.78, fake 76.272 ) gen_loss 632.86\n",
            "iteration 3278, epoch 7, batch 192/481,disc_loss 79.418, (real 81.376, fake 77.46 ) gen_loss 750.31\n",
            "iteration 3279, epoch 7, batch 193/481,disc_loss 76.905, (real 78.598, fake 75.213 ) gen_loss 558.53\n",
            "iteration 3280, epoch 7, batch 194/481,disc_loss 80.31, (real 82.923, fake 77.698 ) gen_loss 597.66\n",
            "iteration 3281, epoch 7, batch 195/481,disc_loss 80.807, (real 82.724, fake 78.891 ) gen_loss 592.61\n",
            "iteration 3282, epoch 7, batch 196/481,disc_loss 80.183, (real 83.103, fake 77.264 ) gen_loss 597.58\n",
            "iteration 3283, epoch 7, batch 197/481,disc_loss 77.347, (real 80.295, fake 74.4 ) gen_loss 565.58\n",
            "iteration 3284, epoch 7, batch 198/481,disc_loss 79.498, (real 82.434, fake 76.561 ) gen_loss 628.79\n",
            "iteration 3285, epoch 7, batch 199/481,disc_loss 75.265, (real 77.497, fake 73.033 ) gen_loss 563.15\n",
            "iteration 3286, epoch 7, batch 200/481,disc_loss 82.894, (real 85.588, fake 80.199 ) gen_loss 548.63\n",
            "iteration 3287, epoch 7, batch 201/481,disc_loss 75.282, (real 78.025, fake 72.539 ) gen_loss 570.79\n",
            "iteration 3288, epoch 7, batch 202/481,disc_loss 79.925, (real 82.447, fake 77.404 ) gen_loss 595.17\n",
            "iteration 3289, epoch 7, batch 203/481,disc_loss 77.598, (real 79.89, fake 75.306 ) gen_loss 634.55\n",
            "iteration 3290, epoch 7, batch 204/481,disc_loss 77.212, (real 78.169, fake 76.255 ) gen_loss 658.0\n",
            "iteration 3291, epoch 7, batch 205/481,disc_loss 82.196, (real 83.02, fake 81.372 ) gen_loss 647.99\n",
            "iteration 3292, epoch 7, batch 206/481,disc_loss 82.399, (real 84.905, fake 79.893 ) gen_loss 553.46\n",
            "iteration 3293, epoch 7, batch 207/481,disc_loss 81.549, (real 84.763, fake 78.335 ) gen_loss 638.5\n",
            "iteration 3294, epoch 7, batch 208/481,disc_loss 74.057, (real 76.06, fake 72.054 ) gen_loss 595.28\n",
            "iteration 3295, epoch 7, batch 209/481,disc_loss 76.375, (real 78.639, fake 74.111 ) gen_loss 654.23\n",
            "iteration 3296, epoch 7, batch 210/481,disc_loss 80.627, (real 82.821, fake 78.434 ) gen_loss 618.59\n",
            "iteration 3297, epoch 7, batch 211/481,disc_loss 78.671, (real 82.027, fake 75.314 ) gen_loss 580.51\n",
            "iteration 3298, epoch 7, batch 212/481,disc_loss 81.871, (real 84.267, fake 79.475 ) gen_loss 580.7\n",
            "iteration 3299, epoch 7, batch 213/481,disc_loss 78.357, (real 81.408, fake 75.305 ) gen_loss 627.55\n",
            "iteration 3300, epoch 7, batch 214/481,disc_loss 79.12, (real 83.592, fake 74.647 ) gen_loss 598.02\n",
            "iteration 3301, epoch 7, batch 215/481,disc_loss 77.867, (real 80.664, fake 75.07 ) gen_loss 577.49\n",
            "iteration 3302, epoch 7, batch 216/481,disc_loss 82.365, (real 85.879, fake 78.851 ) gen_loss 642.81\n",
            "iteration 3303, epoch 7, batch 217/481,disc_loss 78.955, (real 81.969, fake 75.942 ) gen_loss 593.69\n",
            "iteration 3304, epoch 7, batch 218/481,disc_loss 78.464, (real 81.743, fake 75.184 ) gen_loss 625.46\n",
            "iteration 3305, epoch 7, batch 219/481,disc_loss 82.058, (real 84.724, fake 79.391 ) gen_loss 602.75\n",
            "iteration 3306, epoch 7, batch 220/481,disc_loss 78.854, (real 80.88, fake 76.828 ) gen_loss 594.69\n",
            "iteration 3307, epoch 7, batch 221/481,disc_loss 79.028, (real 81.971, fake 76.085 ) gen_loss 604.22\n",
            "iteration 3308, epoch 7, batch 222/481,disc_loss 74.438, (real 78.045, fake 70.831 ) gen_loss 544.59\n",
            "iteration 3309, epoch 7, batch 223/481,disc_loss 73.636, (real 76.086, fake 71.187 ) gen_loss 569.23\n",
            "iteration 3310, epoch 7, batch 224/481,disc_loss 78.032, (real 80.145, fake 75.919 ) gen_loss 650.92\n",
            "iteration 3311, epoch 7, batch 225/481,disc_loss 78.069, (real 80.273, fake 75.865 ) gen_loss 536.89\n",
            "iteration 3312, epoch 7, batch 226/481,disc_loss 78.576, (real 81.066, fake 76.086 ) gen_loss 572.05\n",
            "iteration 3313, epoch 7, batch 227/481,disc_loss 80.811, (real 84.496, fake 77.127 ) gen_loss 522.6\n",
            "iteration 3314, epoch 7, batch 228/481,disc_loss 76.333, (real 78.683, fake 73.984 ) gen_loss 571.29\n",
            "iteration 3315, epoch 7, batch 229/481,disc_loss 77.344, (real 80.182, fake 74.507 ) gen_loss 615.15\n",
            "iteration 3316, epoch 7, batch 230/481,disc_loss 78.645, (real 80.399, fake 76.89 ) gen_loss 561.92\n",
            "iteration 3317, epoch 7, batch 231/481,disc_loss 77.126, (real 79.808, fake 74.444 ) gen_loss 597.86\n",
            "iteration 3318, epoch 7, batch 232/481,disc_loss 80.078, (real 83.523, fake 76.632 ) gen_loss 603.5\n",
            "iteration 3319, epoch 7, batch 233/481,disc_loss 80.382, (real 83.533, fake 77.231 ) gen_loss 638.9\n",
            "iteration 3320, epoch 7, batch 234/481,disc_loss 76.752, (real 79.92, fake 73.585 ) gen_loss 591.33\n",
            "iteration 3321, epoch 7, batch 235/481,disc_loss 76.984, (real 80.391, fake 73.577 ) gen_loss 598.6\n",
            "iteration 3322, epoch 7, batch 236/481,disc_loss 79.558, (real 82.979, fake 76.138 ) gen_loss 588.63\n",
            "iteration 3323, epoch 7, batch 237/481,disc_loss 79.31, (real 81.896, fake 76.725 ) gen_loss 651.89\n",
            "iteration 3324, epoch 7, batch 238/481,disc_loss 79.686, (real 82.013, fake 77.358 ) gen_loss 560.87\n",
            "iteration 3325, epoch 7, batch 239/481,disc_loss 82.329, (real 84.309, fake 80.349 ) gen_loss 623.12\n",
            "iteration 3326, epoch 7, batch 240/481,disc_loss 84.095, (real 86.332, fake 81.858 ) gen_loss 537.23\n",
            "iteration 3327, epoch 7, batch 241/481,disc_loss 76.801, (real 79.64, fake 73.962 ) gen_loss 544.32\n",
            "iteration 3328, epoch 7, batch 242/481,disc_loss 81.723, (real 83.857, fake 79.589 ) gen_loss 728.62\n",
            "iteration 3329, epoch 7, batch 243/481,disc_loss 78.121, (real 81.535, fake 74.706 ) gen_loss 650.37\n",
            "iteration 3330, epoch 7, batch 244/481,disc_loss 80.453, (real 83.65, fake 77.256 ) gen_loss 692.79\n",
            "iteration 3331, epoch 7, batch 245/481,disc_loss 78.721, (real 81.9, fake 75.542 ) gen_loss 657.63\n",
            "iteration 3332, epoch 7, batch 246/481,disc_loss 74.867, (real 77.838, fake 71.897 ) gen_loss 740.25\n",
            "iteration 3333, epoch 7, batch 247/481,disc_loss 79.18, (real 82.234, fake 76.126 ) gen_loss 601.96\n",
            "iteration 3334, epoch 7, batch 248/481,disc_loss 80.958, (real 83.523, fake 78.394 ) gen_loss 605.09\n",
            "iteration 3335, epoch 7, batch 249/481,disc_loss 77.951, (real 81.229, fake 74.673 ) gen_loss 570.98\n",
            "iteration 3336, epoch 7, batch 250/481,disc_loss 79.191, (real 80.944, fake 77.438 ) gen_loss 629.59\n",
            "iteration 3337, epoch 7, batch 251/481,disc_loss 78.272, (real 79.383, fake 77.161 ) gen_loss 649.5\n",
            "iteration 3338, epoch 7, batch 252/481,disc_loss 79.222, (real 80.1, fake 78.344 ) gen_loss 713.99\n",
            "iteration 3339, epoch 7, batch 253/481,disc_loss 78.544, (real 82.004, fake 75.084 ) gen_loss 610.28\n",
            "iteration 3340, epoch 7, batch 254/481,disc_loss 81.753, (real 85.011, fake 78.494 ) gen_loss 612.89\n",
            "iteration 3341, epoch 7, batch 255/481,disc_loss 76.333, (real 80.062, fake 72.603 ) gen_loss 560.5\n",
            "iteration 3342, epoch 7, batch 256/481,disc_loss 80.771, (real 83.285, fake 78.257 ) gen_loss 617.52\n",
            "iteration 3343, epoch 7, batch 257/481,disc_loss 81.293, (real 83.795, fake 78.791 ) gen_loss 611.9\n",
            "iteration 3344, epoch 7, batch 258/481,disc_loss 77.471, (real 80.61, fake 74.332 ) gen_loss 608.53\n",
            "iteration 3345, epoch 7, batch 259/481,disc_loss 76.159, (real 79.107, fake 73.211 ) gen_loss 575.73\n",
            "iteration 3346, epoch 7, batch 260/481,disc_loss 76.51, (real 79.7, fake 73.32 ) gen_loss 633.84\n",
            "iteration 3347, epoch 7, batch 261/481,disc_loss 75.408, (real 78.07, fake 72.745 ) gen_loss 602.19\n",
            "iteration 3348, epoch 7, batch 262/481,disc_loss 79.505, (real 81.842, fake 77.169 ) gen_loss 690.75\n",
            "iteration 3349, epoch 7, batch 263/481,disc_loss 78.287, (real 81.229, fake 75.345 ) gen_loss 652.13\n",
            "iteration 3350, epoch 7, batch 264/481,disc_loss 83.978, (real 86.719, fake 81.237 ) gen_loss 607.16\n",
            "iteration 3351, epoch 7, batch 265/481,disc_loss 81.743, (real 86.382, fake 77.105 ) gen_loss 599.85\n",
            "iteration 3352, epoch 7, batch 266/481,disc_loss 79.999, (real 83.395, fake 76.604 ) gen_loss 587.85\n",
            "iteration 3353, epoch 7, batch 267/481,disc_loss 83.092, (real 86.594, fake 79.59 ) gen_loss 646.97\n",
            "iteration 3354, epoch 7, batch 268/481,disc_loss 79.426, (real 80.966, fake 77.887 ) gen_loss 542.64\n",
            "iteration 3355, epoch 7, batch 269/481,disc_loss 76.953, (real 79.349, fake 74.557 ) gen_loss 590.71\n",
            "iteration 3356, epoch 7, batch 270/481,disc_loss 82.079, (real 84.172, fake 79.987 ) gen_loss 599.7\n",
            "iteration 3357, epoch 7, batch 271/481,disc_loss 77.124, (real 79.375, fake 74.873 ) gen_loss 586.72\n",
            "iteration 3358, epoch 7, batch 272/481,disc_loss 80.432, (real 82.951, fake 77.912 ) gen_loss 601.25\n",
            "iteration 3359, epoch 7, batch 273/481,disc_loss 75.303, (real 78.542, fake 72.063 ) gen_loss 571.69\n",
            "iteration 3360, epoch 7, batch 274/481,disc_loss 76.858, (real 80.152, fake 73.564 ) gen_loss 702.59\n",
            "iteration 3361, epoch 7, batch 275/481,disc_loss 75.724, (real 78.598, fake 72.849 ) gen_loss 538.35\n",
            "iteration 3362, epoch 7, batch 276/481,disc_loss 78.193, (real 80.627, fake 75.758 ) gen_loss 651.09\n",
            "iteration 3363, epoch 7, batch 277/481,disc_loss 78.545, (real 81.834, fake 75.256 ) gen_loss 521.03\n",
            "iteration 3364, epoch 7, batch 278/481,disc_loss 77.178, (real 79.789, fake 74.567 ) gen_loss 590.33\n",
            "iteration 3365, epoch 7, batch 279/481,disc_loss 80.338, (real 84.198, fake 76.478 ) gen_loss 563.66\n",
            "iteration 3366, epoch 7, batch 280/481,disc_loss 77.469, (real 79.286, fake 75.653 ) gen_loss 611.81\n",
            "iteration 3367, epoch 7, batch 281/481,disc_loss 82.308, (real 84.589, fake 80.026 ) gen_loss 586.31\n",
            "iteration 3368, epoch 7, batch 282/481,disc_loss 76.087, (real 78.632, fake 73.543 ) gen_loss 582.61\n",
            "iteration 3369, epoch 7, batch 283/481,disc_loss 79.458, (real 81.982, fake 76.934 ) gen_loss 612.45\n",
            "iteration 3370, epoch 7, batch 284/481,disc_loss 78.575, (real 81.789, fake 75.36 ) gen_loss 612.62\n",
            "iteration 3371, epoch 7, batch 285/481,disc_loss 76.003, (real 80.08, fake 71.927 ) gen_loss 565.97\n",
            "iteration 3372, epoch 7, batch 286/481,disc_loss 80.376, (real 83.267, fake 77.485 ) gen_loss 573.81\n",
            "iteration 3373, epoch 7, batch 287/481,disc_loss 80.527, (real 84.662, fake 76.392 ) gen_loss 635.33\n",
            "iteration 3374, epoch 7, batch 288/481,disc_loss 77.398, (real 79.308, fake 75.488 ) gen_loss 553.96\n",
            "iteration 3375, epoch 7, batch 289/481,disc_loss 75.683, (real 79.401, fake 71.965 ) gen_loss 596.23\n",
            "iteration 3376, epoch 7, batch 290/481,disc_loss 74.157, (real 77.442, fake 70.871 ) gen_loss 589.87\n",
            "iteration 3377, epoch 7, batch 291/481,disc_loss 78.471, (real 81.172, fake 75.77 ) gen_loss 546.54\n",
            "iteration 3378, epoch 7, batch 292/481,disc_loss 77.883, (real 80.614, fake 75.151 ) gen_loss 593.94\n",
            "iteration 3379, epoch 7, batch 293/481,disc_loss 76.557, (real 78.87, fake 74.244 ) gen_loss 617.55\n",
            "iteration 3380, epoch 7, batch 294/481,disc_loss 79.195, (real 82.428, fake 75.961 ) gen_loss 613.44\n",
            "iteration 3381, epoch 7, batch 295/481,disc_loss 78.266, (real 81.671, fake 74.861 ) gen_loss 652.07\n",
            "iteration 3382, epoch 7, batch 296/481,disc_loss 76.073, (real 79.462, fake 72.683 ) gen_loss 710.55\n",
            "iteration 3383, epoch 7, batch 297/481,disc_loss 75.084, (real 77.428, fake 72.74 ) gen_loss 541.07\n",
            "iteration 3384, epoch 7, batch 298/481,disc_loss 80.403, (real 83.4, fake 77.406 ) gen_loss 619.19\n",
            "iteration 3385, epoch 7, batch 299/481,disc_loss 73.833, (real 77.428, fake 70.239 ) gen_loss 559.09\n",
            "iteration 3386, epoch 7, batch 300/481,disc_loss 78.666, (real 81.464, fake 75.869 ) gen_loss 620.39\n",
            "iteration 3387, epoch 7, batch 301/481,disc_loss 82.538, (real 84.738, fake 80.338 ) gen_loss 584.83\n",
            "iteration 3388, epoch 7, batch 302/481,disc_loss 83.951, (real 86.098, fake 81.804 ) gen_loss 487.36\n",
            "iteration 3389, epoch 7, batch 303/481,disc_loss 79.89, (real 81.214, fake 78.567 ) gen_loss 566.56\n",
            "iteration 3390, epoch 7, batch 304/481,disc_loss 78.053, (real 79.955, fake 76.151 ) gen_loss 611.76\n",
            "iteration 3391, epoch 7, batch 305/481,disc_loss 79.477, (real 82.477, fake 76.476 ) gen_loss 590.1\n",
            "iteration 3392, epoch 7, batch 306/481,disc_loss 76.668, (real 79.948, fake 73.387 ) gen_loss 532.68\n",
            "iteration 3393, epoch 7, batch 307/481,disc_loss 86.317, (real 90.013, fake 82.62 ) gen_loss 624.2\n",
            "iteration 3394, epoch 7, batch 308/481,disc_loss 79.81, (real 83.896, fake 75.723 ) gen_loss 515.36\n",
            "iteration 3395, epoch 7, batch 309/481,disc_loss 76.488, (real 79.035, fake 73.941 ) gen_loss 541.98\n",
            "iteration 3396, epoch 7, batch 310/481,disc_loss 76.352, (real 79.39, fake 73.314 ) gen_loss 629.57\n",
            "iteration 3397, epoch 7, batch 311/481,disc_loss 79.924, (real 83.065, fake 76.783 ) gen_loss 588.59\n",
            "iteration 3398, epoch 7, batch 312/481,disc_loss 81.607, (real 84.476, fake 78.738 ) gen_loss 586.85\n",
            "iteration 3399, epoch 7, batch 313/481,disc_loss 80.591, (real 83.074, fake 78.108 ) gen_loss 605.52\n",
            "iteration 3400, epoch 7, batch 314/481,disc_loss 78.698, (real 81.229, fake 76.168 ) gen_loss 611.33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 3401, epoch 7, batch 315/481,disc_loss 81.882, (real 85.101, fake 78.663 ) gen_loss 561.82\n",
            "iteration 3402, epoch 7, batch 316/481,disc_loss 82.425, (real 85.992, fake 78.859 ) gen_loss 673.06\n",
            "iteration 3403, epoch 7, batch 317/481,disc_loss 76.061, (real 79.13, fake 72.993 ) gen_loss 576.66\n",
            "iteration 3404, epoch 7, batch 318/481,disc_loss 76.68, (real 78.923, fake 74.438 ) gen_loss 626.25\n",
            "iteration 3405, epoch 7, batch 319/481,disc_loss 78.029, (real 79.958, fake 76.1 ) gen_loss 699.3\n",
            "iteration 3406, epoch 7, batch 320/481,disc_loss 76.879, (real 79.121, fake 74.638 ) gen_loss 630.68\n",
            "iteration 3407, epoch 7, batch 321/481,disc_loss 80.348, (real 83.162, fake 77.534 ) gen_loss 528.19\n",
            "iteration 3408, epoch 7, batch 322/481,disc_loss 77.271, (real 79.519, fake 75.023 ) gen_loss 705.38\n",
            "iteration 3409, epoch 7, batch 323/481,disc_loss 74.975, (real 77.33, fake 72.621 ) gen_loss 607.66\n",
            "iteration 3410, epoch 7, batch 324/481,disc_loss 84.806, (real 88.758, fake 80.854 ) gen_loss 710.07\n",
            "iteration 3411, epoch 7, batch 325/481,disc_loss 77.501, (real 79.994, fake 75.009 ) gen_loss 586.82\n",
            "iteration 3412, epoch 7, batch 326/481,disc_loss 82.117, (real 83.771, fake 80.462 ) gen_loss 666.79\n",
            "iteration 3413, epoch 7, batch 327/481,disc_loss 78.38, (real 81.161, fake 75.599 ) gen_loss 646.44\n",
            "iteration 3414, epoch 7, batch 328/481,disc_loss 75.112, (real 78.606, fake 71.619 ) gen_loss 666.51\n",
            "iteration 3415, epoch 7, batch 329/481,disc_loss 84.871, (real 88.812, fake 80.931 ) gen_loss 716.18\n",
            "iteration 3416, epoch 7, batch 330/481,disc_loss 81.005, (real 83.872, fake 78.137 ) gen_loss 563.36\n",
            "iteration 3417, epoch 7, batch 331/481,disc_loss 78.815, (real 81.582, fake 76.047 ) gen_loss 590.6\n",
            "iteration 3418, epoch 7, batch 332/481,disc_loss 80.97, (real 84.141, fake 77.799 ) gen_loss 660.8\n",
            "iteration 3419, epoch 7, batch 333/481,disc_loss 75.728, (real 79.079, fake 72.378 ) gen_loss 604.52\n",
            "iteration 3420, epoch 7, batch 334/481,disc_loss 78.467, (real 81.031, fake 75.902 ) gen_loss 643.45\n",
            "iteration 3421, epoch 7, batch 335/481,disc_loss 81.35, (real 84.031, fake 78.669 ) gen_loss 673.26\n",
            "iteration 3422, epoch 7, batch 336/481,disc_loss 77.045, (real 80.007, fake 74.084 ) gen_loss 674.09\n",
            "iteration 3423, epoch 7, batch 337/481,disc_loss 75.849, (real 78.798, fake 72.899 ) gen_loss 702.26\n",
            "iteration 3424, epoch 7, batch 338/481,disc_loss 77.35, (real 80.308, fake 74.392 ) gen_loss 633.35\n",
            "iteration 3425, epoch 7, batch 339/481,disc_loss 76.892, (real 79.914, fake 73.869 ) gen_loss 586.37\n",
            "iteration 3426, epoch 7, batch 340/481,disc_loss 77.126, (real 79.657, fake 74.596 ) gen_loss 586.73\n",
            "iteration 3427, epoch 7, batch 341/481,disc_loss 77.911, (real 80.335, fake 75.488 ) gen_loss 647.78\n",
            "iteration 3428, epoch 7, batch 342/481,disc_loss 76.302, (real 79.076, fake 73.529 ) gen_loss 719.94\n",
            "iteration 3429, epoch 7, batch 343/481,disc_loss 76.869, (real 79.375, fake 74.363 ) gen_loss 588.66\n",
            "iteration 3430, epoch 7, batch 344/481,disc_loss 85.662, (real 86.969, fake 84.356 ) gen_loss 673.73\n",
            "iteration 3431, epoch 7, batch 345/481,disc_loss 87.459, (real 89.432, fake 85.485 ) gen_loss 604.78\n",
            "iteration 3432, epoch 7, batch 346/481,disc_loss 83.343, (real 84.154, fake 82.531 ) gen_loss 627.19\n",
            "iteration 3433, epoch 7, batch 347/481,disc_loss 75.533, (real 77.962, fake 73.104 ) gen_loss 607.43\n",
            "iteration 3434, epoch 7, batch 348/481,disc_loss 79.295, (real 82.053, fake 76.537 ) gen_loss 588.89\n",
            "iteration 3435, epoch 7, batch 349/481,disc_loss 74.849, (real 78.064, fake 71.635 ) gen_loss 647.84\n",
            "iteration 3436, epoch 7, batch 350/481,disc_loss 75.479, (real 77.813, fake 73.144 ) gen_loss 634.73\n",
            "iteration 3437, epoch 7, batch 351/481,disc_loss 79.23, (real 82.648, fake 75.813 ) gen_loss 591.17\n",
            "iteration 3438, epoch 7, batch 352/481,disc_loss 77.917, (real 81.211, fake 74.622 ) gen_loss 583.0\n",
            "iteration 3439, epoch 7, batch 353/481,disc_loss 79.317, (real 82.009, fake 76.625 ) gen_loss 640.1\n",
            "iteration 3440, epoch 7, batch 354/481,disc_loss 78.609, (real 81.573, fake 75.646 ) gen_loss 598.49\n",
            "iteration 3441, epoch 7, batch 355/481,disc_loss 78.429, (real 81.42, fake 75.438 ) gen_loss 615.06\n",
            "iteration 3442, epoch 7, batch 356/481,disc_loss 77.978, (real 81.762, fake 74.194 ) gen_loss 603.55\n",
            "iteration 3443, epoch 7, batch 357/481,disc_loss 81.214, (real 85.075, fake 77.354 ) gen_loss 700.06\n",
            "iteration 3444, epoch 7, batch 358/481,disc_loss 76.962, (real 79.638, fake 74.287 ) gen_loss 646.65\n",
            "iteration 3445, epoch 7, batch 359/481,disc_loss 80.849, (real 83.265, fake 78.432 ) gen_loss 633.18\n",
            "iteration 3446, epoch 7, batch 360/481,disc_loss 82.772, (real 86.136, fake 79.407 ) gen_loss 595.45\n",
            "iteration 3447, epoch 7, batch 361/481,disc_loss 78.836, (real 82.93, fake 74.743 ) gen_loss 685.92\n",
            "iteration 3448, epoch 7, batch 362/481,disc_loss 80.689, (real 83.565, fake 77.813 ) gen_loss 778.17\n",
            "iteration 3449, epoch 7, batch 363/481,disc_loss 76.611, (real 79.769, fake 73.453 ) gen_loss 631.65\n",
            "iteration 3450, epoch 7, batch 364/481,disc_loss 80.676, (real 83.6, fake 77.752 ) gen_loss 641.02\n",
            "iteration 3451, epoch 7, batch 365/481,disc_loss 75.173, (real 77.139, fake 73.208 ) gen_loss 599.76\n",
            "iteration 3452, epoch 7, batch 366/481,disc_loss 81.272, (real 82.757, fake 79.787 ) gen_loss 611.27\n",
            "iteration 3453, epoch 7, batch 367/481,disc_loss 79.885, (real 83.054, fake 76.716 ) gen_loss 606.49\n",
            "iteration 3454, epoch 7, batch 368/481,disc_loss 76.8, (real 79.492, fake 74.108 ) gen_loss 611.29\n",
            "iteration 3455, epoch 7, batch 369/481,disc_loss 75.562, (real 78.83, fake 72.293 ) gen_loss 655.22\n",
            "iteration 3456, epoch 7, batch 370/481,disc_loss 74.496, (real 76.824, fake 72.167 ) gen_loss 636.68\n",
            "iteration 3457, epoch 7, batch 371/481,disc_loss 82.682, (real 85.611, fake 79.754 ) gen_loss 616.88\n",
            "iteration 3458, epoch 7, batch 372/481,disc_loss 79.368, (real 81.649, fake 77.087 ) gen_loss 603.08\n",
            "iteration 3459, epoch 7, batch 373/481,disc_loss 81.671, (real 84.364, fake 78.977 ) gen_loss 608.41\n",
            "iteration 3460, epoch 7, batch 374/481,disc_loss 81.376, (real 83.039, fake 79.712 ) gen_loss 683.69\n",
            "iteration 3461, epoch 7, batch 375/481,disc_loss 81.002, (real 84.082, fake 77.922 ) gen_loss 615.7\n",
            "iteration 3462, epoch 7, batch 376/481,disc_loss 77.896, (real 80.5, fake 75.292 ) gen_loss 607.6\n",
            "iteration 3463, epoch 7, batch 377/481,disc_loss 81.501, (real 84.074, fake 78.927 ) gen_loss 697.55\n",
            "iteration 3464, epoch 7, batch 378/481,disc_loss 81.051, (real 83.884, fake 78.217 ) gen_loss 667.91\n",
            "iteration 3465, epoch 7, batch 379/481,disc_loss 77.2, (real 79.47, fake 74.929 ) gen_loss 580.96\n",
            "iteration 3466, epoch 7, batch 380/481,disc_loss 75.359, (real 78.176, fake 72.542 ) gen_loss 618.84\n",
            "iteration 3467, epoch 7, batch 381/481,disc_loss 81.673, (real 84.626, fake 78.72 ) gen_loss 583.39\n",
            "iteration 3468, epoch 7, batch 382/481,disc_loss 82.233, (real 86.64, fake 77.826 ) gen_loss 680.99\n",
            "iteration 3469, epoch 7, batch 383/481,disc_loss 84.37, (real 86.781, fake 81.959 ) gen_loss 724.29\n",
            "iteration 3470, epoch 7, batch 384/481,disc_loss 76.066, (real 79.265, fake 72.867 ) gen_loss 622.03\n",
            "iteration 3471, epoch 7, batch 385/481,disc_loss 81.46, (real 84.291, fake 78.629 ) gen_loss 621.36\n",
            "iteration 3472, epoch 7, batch 386/481,disc_loss 73.749, (real 77.029, fake 70.469 ) gen_loss 668.88\n",
            "iteration 3473, epoch 7, batch 387/481,disc_loss 75.573, (real 78.314, fake 72.831 ) gen_loss 616.86\n",
            "iteration 3474, epoch 7, batch 388/481,disc_loss 79.095, (real 81.933, fake 76.258 ) gen_loss 651.31\n",
            "iteration 3475, epoch 7, batch 389/481,disc_loss 79.49, (real 82.124, fake 76.855 ) gen_loss 671.2\n",
            "iteration 3476, epoch 7, batch 390/481,disc_loss 77.044, (real 79.265, fake 74.824 ) gen_loss 628.45\n",
            "iteration 3477, epoch 7, batch 391/481,disc_loss 81.039, (real 84.127, fake 77.95 ) gen_loss 690.58\n",
            "iteration 3478, epoch 7, batch 392/481,disc_loss 75.061, (real 78.081, fake 72.04 ) gen_loss 587.53\n",
            "iteration 3479, epoch 7, batch 393/481,disc_loss 77.92, (real 80.998, fake 74.842 ) gen_loss 652.5\n",
            "iteration 3480, epoch 7, batch 394/481,disc_loss 77.096, (real 78.834, fake 75.359 ) gen_loss 557.72\n",
            "iteration 3481, epoch 7, batch 395/481,disc_loss 77.4, (real 80.628, fake 74.173 ) gen_loss 578.16\n",
            "iteration 3482, epoch 7, batch 396/481,disc_loss 78.798, (real 81.581, fake 76.014 ) gen_loss 633.94\n",
            "iteration 3483, epoch 7, batch 397/481,disc_loss 77.796, (real 80.313, fake 75.279 ) gen_loss 609.9\n",
            "iteration 3484, epoch 7, batch 398/481,disc_loss 77.075, (real 79.639, fake 74.51 ) gen_loss 615.83\n",
            "iteration 3485, epoch 7, batch 399/481,disc_loss 77.833, (real 79.65, fake 76.016 ) gen_loss 589.69\n",
            "iteration 3486, epoch 7, batch 400/481,disc_loss 80.876, (real 84.569, fake 77.183 ) gen_loss 579.99\n",
            "iteration 3487, epoch 7, batch 401/481,disc_loss 79.308, (real 83.546, fake 75.069 ) gen_loss 565.57\n",
            "iteration 3488, epoch 7, batch 402/481,disc_loss 74.552, (real 76.811, fake 72.293 ) gen_loss 571.27\n",
            "iteration 3489, epoch 7, batch 403/481,disc_loss 78.52, (real 81.413, fake 75.627 ) gen_loss 667.26\n",
            "iteration 3490, epoch 7, batch 404/481,disc_loss 77.631, (real 81.634, fake 73.628 ) gen_loss 699.19\n",
            "iteration 3491, epoch 7, batch 405/481,disc_loss 78.081, (real 81.024, fake 75.139 ) gen_loss 574.55\n",
            "iteration 3492, epoch 7, batch 406/481,disc_loss 81.195, (real 85.674, fake 76.716 ) gen_loss 634.02\n",
            "iteration 3493, epoch 7, batch 407/481,disc_loss 80.451, (real 84.051, fake 76.851 ) gen_loss 629.62\n",
            "iteration 3494, epoch 7, batch 408/481,disc_loss 77.271, (real 79.271, fake 75.271 ) gen_loss 658.12\n",
            "iteration 3495, epoch 7, batch 409/481,disc_loss 79.516, (real 82.151, fake 76.88 ) gen_loss 665.29\n",
            "iteration 3496, epoch 7, batch 410/481,disc_loss 78.164, (real 80.8, fake 75.527 ) gen_loss 576.94\n",
            "iteration 3497, epoch 7, batch 411/481,disc_loss 77.946, (real 80.648, fake 75.244 ) gen_loss 636.07\n",
            "iteration 3498, epoch 7, batch 412/481,disc_loss 82.345, (real 85.745, fake 78.945 ) gen_loss 573.85\n",
            "iteration 3499, epoch 7, batch 413/481,disc_loss 79.651, (real 81.906, fake 77.395 ) gen_loss 570.95\n",
            "iteration 3500, epoch 7, batch 414/481,disc_loss 80.383, (real 83.592, fake 77.174 ) gen_loss 601.85\n",
            "iteration 3501, epoch 7, batch 415/481,disc_loss 77.206, (real 80.046, fake 74.365 ) gen_loss 581.79\n",
            "iteration 3502, epoch 7, batch 416/481,disc_loss 80.781, (real 84.455, fake 77.108 ) gen_loss 532.58\n",
            "iteration 3503, epoch 7, batch 417/481,disc_loss 81.207, (real 84.047, fake 78.368 ) gen_loss 553.44\n",
            "iteration 3504, epoch 7, batch 418/481,disc_loss 79.331, (real 83.143, fake 75.519 ) gen_loss 551.2\n",
            "iteration 3505, epoch 7, batch 419/481,disc_loss 80.453, (real 83.205, fake 77.7 ) gen_loss 585.72\n",
            "iteration 3506, epoch 7, batch 420/481,disc_loss 79.08, (real 82.116, fake 76.044 ) gen_loss 566.22\n",
            "iteration 3507, epoch 7, batch 421/481,disc_loss 81.387, (real 84.108, fake 78.666 ) gen_loss 586.76\n",
            "iteration 3508, epoch 7, batch 422/481,disc_loss 75.351, (real 78.544, fake 72.159 ) gen_loss 627.63\n",
            "iteration 3509, epoch 7, batch 423/481,disc_loss 75.658, (real 79.084, fake 72.232 ) gen_loss 661.77\n",
            "iteration 3510, epoch 7, batch 424/481,disc_loss 77.748, (real 80.778, fake 74.717 ) gen_loss 598.16\n",
            "iteration 3511, epoch 7, batch 425/481,disc_loss 82.478, (real 86.372, fake 78.583 ) gen_loss 636.41\n",
            "iteration 3512, epoch 7, batch 426/481,disc_loss 79.969, (real 82.59, fake 77.348 ) gen_loss 666.19\n",
            "iteration 3513, epoch 7, batch 427/481,disc_loss 79.298, (real 82.299, fake 76.297 ) gen_loss 690.22\n",
            "iteration 3514, epoch 7, batch 428/481,disc_loss 74.905, (real 76.373, fake 73.437 ) gen_loss 695.29\n",
            "iteration 3515, epoch 7, batch 429/481,disc_loss 79.356, (real 81.621, fake 77.091 ) gen_loss 630.34\n",
            "iteration 3516, epoch 7, batch 430/481,disc_loss 82.294, (real 85.091, fake 79.496 ) gen_loss 568.37\n",
            "iteration 3517, epoch 7, batch 431/481,disc_loss 79.865, (real 82.677, fake 77.052 ) gen_loss 544.21\n",
            "iteration 3518, epoch 7, batch 432/481,disc_loss 80.946, (real 83.774, fake 78.117 ) gen_loss 609.17\n",
            "iteration 3519, epoch 7, batch 433/481,disc_loss 78.648, (real 81.485, fake 75.811 ) gen_loss 612.37\n",
            "iteration 3520, epoch 7, batch 434/481,disc_loss 81.571, (real 83.384, fake 79.758 ) gen_loss 651.15\n",
            "iteration 3521, epoch 7, batch 435/481,disc_loss 81.544, (real 83.97, fake 79.117 ) gen_loss 592.18\n",
            "iteration 3522, epoch 7, batch 436/481,disc_loss 81.292, (real 83.569, fake 79.016 ) gen_loss 682.48\n",
            "iteration 3523, epoch 7, batch 437/481,disc_loss 80.556, (real 83.885, fake 77.227 ) gen_loss 586.54\n",
            "iteration 3524, epoch 7, batch 438/481,disc_loss 77.499, (real 79.909, fake 75.09 ) gen_loss 541.04\n",
            "iteration 3525, epoch 7, batch 439/481,disc_loss 75.977, (real 79.571, fake 72.383 ) gen_loss 561.51\n",
            "iteration 3526, epoch 7, batch 440/481,disc_loss 74.211, (real 76.888, fake 71.535 ) gen_loss 589.12\n",
            "iteration 3527, epoch 7, batch 441/481,disc_loss 80.147, (real 83.721, fake 76.574 ) gen_loss 551.93\n",
            "iteration 3528, epoch 7, batch 442/481,disc_loss 80.908, (real 84.449, fake 77.367 ) gen_loss 648.35\n",
            "iteration 3529, epoch 7, batch 443/481,disc_loss 81.116, (real 84.026, fake 78.206 ) gen_loss 639.69\n",
            "iteration 3530, epoch 7, batch 444/481,disc_loss 71.411, (real 73.625, fake 69.197 ) gen_loss 656.93\n",
            "iteration 3531, epoch 7, batch 445/481,disc_loss 83.676, (real 86.147, fake 81.204 ) gen_loss 622.42\n",
            "iteration 3532, epoch 7, batch 446/481,disc_loss 77.986, (real 78.656, fake 77.317 ) gen_loss 553.68\n",
            "iteration 3533, epoch 7, batch 447/481,disc_loss 79.787, (real 83.795, fake 75.779 ) gen_loss 578.02\n",
            "iteration 3534, epoch 7, batch 448/481,disc_loss 80.689, (real 82.9, fake 78.478 ) gen_loss 628.93\n",
            "iteration 3535, epoch 7, batch 449/481,disc_loss 80.799, (real 83.146, fake 78.452 ) gen_loss 617.13\n",
            "iteration 3536, epoch 7, batch 450/481,disc_loss 78.654, (real 81.357, fake 75.952 ) gen_loss 630.17\n",
            "iteration 3537, epoch 7, batch 451/481,disc_loss 75.883, (real 78.832, fake 72.934 ) gen_loss 583.34\n",
            "iteration 3538, epoch 7, batch 452/481,disc_loss 76.585, (real 80.102, fake 73.069 ) gen_loss 607.68\n",
            "iteration 3539, epoch 7, batch 453/481,disc_loss 72.185, (real 75.006, fake 69.363 ) gen_loss 640.46\n",
            "iteration 3540, epoch 7, batch 454/481,disc_loss 81.075, (real 84.527, fake 77.622 ) gen_loss 601.27\n",
            "iteration 3541, epoch 7, batch 455/481,disc_loss 76.551, (real 79.585, fake 73.517 ) gen_loss 541.55\n",
            "iteration 3542, epoch 7, batch 456/481,disc_loss 80.081, (real 82.299, fake 77.864 ) gen_loss 572.82\n",
            "iteration 3543, epoch 7, batch 457/481,disc_loss 72.934, (real 75.22, fake 70.648 ) gen_loss 540.6\n",
            "iteration 3544, epoch 7, batch 458/481,disc_loss 80.47, (real 83.798, fake 77.142 ) gen_loss 624.72\n",
            "iteration 3545, epoch 7, batch 459/481,disc_loss 79.954, (real 83.514, fake 76.394 ) gen_loss 599.86\n",
            "iteration 3546, epoch 7, batch 460/481,disc_loss 76.734, (real 79.052, fake 74.417 ) gen_loss 565.15\n",
            "iteration 3547, epoch 7, batch 461/481,disc_loss 81.759, (real 83.385, fake 80.134 ) gen_loss 647.86\n",
            "iteration 3548, epoch 7, batch 462/481,disc_loss 81.059, (real 83.351, fake 78.767 ) gen_loss 559.97\n",
            "iteration 3549, epoch 7, batch 463/481,disc_loss 81.557, (real 84.712, fake 78.402 ) gen_loss 596.2\n",
            "iteration 3550, epoch 7, batch 464/481,disc_loss 73.218, (real 76.527, fake 69.909 ) gen_loss 685.02\n",
            "iteration 3551, epoch 7, batch 465/481,disc_loss 76.183, (real 79.073, fake 73.294 ) gen_loss 681.14\n",
            "iteration 3552, epoch 7, batch 466/481,disc_loss 76.626, (real 80.466, fake 72.786 ) gen_loss 686.02\n",
            "iteration 3553, epoch 7, batch 467/481,disc_loss 78.802, (real 81.236, fake 76.368 ) gen_loss 573.19\n",
            "iteration 3554, epoch 7, batch 468/481,disc_loss 78.777, (real 81.876, fake 75.679 ) gen_loss 595.79\n",
            "iteration 3555, epoch 7, batch 469/481,disc_loss 84.184, (real 87.23, fake 81.137 ) gen_loss 666.17\n",
            "iteration 3556, epoch 7, batch 470/481,disc_loss 76.83, (real 79.862, fake 73.799 ) gen_loss 562.66\n",
            "iteration 3557, epoch 7, batch 471/481,disc_loss 83.819, (real 86.737, fake 80.9 ) gen_loss 532.36\n",
            "iteration 3558, epoch 7, batch 472/481,disc_loss 77.816, (real 80.507, fake 75.124 ) gen_loss 636.56\n",
            "iteration 3559, epoch 7, batch 473/481,disc_loss 78.698, (real 82.075, fake 75.321 ) gen_loss 681.53\n",
            "iteration 3560, epoch 7, batch 474/481,disc_loss 76.48, (real 79.232, fake 73.729 ) gen_loss 614.01\n",
            "iteration 3561, epoch 7, batch 475/481,disc_loss 80.281, (real 83.355, fake 77.207 ) gen_loss 683.31\n",
            "iteration 3562, epoch 7, batch 476/481,disc_loss 76.581, (real 78.913, fake 74.25 ) gen_loss 664.6\n",
            "iteration 3563, epoch 7, batch 477/481,disc_loss 80.648, (real 82.988, fake 78.308 ) gen_loss 605.86\n",
            "iteration 3564, epoch 7, batch 478/481,disc_loss 80.951, (real 83.751, fake 78.15 ) gen_loss 559.05\n",
            "iteration 3565, epoch 7, batch 479/481,disc_loss 74.867, (real 77.206, fake 72.528 ) gen_loss 615.5\n",
            "iteration 3566, epoch 7, batch 480/481,disc_loss 77.915, (real 80.477, fake 75.353 ) gen_loss 611.05\n",
            "iteration 3567, epoch 7, batch 481/481,disc_loss 75.632, (real 78.685, fake 72.578 ) gen_loss 612.75\n",
            "iteration 3568, epoch 8, batch 1/481,disc_loss 80.887, (real 83.533, fake 78.241 ) gen_loss 608.38\n",
            "iteration 3569, epoch 8, batch 2/481,disc_loss 78.994, (real 82.422, fake 75.567 ) gen_loss 674.47\n",
            "iteration 3570, epoch 8, batch 3/481,disc_loss 77.627, (real 80.309, fake 74.945 ) gen_loss 692.59\n",
            "iteration 3571, epoch 8, batch 4/481,disc_loss 77.493, (real 79.974, fake 75.011 ) gen_loss 653.07\n",
            "iteration 3572, epoch 8, batch 5/481,disc_loss 82.515, (real 85.399, fake 79.63 ) gen_loss 655.04\n",
            "iteration 3573, epoch 8, batch 6/481,disc_loss 78.804, (real 82.425, fake 75.183 ) gen_loss 639.61\n",
            "iteration 3574, epoch 8, batch 7/481,disc_loss 79.353, (real 81.713, fake 76.993 ) gen_loss 602.6\n",
            "iteration 3575, epoch 8, batch 8/481,disc_loss 75.198, (real 79.036, fake 71.36 ) gen_loss 679.45\n",
            "iteration 3576, epoch 8, batch 9/481,disc_loss 78.799, (real 80.472, fake 77.126 ) gen_loss 615.8\n",
            "iteration 3577, epoch 8, batch 10/481,disc_loss 77.549, (real 79.112, fake 75.985 ) gen_loss 622.07\n",
            "iteration 3578, epoch 8, batch 11/481,disc_loss 77.158, (real 79.796, fake 74.52 ) gen_loss 590.61\n",
            "iteration 3579, epoch 8, batch 12/481,disc_loss 75.498, (real 78.147, fake 72.85 ) gen_loss 645.26\n",
            "iteration 3580, epoch 8, batch 13/481,disc_loss 78.234, (real 81.461, fake 75.006 ) gen_loss 666.18\n",
            "iteration 3581, epoch 8, batch 14/481,disc_loss 80.147, (real 82.696, fake 77.597 ) gen_loss 638.42\n",
            "iteration 3582, epoch 8, batch 15/481,disc_loss 77.483, (real 80.105, fake 74.86 ) gen_loss 622.66\n",
            "iteration 3583, epoch 8, batch 16/481,disc_loss 82.348, (real 85.643, fake 79.053 ) gen_loss 653.84\n",
            "iteration 3584, epoch 8, batch 17/481,disc_loss 79.672, (real 81.232, fake 78.111 ) gen_loss 639.41\n",
            "iteration 3585, epoch 8, batch 18/481,disc_loss 80.065, (real 83.312, fake 76.818 ) gen_loss 629.88\n",
            "iteration 3586, epoch 8, batch 19/481,disc_loss 76.734, (real 79.314, fake 74.154 ) gen_loss 595.55\n",
            "iteration 3587, epoch 8, batch 20/481,disc_loss 82.708, (real 85.049, fake 80.367 ) gen_loss 563.43\n",
            "iteration 3588, epoch 8, batch 21/481,disc_loss 81.368, (real 84.295, fake 78.441 ) gen_loss 582.04\n",
            "iteration 3589, epoch 8, batch 22/481,disc_loss 76.567, (real 79.215, fake 73.92 ) gen_loss 588.16\n",
            "iteration 3590, epoch 8, batch 23/481,disc_loss 82.814, (real 85.897, fake 79.73 ) gen_loss 615.2\n",
            "iteration 3591, epoch 8, batch 24/481,disc_loss 76.336, (real 78.763, fake 73.91 ) gen_loss 686.59\n",
            "iteration 3592, epoch 8, batch 25/481,disc_loss 77.627, (real 79.61, fake 75.644 ) gen_loss 675.76\n",
            "iteration 3593, epoch 8, batch 26/481,disc_loss 77.671, (real 79.624, fake 75.718 ) gen_loss 618.8\n",
            "iteration 3594, epoch 8, batch 27/481,disc_loss 78.057, (real 80.906, fake 75.209 ) gen_loss 654.42\n",
            "iteration 3595, epoch 8, batch 28/481,disc_loss 79.729, (real 82.039, fake 77.419 ) gen_loss 647.65\n",
            "iteration 3596, epoch 8, batch 29/481,disc_loss 81.591, (real 84.878, fake 78.305 ) gen_loss 607.33\n",
            "iteration 3597, epoch 8, batch 30/481,disc_loss 74.467, (real 76.981, fake 71.952 ) gen_loss 632.25\n",
            "iteration 3598, epoch 8, batch 31/481,disc_loss 77.057, (real 79.63, fake 74.485 ) gen_loss 634.31\n",
            "iteration 3599, epoch 8, batch 32/481,disc_loss 76.12, (real 79.558, fake 72.683 ) gen_loss 664.58\n",
            "iteration 3600, epoch 8, batch 33/481,disc_loss 85.306, (real 88.475, fake 82.136 ) gen_loss 647.12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 3601, epoch 8, batch 34/481,disc_loss 79.21, (real 81.301, fake 77.12 ) gen_loss 591.29\n",
            "iteration 3602, epoch 8, batch 35/481,disc_loss 81.043, (real 82.856, fake 79.229 ) gen_loss 648.19\n",
            "iteration 3603, epoch 8, batch 36/481,disc_loss 77.938, (real 80.006, fake 75.871 ) gen_loss 560.44\n",
            "iteration 3604, epoch 8, batch 37/481,disc_loss 78.609, (real 81.34, fake 75.879 ) gen_loss 556.74\n",
            "iteration 3605, epoch 8, batch 38/481,disc_loss 78.945, (real 81.929, fake 75.962 ) gen_loss 612.84\n",
            "iteration 3606, epoch 8, batch 39/481,disc_loss 83.417, (real 85.747, fake 81.088 ) gen_loss 602.73\n",
            "iteration 3607, epoch 8, batch 40/481,disc_loss 78.201, (real 81.084, fake 75.319 ) gen_loss 628.39\n",
            "iteration 3608, epoch 8, batch 41/481,disc_loss 75.122, (real 76.577, fake 73.667 ) gen_loss 572.05\n",
            "iteration 3609, epoch 8, batch 42/481,disc_loss 73.228, (real 76.191, fake 70.266 ) gen_loss 576.56\n",
            "iteration 3610, epoch 8, batch 43/481,disc_loss 79.898, (real 81.746, fake 78.05 ) gen_loss 585.75\n",
            "iteration 3611, epoch 8, batch 44/481,disc_loss 78.587, (real 81.549, fake 75.624 ) gen_loss 662.91\n",
            "iteration 3612, epoch 8, batch 45/481,disc_loss 77.299, (real 79.698, fake 74.9 ) gen_loss 565.27\n",
            "iteration 3613, epoch 8, batch 46/481,disc_loss 74.68, (real 77.329, fake 72.03 ) gen_loss 622.43\n",
            "iteration 3614, epoch 8, batch 47/481,disc_loss 77.833, (real 81.044, fake 74.621 ) gen_loss 683.77\n",
            "iteration 3615, epoch 8, batch 48/481,disc_loss 82.927, (real 85.624, fake 80.231 ) gen_loss 647.22\n",
            "iteration 3616, epoch 8, batch 49/481,disc_loss 74.805, (real 78.02, fake 71.591 ) gen_loss 561.02\n",
            "iteration 3617, epoch 8, batch 50/481,disc_loss 76.474, (real 79.944, fake 73.003 ) gen_loss 633.42\n",
            "iteration 3618, epoch 8, batch 51/481,disc_loss 77.322, (real 80.675, fake 73.969 ) gen_loss 599.85\n",
            "iteration 3619, epoch 8, batch 52/481,disc_loss 78.883, (real 81.418, fake 76.349 ) gen_loss 633.61\n",
            "iteration 3620, epoch 8, batch 53/481,disc_loss 77.495, (real 79.228, fake 75.762 ) gen_loss 575.46\n",
            "iteration 3621, epoch 8, batch 54/481,disc_loss 76.102, (real 78.023, fake 74.181 ) gen_loss 567.07\n",
            "iteration 3622, epoch 8, batch 55/481,disc_loss 72.376, (real 75.417, fake 69.335 ) gen_loss 600.98\n",
            "iteration 3623, epoch 8, batch 56/481,disc_loss 79.106, (real 81.654, fake 76.557 ) gen_loss 599.07\n",
            "iteration 3624, epoch 8, batch 57/481,disc_loss 80.449, (real 83.356, fake 77.542 ) gen_loss 668.56\n",
            "iteration 3625, epoch 8, batch 58/481,disc_loss 79.362, (real 82.458, fake 76.266 ) gen_loss 608.07\n",
            "iteration 3626, epoch 8, batch 59/481,disc_loss 77.751, (real 79.294, fake 76.208 ) gen_loss 585.73\n",
            "iteration 3627, epoch 8, batch 60/481,disc_loss 80.636, (real 82.729, fake 78.543 ) gen_loss 603.35\n",
            "iteration 3628, epoch 8, batch 61/481,disc_loss 74.275, (real 77.355, fake 71.195 ) gen_loss 687.73\n",
            "iteration 3629, epoch 8, batch 62/481,disc_loss 81.201, (real 83.708, fake 78.695 ) gen_loss 565.64\n",
            "iteration 3630, epoch 8, batch 63/481,disc_loss 81.116, (real 84.763, fake 77.468 ) gen_loss 582.37\n",
            "iteration 3631, epoch 8, batch 64/481,disc_loss 78.528, (real 81.196, fake 75.86 ) gen_loss 734.41\n",
            "iteration 3632, epoch 8, batch 65/481,disc_loss 78.62, (real 80.294, fake 76.947 ) gen_loss 542.66\n",
            "iteration 3633, epoch 8, batch 66/481,disc_loss 77.734, (real 79.769, fake 75.698 ) gen_loss 587.01\n",
            "iteration 3634, epoch 8, batch 67/481,disc_loss 82.725, (real 85.574, fake 79.876 ) gen_loss 611.25\n",
            "iteration 3635, epoch 8, batch 68/481,disc_loss 75.629, (real 78.937, fake 72.321 ) gen_loss 659.74\n",
            "iteration 3636, epoch 8, batch 69/481,disc_loss 74.569, (real 77.85, fake 71.288 ) gen_loss 595.96\n",
            "iteration 3637, epoch 8, batch 70/481,disc_loss 78.264, (real 80.947, fake 75.581 ) gen_loss 619.28\n",
            "iteration 3638, epoch 8, batch 71/481,disc_loss 80.636, (real 82.721, fake 78.551 ) gen_loss 558.59\n",
            "iteration 3639, epoch 8, batch 72/481,disc_loss 80.037, (real 82.909, fake 77.164 ) gen_loss 550.36\n",
            "iteration 3640, epoch 8, batch 73/481,disc_loss 80.183, (real 83.203, fake 77.162 ) gen_loss 536.92\n",
            "iteration 3641, epoch 8, batch 74/481,disc_loss 77.063, (real 79.245, fake 74.881 ) gen_loss 589.07\n",
            "iteration 3642, epoch 8, batch 75/481,disc_loss 75.438, (real 78.263, fake 72.614 ) gen_loss 573.75\n",
            "iteration 3643, epoch 8, batch 76/481,disc_loss 75.997, (real 80.512, fake 71.482 ) gen_loss 592.24\n",
            "iteration 3644, epoch 8, batch 77/481,disc_loss 75.167, (real 78.016, fake 72.317 ) gen_loss 626.47\n",
            "iteration 3645, epoch 8, batch 78/481,disc_loss 77.972, (real 81.382, fake 74.562 ) gen_loss 583.22\n",
            "iteration 3646, epoch 8, batch 79/481,disc_loss 77.064, (real 78.936, fake 75.193 ) gen_loss 714.75\n",
            "iteration 3647, epoch 8, batch 80/481,disc_loss 75.245, (real 77.828, fake 72.661 ) gen_loss 664.46\n",
            "iteration 3648, epoch 8, batch 81/481,disc_loss 80.918, (real 83.126, fake 78.711 ) gen_loss 671.44\n",
            "iteration 3649, epoch 8, batch 82/481,disc_loss 75.507, (real 79.251, fake 71.763 ) gen_loss 639.41\n",
            "iteration 3650, epoch 8, batch 83/481,disc_loss 78.317, (real 81.115, fake 75.52 ) gen_loss 655.8\n",
            "iteration 3651, epoch 8, batch 84/481,disc_loss 81.524, (real 86.414, fake 76.634 ) gen_loss 566.05\n",
            "iteration 3652, epoch 8, batch 85/481,disc_loss 77.412, (real 80.532, fake 74.293 ) gen_loss 541.94\n",
            "iteration 3653, epoch 8, batch 86/481,disc_loss 78.971, (real 80.431, fake 77.51 ) gen_loss 558.37\n",
            "iteration 3654, epoch 8, batch 87/481,disc_loss 76.78, (real 79.764, fake 73.797 ) gen_loss 563.65\n",
            "iteration 3655, epoch 8, batch 88/481,disc_loss 78.248, (real 80.816, fake 75.679 ) gen_loss 616.46\n",
            "iteration 3656, epoch 8, batch 89/481,disc_loss 77.093, (real 79.997, fake 74.189 ) gen_loss 612.76\n",
            "iteration 3657, epoch 8, batch 90/481,disc_loss 77.883, (real 80.771, fake 74.995 ) gen_loss 621.97\n",
            "iteration 3658, epoch 8, batch 91/481,disc_loss 75.826, (real 78.539, fake 73.113 ) gen_loss 613.45\n",
            "iteration 3659, epoch 8, batch 92/481,disc_loss 82.328, (real 84.416, fake 80.239 ) gen_loss 541.45\n",
            "iteration 3660, epoch 8, batch 93/481,disc_loss 79.393, (real 82.506, fake 76.279 ) gen_loss 549.7\n",
            "iteration 3661, epoch 8, batch 94/481,disc_loss 81.775, (real 84.789, fake 78.762 ) gen_loss 555.88\n",
            "iteration 3662, epoch 8, batch 95/481,disc_loss 76.636, (real 80.227, fake 73.045 ) gen_loss 657.24\n",
            "iteration 3663, epoch 8, batch 96/481,disc_loss 78.339, (real 81.172, fake 75.506 ) gen_loss 684.52\n",
            "iteration 3664, epoch 8, batch 97/481,disc_loss 73.95, (real 76.766, fake 71.133 ) gen_loss 554.16\n",
            "iteration 3665, epoch 8, batch 98/481,disc_loss 79.435, (real 82.47, fake 76.401 ) gen_loss 573.25\n",
            "iteration 3666, epoch 8, batch 99/481,disc_loss 81.189, (real 84.568, fake 77.811 ) gen_loss 552.16\n",
            "iteration 3667, epoch 8, batch 100/481,disc_loss 77.354, (real 80.037, fake 74.67 ) gen_loss 545.66\n",
            "iteration 3668, epoch 8, batch 101/481,disc_loss 86.068, (real 89.064, fake 83.073 ) gen_loss 607.41\n",
            "iteration 3669, epoch 8, batch 102/481,disc_loss 84.103, (real 86.871, fake 81.334 ) gen_loss 655.87\n",
            "iteration 3670, epoch 8, batch 103/481,disc_loss 77.604, (real 81.446, fake 73.761 ) gen_loss 552.04\n",
            "iteration 3671, epoch 8, batch 104/481,disc_loss 79.284, (real 81.733, fake 76.834 ) gen_loss 561.61\n",
            "iteration 3672, epoch 8, batch 105/481,disc_loss 80.493, (real 83.516, fake 77.471 ) gen_loss 563.59\n",
            "iteration 3673, epoch 8, batch 106/481,disc_loss 73.355, (real 76.613, fake 70.097 ) gen_loss 584.27\n",
            "iteration 3674, epoch 8, batch 107/481,disc_loss 77.499, (real 80.125, fake 74.873 ) gen_loss 600.39\n",
            "iteration 3675, epoch 8, batch 108/481,disc_loss 75.395, (real 78.693, fake 72.097 ) gen_loss 606.0\n",
            "iteration 3676, epoch 8, batch 109/481,disc_loss 77.895, (real 81.014, fake 74.776 ) gen_loss 628.92\n",
            "iteration 3677, epoch 8, batch 110/481,disc_loss 76.423, (real 80.072, fake 72.774 ) gen_loss 560.62\n",
            "iteration 3678, epoch 8, batch 111/481,disc_loss 81.885, (real 84.858, fake 78.911 ) gen_loss 555.12\n",
            "iteration 3679, epoch 8, batch 112/481,disc_loss 76.796, (real 79.922, fake 73.67 ) gen_loss 640.26\n",
            "iteration 3680, epoch 8, batch 113/481,disc_loss 81.325, (real 85.909, fake 76.741 ) gen_loss 603.15\n",
            "iteration 3681, epoch 8, batch 114/481,disc_loss 75.905, (real 77.985, fake 73.825 ) gen_loss 616.53\n",
            "iteration 3682, epoch 8, batch 115/481,disc_loss 82.495, (real 85.149, fake 79.841 ) gen_loss 540.47\n",
            "iteration 3683, epoch 8, batch 116/481,disc_loss 79.267, (real 82.707, fake 75.826 ) gen_loss 600.58\n",
            "iteration 3684, epoch 8, batch 117/481,disc_loss 78.156, (real 82.009, fake 74.302 ) gen_loss 580.72\n",
            "iteration 3685, epoch 8, batch 118/481,disc_loss 79.566, (real 82.872, fake 76.261 ) gen_loss 556.05\n",
            "iteration 3686, epoch 8, batch 119/481,disc_loss 79.925, (real 83.204, fake 76.646 ) gen_loss 631.09\n",
            "iteration 3687, epoch 8, batch 120/481,disc_loss 79.875, (real 82.393, fake 77.357 ) gen_loss 633.91\n",
            "iteration 3688, epoch 8, batch 121/481,disc_loss 82.65, (real 85.532, fake 79.768 ) gen_loss 582.83\n",
            "iteration 3689, epoch 8, batch 122/481,disc_loss 78.414, (real 80.283, fake 76.545 ) gen_loss 615.05\n",
            "iteration 3690, epoch 8, batch 123/481,disc_loss 77.921, (real 80.872, fake 74.97 ) gen_loss 612.78\n",
            "iteration 3691, epoch 8, batch 124/481,disc_loss 78.96, (real 81.444, fake 76.477 ) gen_loss 571.84\n",
            "iteration 3692, epoch 8, batch 125/481,disc_loss 83.146, (real 85.852, fake 80.441 ) gen_loss 575.71\n",
            "iteration 3693, epoch 8, batch 126/481,disc_loss 82.008, (real 85.633, fake 78.384 ) gen_loss 582.34\n",
            "iteration 3694, epoch 8, batch 127/481,disc_loss 78.582, (real 80.762, fake 76.402 ) gen_loss 619.34\n",
            "iteration 3695, epoch 8, batch 128/481,disc_loss 75.968, (real 78.608, fake 73.328 ) gen_loss 563.38\n",
            "iteration 3696, epoch 8, batch 129/481,disc_loss 76.024, (real 78.681, fake 73.367 ) gen_loss 620.48\n",
            "iteration 3697, epoch 8, batch 130/481,disc_loss 73.239, (real 75.383, fake 71.094 ) gen_loss 570.94\n",
            "iteration 3698, epoch 8, batch 131/481,disc_loss 83.227, (real 86.706, fake 79.748 ) gen_loss 597.41\n",
            "iteration 3699, epoch 8, batch 132/481,disc_loss 80.026, (real 82.886, fake 77.166 ) gen_loss 535.63\n",
            "iteration 3700, epoch 8, batch 133/481,disc_loss 79.143, (real 81.673, fake 76.613 ) gen_loss 508.47\n",
            "iteration 3701, epoch 8, batch 134/481,disc_loss 79.188, (real 83.043, fake 75.333 ) gen_loss 567.68\n",
            "iteration 3702, epoch 8, batch 135/481,disc_loss 76.649, (real 80.036, fake 73.263 ) gen_loss 566.97\n",
            "iteration 3703, epoch 8, batch 136/481,disc_loss 77.302, (real 80.671, fake 73.934 ) gen_loss 717.31\n",
            "iteration 3704, epoch 8, batch 137/481,disc_loss 76.443, (real 79.927, fake 72.959 ) gen_loss 719.02\n",
            "iteration 3705, epoch 8, batch 138/481,disc_loss 79.336, (real 82.164, fake 76.507 ) gen_loss 675.74\n",
            "iteration 3706, epoch 8, batch 139/481,disc_loss 79.951, (real 82.524, fake 77.377 ) gen_loss 624.36\n",
            "iteration 3707, epoch 8, batch 140/481,disc_loss 80.515, (real 84.913, fake 76.116 ) gen_loss 637.04\n",
            "iteration 3708, epoch 8, batch 141/481,disc_loss 77.214, (real 81.221, fake 73.207 ) gen_loss 623.08\n",
            "iteration 3709, epoch 8, batch 142/481,disc_loss 77.331, (real 80.552, fake 74.11 ) gen_loss 588.13\n",
            "iteration 3710, epoch 8, batch 143/481,disc_loss 78.814, (real 81.919, fake 75.708 ) gen_loss 666.89\n",
            "iteration 3711, epoch 8, batch 144/481,disc_loss 82.756, (real 85.571, fake 79.941 ) gen_loss 666.61\n",
            "iteration 3712, epoch 8, batch 145/481,disc_loss 79.957, (real 82.487, fake 77.428 ) gen_loss 639.38\n",
            "iteration 3713, epoch 8, batch 146/481,disc_loss 75.964, (real 78.733, fake 73.196 ) gen_loss 624.51\n",
            "iteration 3714, epoch 8, batch 147/481,disc_loss 77.735, (real 80.001, fake 75.468 ) gen_loss 655.23\n",
            "iteration 3715, epoch 8, batch 148/481,disc_loss 75.905, (real 78.575, fake 73.235 ) gen_loss 606.93\n",
            "iteration 3716, epoch 8, batch 149/481,disc_loss 80.63, (real 84.034, fake 77.225 ) gen_loss 612.28\n",
            "iteration 3717, epoch 8, batch 150/481,disc_loss 76.166, (real 78.59, fake 73.742 ) gen_loss 588.59\n",
            "iteration 3718, epoch 8, batch 151/481,disc_loss 78.936, (real 82.381, fake 75.49 ) gen_loss 583.92\n",
            "iteration 3719, epoch 8, batch 152/481,disc_loss 76.885, (real 79.903, fake 73.868 ) gen_loss 595.5\n",
            "iteration 3720, epoch 8, batch 153/481,disc_loss 77.434, (real 80.199, fake 74.668 ) gen_loss 617.56\n",
            "iteration 3721, epoch 8, batch 154/481,disc_loss 77.981, (real 80.218, fake 75.744 ) gen_loss 593.17\n",
            "iteration 3722, epoch 8, batch 155/481,disc_loss 79.395, (real 83.165, fake 75.625 ) gen_loss 596.74\n",
            "iteration 3723, epoch 8, batch 156/481,disc_loss 77.29, (real 79.592, fake 74.988 ) gen_loss 597.4\n",
            "iteration 3724, epoch 8, batch 157/481,disc_loss 77.702, (real 82.365, fake 73.038 ) gen_loss 574.25\n",
            "iteration 3725, epoch 8, batch 158/481,disc_loss 81.094, (real 84.619, fake 77.569 ) gen_loss 595.88\n",
            "iteration 3726, epoch 8, batch 159/481,disc_loss 77.736, (real 81.215, fake 74.258 ) gen_loss 608.4\n",
            "iteration 3727, epoch 8, batch 160/481,disc_loss 77.943, (real 81.743, fake 74.143 ) gen_loss 555.4\n",
            "iteration 3728, epoch 8, batch 161/481,disc_loss 77.766, (real 80.474, fake 75.058 ) gen_loss 610.36\n",
            "iteration 3729, epoch 8, batch 162/481,disc_loss 79.173, (real 82.097, fake 76.25 ) gen_loss 760.4\n",
            "iteration 3730, epoch 8, batch 163/481,disc_loss 76.442, (real 79.951, fake 72.934 ) gen_loss 683.64\n",
            "iteration 3731, epoch 8, batch 164/481,disc_loss 74.162, (real 78.09, fake 70.234 ) gen_loss 666.03\n",
            "iteration 3732, epoch 8, batch 165/481,disc_loss 75.599, (real 78.813, fake 72.386 ) gen_loss 626.29\n",
            "iteration 3733, epoch 8, batch 166/481,disc_loss 75.549, (real 78.676, fake 72.422 ) gen_loss 673.59\n",
            "iteration 3734, epoch 8, batch 167/481,disc_loss 72.701, (real 75.078, fake 70.324 ) gen_loss 636.95\n",
            "iteration 3735, epoch 8, batch 168/481,disc_loss 80.717, (real 83.238, fake 78.197 ) gen_loss 552.73\n",
            "iteration 3736, epoch 8, batch 169/481,disc_loss 76.231, (real 79.702, fake 72.761 ) gen_loss 629.65\n",
            "iteration 3737, epoch 8, batch 170/481,disc_loss 72.077, (real 74.041, fake 70.113 ) gen_loss 707.22\n",
            "iteration 3738, epoch 8, batch 171/481,disc_loss 78.896, (real 80.951, fake 76.841 ) gen_loss 636.15\n",
            "iteration 3739, epoch 8, batch 172/481,disc_loss 79.385, (real 81.522, fake 77.249 ) gen_loss 580.12\n",
            "iteration 3740, epoch 8, batch 173/481,disc_loss 78.56, (real 81.574, fake 75.545 ) gen_loss 664.18\n",
            "iteration 3741, epoch 8, batch 174/481,disc_loss 78.162, (real 80.913, fake 75.411 ) gen_loss 610.49\n",
            "iteration 3742, epoch 8, batch 175/481,disc_loss 78.556, (real 81.558, fake 75.555 ) gen_loss 610.13\n",
            "iteration 3743, epoch 8, batch 176/481,disc_loss 79.146, (real 81.392, fake 76.901 ) gen_loss 520.08\n",
            "iteration 3744, epoch 8, batch 177/481,disc_loss 76.898, (real 79.288, fake 74.508 ) gen_loss 610.44\n",
            "iteration 3745, epoch 8, batch 178/481,disc_loss 75.682, (real 79.241, fake 72.124 ) gen_loss 590.54\n",
            "iteration 3746, epoch 8, batch 179/481,disc_loss 77.112, (real 80.095, fake 74.13 ) gen_loss 649.86\n",
            "iteration 3747, epoch 8, batch 180/481,disc_loss 75.451, (real 78.606, fake 72.297 ) gen_loss 690.95\n",
            "iteration 3748, epoch 8, batch 181/481,disc_loss 78.377, (real 81.297, fake 75.458 ) gen_loss 659.83\n",
            "iteration 3749, epoch 8, batch 182/481,disc_loss 79.729, (real 83.345, fake 76.113 ) gen_loss 579.52\n",
            "iteration 3750, epoch 8, batch 183/481,disc_loss 78.3, (real 82.754, fake 73.847 ) gen_loss 609.82\n",
            "iteration 3751, epoch 8, batch 184/481,disc_loss 80.424, (real 83.985, fake 76.863 ) gen_loss 615.72\n",
            "iteration 3752, epoch 8, batch 185/481,disc_loss 81.076, (real 84.219, fake 77.932 ) gen_loss 630.6\n",
            "iteration 3753, epoch 8, batch 186/481,disc_loss 80.646, (real 82.745, fake 78.546 ) gen_loss 724.02\n",
            "iteration 3754, epoch 8, batch 187/481,disc_loss 79.294, (real 81.852, fake 76.735 ) gen_loss 634.83\n",
            "iteration 3755, epoch 8, batch 188/481,disc_loss 79.236, (real 82.188, fake 76.285 ) gen_loss 659.87\n",
            "iteration 3756, epoch 8, batch 189/481,disc_loss 83.446, (real 86.742, fake 80.151 ) gen_loss 603.64\n",
            "iteration 3757, epoch 8, batch 190/481,disc_loss 79.333, (real 82.746, fake 75.919 ) gen_loss 627.95\n",
            "iteration 3758, epoch 8, batch 191/481,disc_loss 80.352, (real 82.626, fake 78.078 ) gen_loss 614.51\n",
            "iteration 3759, epoch 8, batch 192/481,disc_loss 77.812, (real 80.592, fake 75.033 ) gen_loss 651.53\n",
            "iteration 3760, epoch 8, batch 193/481,disc_loss 77.243, (real 80.053, fake 74.434 ) gen_loss 680.71\n",
            "iteration 3761, epoch 8, batch 194/481,disc_loss 83.265, (real 86.836, fake 79.694 ) gen_loss 723.75\n",
            "iteration 3762, epoch 8, batch 195/481,disc_loss 75.775, (real 78.265, fake 73.285 ) gen_loss 639.47\n",
            "iteration 3763, epoch 8, batch 196/481,disc_loss 80.088, (real 83.295, fake 76.882 ) gen_loss 621.25\n",
            "iteration 3764, epoch 8, batch 197/481,disc_loss 77.518, (real 80.706, fake 74.331 ) gen_loss 605.99\n",
            "iteration 3765, epoch 8, batch 198/481,disc_loss 81.702, (real 84.849, fake 78.554 ) gen_loss 653.42\n",
            "iteration 3766, epoch 8, batch 199/481,disc_loss 76.297, (real 78.881, fake 73.713 ) gen_loss 505.27\n",
            "iteration 3767, epoch 8, batch 200/481,disc_loss 81.165, (real 84.062, fake 78.268 ) gen_loss 614.19\n",
            "iteration 3768, epoch 8, batch 201/481,disc_loss 77.385, (real 81.234, fake 73.536 ) gen_loss 706.61\n",
            "iteration 3769, epoch 8, batch 202/481,disc_loss 79.729, (real 82.124, fake 77.333 ) gen_loss 574.81\n",
            "iteration 3770, epoch 8, batch 203/481,disc_loss 79.566, (real 82.002, fake 77.131 ) gen_loss 709.16\n",
            "iteration 3771, epoch 8, batch 204/481,disc_loss 77.635, (real 81.244, fake 74.026 ) gen_loss 668.5\n",
            "iteration 3772, epoch 8, batch 205/481,disc_loss 78.056, (real 81.738, fake 74.374 ) gen_loss 627.4\n",
            "iteration 3773, epoch 8, batch 206/481,disc_loss 81.405, (real 84.212, fake 78.599 ) gen_loss 573.13\n",
            "iteration 3774, epoch 8, batch 207/481,disc_loss 78.369, (real 81.529, fake 75.209 ) gen_loss 568.07\n",
            "iteration 3775, epoch 8, batch 208/481,disc_loss 81.852, (real 85.006, fake 78.697 ) gen_loss 638.6\n",
            "iteration 3776, epoch 8, batch 209/481,disc_loss 76.803, (real 79.942, fake 73.664 ) gen_loss 587.13\n",
            "iteration 3777, epoch 8, batch 210/481,disc_loss 76.598, (real 79.255, fake 73.94 ) gen_loss 667.95\n",
            "iteration 3778, epoch 8, batch 211/481,disc_loss 78.372, (real 81.187, fake 75.558 ) gen_loss 649.55\n",
            "iteration 3779, epoch 8, batch 212/481,disc_loss 82.193, (real 84.715, fake 79.672 ) gen_loss 665.15\n",
            "iteration 3780, epoch 8, batch 213/481,disc_loss 75.539, (real 78.835, fake 72.244 ) gen_loss 579.59\n",
            "iteration 3781, epoch 8, batch 214/481,disc_loss 79.524, (real 82.894, fake 76.155 ) gen_loss 647.28\n",
            "iteration 3782, epoch 8, batch 215/481,disc_loss 77.129, (real 80.158, fake 74.101 ) gen_loss 771.68\n",
            "iteration 3783, epoch 8, batch 216/481,disc_loss 77.105, (real 79.739, fake 74.471 ) gen_loss 557.62\n",
            "iteration 3784, epoch 8, batch 217/481,disc_loss 74.577, (real 77.887, fake 71.266 ) gen_loss 644.88\n",
            "iteration 3785, epoch 8, batch 218/481,disc_loss 78.597, (real 81.88, fake 75.314 ) gen_loss 804.89\n",
            "iteration 3786, epoch 8, batch 219/481,disc_loss 81.865, (real 85.722, fake 78.009 ) gen_loss 668.7\n",
            "iteration 3787, epoch 8, batch 220/481,disc_loss 79.024, (real 81.157, fake 76.89 ) gen_loss 628.41\n",
            "iteration 3788, epoch 8, batch 221/481,disc_loss 83.038, (real 86.494, fake 79.582 ) gen_loss 579.02\n",
            "iteration 3789, epoch 8, batch 222/481,disc_loss 77.609, (real 81.162, fake 74.056 ) gen_loss 595.97\n",
            "iteration 3790, epoch 8, batch 223/481,disc_loss 74.022, (real 76.874, fake 71.171 ) gen_loss 608.73\n",
            "iteration 3791, epoch 8, batch 224/481,disc_loss 85.54, (real 88.727, fake 82.354 ) gen_loss 669.1\n",
            "iteration 3792, epoch 8, batch 225/481,disc_loss 78.822, (real 81.74, fake 75.903 ) gen_loss 635.65\n",
            "iteration 3793, epoch 8, batch 226/481,disc_loss 79.492, (real 82.743, fake 76.241 ) gen_loss 576.62\n",
            "iteration 3794, epoch 8, batch 227/481,disc_loss 81.559, (real 84.125, fake 78.994 ) gen_loss 665.62\n",
            "iteration 3795, epoch 8, batch 228/481,disc_loss 76.848, (real 79.871, fake 73.825 ) gen_loss 617.78\n",
            "iteration 3796, epoch 8, batch 229/481,disc_loss 76.978, (real 79.625, fake 74.331 ) gen_loss 668.4\n",
            "iteration 3797, epoch 8, batch 230/481,disc_loss 78.263, (real 81.007, fake 75.519 ) gen_loss 676.15\n",
            "iteration 3798, epoch 8, batch 231/481,disc_loss 73.555, (real 75.771, fake 71.339 ) gen_loss 645.66\n",
            "iteration 3799, epoch 8, batch 232/481,disc_loss 77.952, (real 80.587, fake 75.316 ) gen_loss 642.61\n",
            "iteration 3800, epoch 8, batch 233/481,disc_loss 79.08, (real 82.411, fake 75.75 ) gen_loss 720.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 3801, epoch 8, batch 234/481,disc_loss 81.688, (real 83.52, fake 79.857 ) gen_loss 610.41\n",
            "iteration 3802, epoch 8, batch 235/481,disc_loss 77.231, (real 80.815, fake 73.647 ) gen_loss 687.24\n",
            "iteration 3803, epoch 8, batch 236/481,disc_loss 79.063, (real 82.578, fake 75.549 ) gen_loss 580.77\n",
            "iteration 3804, epoch 8, batch 237/481,disc_loss 79.388, (real 83.102, fake 75.674 ) gen_loss 659.11\n",
            "iteration 3805, epoch 8, batch 238/481,disc_loss 80.065, (real 83.17, fake 76.96 ) gen_loss 601.58\n",
            "iteration 3806, epoch 8, batch 239/481,disc_loss 76.407, (real 79.901, fake 72.914 ) gen_loss 646.85\n",
            "iteration 3807, epoch 8, batch 240/481,disc_loss 71.365, (real 73.176, fake 69.555 ) gen_loss 612.63\n",
            "iteration 3808, epoch 8, batch 241/481,disc_loss 82.868, (real 85.973, fake 79.763 ) gen_loss 649.55\n",
            "iteration 3809, epoch 8, batch 242/481,disc_loss 79.908, (real 82.562, fake 77.255 ) gen_loss 581.73\n",
            "iteration 3810, epoch 8, batch 243/481,disc_loss 81.184, (real 83.97, fake 78.397 ) gen_loss 589.12\n",
            "iteration 3811, epoch 8, batch 244/481,disc_loss 80.297, (real 83.481, fake 77.114 ) gen_loss 705.04\n",
            "iteration 3812, epoch 8, batch 245/481,disc_loss 78.547, (real 80.862, fake 76.231 ) gen_loss 758.78\n",
            "iteration 3813, epoch 8, batch 246/481,disc_loss 82.755, (real 84.796, fake 80.714 ) gen_loss 714.2\n",
            "iteration 3814, epoch 8, batch 247/481,disc_loss 77.67, (real 81.044, fake 74.297 ) gen_loss 683.66\n",
            "iteration 3815, epoch 8, batch 248/481,disc_loss 79.899, (real 81.931, fake 77.866 ) gen_loss 574.22\n",
            "iteration 3816, epoch 8, batch 249/481,disc_loss 78.261, (real 81.041, fake 75.481 ) gen_loss 566.46\n",
            "iteration 3817, epoch 8, batch 250/481,disc_loss 75.157, (real 77.916, fake 72.397 ) gen_loss 588.82\n",
            "iteration 3818, epoch 8, batch 251/481,disc_loss 82.105, (real 84.674, fake 79.537 ) gen_loss 576.39\n",
            "iteration 3819, epoch 8, batch 252/481,disc_loss 77.336, (real 79.517, fake 75.155 ) gen_loss 651.66\n",
            "iteration 3820, epoch 8, batch 253/481,disc_loss 75.281, (real 78.45, fake 72.111 ) gen_loss 634.0\n",
            "iteration 3821, epoch 8, batch 254/481,disc_loss 82.862, (real 86.118, fake 79.606 ) gen_loss 643.45\n",
            "iteration 3822, epoch 8, batch 255/481,disc_loss 78.82, (real 81.835, fake 75.805 ) gen_loss 574.24\n",
            "iteration 3823, epoch 8, batch 256/481,disc_loss 79.67, (real 83.131, fake 76.209 ) gen_loss 632.28\n",
            "iteration 3824, epoch 8, batch 257/481,disc_loss 81.913, (real 85.109, fake 78.717 ) gen_loss 638.82\n",
            "iteration 3825, epoch 8, batch 258/481,disc_loss 77.81, (real 81.72, fake 73.899 ) gen_loss 674.71\n",
            "iteration 3826, epoch 8, batch 259/481,disc_loss 78.747, (real 81.617, fake 75.877 ) gen_loss 592.58\n",
            "iteration 3827, epoch 8, batch 260/481,disc_loss 81.294, (real 85.272, fake 77.316 ) gen_loss 602.64\n",
            "iteration 3828, epoch 8, batch 261/481,disc_loss 80.322, (real 83.861, fake 76.783 ) gen_loss 640.81\n",
            "iteration 3829, epoch 8, batch 262/481,disc_loss 78.386, (real 80.87, fake 75.901 ) gen_loss 696.83\n",
            "iteration 3830, epoch 8, batch 263/481,disc_loss 77.769, (real 80.083, fake 75.455 ) gen_loss 614.01\n",
            "iteration 3831, epoch 8, batch 264/481,disc_loss 79.348, (real 81.969, fake 76.726 ) gen_loss 589.95\n",
            "iteration 3832, epoch 8, batch 265/481,disc_loss 80.21, (real 83.361, fake 77.059 ) gen_loss 635.19\n",
            "iteration 3833, epoch 8, batch 266/481,disc_loss 77.378, (real 80.639, fake 74.117 ) gen_loss 565.3\n",
            "iteration 3834, epoch 8, batch 267/481,disc_loss 78.882, (real 81.722, fake 76.042 ) gen_loss 641.24\n",
            "iteration 3835, epoch 8, batch 268/481,disc_loss 76.127, (real 78.913, fake 73.34 ) gen_loss 665.27\n",
            "iteration 3836, epoch 8, batch 269/481,disc_loss 78.099, (real 80.287, fake 75.911 ) gen_loss 586.54\n",
            "iteration 3837, epoch 8, batch 270/481,disc_loss 78.104, (real 80.999, fake 75.21 ) gen_loss 594.13\n",
            "iteration 3838, epoch 8, batch 271/481,disc_loss 75.818, (real 79.612, fake 72.024 ) gen_loss 664.83\n",
            "iteration 3839, epoch 8, batch 272/481,disc_loss 81.025, (real 84.211, fake 77.838 ) gen_loss 621.79\n",
            "iteration 3840, epoch 8, batch 273/481,disc_loss 75.653, (real 78.928, fake 72.377 ) gen_loss 600.65\n",
            "iteration 3841, epoch 8, batch 274/481,disc_loss 82.075, (real 85.737, fake 78.413 ) gen_loss 610.59\n",
            "iteration 3842, epoch 8, batch 275/481,disc_loss 77.822, (real 80.774, fake 74.87 ) gen_loss 586.7\n",
            "iteration 3843, epoch 8, batch 276/481,disc_loss 80.175, (real 82.698, fake 77.652 ) gen_loss 706.84\n",
            "iteration 3844, epoch 8, batch 277/481,disc_loss 75.574, (real 78.25, fake 72.898 ) gen_loss 703.84\n",
            "iteration 3845, epoch 8, batch 278/481,disc_loss 74.619, (real 78.277, fake 70.961 ) gen_loss 626.96\n",
            "iteration 3846, epoch 8, batch 279/481,disc_loss 78.037, (real 81.003, fake 75.072 ) gen_loss 654.83\n",
            "iteration 3847, epoch 8, batch 280/481,disc_loss 77.969, (real 80.637, fake 75.302 ) gen_loss 597.85\n",
            "iteration 3848, epoch 8, batch 281/481,disc_loss 79.945, (real 83.834, fake 76.056 ) gen_loss 695.05\n",
            "iteration 3849, epoch 8, batch 282/481,disc_loss 77.679, (real 80.023, fake 75.335 ) gen_loss 576.25\n",
            "iteration 3850, epoch 8, batch 283/481,disc_loss 81.353, (real 83.987, fake 78.719 ) gen_loss 676.04\n",
            "iteration 3851, epoch 8, batch 284/481,disc_loss 80.152, (real 83.622, fake 76.682 ) gen_loss 655.45\n",
            "iteration 3852, epoch 8, batch 285/481,disc_loss 76.306, (real 79.135, fake 73.478 ) gen_loss 704.43\n",
            "iteration 3853, epoch 8, batch 286/481,disc_loss 77.411, (real 80.083, fake 74.739 ) gen_loss 611.34\n",
            "iteration 3854, epoch 8, batch 287/481,disc_loss 76.147, (real 78.264, fake 74.03 ) gen_loss 595.81\n",
            "iteration 3855, epoch 8, batch 288/481,disc_loss 77.622, (real 80.328, fake 74.916 ) gen_loss 707.74\n",
            "iteration 3856, epoch 8, batch 289/481,disc_loss 78.695, (real 81.838, fake 75.552 ) gen_loss 735.71\n",
            "iteration 3857, epoch 8, batch 290/481,disc_loss 76.117, (real 79.207, fake 73.028 ) gen_loss 592.44\n",
            "iteration 3858, epoch 8, batch 291/481,disc_loss 75.595, (real 78.118, fake 73.073 ) gen_loss 625.91\n",
            "iteration 3859, epoch 8, batch 292/481,disc_loss 76.097, (real 78.384, fake 73.811 ) gen_loss 586.11\n",
            "iteration 3860, epoch 8, batch 293/481,disc_loss 74.863, (real 77.879, fake 71.847 ) gen_loss 620.29\n",
            "iteration 3861, epoch 8, batch 294/481,disc_loss 77.021, (real 79.911, fake 74.131 ) gen_loss 581.76\n",
            "iteration 3862, epoch 8, batch 295/481,disc_loss 77.837, (real 81.24, fake 74.433 ) gen_loss 557.66\n",
            "iteration 3863, epoch 8, batch 296/481,disc_loss 75.448, (real 78.014, fake 72.881 ) gen_loss 601.73\n",
            "iteration 3864, epoch 8, batch 297/481,disc_loss 80.735, (real 83.612, fake 77.859 ) gen_loss 709.02\n",
            "iteration 3865, epoch 8, batch 298/481,disc_loss 77.369, (real 80.861, fake 73.878 ) gen_loss 634.47\n",
            "iteration 3866, epoch 8, batch 299/481,disc_loss 80.615, (real 83.169, fake 78.061 ) gen_loss 657.9\n",
            "iteration 3867, epoch 8, batch 300/481,disc_loss 80.766, (real 84.444, fake 77.088 ) gen_loss 592.35\n",
            "iteration 3868, epoch 8, batch 301/481,disc_loss 76.678, (real 79.7, fake 73.656 ) gen_loss 646.6\n",
            "iteration 3869, epoch 8, batch 302/481,disc_loss 77.241, (real 79.981, fake 74.5 ) gen_loss 646.79\n",
            "iteration 3870, epoch 8, batch 303/481,disc_loss 78.544, (real 81.645, fake 75.442 ) gen_loss 635.24\n",
            "iteration 3871, epoch 8, batch 304/481,disc_loss 75.839, (real 78.662, fake 73.016 ) gen_loss 687.79\n",
            "iteration 3872, epoch 8, batch 305/481,disc_loss 75.658, (real 78.977, fake 72.339 ) gen_loss 691.32\n",
            "iteration 3873, epoch 8, batch 306/481,disc_loss 78.081, (real 82.064, fake 74.097 ) gen_loss 696.27\n",
            "iteration 3874, epoch 8, batch 307/481,disc_loss 76.452, (real 78.761, fake 74.144 ) gen_loss 658.82\n",
            "iteration 3875, epoch 8, batch 308/481,disc_loss 80.186, (real 83.292, fake 77.081 ) gen_loss 638.02\n",
            "iteration 3876, epoch 8, batch 309/481,disc_loss 75.211, (real 78.155, fake 72.268 ) gen_loss 567.14\n",
            "iteration 3877, epoch 8, batch 310/481,disc_loss 79.864, (real 81.837, fake 77.891 ) gen_loss 614.6\n",
            "iteration 3878, epoch 8, batch 311/481,disc_loss 80.407, (real 84.282, fake 76.532 ) gen_loss 617.32\n",
            "iteration 3879, epoch 8, batch 312/481,disc_loss 75.944, (real 78.487, fake 73.401 ) gen_loss 524.19\n",
            "iteration 3880, epoch 8, batch 313/481,disc_loss 71.526, (real 74.205, fake 68.847 ) gen_loss 597.86\n",
            "iteration 3881, epoch 8, batch 314/481,disc_loss 73.776, (real 77.861, fake 69.691 ) gen_loss 602.55\n",
            "iteration 3882, epoch 8, batch 315/481,disc_loss 80.413, (real 83.83, fake 76.995 ) gen_loss 685.97\n",
            "iteration 3883, epoch 8, batch 316/481,disc_loss 74.983, (real 77.277, fake 72.688 ) gen_loss 788.64\n",
            "iteration 3884, epoch 8, batch 317/481,disc_loss 77.925, (real 80.001, fake 75.848 ) gen_loss 587.76\n",
            "iteration 3885, epoch 8, batch 318/481,disc_loss 76.925, (real 80.268, fake 73.582 ) gen_loss 696.72\n",
            "iteration 3886, epoch 8, batch 319/481,disc_loss 74.092, (real 77.678, fake 70.507 ) gen_loss 675.73\n",
            "iteration 3887, epoch 8, batch 320/481,disc_loss 79.648, (real 82.37, fake 76.927 ) gen_loss 621.68\n",
            "iteration 3888, epoch 8, batch 321/481,disc_loss 81.793, (real 84.164, fake 79.423 ) gen_loss 617.67\n",
            "iteration 3889, epoch 8, batch 322/481,disc_loss 76.62, (real 79.066, fake 74.174 ) gen_loss 663.73\n",
            "iteration 3890, epoch 8, batch 323/481,disc_loss 82.108, (real 85.804, fake 78.411 ) gen_loss 669.3\n",
            "iteration 3891, epoch 8, batch 324/481,disc_loss 75.099, (real 78.664, fake 71.534 ) gen_loss 716.15\n",
            "iteration 3892, epoch 8, batch 325/481,disc_loss 80.444, (real 83.158, fake 77.73 ) gen_loss 686.48\n",
            "iteration 3893, epoch 8, batch 326/481,disc_loss 82.575, (real 85.347, fake 79.803 ) gen_loss 631.23\n",
            "iteration 3894, epoch 8, batch 327/481,disc_loss 78.961, (real 81.546, fake 76.377 ) gen_loss 554.18\n",
            "iteration 3895, epoch 8, batch 328/481,disc_loss 79.131, (real 81.587, fake 76.675 ) gen_loss 603.71\n",
            "iteration 3896, epoch 8, batch 329/481,disc_loss 79.648, (real 82.382, fake 76.914 ) gen_loss 669.9\n",
            "iteration 3897, epoch 8, batch 330/481,disc_loss 79.355, (real 81.792, fake 76.919 ) gen_loss 588.64\n",
            "iteration 3898, epoch 8, batch 331/481,disc_loss 78.255, (real 81.473, fake 75.037 ) gen_loss 648.43\n",
            "iteration 3899, epoch 8, batch 332/481,disc_loss 80.074, (real 83.787, fake 76.362 ) gen_loss 602.9\n",
            "iteration 3900, epoch 8, batch 333/481,disc_loss 81.102, (real 83.849, fake 78.355 ) gen_loss 708.48\n",
            "iteration 3901, epoch 8, batch 334/481,disc_loss 79.886, (real 83.59, fake 76.182 ) gen_loss 635.58\n",
            "iteration 3902, epoch 8, batch 335/481,disc_loss 77.735, (real 80.738, fake 74.733 ) gen_loss 592.1\n",
            "iteration 3903, epoch 8, batch 336/481,disc_loss 80.187, (real 83.787, fake 76.586 ) gen_loss 671.89\n",
            "iteration 3904, epoch 8, batch 337/481,disc_loss 80.893, (real 84.458, fake 77.328 ) gen_loss 596.32\n",
            "iteration 3905, epoch 8, batch 338/481,disc_loss 76.552, (real 79.026, fake 74.078 ) gen_loss 584.48\n",
            "iteration 3906, epoch 8, batch 339/481,disc_loss 79.584, (real 83.362, fake 75.806 ) gen_loss 632.97\n",
            "iteration 3907, epoch 8, batch 340/481,disc_loss 78.808, (real 80.993, fake 76.623 ) gen_loss 574.15\n",
            "iteration 3908, epoch 8, batch 341/481,disc_loss 81.889, (real 84.138, fake 79.639 ) gen_loss 569.41\n",
            "iteration 3909, epoch 8, batch 342/481,disc_loss 79.362, (real 82.072, fake 76.652 ) gen_loss 587.17\n",
            "iteration 3910, epoch 8, batch 343/481,disc_loss 77.629, (real 80.497, fake 74.762 ) gen_loss 625.65\n",
            "iteration 3911, epoch 8, batch 344/481,disc_loss 77.828, (real 81.529, fake 74.128 ) gen_loss 667.99\n",
            "iteration 3912, epoch 8, batch 345/481,disc_loss 78.312, (real 81.278, fake 75.345 ) gen_loss 609.91\n",
            "iteration 3913, epoch 8, batch 346/481,disc_loss 76.693, (real 79.82, fake 73.567 ) gen_loss 641.84\n",
            "iteration 3914, epoch 8, batch 347/481,disc_loss 75.315, (real 78.458, fake 72.172 ) gen_loss 603.56\n",
            "iteration 3915, epoch 8, batch 348/481,disc_loss 76.332, (real 79.419, fake 73.244 ) gen_loss 664.25\n",
            "iteration 3916, epoch 8, batch 349/481,disc_loss 83.699, (real 85.816, fake 81.583 ) gen_loss 662.19\n",
            "iteration 3917, epoch 8, batch 350/481,disc_loss 79.13, (real 81.6, fake 76.66 ) gen_loss 636.29\n",
            "iteration 3918, epoch 8, batch 351/481,disc_loss 76.794, (real 80.326, fake 73.263 ) gen_loss 634.31\n",
            "iteration 3919, epoch 8, batch 352/481,disc_loss 77.223, (real 80.278, fake 74.168 ) gen_loss 671.39\n",
            "iteration 3920, epoch 8, batch 353/481,disc_loss 75.025, (real 77.573, fake 72.478 ) gen_loss 595.2\n",
            "iteration 3921, epoch 8, batch 354/481,disc_loss 77.908, (real 81.322, fake 74.494 ) gen_loss 662.03\n",
            "iteration 3922, epoch 8, batch 355/481,disc_loss 78.293, (real 81.837, fake 74.749 ) gen_loss 597.08\n",
            "iteration 3923, epoch 8, batch 356/481,disc_loss 78.664, (real 81.421, fake 75.906 ) gen_loss 638.99\n",
            "iteration 3924, epoch 8, batch 357/481,disc_loss 80.18, (real 83.393, fake 76.968 ) gen_loss 660.41\n",
            "iteration 3925, epoch 8, batch 358/481,disc_loss 77.345, (real 81.137, fake 73.552 ) gen_loss 694.54\n",
            "iteration 3926, epoch 8, batch 359/481,disc_loss 81.081, (real 84.082, fake 78.079 ) gen_loss 647.42\n",
            "iteration 3927, epoch 8, batch 360/481,disc_loss 75.21, (real 78.251, fake 72.17 ) gen_loss 648.1\n",
            "iteration 3928, epoch 8, batch 361/481,disc_loss 77.289, (real 81.44, fake 73.137 ) gen_loss 604.01\n",
            "iteration 3929, epoch 8, batch 362/481,disc_loss 75.013, (real 78.712, fake 71.315 ) gen_loss 685.49\n",
            "iteration 3930, epoch 8, batch 363/481,disc_loss 82.302, (real 85.124, fake 79.48 ) gen_loss 620.51\n",
            "iteration 3931, epoch 8, batch 364/481,disc_loss 80.232, (real 82.269, fake 78.195 ) gen_loss 597.62\n",
            "iteration 3932, epoch 8, batch 365/481,disc_loss 77.447, (real 81.296, fake 73.598 ) gen_loss 634.71\n",
            "iteration 3933, epoch 8, batch 366/481,disc_loss 76.635, (real 79.671, fake 73.598 ) gen_loss 601.93\n",
            "iteration 3934, epoch 8, batch 367/481,disc_loss 76.618, (real 79.526, fake 73.711 ) gen_loss 652.32\n",
            "iteration 3935, epoch 8, batch 368/481,disc_loss 77.502, (real 80.846, fake 74.158 ) gen_loss 712.51\n",
            "iteration 3936, epoch 8, batch 369/481,disc_loss 80.181, (real 83.103, fake 77.26 ) gen_loss 752.93\n",
            "iteration 3937, epoch 8, batch 370/481,disc_loss 78.112, (real 81.545, fake 74.679 ) gen_loss 662.78\n",
            "iteration 3938, epoch 8, batch 371/481,disc_loss 74.469, (real 77.733, fake 71.205 ) gen_loss 732.74\n",
            "iteration 3939, epoch 8, batch 372/481,disc_loss 76.002, (real 79.667, fake 72.337 ) gen_loss 561.23\n",
            "iteration 3940, epoch 8, batch 373/481,disc_loss 77.739, (real 80.578, fake 74.901 ) gen_loss 599.9\n",
            "iteration 3941, epoch 8, batch 374/481,disc_loss 78.516, (real 81.463, fake 75.57 ) gen_loss 576.56\n",
            "iteration 3942, epoch 8, batch 375/481,disc_loss 76.897, (real 80.074, fake 73.72 ) gen_loss 623.92\n",
            "iteration 3943, epoch 8, batch 376/481,disc_loss 79.1, (real 81.522, fake 76.678 ) gen_loss 599.44\n",
            "iteration 3944, epoch 8, batch 377/481,disc_loss 81.774, (real 84.491, fake 79.056 ) gen_loss 578.31\n",
            "iteration 3945, epoch 8, batch 378/481,disc_loss 73.176, (real 76.305, fake 70.047 ) gen_loss 612.13\n",
            "iteration 3946, epoch 8, batch 379/481,disc_loss 79.698, (real 81.434, fake 77.963 ) gen_loss 675.54\n",
            "iteration 3947, epoch 8, batch 380/481,disc_loss 77.202, (real 79.75, fake 74.655 ) gen_loss 623.82\n",
            "iteration 3948, epoch 8, batch 381/481,disc_loss 81.065, (real 84.158, fake 77.971 ) gen_loss 655.99\n",
            "iteration 3949, epoch 8, batch 382/481,disc_loss 79.142, (real 82.096, fake 76.188 ) gen_loss 688.53\n",
            "iteration 3950, epoch 8, batch 383/481,disc_loss 72.578, (real 74.677, fake 70.478 ) gen_loss 584.2\n",
            "iteration 3951, epoch 8, batch 384/481,disc_loss 78.655, (real 82.214, fake 75.096 ) gen_loss 600.31\n",
            "iteration 3952, epoch 8, batch 385/481,disc_loss 82.105, (real 84.676, fake 79.534 ) gen_loss 639.74\n",
            "iteration 3953, epoch 8, batch 386/481,disc_loss 75.601, (real 77.925, fake 73.277 ) gen_loss 664.6\n",
            "iteration 3954, epoch 8, batch 387/481,disc_loss 76.902, (real 79.505, fake 74.298 ) gen_loss 669.03\n",
            "iteration 3955, epoch 8, batch 388/481,disc_loss 77.471, (real 80.948, fake 73.994 ) gen_loss 688.5\n",
            "iteration 3956, epoch 8, batch 389/481,disc_loss 76.446, (real 80.049, fake 72.843 ) gen_loss 619.29\n",
            "iteration 3957, epoch 8, batch 390/481,disc_loss 78.23, (real 80.999, fake 75.461 ) gen_loss 577.64\n",
            "iteration 3958, epoch 8, batch 391/481,disc_loss 81.385, (real 83.96, fake 78.81 ) gen_loss 592.81\n",
            "iteration 3959, epoch 8, batch 392/481,disc_loss 75.025, (real 77.704, fake 72.345 ) gen_loss 579.72\n",
            "iteration 3960, epoch 8, batch 393/481,disc_loss 77.266, (real 79.987, fake 74.544 ) gen_loss 619.23\n",
            "iteration 3961, epoch 8, batch 394/481,disc_loss 78.059, (real 81.415, fake 74.704 ) gen_loss 602.44\n",
            "iteration 3962, epoch 8, batch 395/481,disc_loss 79.763, (real 83.287, fake 76.238 ) gen_loss 614.92\n",
            "iteration 3963, epoch 8, batch 396/481,disc_loss 79.153, (real 81.4, fake 76.907 ) gen_loss 588.99\n",
            "iteration 3964, epoch 8, batch 397/481,disc_loss 82.296, (real 85.576, fake 79.016 ) gen_loss 650.98\n",
            "iteration 3965, epoch 8, batch 398/481,disc_loss 83.342, (real 85.911, fake 80.774 ) gen_loss 545.45\n",
            "iteration 3966, epoch 8, batch 399/481,disc_loss 82.288, (real 86.424, fake 78.152 ) gen_loss 613.63\n",
            "iteration 3967, epoch 8, batch 400/481,disc_loss 77.272, (real 81.33, fake 73.214 ) gen_loss 700.08\n",
            "iteration 3968, epoch 8, batch 401/481,disc_loss 79.673, (real 83.299, fake 76.047 ) gen_loss 782.08\n",
            "iteration 3969, epoch 8, batch 402/481,disc_loss 79.048, (real 81.555, fake 76.542 ) gen_loss 749.03\n",
            "iteration 3970, epoch 8, batch 403/481,disc_loss 75.02, (real 77.799, fake 72.24 ) gen_loss 669.01\n",
            "iteration 3971, epoch 8, batch 404/481,disc_loss 79.38, (real 83.323, fake 75.438 ) gen_loss 692.49\n",
            "iteration 3972, epoch 8, batch 405/481,disc_loss 77.472, (real 80.214, fake 74.729 ) gen_loss 708.44\n",
            "iteration 3973, epoch 8, batch 406/481,disc_loss 82.015, (real 84.708, fake 79.322 ) gen_loss 570.57\n",
            "iteration 3974, epoch 8, batch 407/481,disc_loss 80.823, (real 83.141, fake 78.506 ) gen_loss 600.08\n",
            "iteration 3975, epoch 8, batch 408/481,disc_loss 83.955, (real 86.769, fake 81.141 ) gen_loss 658.94\n",
            "iteration 3976, epoch 8, batch 409/481,disc_loss 76.608, (real 79.203, fake 74.012 ) gen_loss 606.39\n",
            "iteration 3977, epoch 8, batch 410/481,disc_loss 72.651, (real 74.982, fake 70.319 ) gen_loss 608.77\n",
            "iteration 3978, epoch 8, batch 411/481,disc_loss 79.102, (real 82.041, fake 76.163 ) gen_loss 618.73\n",
            "iteration 3979, epoch 8, batch 412/481,disc_loss 76.547, (real 79.539, fake 73.556 ) gen_loss 633.77\n",
            "iteration 3980, epoch 8, batch 413/481,disc_loss 84.259, (real 86.804, fake 81.714 ) gen_loss 646.13\n",
            "iteration 3981, epoch 8, batch 414/481,disc_loss 79.612, (real 81.821, fake 77.403 ) gen_loss 605.22\n",
            "iteration 3982, epoch 8, batch 415/481,disc_loss 73.382, (real 76.488, fake 70.275 ) gen_loss 657.52\n",
            "iteration 3983, epoch 8, batch 416/481,disc_loss 83.206, (real 87.175, fake 79.238 ) gen_loss 683.8\n",
            "iteration 3984, epoch 8, batch 417/481,disc_loss 76.59, (real 80.056, fake 73.125 ) gen_loss 702.93\n",
            "iteration 3985, epoch 8, batch 418/481,disc_loss 81.949, (real 85.856, fake 78.041 ) gen_loss 686.04\n",
            "iteration 3986, epoch 8, batch 419/481,disc_loss 76.874, (real 79.373, fake 74.375 ) gen_loss 750.28\n",
            "iteration 3987, epoch 8, batch 420/481,disc_loss 81.121, (real 84.124, fake 78.117 ) gen_loss 660.99\n",
            "iteration 3988, epoch 8, batch 421/481,disc_loss 82.825, (real 84.617, fake 81.032 ) gen_loss 569.43\n",
            "iteration 3989, epoch 8, batch 422/481,disc_loss 82.03, (real 85.503, fake 78.556 ) gen_loss 636.29\n",
            "iteration 3990, epoch 8, batch 423/481,disc_loss 79.819, (real 82.193, fake 77.444 ) gen_loss 735.79\n",
            "iteration 3991, epoch 8, batch 424/481,disc_loss 75.95, (real 79.016, fake 72.884 ) gen_loss 705.97\n",
            "iteration 3992, epoch 8, batch 425/481,disc_loss 78.469, (real 81.721, fake 75.217 ) gen_loss 682.96\n",
            "iteration 3993, epoch 8, batch 426/481,disc_loss 79.432, (real 82.228, fake 76.636 ) gen_loss 640.79\n",
            "iteration 3994, epoch 8, batch 427/481,disc_loss 78.65, (real 81.461, fake 75.839 ) gen_loss 650.84\n",
            "iteration 3995, epoch 8, batch 428/481,disc_loss 79.868, (real 82.579, fake 77.156 ) gen_loss 694.51\n",
            "iteration 3996, epoch 8, batch 429/481,disc_loss 78.478, (real 82.587, fake 74.368 ) gen_loss 732.46\n",
            "iteration 3997, epoch 8, batch 430/481,disc_loss 79.184, (real 82.002, fake 76.365 ) gen_loss 674.25\n",
            "iteration 3998, epoch 8, batch 431/481,disc_loss 78.072, (real 81.189, fake 74.955 ) gen_loss 618.73\n",
            "iteration 3999, epoch 8, batch 432/481,disc_loss 75.828, (real 79.116, fake 72.539 ) gen_loss 601.67\n",
            "iteration 4000, epoch 8, batch 433/481,disc_loss 81.522, (real 84.13, fake 78.915 ) gen_loss 655.68\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 4001, epoch 8, batch 434/481,disc_loss 77.875, (real 80.811, fake 74.938 ) gen_loss 595.03\n",
            "iteration 4002, epoch 8, batch 435/481,disc_loss 77.346, (real 81.456, fake 73.235 ) gen_loss 687.83\n",
            "iteration 4003, epoch 8, batch 436/481,disc_loss 76.419, (real 78.957, fake 73.881 ) gen_loss 601.84\n",
            "iteration 4004, epoch 8, batch 437/481,disc_loss 83.609, (real 87.903, fake 79.315 ) gen_loss 624.57\n",
            "iteration 4005, epoch 8, batch 438/481,disc_loss 82.124, (real 86.002, fake 78.245 ) gen_loss 777.35\n",
            "iteration 4006, epoch 8, batch 439/481,disc_loss 79.458, (real 82.425, fake 76.492 ) gen_loss 685.33\n",
            "iteration 4007, epoch 8, batch 440/481,disc_loss 79.068, (real 83.123, fake 75.013 ) gen_loss 628.95\n",
            "iteration 4008, epoch 8, batch 441/481,disc_loss 75.176, (real 77.146, fake 73.205 ) gen_loss 575.88\n",
            "iteration 4009, epoch 8, batch 442/481,disc_loss 75.437, (real 78.868, fake 72.007 ) gen_loss 647.89\n",
            "iteration 4010, epoch 8, batch 443/481,disc_loss 75.416, (real 78.276, fake 72.557 ) gen_loss 638.33\n",
            "iteration 4011, epoch 8, batch 444/481,disc_loss 77.819, (real 80.505, fake 75.133 ) gen_loss 662.52\n",
            "iteration 4012, epoch 8, batch 445/481,disc_loss 81.269, (real 84.44, fake 78.097 ) gen_loss 628.58\n",
            "iteration 4013, epoch 8, batch 446/481,disc_loss 80.142, (real 83.695, fake 76.589 ) gen_loss 736.14\n",
            "iteration 4014, epoch 8, batch 447/481,disc_loss 78.09, (real 80.728, fake 75.452 ) gen_loss 694.15\n",
            "iteration 4015, epoch 8, batch 448/481,disc_loss 75.265, (real 78.856, fake 71.675 ) gen_loss 647.47\n",
            "iteration 4016, epoch 8, batch 449/481,disc_loss 80.563, (real 84.405, fake 76.721 ) gen_loss 687.43\n",
            "iteration 4017, epoch 8, batch 450/481,disc_loss 79.262, (real 82.199, fake 76.325 ) gen_loss 876.86\n",
            "iteration 4018, epoch 8, batch 451/481,disc_loss 80.185, (real 83.638, fake 76.732 ) gen_loss 626.56\n",
            "iteration 4019, epoch 8, batch 452/481,disc_loss 71.29, (real 73.825, fake 68.756 ) gen_loss 551.3\n",
            "iteration 4020, epoch 8, batch 453/481,disc_loss 80.045, (real 83.913, fake 76.177 ) gen_loss 577.41\n",
            "iteration 4021, epoch 8, batch 454/481,disc_loss 78.983, (real 82.123, fake 75.843 ) gen_loss 591.79\n",
            "iteration 4022, epoch 8, batch 455/481,disc_loss 77.458, (real 81.403, fake 73.514 ) gen_loss 701.18\n",
            "iteration 4023, epoch 8, batch 456/481,disc_loss 81.552, (real 82.84, fake 80.265 ) gen_loss 707.99\n",
            "iteration 4024, epoch 8, batch 457/481,disc_loss 81.299, (real 84.404, fake 78.194 ) gen_loss 622.16\n",
            "iteration 4025, epoch 8, batch 458/481,disc_loss 82.154, (real 84.617, fake 79.691 ) gen_loss 716.65\n",
            "iteration 4026, epoch 8, batch 459/481,disc_loss 80.98, (real 84.539, fake 77.422 ) gen_loss 668.33\n",
            "iteration 4027, epoch 8, batch 460/481,disc_loss 77.27, (real 79.704, fake 74.837 ) gen_loss 641.11\n",
            "iteration 4028, epoch 8, batch 461/481,disc_loss 77.02, (real 79.805, fake 74.235 ) gen_loss 653.73\n",
            "iteration 4029, epoch 8, batch 462/481,disc_loss 78.673, (real 82.766, fake 74.58 ) gen_loss 653.61\n",
            "iteration 4030, epoch 8, batch 463/481,disc_loss 80.946, (real 84.052, fake 77.839 ) gen_loss 615.75\n",
            "iteration 4031, epoch 8, batch 464/481,disc_loss 80.539, (real 83.571, fake 77.507 ) gen_loss 605.0\n",
            "iteration 4032, epoch 8, batch 465/481,disc_loss 78.681, (real 80.906, fake 76.457 ) gen_loss 615.02\n",
            "iteration 4033, epoch 8, batch 466/481,disc_loss 78.551, (real 80.959, fake 76.142 ) gen_loss 633.78\n",
            "iteration 4034, epoch 8, batch 467/481,disc_loss 84.745, (real 89.306, fake 80.184 ) gen_loss 675.76\n",
            "iteration 4035, epoch 8, batch 468/481,disc_loss 78.519, (real 81.538, fake 75.5 ) gen_loss 609.04\n",
            "iteration 4036, epoch 8, batch 469/481,disc_loss 76.304, (real 78.942, fake 73.666 ) gen_loss 680.97\n",
            "iteration 4037, epoch 8, batch 470/481,disc_loss 78.465, (real 80.917, fake 76.013 ) gen_loss 691.45\n",
            "iteration 4038, epoch 8, batch 471/481,disc_loss 76.795, (real 79.928, fake 73.661 ) gen_loss 618.83\n",
            "iteration 4039, epoch 8, batch 472/481,disc_loss 80.109, (real 83.15, fake 77.068 ) gen_loss 680.83\n",
            "iteration 4040, epoch 8, batch 473/481,disc_loss 77.665, (real 81.526, fake 73.803 ) gen_loss 611.81\n",
            "iteration 4041, epoch 8, batch 474/481,disc_loss 74.992, (real 77.94, fake 72.043 ) gen_loss 654.55\n",
            "iteration 4042, epoch 8, batch 475/481,disc_loss 78.214, (real 80.912, fake 75.516 ) gen_loss 615.13\n",
            "iteration 4043, epoch 8, batch 476/481,disc_loss 80.167, (real 82.839, fake 77.495 ) gen_loss 586.12\n",
            "iteration 4044, epoch 8, batch 477/481,disc_loss 75.328, (real 79.555, fake 71.101 ) gen_loss 602.28\n",
            "iteration 4045, epoch 8, batch 478/481,disc_loss 84.083, (real 88.494, fake 79.671 ) gen_loss 694.93\n",
            "iteration 4046, epoch 8, batch 479/481,disc_loss 79.438, (real 82.61, fake 76.265 ) gen_loss 725.92\n",
            "iteration 4047, epoch 8, batch 480/481,disc_loss 76.503, (real 79.209, fake 73.797 ) gen_loss 590.68\n",
            "iteration 4048, epoch 8, batch 481/481,disc_loss 77.243, (real 80.686, fake 73.8 ) gen_loss 618.62\n",
            "iteration 4049, epoch 9, batch 1/481,disc_loss 82.246, (real 84.808, fake 79.684 ) gen_loss 605.26\n",
            "iteration 4050, epoch 9, batch 2/481,disc_loss 75.615, (real 78.179, fake 73.051 ) gen_loss 586.29\n",
            "iteration 4051, epoch 9, batch 3/481,disc_loss 77.522, (real 80.155, fake 74.889 ) gen_loss 581.93\n",
            "iteration 4052, epoch 9, batch 4/481,disc_loss 78.351, (real 81.513, fake 75.189 ) gen_loss 618.76\n",
            "iteration 4053, epoch 9, batch 5/481,disc_loss 81.386, (real 84.051, fake 78.721 ) gen_loss 671.76\n",
            "iteration 4054, epoch 9, batch 6/481,disc_loss 80.125, (real 83.362, fake 76.888 ) gen_loss 628.76\n",
            "iteration 4055, epoch 9, batch 7/481,disc_loss 78.214, (real 80.754, fake 75.674 ) gen_loss 626.82\n",
            "iteration 4056, epoch 9, batch 8/481,disc_loss 80.481, (real 83.307, fake 77.654 ) gen_loss 631.69\n",
            "iteration 4057, epoch 9, batch 9/481,disc_loss 77.045, (real 80.005, fake 74.085 ) gen_loss 628.71\n",
            "iteration 4058, epoch 9, batch 10/481,disc_loss 80.809, (real 83.857, fake 77.762 ) gen_loss 667.0\n",
            "iteration 4059, epoch 9, batch 11/481,disc_loss 78.869, (real 82.444, fake 75.294 ) gen_loss 588.87\n",
            "iteration 4060, epoch 9, batch 12/481,disc_loss 79.291, (real 82.067, fake 76.515 ) gen_loss 657.16\n",
            "iteration 4061, epoch 9, batch 13/481,disc_loss 76.559, (real 79.019, fake 74.099 ) gen_loss 739.91\n",
            "iteration 4062, epoch 9, batch 14/481,disc_loss 81.247, (real 83.704, fake 78.79 ) gen_loss 683.48\n",
            "iteration 4063, epoch 9, batch 15/481,disc_loss 78.819, (real 82.179, fake 75.458 ) gen_loss 666.47\n",
            "iteration 4064, epoch 9, batch 16/481,disc_loss 79.014, (real 81.1, fake 76.929 ) gen_loss 670.89\n",
            "iteration 4065, epoch 9, batch 17/481,disc_loss 76.467, (real 79.568, fake 73.366 ) gen_loss 717.0\n",
            "iteration 4066, epoch 9, batch 18/481,disc_loss 73.622, (real 76.051, fake 71.193 ) gen_loss 647.45\n",
            "iteration 4067, epoch 9, batch 19/481,disc_loss 76.21, (real 78.427, fake 73.994 ) gen_loss 611.12\n",
            "iteration 4068, epoch 9, batch 20/481,disc_loss 76.773, (real 79.176, fake 74.37 ) gen_loss 695.89\n",
            "iteration 4069, epoch 9, batch 21/481,disc_loss 75.799, (real 77.847, fake 73.75 ) gen_loss 661.79\n",
            "iteration 4070, epoch 9, batch 22/481,disc_loss 82.104, (real 85.353, fake 78.855 ) gen_loss 748.28\n",
            "iteration 4071, epoch 9, batch 23/481,disc_loss 76.342, (real 78.278, fake 74.405 ) gen_loss 710.05\n",
            "iteration 4072, epoch 9, batch 24/481,disc_loss 77.29, (real 80.624, fake 73.956 ) gen_loss 620.27\n",
            "iteration 4073, epoch 9, batch 25/481,disc_loss 79.73, (real 82.417, fake 77.043 ) gen_loss 660.23\n",
            "iteration 4074, epoch 9, batch 26/481,disc_loss 75.172, (real 77.833, fake 72.511 ) gen_loss 638.18\n",
            "iteration 4075, epoch 9, batch 27/481,disc_loss 77.872, (real 79.832, fake 75.912 ) gen_loss 643.06\n",
            "iteration 4076, epoch 9, batch 28/481,disc_loss 80.662, (real 83.074, fake 78.25 ) gen_loss 578.51\n",
            "iteration 4077, epoch 9, batch 29/481,disc_loss 75.679, (real 78.331, fake 73.026 ) gen_loss 550.75\n",
            "iteration 4078, epoch 9, batch 30/481,disc_loss 76.417, (real 79.197, fake 73.638 ) gen_loss 684.32\n",
            "iteration 4079, epoch 9, batch 31/481,disc_loss 81.082, (real 82.846, fake 79.319 ) gen_loss 727.95\n",
            "iteration 4080, epoch 9, batch 32/481,disc_loss 76.226, (real 79.463, fake 72.989 ) gen_loss 697.49\n",
            "iteration 4081, epoch 9, batch 33/481,disc_loss 76.858, (real 78.803, fake 74.913 ) gen_loss 682.92\n",
            "iteration 4082, epoch 9, batch 34/481,disc_loss 80.781, (real 84.612, fake 76.95 ) gen_loss 682.37\n",
            "iteration 4083, epoch 9, batch 35/481,disc_loss 77.592, (real 81.568, fake 73.616 ) gen_loss 710.22\n",
            "iteration 4084, epoch 9, batch 36/481,disc_loss 79.389, (real 82.5, fake 76.278 ) gen_loss 781.3\n",
            "iteration 4085, epoch 9, batch 37/481,disc_loss 80.382, (real 83.879, fake 76.885 ) gen_loss 714.71\n",
            "iteration 4086, epoch 9, batch 38/481,disc_loss 77.534, (real 81.352, fake 73.716 ) gen_loss 643.31\n",
            "iteration 4087, epoch 9, batch 39/481,disc_loss 77.928, (real 81.435, fake 74.421 ) gen_loss 677.3\n",
            "iteration 4088, epoch 9, batch 40/481,disc_loss 76.078, (real 79.099, fake 73.057 ) gen_loss 686.66\n",
            "iteration 4089, epoch 9, batch 41/481,disc_loss 75.705, (real 77.954, fake 73.456 ) gen_loss 687.39\n",
            "iteration 4090, epoch 9, batch 42/481,disc_loss 81.269, (real 84.531, fake 78.007 ) gen_loss 634.34\n",
            "iteration 4091, epoch 9, batch 43/481,disc_loss 79.762, (real 82.386, fake 77.139 ) gen_loss 731.42\n",
            "iteration 4092, epoch 9, batch 44/481,disc_loss 79.515, (real 82.231, fake 76.8 ) gen_loss 589.78\n",
            "iteration 4093, epoch 9, batch 45/481,disc_loss 76.167, (real 79.113, fake 73.221 ) gen_loss 614.06\n",
            "iteration 4094, epoch 9, batch 46/481,disc_loss 83.185, (real 86.516, fake 79.854 ) gen_loss 620.56\n",
            "iteration 4095, epoch 9, batch 47/481,disc_loss 75.557, (real 78.133, fake 72.981 ) gen_loss 671.97\n",
            "iteration 4096, epoch 9, batch 48/481,disc_loss 79.974, (real 82.965, fake 76.983 ) gen_loss 607.23\n",
            "iteration 4097, epoch 9, batch 49/481,disc_loss 75.916, (real 79.089, fake 72.744 ) gen_loss 672.8\n",
            "iteration 4098, epoch 9, batch 50/481,disc_loss 79.421, (real 83.602, fake 75.24 ) gen_loss 716.75\n",
            "iteration 4099, epoch 9, batch 51/481,disc_loss 81.579, (real 83.459, fake 79.699 ) gen_loss 675.59\n",
            "iteration 4100, epoch 9, batch 52/481,disc_loss 78.195, (real 80.473, fake 75.917 ) gen_loss 681.67\n",
            "iteration 4101, epoch 9, batch 53/481,disc_loss 72.151, (real 74.761, fake 69.54 ) gen_loss 620.01\n",
            "iteration 4102, epoch 9, batch 54/481,disc_loss 72.354, (real 74.937, fake 69.771 ) gen_loss 632.16\n",
            "iteration 4103, epoch 9, batch 55/481,disc_loss 79.495, (real 82.827, fake 76.164 ) gen_loss 600.12\n",
            "iteration 4104, epoch 9, batch 56/481,disc_loss 76.084, (real 78.329, fake 73.838 ) gen_loss 557.83\n",
            "iteration 4105, epoch 9, batch 57/481,disc_loss 78.918, (real 81.647, fake 76.19 ) gen_loss 673.8\n",
            "iteration 4106, epoch 9, batch 58/481,disc_loss 79.088, (real 81.558, fake 76.618 ) gen_loss 721.56\n",
            "iteration 4107, epoch 9, batch 59/481,disc_loss 77.358, (real 79.759, fake 74.957 ) gen_loss 654.63\n",
            "iteration 4108, epoch 9, batch 60/481,disc_loss 82.711, (real 85.355, fake 80.067 ) gen_loss 698.93\n",
            "iteration 4109, epoch 9, batch 61/481,disc_loss 78.809, (real 81.406, fake 76.211 ) gen_loss 673.72\n",
            "iteration 4110, epoch 9, batch 62/481,disc_loss 77.165, (real 81.535, fake 72.794 ) gen_loss 701.96\n",
            "iteration 4111, epoch 9, batch 63/481,disc_loss 78.035, (real 80.464, fake 75.607 ) gen_loss 662.88\n",
            "iteration 4112, epoch 9, batch 64/481,disc_loss 74.115, (real 77.258, fake 70.973 ) gen_loss 697.08\n",
            "iteration 4113, epoch 9, batch 65/481,disc_loss 80.936, (real 83.835, fake 78.038 ) gen_loss 669.45\n",
            "iteration 4114, epoch 9, batch 66/481,disc_loss 76.529, (real 80.36, fake 72.698 ) gen_loss 664.18\n",
            "iteration 4115, epoch 9, batch 67/481,disc_loss 77.423, (real 80.497, fake 74.35 ) gen_loss 621.04\n",
            "iteration 4116, epoch 9, batch 68/481,disc_loss 77.304, (real 80.086, fake 74.523 ) gen_loss 738.37\n",
            "iteration 4117, epoch 9, batch 69/481,disc_loss 79.127, (real 82.597, fake 75.656 ) gen_loss 675.18\n",
            "iteration 4118, epoch 9, batch 70/481,disc_loss 79.528, (real 82.156, fake 76.9 ) gen_loss 597.4\n",
            "iteration 4119, epoch 9, batch 71/481,disc_loss 85.392, (real 88.686, fake 82.098 ) gen_loss 660.17\n",
            "iteration 4120, epoch 9, batch 72/481,disc_loss 81.688, (real 84.848, fake 78.527 ) gen_loss 621.26\n",
            "iteration 4121, epoch 9, batch 73/481,disc_loss 81.118, (real 84.233, fake 78.003 ) gen_loss 638.17\n",
            "iteration 4122, epoch 9, batch 74/481,disc_loss 75.687, (real 79.248, fake 72.126 ) gen_loss 756.97\n",
            "iteration 4123, epoch 9, batch 75/481,disc_loss 76.575, (real 79.324, fake 73.825 ) gen_loss 594.75\n",
            "iteration 4124, epoch 9, batch 76/481,disc_loss 81.021, (real 83.859, fake 78.182 ) gen_loss 594.63\n",
            "iteration 4125, epoch 9, batch 77/481,disc_loss 78.444, (real 80.929, fake 75.958 ) gen_loss 564.27\n",
            "iteration 4126, epoch 9, batch 78/481,disc_loss 78.274, (real 80.667, fake 75.882 ) gen_loss 687.7\n",
            "iteration 4127, epoch 9, batch 79/481,disc_loss 78.696, (real 81.036, fake 76.357 ) gen_loss 662.93\n",
            "iteration 4128, epoch 9, batch 80/481,disc_loss 78.635, (real 81.699, fake 75.571 ) gen_loss 745.98\n",
            "iteration 4129, epoch 9, batch 81/481,disc_loss 75.573, (real 78.819, fake 72.328 ) gen_loss 630.54\n",
            "iteration 4130, epoch 9, batch 82/481,disc_loss 78.568, (real 81.474, fake 75.663 ) gen_loss 651.04\n",
            "iteration 4131, epoch 9, batch 83/481,disc_loss 81.043, (real 84.24, fake 77.847 ) gen_loss 709.97\n",
            "iteration 4132, epoch 9, batch 84/481,disc_loss 77.08, (real 79.59, fake 74.57 ) gen_loss 695.28\n",
            "iteration 4133, epoch 9, batch 85/481,disc_loss 75.653, (real 78.571, fake 72.735 ) gen_loss 726.45\n",
            "iteration 4134, epoch 9, batch 86/481,disc_loss 80.332, (real 82.826, fake 77.838 ) gen_loss 620.55\n",
            "iteration 4135, epoch 9, batch 87/481,disc_loss 77.517, (real 80.412, fake 74.621 ) gen_loss 630.79\n",
            "iteration 4136, epoch 9, batch 88/481,disc_loss 76.9, (real 79.74, fake 74.061 ) gen_loss 597.93\n",
            "iteration 4137, epoch 9, batch 89/481,disc_loss 71.493, (real 74.228, fake 68.758 ) gen_loss 629.62\n",
            "iteration 4138, epoch 9, batch 90/481,disc_loss 76.995, (real 79.633, fake 74.356 ) gen_loss 629.84\n",
            "iteration 4139, epoch 9, batch 91/481,disc_loss 77.396, (real 80.433, fake 74.358 ) gen_loss 749.0\n",
            "iteration 4140, epoch 9, batch 92/481,disc_loss 75.283, (real 78.225, fake 72.341 ) gen_loss 684.52\n",
            "iteration 4141, epoch 9, batch 93/481,disc_loss 74.153, (real 76.586, fake 71.719 ) gen_loss 664.55\n",
            "iteration 4142, epoch 9, batch 94/481,disc_loss 76.701, (real 78.884, fake 74.517 ) gen_loss 720.17\n",
            "iteration 4143, epoch 9, batch 95/481,disc_loss 78.283, (real 81.182, fake 75.383 ) gen_loss 672.13\n",
            "iteration 4144, epoch 9, batch 96/481,disc_loss 79.254, (real 82.327, fake 76.18 ) gen_loss 646.56\n",
            "iteration 4145, epoch 9, batch 97/481,disc_loss 74.113, (real 76.948, fake 71.278 ) gen_loss 641.9\n",
            "iteration 4146, epoch 9, batch 98/481,disc_loss 79.131, (real 81.871, fake 76.392 ) gen_loss 686.66\n",
            "iteration 4147, epoch 9, batch 99/481,disc_loss 78.222, (real 82.121, fake 74.322 ) gen_loss 740.09\n",
            "iteration 4148, epoch 9, batch 100/481,disc_loss 79.204, (real 82.407, fake 76.001 ) gen_loss 848.76\n",
            "iteration 4149, epoch 9, batch 101/481,disc_loss 81.96, (real 83.812, fake 80.107 ) gen_loss 671.6\n",
            "iteration 4150, epoch 9, batch 102/481,disc_loss 77.787, (real 80.402, fake 75.172 ) gen_loss 664.66\n",
            "iteration 4151, epoch 9, batch 103/481,disc_loss 78.453, (real 81.476, fake 75.429 ) gen_loss 649.64\n",
            "iteration 4152, epoch 9, batch 104/481,disc_loss 81.893, (real 85.997, fake 77.79 ) gen_loss 722.14\n",
            "iteration 4153, epoch 9, batch 105/481,disc_loss 76.354, (real 79.709, fake 73.0 ) gen_loss 600.58\n",
            "iteration 4154, epoch 9, batch 106/481,disc_loss 80.29, (real 83.135, fake 77.445 ) gen_loss 644.84\n",
            "iteration 4155, epoch 9, batch 107/481,disc_loss 80.349, (real 83.051, fake 77.647 ) gen_loss 611.12\n",
            "iteration 4156, epoch 9, batch 108/481,disc_loss 78.328, (real 81.079, fake 75.577 ) gen_loss 581.41\n",
            "iteration 4157, epoch 9, batch 109/481,disc_loss 77.13, (real 80.175, fake 74.086 ) gen_loss 651.42\n",
            "iteration 4158, epoch 9, batch 110/481,disc_loss 76.906, (real 80.237, fake 73.576 ) gen_loss 690.81\n",
            "iteration 4159, epoch 9, batch 111/481,disc_loss 82.075, (real 84.754, fake 79.397 ) gen_loss 640.18\n",
            "iteration 4160, epoch 9, batch 112/481,disc_loss 77.061, (real 80.158, fake 73.965 ) gen_loss 705.12\n",
            "iteration 4161, epoch 9, batch 113/481,disc_loss 78.18, (real 80.821, fake 75.54 ) gen_loss 636.24\n",
            "iteration 4162, epoch 9, batch 114/481,disc_loss 78.835, (real 81.954, fake 75.716 ) gen_loss 622.31\n",
            "iteration 4163, epoch 9, batch 115/481,disc_loss 77.797, (real 80.078, fake 75.515 ) gen_loss 751.17\n",
            "iteration 4164, epoch 9, batch 116/481,disc_loss 77.895, (real 80.539, fake 75.251 ) gen_loss 581.17\n",
            "iteration 4165, epoch 9, batch 117/481,disc_loss 75.77, (real 78.8, fake 72.74 ) gen_loss 585.7\n",
            "iteration 4166, epoch 9, batch 118/481,disc_loss 79.539, (real 82.687, fake 76.391 ) gen_loss 577.68\n",
            "iteration 4167, epoch 9, batch 119/481,disc_loss 78.887, (real 82.859, fake 74.916 ) gen_loss 690.32\n",
            "iteration 4168, epoch 9, batch 120/481,disc_loss 81.78, (real 84.558, fake 79.003 ) gen_loss 601.22\n",
            "iteration 4169, epoch 9, batch 121/481,disc_loss 78.123, (real 80.871, fake 75.375 ) gen_loss 598.3\n",
            "iteration 4170, epoch 9, batch 122/481,disc_loss 77.001, (real 79.94, fake 74.062 ) gen_loss 626.24\n",
            "iteration 4171, epoch 9, batch 123/481,disc_loss 74.205, (real 77.295, fake 71.114 ) gen_loss 772.47\n",
            "iteration 4172, epoch 9, batch 124/481,disc_loss 73.914, (real 76.874, fake 70.954 ) gen_loss 641.7\n",
            "iteration 4173, epoch 9, batch 125/481,disc_loss 80.707, (real 84.259, fake 77.155 ) gen_loss 805.45\n",
            "iteration 4174, epoch 9, batch 126/481,disc_loss 84.004, (real 86.117, fake 81.892 ) gen_loss 537.23\n",
            "iteration 4175, epoch 9, batch 127/481,disc_loss 81.324, (real 83.836, fake 78.812 ) gen_loss 726.87\n",
            "iteration 4176, epoch 9, batch 128/481,disc_loss 83.785, (real 85.678, fake 81.893 ) gen_loss 641.85\n",
            "iteration 4177, epoch 9, batch 129/481,disc_loss 75.187, (real 78.305, fake 72.068 ) gen_loss 618.94\n",
            "iteration 4178, epoch 9, batch 130/481,disc_loss 78.983, (real 81.334, fake 76.631 ) gen_loss 661.42\n",
            "iteration 4179, epoch 9, batch 131/481,disc_loss 77.355, (real 80.147, fake 74.562 ) gen_loss 647.0\n",
            "iteration 4180, epoch 9, batch 132/481,disc_loss 78.89, (real 80.667, fake 77.112 ) gen_loss 562.92\n",
            "iteration 4181, epoch 9, batch 133/481,disc_loss 79.79, (real 83.467, fake 76.113 ) gen_loss 702.98\n",
            "iteration 4182, epoch 9, batch 134/481,disc_loss 77.362, (real 80.641, fake 74.083 ) gen_loss 684.83\n",
            "iteration 4183, epoch 9, batch 135/481,disc_loss 80.296, (real 83.375, fake 77.217 ) gen_loss 616.86\n",
            "iteration 4184, epoch 9, batch 136/481,disc_loss 78.965, (real 81.443, fake 76.486 ) gen_loss 655.15\n",
            "iteration 4185, epoch 9, batch 137/481,disc_loss 79.271, (real 82.197, fake 76.345 ) gen_loss 620.95\n",
            "iteration 4186, epoch 9, batch 138/481,disc_loss 79.789, (real 83.146, fake 76.432 ) gen_loss 623.34\n",
            "iteration 4187, epoch 9, batch 139/481,disc_loss 74.655, (real 77.882, fake 71.428 ) gen_loss 660.54\n",
            "iteration 4188, epoch 9, batch 140/481,disc_loss 75.833, (real 78.82, fake 72.847 ) gen_loss 662.98\n",
            "iteration 4189, epoch 9, batch 141/481,disc_loss 77.608, (real 80.234, fake 74.981 ) gen_loss 628.6\n",
            "iteration 4190, epoch 9, batch 142/481,disc_loss 78.83, (real 80.883, fake 76.776 ) gen_loss 605.74\n",
            "iteration 4191, epoch 9, batch 143/481,disc_loss 77.948, (real 80.843, fake 75.053 ) gen_loss 640.54\n",
            "iteration 4192, epoch 9, batch 144/481,disc_loss 72.48, (real 75.605, fake 69.356 ) gen_loss 689.69\n",
            "iteration 4193, epoch 9, batch 145/481,disc_loss 82.386, (real 84.862, fake 79.911 ) gen_loss 654.19\n",
            "iteration 4194, epoch 9, batch 146/481,disc_loss 79.172, (real 81.727, fake 76.616 ) gen_loss 685.3\n",
            "iteration 4195, epoch 9, batch 147/481,disc_loss 75.851, (real 78.934, fake 72.768 ) gen_loss 641.19\n",
            "iteration 4196, epoch 9, batch 148/481,disc_loss 76.647, (real 79.213, fake 74.08 ) gen_loss 673.59\n",
            "iteration 4197, epoch 9, batch 149/481,disc_loss 78.473, (real 82.872, fake 74.074 ) gen_loss 716.3\n",
            "iteration 4198, epoch 9, batch 150/481,disc_loss 76.446, (real 78.608, fake 74.285 ) gen_loss 665.93\n",
            "iteration 4199, epoch 9, batch 151/481,disc_loss 75.521, (real 77.814, fake 73.229 ) gen_loss 607.59\n",
            "iteration 4200, epoch 9, batch 152/481,disc_loss 75.952, (real 78.107, fake 73.796 ) gen_loss 707.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 4201, epoch 9, batch 153/481,disc_loss 74.669, (real 77.571, fake 71.768 ) gen_loss 621.7\n",
            "iteration 4202, epoch 9, batch 154/481,disc_loss 75.981, (real 78.551, fake 73.411 ) gen_loss 632.92\n",
            "iteration 4203, epoch 9, batch 155/481,disc_loss 80.946, (real 83.874, fake 78.018 ) gen_loss 645.62\n",
            "iteration 4204, epoch 9, batch 156/481,disc_loss 72.065, (real 74.725, fake 69.404 ) gen_loss 648.72\n",
            "iteration 4205, epoch 9, batch 157/481,disc_loss 77.119, (real 80.119, fake 74.12 ) gen_loss 626.28\n",
            "iteration 4206, epoch 9, batch 158/481,disc_loss 77.299, (real 79.648, fake 74.949 ) gen_loss 724.27\n",
            "iteration 4207, epoch 9, batch 159/481,disc_loss 74.154, (real 76.799, fake 71.51 ) gen_loss 649.85\n",
            "iteration 4208, epoch 9, batch 160/481,disc_loss 75.543, (real 78.12, fake 72.965 ) gen_loss 589.58\n",
            "iteration 4209, epoch 9, batch 161/481,disc_loss 80.514, (real 82.845, fake 78.183 ) gen_loss 612.56\n",
            "iteration 4210, epoch 9, batch 162/481,disc_loss 76.977, (real 79.547, fake 74.407 ) gen_loss 575.75\n",
            "iteration 4211, epoch 9, batch 163/481,disc_loss 76.432, (real 79.366, fake 73.499 ) gen_loss 605.99\n",
            "iteration 4212, epoch 9, batch 164/481,disc_loss 77.252, (real 79.797, fake 74.708 ) gen_loss 627.9\n",
            "iteration 4213, epoch 9, batch 165/481,disc_loss 77.428, (real 80.681, fake 74.175 ) gen_loss 685.13\n",
            "iteration 4214, epoch 9, batch 166/481,disc_loss 80.495, (real 82.725, fake 78.265 ) gen_loss 700.32\n",
            "iteration 4215, epoch 9, batch 167/481,disc_loss 82.873, (real 85.422, fake 80.323 ) gen_loss 638.7\n",
            "iteration 4216, epoch 9, batch 168/481,disc_loss 77.207, (real 79.387, fake 75.027 ) gen_loss 623.54\n",
            "iteration 4217, epoch 9, batch 169/481,disc_loss 81.503, (real 84.498, fake 78.507 ) gen_loss 662.75\n",
            "iteration 4218, epoch 9, batch 170/481,disc_loss 79.687, (real 83.611, fake 75.763 ) gen_loss 633.46\n",
            "iteration 4219, epoch 9, batch 171/481,disc_loss 78.183, (real 80.47, fake 75.896 ) gen_loss 613.04\n",
            "iteration 4220, epoch 9, batch 172/481,disc_loss 75.801, (real 78.159, fake 73.443 ) gen_loss 616.05\n",
            "iteration 4221, epoch 9, batch 173/481,disc_loss 79.097, (real 81.651, fake 76.542 ) gen_loss 664.32\n",
            "iteration 4222, epoch 9, batch 174/481,disc_loss 79.083, (real 81.726, fake 76.439 ) gen_loss 629.53\n",
            "iteration 4223, epoch 9, batch 175/481,disc_loss 78.459, (real 81.685, fake 75.233 ) gen_loss 607.23\n",
            "iteration 4224, epoch 9, batch 176/481,disc_loss 73.701, (real 76.344, fake 71.058 ) gen_loss 553.05\n",
            "iteration 4225, epoch 9, batch 177/481,disc_loss 80.766, (real 83.417, fake 78.114 ) gen_loss 632.9\n",
            "iteration 4226, epoch 9, batch 178/481,disc_loss 72.082, (real 74.573, fake 69.591 ) gen_loss 723.13\n",
            "iteration 4227, epoch 9, batch 179/481,disc_loss 75.899, (real 78.448, fake 73.351 ) gen_loss 675.51\n",
            "iteration 4228, epoch 9, batch 180/481,disc_loss 74.131, (real 75.87, fake 72.391 ) gen_loss 602.08\n",
            "iteration 4229, epoch 9, batch 181/481,disc_loss 78.912, (real 82.612, fake 75.212 ) gen_loss 778.21\n",
            "iteration 4230, epoch 9, batch 182/481,disc_loss 78.19, (real 80.705, fake 75.676 ) gen_loss 716.59\n",
            "iteration 4231, epoch 9, batch 183/481,disc_loss 76.383, (real 79.411, fake 73.356 ) gen_loss 634.77\n",
            "iteration 4232, epoch 9, batch 184/481,disc_loss 74.992, (real 78.23, fake 71.753 ) gen_loss 667.21\n",
            "iteration 4233, epoch 9, batch 185/481,disc_loss 76.037, (real 79.598, fake 72.476 ) gen_loss 658.19\n",
            "iteration 4234, epoch 9, batch 186/481,disc_loss 76.949, (real 79.358, fake 74.54 ) gen_loss 662.92\n",
            "iteration 4235, epoch 9, batch 187/481,disc_loss 78.373, (real 80.562, fake 76.185 ) gen_loss 610.34\n",
            "iteration 4236, epoch 9, batch 188/481,disc_loss 76.304, (real 79.84, fake 72.769 ) gen_loss 709.22\n",
            "iteration 4237, epoch 9, batch 189/481,disc_loss 81.001, (real 83.523, fake 78.48 ) gen_loss 677.32\n",
            "iteration 4238, epoch 9, batch 190/481,disc_loss 82.307, (real 85.935, fake 78.679 ) gen_loss 643.1\n",
            "iteration 4239, epoch 9, batch 191/481,disc_loss 82.225, (real 84.744, fake 79.705 ) gen_loss 632.6\n",
            "iteration 4240, epoch 9, batch 192/481,disc_loss 81.508, (real 84.575, fake 78.441 ) gen_loss 669.6\n",
            "iteration 4241, epoch 9, batch 193/481,disc_loss 77.385, (real 80.677, fake 74.093 ) gen_loss 753.93\n",
            "iteration 4242, epoch 9, batch 194/481,disc_loss 78.552, (real 80.921, fake 76.184 ) gen_loss 820.31\n",
            "iteration 4243, epoch 9, batch 195/481,disc_loss 80.75, (real 83.991, fake 77.509 ) gen_loss 734.13\n",
            "iteration 4244, epoch 9, batch 196/481,disc_loss 71.262, (real 73.778, fake 68.745 ) gen_loss 704.97\n",
            "iteration 4245, epoch 9, batch 197/481,disc_loss 79.112, (real 82.635, fake 75.588 ) gen_loss 741.37\n",
            "iteration 4246, epoch 9, batch 198/481,disc_loss 82.671, (real 84.998, fake 80.343 ) gen_loss 651.31\n",
            "iteration 4247, epoch 9, batch 199/481,disc_loss 76.638, (real 78.947, fake 74.328 ) gen_loss 605.29\n",
            "iteration 4248, epoch 9, batch 200/481,disc_loss 76.716, (real 81.194, fake 72.238 ) gen_loss 634.82\n",
            "iteration 4249, epoch 9, batch 201/481,disc_loss 75.643, (real 78.662, fake 72.625 ) gen_loss 680.27\n",
            "iteration 4250, epoch 9, batch 202/481,disc_loss 75.673, (real 79.222, fake 72.123 ) gen_loss 711.16\n",
            "iteration 4251, epoch 9, batch 203/481,disc_loss 78.08, (real 81.36, fake 74.8 ) gen_loss 651.01\n",
            "iteration 4252, epoch 9, batch 204/481,disc_loss 77.437, (real 80.761, fake 74.113 ) gen_loss 704.45\n",
            "iteration 4253, epoch 9, batch 205/481,disc_loss 71.379, (real 74.704, fake 68.054 ) gen_loss 708.96\n",
            "iteration 4254, epoch 9, batch 206/481,disc_loss 77.272, (real 80.814, fake 73.73 ) gen_loss 720.97\n",
            "iteration 4255, epoch 9, batch 207/481,disc_loss 73.72, (real 76.712, fake 70.727 ) gen_loss 738.32\n",
            "iteration 4256, epoch 9, batch 208/481,disc_loss 80.748, (real 84.0, fake 77.495 ) gen_loss 699.68\n",
            "iteration 4257, epoch 9, batch 209/481,disc_loss 77.423, (real 80.127, fake 74.718 ) gen_loss 697.56\n",
            "iteration 4258, epoch 9, batch 210/481,disc_loss 81.969, (real 84.679, fake 79.258 ) gen_loss 645.17\n",
            "iteration 4259, epoch 9, batch 211/481,disc_loss 79.251, (real 82.311, fake 76.191 ) gen_loss 681.94\n",
            "iteration 4260, epoch 9, batch 212/481,disc_loss 77.116, (real 79.834, fake 74.398 ) gen_loss 643.63\n",
            "iteration 4261, epoch 9, batch 213/481,disc_loss 79.119, (real 81.989, fake 76.248 ) gen_loss 646.03\n",
            "iteration 4262, epoch 9, batch 214/481,disc_loss 80.891, (real 84.593, fake 77.188 ) gen_loss 737.82\n",
            "iteration 4263, epoch 9, batch 215/481,disc_loss 77.393, (real 80.993, fake 73.793 ) gen_loss 706.66\n",
            "iteration 4264, epoch 9, batch 216/481,disc_loss 78.547, (real 81.819, fake 75.275 ) gen_loss 634.27\n",
            "iteration 4265, epoch 9, batch 217/481,disc_loss 78.105, (real 80.617, fake 75.593 ) gen_loss 692.84\n",
            "iteration 4266, epoch 9, batch 218/481,disc_loss 77.55, (real 79.953, fake 75.146 ) gen_loss 696.86\n",
            "iteration 4267, epoch 9, batch 219/481,disc_loss 77.99, (real 81.014, fake 74.966 ) gen_loss 655.65\n",
            "iteration 4268, epoch 9, batch 220/481,disc_loss 73.491, (real 76.544, fake 70.439 ) gen_loss 766.46\n",
            "iteration 4269, epoch 9, batch 221/481,disc_loss 77.975, (real 80.753, fake 75.197 ) gen_loss 715.71\n",
            "iteration 4270, epoch 9, batch 222/481,disc_loss 78.296, (real 82.533, fake 74.06 ) gen_loss 675.53\n",
            "iteration 4271, epoch 9, batch 223/481,disc_loss 76.195, (real 78.772, fake 73.618 ) gen_loss 684.62\n",
            "iteration 4272, epoch 9, batch 224/481,disc_loss 79.846, (real 82.866, fake 76.827 ) gen_loss 712.28\n",
            "iteration 4273, epoch 9, batch 225/481,disc_loss 78.768, (real 81.321, fake 76.215 ) gen_loss 757.23\n",
            "iteration 4274, epoch 9, batch 226/481,disc_loss 85.287, (real 88.795, fake 81.779 ) gen_loss 718.69\n",
            "iteration 4275, epoch 9, batch 227/481,disc_loss 79.803, (real 82.649, fake 76.957 ) gen_loss 663.71\n",
            "iteration 4276, epoch 9, batch 228/481,disc_loss 74.455, (real 77.78, fake 71.13 ) gen_loss 739.0\n",
            "iteration 4277, epoch 9, batch 229/481,disc_loss 78.677, (real 81.144, fake 76.21 ) gen_loss 659.97\n",
            "iteration 4278, epoch 9, batch 230/481,disc_loss 81.609, (real 84.347, fake 78.872 ) gen_loss 633.43\n",
            "iteration 4279, epoch 9, batch 231/481,disc_loss 76.486, (real 80.082, fake 72.891 ) gen_loss 697.86\n",
            "iteration 4280, epoch 9, batch 232/481,disc_loss 75.596, (real 78.233, fake 72.96 ) gen_loss 725.63\n",
            "iteration 4281, epoch 9, batch 233/481,disc_loss 76.064, (real 78.727, fake 73.402 ) gen_loss 667.17\n",
            "iteration 4282, epoch 9, batch 234/481,disc_loss 79.757, (real 83.037, fake 76.477 ) gen_loss 663.59\n",
            "iteration 4283, epoch 9, batch 235/481,disc_loss 79.583, (real 82.688, fake 76.478 ) gen_loss 680.68\n",
            "iteration 4284, epoch 9, batch 236/481,disc_loss 77.406, (real 80.064, fake 74.749 ) gen_loss 737.29\n",
            "iteration 4285, epoch 9, batch 237/481,disc_loss 82.301, (real 85.179, fake 79.423 ) gen_loss 665.23\n",
            "iteration 4286, epoch 9, batch 238/481,disc_loss 73.733, (real 77.191, fake 70.274 ) gen_loss 710.75\n",
            "iteration 4287, epoch 9, batch 239/481,disc_loss 79.966, (real 82.818, fake 77.114 ) gen_loss 754.26\n",
            "iteration 4288, epoch 9, batch 240/481,disc_loss 78.664, (real 83.09, fake 74.238 ) gen_loss 666.72\n",
            "iteration 4289, epoch 9, batch 241/481,disc_loss 76.135, (real 78.35, fake 73.92 ) gen_loss 654.55\n",
            "iteration 4290, epoch 9, batch 242/481,disc_loss 81.114, (real 84.259, fake 77.97 ) gen_loss 692.09\n",
            "iteration 4291, epoch 9, batch 243/481,disc_loss 76.849, (real 79.543, fake 74.154 ) gen_loss 721.41\n",
            "iteration 4292, epoch 9, batch 244/481,disc_loss 74.745, (real 78.46, fake 71.03 ) gen_loss 648.5\n",
            "iteration 4293, epoch 9, batch 245/481,disc_loss 73.378, (real 75.872, fake 70.884 ) gen_loss 645.1\n",
            "iteration 4294, epoch 9, batch 246/481,disc_loss 79.551, (real 82.777, fake 76.325 ) gen_loss 627.53\n",
            "iteration 4295, epoch 9, batch 247/481,disc_loss 77.812, (real 80.699, fake 74.925 ) gen_loss 649.81\n",
            "iteration 4296, epoch 9, batch 248/481,disc_loss 76.862, (real 80.38, fake 73.344 ) gen_loss 728.25\n",
            "iteration 4297, epoch 9, batch 249/481,disc_loss 79.9, (real 82.605, fake 77.195 ) gen_loss 603.09\n",
            "iteration 4298, epoch 9, batch 250/481,disc_loss 76.713, (real 80.334, fake 73.093 ) gen_loss 635.39\n",
            "iteration 4299, epoch 9, batch 251/481,disc_loss 73.105, (real 76.242, fake 69.968 ) gen_loss 648.89\n",
            "iteration 4300, epoch 9, batch 252/481,disc_loss 82.279, (real 84.818, fake 79.741 ) gen_loss 698.05\n",
            "iteration 4301, epoch 9, batch 253/481,disc_loss 78.675, (real 81.629, fake 75.722 ) gen_loss 636.85\n",
            "iteration 4302, epoch 9, batch 254/481,disc_loss 77.725, (real 80.759, fake 74.69 ) gen_loss 689.57\n",
            "iteration 4303, epoch 9, batch 255/481,disc_loss 74.928, (real 78.14, fake 71.717 ) gen_loss 666.37\n",
            "iteration 4304, epoch 9, batch 256/481,disc_loss 79.297, (real 82.522, fake 76.071 ) gen_loss 724.51\n",
            "iteration 4305, epoch 9, batch 257/481,disc_loss 75.175, (real 77.705, fake 72.645 ) gen_loss 662.53\n",
            "iteration 4306, epoch 9, batch 258/481,disc_loss 80.361, (real 83.447, fake 77.276 ) gen_loss 661.9\n",
            "iteration 4307, epoch 9, batch 259/481,disc_loss 74.418, (real 77.575, fake 71.261 ) gen_loss 706.68\n",
            "iteration 4308, epoch 9, batch 260/481,disc_loss 75.77, (real 80.082, fake 71.458 ) gen_loss 652.55\n",
            "iteration 4309, epoch 9, batch 261/481,disc_loss 81.128, (real 84.974, fake 77.281 ) gen_loss 651.31\n",
            "iteration 4310, epoch 9, batch 262/481,disc_loss 76.583, (real 79.752, fake 73.413 ) gen_loss 713.56\n",
            "iteration 4311, epoch 9, batch 263/481,disc_loss 76.341, (real 78.554, fake 74.128 ) gen_loss 704.45\n",
            "iteration 4312, epoch 9, batch 264/481,disc_loss 79.369, (real 82.42, fake 76.318 ) gen_loss 715.48\n",
            "iteration 4313, epoch 9, batch 265/481,disc_loss 78.346, (real 81.127, fake 75.564 ) gen_loss 660.25\n",
            "iteration 4314, epoch 9, batch 266/481,disc_loss 76.161, (real 80.028, fake 72.295 ) gen_loss 667.15\n",
            "iteration 4315, epoch 9, batch 267/481,disc_loss 82.478, (real 85.749, fake 79.207 ) gen_loss 729.22\n",
            "iteration 4316, epoch 9, batch 268/481,disc_loss 79.934, (real 83.241, fake 76.626 ) gen_loss 651.2\n",
            "iteration 4317, epoch 9, batch 269/481,disc_loss 71.143, (real 73.516, fake 68.77 ) gen_loss 854.53\n",
            "iteration 4318, epoch 9, batch 270/481,disc_loss 83.005, (real 86.368, fake 79.641 ) gen_loss 744.47\n",
            "iteration 4319, epoch 9, batch 271/481,disc_loss 76.202, (real 78.38, fake 74.024 ) gen_loss 575.07\n",
            "iteration 4320, epoch 9, batch 272/481,disc_loss 76.503, (real 78.894, fake 74.112 ) gen_loss 640.33\n",
            "iteration 4321, epoch 9, batch 273/481,disc_loss 76.998, (real 79.309, fake 74.688 ) gen_loss 632.92\n",
            "iteration 4322, epoch 9, batch 274/481,disc_loss 73.596, (real 77.661, fake 69.53 ) gen_loss 663.75\n",
            "iteration 4323, epoch 9, batch 275/481,disc_loss 76.517, (real 79.184, fake 73.85 ) gen_loss 693.75\n",
            "iteration 4324, epoch 9, batch 276/481,disc_loss 78.148, (real 80.798, fake 75.497 ) gen_loss 579.64\n",
            "iteration 4325, epoch 9, batch 277/481,disc_loss 75.777, (real 77.752, fake 73.803 ) gen_loss 651.07\n",
            "iteration 4326, epoch 9, batch 278/481,disc_loss 78.383, (real 81.833, fake 74.933 ) gen_loss 672.82\n",
            "iteration 4327, epoch 9, batch 279/481,disc_loss 77.567, (real 80.924, fake 74.211 ) gen_loss 759.16\n",
            "iteration 4328, epoch 9, batch 280/481,disc_loss 77.141, (real 79.18, fake 75.101 ) gen_loss 665.56\n",
            "iteration 4329, epoch 9, batch 281/481,disc_loss 85.887, (real 90.198, fake 81.577 ) gen_loss 745.4\n",
            "iteration 4330, epoch 9, batch 282/481,disc_loss 83.694, (real 87.593, fake 79.796 ) gen_loss 712.64\n",
            "iteration 4331, epoch 9, batch 283/481,disc_loss 75.148, (real 78.758, fake 71.538 ) gen_loss 668.63\n",
            "iteration 4332, epoch 9, batch 284/481,disc_loss 76.604, (real 79.046, fake 74.162 ) gen_loss 730.09\n",
            "iteration 4333, epoch 9, batch 285/481,disc_loss 77.305, (real 79.949, fake 74.661 ) gen_loss 658.78\n",
            "iteration 4334, epoch 9, batch 286/481,disc_loss 79.544, (real 82.951, fake 76.136 ) gen_loss 680.63\n",
            "iteration 4335, epoch 9, batch 287/481,disc_loss 81.095, (real 83.68, fake 78.509 ) gen_loss 652.92\n",
            "iteration 4336, epoch 9, batch 288/481,disc_loss 78.998, (real 82.351, fake 75.644 ) gen_loss 599.38\n",
            "iteration 4337, epoch 9, batch 289/481,disc_loss 77.684, (real 80.082, fake 75.286 ) gen_loss 684.53\n",
            "iteration 4338, epoch 9, batch 290/481,disc_loss 75.001, (real 78.122, fake 71.88 ) gen_loss 744.53\n",
            "iteration 4339, epoch 9, batch 291/481,disc_loss 76.757, (real 81.221, fake 72.294 ) gen_loss 693.7\n",
            "iteration 4340, epoch 9, batch 292/481,disc_loss 79.751, (real 82.911, fake 76.59 ) gen_loss 643.69\n",
            "iteration 4341, epoch 9, batch 293/481,disc_loss 74.728, (real 78.244, fake 71.212 ) gen_loss 670.35\n",
            "iteration 4342, epoch 9, batch 294/481,disc_loss 78.082, (real 81.885, fake 74.279 ) gen_loss 660.0\n",
            "iteration 4343, epoch 9, batch 295/481,disc_loss 77.86, (real 81.481, fake 74.24 ) gen_loss 650.48\n",
            "iteration 4344, epoch 9, batch 296/481,disc_loss 79.766, (real 82.934, fake 76.597 ) gen_loss 773.02\n",
            "iteration 4345, epoch 9, batch 297/481,disc_loss 74.739, (real 77.938, fake 71.54 ) gen_loss 705.68\n",
            "iteration 4346, epoch 9, batch 298/481,disc_loss 76.125, (real 79.009, fake 73.24 ) gen_loss 773.33\n",
            "iteration 4347, epoch 9, batch 299/481,disc_loss 81.091, (real 83.553, fake 78.63 ) gen_loss 673.22\n",
            "iteration 4348, epoch 9, batch 300/481,disc_loss 76.541, (real 79.621, fake 73.462 ) gen_loss 752.8\n",
            "iteration 4349, epoch 9, batch 301/481,disc_loss 73.656, (real 76.379, fake 70.934 ) gen_loss 734.17\n",
            "iteration 4350, epoch 9, batch 302/481,disc_loss 74.326, (real 76.921, fake 71.731 ) gen_loss 749.73\n",
            "iteration 4351, epoch 9, batch 303/481,disc_loss 77.283, (real 80.697, fake 73.868 ) gen_loss 639.07\n",
            "iteration 4352, epoch 9, batch 304/481,disc_loss 79.661, (real 82.326, fake 76.995 ) gen_loss 723.5\n",
            "iteration 4353, epoch 9, batch 305/481,disc_loss 80.212, (real 83.374, fake 77.05 ) gen_loss 741.05\n",
            "iteration 4354, epoch 9, batch 306/481,disc_loss 77.92, (real 80.562, fake 75.277 ) gen_loss 728.56\n",
            "iteration 4355, epoch 9, batch 307/481,disc_loss 78.826, (real 81.88, fake 75.771 ) gen_loss 675.18\n",
            "iteration 4356, epoch 9, batch 308/481,disc_loss 79.379, (real 81.855, fake 76.904 ) gen_loss 742.85\n",
            "iteration 4357, epoch 9, batch 309/481,disc_loss 78.244, (real 82.215, fake 74.273 ) gen_loss 732.93\n",
            "iteration 4358, epoch 9, batch 310/481,disc_loss 78.528, (real 81.823, fake 75.233 ) gen_loss 657.63\n",
            "iteration 4359, epoch 9, batch 311/481,disc_loss 80.6, (real 84.296, fake 76.904 ) gen_loss 625.08\n",
            "iteration 4360, epoch 9, batch 312/481,disc_loss 81.851, (real 84.962, fake 78.74 ) gen_loss 659.04\n",
            "iteration 4361, epoch 9, batch 313/481,disc_loss 77.589, (real 80.182, fake 74.997 ) gen_loss 777.46\n",
            "iteration 4362, epoch 9, batch 314/481,disc_loss 75.576, (real 78.647, fake 72.505 ) gen_loss 753.9\n",
            "iteration 4363, epoch 9, batch 315/481,disc_loss 80.766, (real 83.821, fake 77.711 ) gen_loss 701.77\n",
            "iteration 4364, epoch 9, batch 316/481,disc_loss 75.948, (real 78.139, fake 73.756 ) gen_loss 682.42\n",
            "iteration 4365, epoch 9, batch 317/481,disc_loss 77.503, (real 80.232, fake 74.774 ) gen_loss 740.86\n",
            "iteration 4366, epoch 9, batch 318/481,disc_loss 78.385, (real 81.181, fake 75.59 ) gen_loss 689.94\n",
            "iteration 4367, epoch 9, batch 319/481,disc_loss 75.835, (real 79.392, fake 72.278 ) gen_loss 711.11\n",
            "iteration 4368, epoch 9, batch 320/481,disc_loss 80.784, (real 83.824, fake 77.744 ) gen_loss 724.66\n",
            "iteration 4369, epoch 9, batch 321/481,disc_loss 82.256, (real 86.09, fake 78.422 ) gen_loss 652.56\n",
            "iteration 4370, epoch 9, batch 322/481,disc_loss 81.046, (real 84.031, fake 78.061 ) gen_loss 631.17\n",
            "iteration 4371, epoch 9, batch 323/481,disc_loss 77.039, (real 79.856, fake 74.222 ) gen_loss 654.37\n",
            "iteration 4372, epoch 9, batch 324/481,disc_loss 78.314, (real 82.257, fake 74.372 ) gen_loss 667.56\n",
            "iteration 4373, epoch 9, batch 325/481,disc_loss 81.705, (real 84.819, fake 78.591 ) gen_loss 676.88\n",
            "iteration 4374, epoch 9, batch 326/481,disc_loss 79.227, (real 84.3, fake 74.153 ) gen_loss 733.92\n",
            "iteration 4375, epoch 9, batch 327/481,disc_loss 74.625, (real 78.424, fake 70.827 ) gen_loss 705.02\n",
            "iteration 4376, epoch 9, batch 328/481,disc_loss 81.131, (real 84.592, fake 77.67 ) gen_loss 692.78\n",
            "iteration 4377, epoch 9, batch 329/481,disc_loss 75.532, (real 79.329, fake 71.736 ) gen_loss 753.11\n",
            "iteration 4378, epoch 9, batch 330/481,disc_loss 80.349, (real 82.927, fake 77.772 ) gen_loss 791.14\n",
            "iteration 4379, epoch 9, batch 331/481,disc_loss 77.326, (real 80.533, fake 74.119 ) gen_loss 684.17\n",
            "iteration 4380, epoch 9, batch 332/481,disc_loss 78.928, (real 81.222, fake 76.634 ) gen_loss 609.93\n",
            "iteration 4381, epoch 9, batch 333/481,disc_loss 77.576, (real 80.854, fake 74.298 ) gen_loss 710.45\n",
            "iteration 4382, epoch 9, batch 334/481,disc_loss 76.14, (real 78.903, fake 73.376 ) gen_loss 689.38\n",
            "iteration 4383, epoch 9, batch 335/481,disc_loss 75.407, (real 79.114, fake 71.7 ) gen_loss 706.83\n",
            "iteration 4384, epoch 9, batch 336/481,disc_loss 75.183, (real 77.961, fake 72.406 ) gen_loss 623.53\n",
            "iteration 4385, epoch 9, batch 337/481,disc_loss 77.147, (real 80.28, fake 74.014 ) gen_loss 669.19\n",
            "iteration 4386, epoch 9, batch 338/481,disc_loss 74.378, (real 78.156, fake 70.601 ) gen_loss 607.15\n",
            "iteration 4387, epoch 9, batch 339/481,disc_loss 76.361, (real 79.192, fake 73.53 ) gen_loss 611.68\n",
            "iteration 4388, epoch 9, batch 340/481,disc_loss 80.151, (real 83.437, fake 76.866 ) gen_loss 625.61\n",
            "iteration 4389, epoch 9, batch 341/481,disc_loss 80.163, (real 83.335, fake 76.992 ) gen_loss 680.6\n",
            "iteration 4390, epoch 9, batch 342/481,disc_loss 79.568, (real 82.862, fake 76.274 ) gen_loss 684.3\n",
            "iteration 4391, epoch 9, batch 343/481,disc_loss 79.947, (real 82.208, fake 77.686 ) gen_loss 664.91\n",
            "iteration 4392, epoch 9, batch 344/481,disc_loss 74.92, (real 77.262, fake 72.578 ) gen_loss 646.47\n",
            "iteration 4393, epoch 9, batch 345/481,disc_loss 74.997, (real 77.722, fake 72.272 ) gen_loss 732.23\n",
            "iteration 4394, epoch 9, batch 346/481,disc_loss 75.674, (real 78.44, fake 72.907 ) gen_loss 728.73\n",
            "iteration 4395, epoch 9, batch 347/481,disc_loss 80.62, (real 84.226, fake 77.013 ) gen_loss 691.91\n",
            "iteration 4396, epoch 9, batch 348/481,disc_loss 78.604, (real 80.605, fake 76.604 ) gen_loss 716.63\n",
            "iteration 4397, epoch 9, batch 349/481,disc_loss 80.548, (real 83.778, fake 77.318 ) gen_loss 640.55\n",
            "iteration 4398, epoch 9, batch 350/481,disc_loss 76.13, (real 79.511, fake 72.749 ) gen_loss 676.46\n",
            "iteration 4399, epoch 9, batch 351/481,disc_loss 74.244, (real 76.573, fake 71.915 ) gen_loss 670.38\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 4400, epoch 9, batch 352/481,disc_loss 77.574, (real 80.787, fake 74.36 ) gen_loss 678.47\n",
            "iteration 4401, epoch 9, batch 353/481,disc_loss 78.845, (real 82.177, fake 75.514 ) gen_loss 610.49\n",
            "iteration 4402, epoch 9, batch 354/481,disc_loss 77.51, (real 80.506, fake 74.514 ) gen_loss 673.52\n",
            "iteration 4403, epoch 9, batch 355/481,disc_loss 79.98, (real 82.654, fake 77.306 ) gen_loss 635.48\n",
            "iteration 4404, epoch 9, batch 356/481,disc_loss 80.333, (real 83.695, fake 76.971 ) gen_loss 655.78\n",
            "iteration 4405, epoch 9, batch 357/481,disc_loss 72.981, (real 75.278, fake 70.685 ) gen_loss 705.19\n",
            "iteration 4406, epoch 9, batch 358/481,disc_loss 73.327, (real 76.898, fake 69.755 ) gen_loss 778.33\n",
            "iteration 4407, epoch 9, batch 359/481,disc_loss 78.827, (real 82.399, fake 75.254 ) gen_loss 793.37\n",
            "iteration 4408, epoch 9, batch 360/481,disc_loss 79.854, (real 82.556, fake 77.152 ) gen_loss 742.47\n",
            "iteration 4409, epoch 9, batch 361/481,disc_loss 82.187, (real 85.145, fake 79.228 ) gen_loss 720.66\n",
            "iteration 4410, epoch 9, batch 362/481,disc_loss 77.348, (real 79.97, fake 74.727 ) gen_loss 647.8\n",
            "iteration 4411, epoch 9, batch 363/481,disc_loss 81.318, (real 83.729, fake 78.906 ) gen_loss 671.63\n",
            "iteration 4412, epoch 9, batch 364/481,disc_loss 74.763, (real 77.604, fake 71.922 ) gen_loss 726.13\n",
            "iteration 4413, epoch 9, batch 365/481,disc_loss 77.412, (real 80.598, fake 74.226 ) gen_loss 647.42\n",
            "iteration 4414, epoch 9, batch 366/481,disc_loss 80.458, (real 83.433, fake 77.483 ) gen_loss 650.19\n",
            "iteration 4415, epoch 9, batch 367/481,disc_loss 81.42, (real 84.032, fake 78.808 ) gen_loss 703.86\n",
            "iteration 4416, epoch 9, batch 368/481,disc_loss 76.333, (real 78.855, fake 73.812 ) gen_loss 593.74\n",
            "iteration 4417, epoch 9, batch 369/481,disc_loss 80.22, (real 82.716, fake 77.724 ) gen_loss 735.28\n",
            "iteration 4418, epoch 9, batch 370/481,disc_loss 80.437, (real 83.809, fake 77.065 ) gen_loss 682.2\n",
            "iteration 4419, epoch 9, batch 371/481,disc_loss 77.521, (real 80.535, fake 74.508 ) gen_loss 696.23\n",
            "iteration 4420, epoch 9, batch 372/481,disc_loss 78.508, (real 82.219, fake 74.797 ) gen_loss 679.04\n",
            "iteration 4421, epoch 9, batch 373/481,disc_loss 80.281, (real 83.279, fake 77.282 ) gen_loss 755.74\n",
            "iteration 4422, epoch 9, batch 374/481,disc_loss 77.09, (real 79.097, fake 75.084 ) gen_loss 670.94\n",
            "iteration 4423, epoch 9, batch 375/481,disc_loss 78.642, (real 80.986, fake 76.299 ) gen_loss 640.56\n",
            "iteration 4424, epoch 9, batch 376/481,disc_loss 78.763, (real 82.503, fake 75.023 ) gen_loss 681.04\n",
            "iteration 4425, epoch 9, batch 377/481,disc_loss 76.064, (real 78.741, fake 73.387 ) gen_loss 719.62\n",
            "iteration 4426, epoch 9, batch 378/481,disc_loss 75.272, (real 77.959, fake 72.584 ) gen_loss 607.67\n",
            "iteration 4427, epoch 9, batch 379/481,disc_loss 79.005, (real 82.747, fake 75.264 ) gen_loss 680.15\n",
            "iteration 4428, epoch 9, batch 380/481,disc_loss 79.235, (real 82.291, fake 76.179 ) gen_loss 629.78\n",
            "iteration 4429, epoch 9, batch 381/481,disc_loss 78.968, (real 82.546, fake 75.389 ) gen_loss 727.26\n",
            "iteration 4430, epoch 9, batch 382/481,disc_loss 78.134, (real 81.274, fake 74.995 ) gen_loss 620.12\n",
            "iteration 4431, epoch 9, batch 383/481,disc_loss 82.974, (real 87.086, fake 78.863 ) gen_loss 722.39\n",
            "iteration 4432, epoch 9, batch 384/481,disc_loss 81.708, (real 85.034, fake 78.383 ) gen_loss 789.31\n",
            "iteration 4433, epoch 9, batch 385/481,disc_loss 78.678, (real 81.757, fake 75.599 ) gen_loss 737.34\n",
            "iteration 4434, epoch 9, batch 386/481,disc_loss 74.369, (real 77.78, fake 70.957 ) gen_loss 719.56\n",
            "iteration 4435, epoch 9, batch 387/481,disc_loss 77.612, (real 81.083, fake 74.142 ) gen_loss 689.84\n",
            "iteration 4436, epoch 9, batch 388/481,disc_loss 74.374, (real 76.9, fake 71.849 ) gen_loss 637.18\n",
            "iteration 4437, epoch 9, batch 389/481,disc_loss 78.285, (real 81.085, fake 75.485 ) gen_loss 721.4\n",
            "iteration 4438, epoch 9, batch 390/481,disc_loss 81.497, (real 84.656, fake 78.339 ) gen_loss 872.69\n",
            "iteration 4439, epoch 9, batch 391/481,disc_loss 78.907, (real 82.228, fake 75.587 ) gen_loss 646.91\n",
            "iteration 4440, epoch 9, batch 392/481,disc_loss 75.396, (real 78.348, fake 72.444 ) gen_loss 707.71\n",
            "iteration 4441, epoch 9, batch 393/481,disc_loss 77.962, (real 81.859, fake 74.066 ) gen_loss 720.95\n",
            "iteration 4442, epoch 9, batch 394/481,disc_loss 74.702, (real 77.529, fake 71.874 ) gen_loss 703.78\n",
            "iteration 4443, epoch 9, batch 395/481,disc_loss 76.081, (real 78.474, fake 73.688 ) gen_loss 668.25\n",
            "iteration 4444, epoch 9, batch 396/481,disc_loss 77.76, (real 80.467, fake 75.052 ) gen_loss 690.74\n",
            "iteration 4445, epoch 9, batch 397/481,disc_loss 79.477, (real 82.382, fake 76.573 ) gen_loss 693.81\n",
            "iteration 4446, epoch 9, batch 398/481,disc_loss 81.805, (real 84.136, fake 79.474 ) gen_loss 639.1\n",
            "iteration 4447, epoch 9, batch 399/481,disc_loss 80.153, (real 83.071, fake 77.235 ) gen_loss 634.65\n",
            "iteration 4448, epoch 9, batch 400/481,disc_loss 79.039, (real 81.705, fake 76.373 ) gen_loss 623.23\n",
            "iteration 4449, epoch 9, batch 401/481,disc_loss 76.053, (real 79.109, fake 72.998 ) gen_loss 677.36\n",
            "iteration 4450, epoch 9, batch 402/481,disc_loss 73.485, (real 75.84, fake 71.131 ) gen_loss 665.54\n",
            "iteration 4451, epoch 9, batch 403/481,disc_loss 79.799, (real 83.839, fake 75.76 ) gen_loss 737.32\n",
            "iteration 4452, epoch 9, batch 404/481,disc_loss 74.96, (real 78.713, fake 71.207 ) gen_loss 716.37\n",
            "iteration 4453, epoch 9, batch 405/481,disc_loss 77.397, (real 81.253, fake 73.541 ) gen_loss 691.21\n",
            "iteration 4454, epoch 9, batch 406/481,disc_loss 75.643, (real 78.297, fake 72.988 ) gen_loss 717.43\n",
            "iteration 4455, epoch 9, batch 407/481,disc_loss 78.373, (real 81.603, fake 75.143 ) gen_loss 679.18\n",
            "iteration 4456, epoch 9, batch 408/481,disc_loss 77.34, (real 80.465, fake 74.216 ) gen_loss 743.18\n",
            "iteration 4457, epoch 9, batch 409/481,disc_loss 80.759, (real 84.017, fake 77.5 ) gen_loss 650.22\n",
            "iteration 4458, epoch 9, batch 410/481,disc_loss 76.857, (real 79.804, fake 73.91 ) gen_loss 754.49\n",
            "iteration 4459, epoch 9, batch 411/481,disc_loss 79.912, (real 82.814, fake 77.009 ) gen_loss 692.89\n",
            "iteration 4460, epoch 9, batch 412/481,disc_loss 76.627, (real 80.273, fake 72.981 ) gen_loss 746.01\n",
            "iteration 4461, epoch 9, batch 413/481,disc_loss 80.325, (real 83.068, fake 77.582 ) gen_loss 707.5\n",
            "iteration 4462, epoch 9, batch 414/481,disc_loss 80.359, (real 82.948, fake 77.771 ) gen_loss 700.76\n",
            "iteration 4463, epoch 9, batch 415/481,disc_loss 81.266, (real 83.76, fake 78.773 ) gen_loss 612.1\n",
            "iteration 4464, epoch 9, batch 416/481,disc_loss 76.385, (real 78.912, fake 73.858 ) gen_loss 720.32\n",
            "iteration 4465, epoch 9, batch 417/481,disc_loss 82.195, (real 85.578, fake 78.813 ) gen_loss 724.16\n",
            "iteration 4466, epoch 9, batch 418/481,disc_loss 75.988, (real 79.769, fake 72.206 ) gen_loss 789.92\n",
            "iteration 4467, epoch 9, batch 419/481,disc_loss 77.888, (real 80.762, fake 75.013 ) gen_loss 697.14\n",
            "iteration 4468, epoch 9, batch 420/481,disc_loss 77.358, (real 80.05, fake 74.665 ) gen_loss 680.56\n",
            "iteration 4469, epoch 9, batch 421/481,disc_loss 74.036, (real 76.693, fake 71.379 ) gen_loss 674.93\n",
            "iteration 4470, epoch 9, batch 422/481,disc_loss 77.828, (real 80.368, fake 75.287 ) gen_loss 667.13\n",
            "iteration 4471, epoch 9, batch 423/481,disc_loss 78.104, (real 82.217, fake 73.991 ) gen_loss 696.25\n",
            "iteration 4472, epoch 9, batch 424/481,disc_loss 78.26, (real 80.884, fake 75.636 ) gen_loss 690.15\n",
            "iteration 4473, epoch 9, batch 425/481,disc_loss 79.838, (real 83.596, fake 76.08 ) gen_loss 668.9\n",
            "iteration 4474, epoch 9, batch 426/481,disc_loss 81.334, (real 83.591, fake 79.076 ) gen_loss 759.79\n",
            "iteration 4475, epoch 9, batch 427/481,disc_loss 78.497, (real 81.043, fake 75.951 ) gen_loss 836.43\n",
            "iteration 4476, epoch 9, batch 428/481,disc_loss 80.088, (real 82.782, fake 77.394 ) gen_loss 762.86\n",
            "iteration 4477, epoch 9, batch 429/481,disc_loss 77.155, (real 80.216, fake 74.093 ) gen_loss 695.89\n",
            "iteration 4478, epoch 9, batch 430/481,disc_loss 82.829, (real 85.942, fake 79.715 ) gen_loss 666.27\n",
            "iteration 4479, epoch 9, batch 431/481,disc_loss 80.45, (real 83.111, fake 77.789 ) gen_loss 686.17\n",
            "iteration 4480, epoch 9, batch 432/481,disc_loss 82.091, (real 85.076, fake 79.107 ) gen_loss 774.08\n",
            "iteration 4481, epoch 9, batch 433/481,disc_loss 80.78, (real 83.933, fake 77.628 ) gen_loss 664.81\n",
            "iteration 4482, epoch 9, batch 434/481,disc_loss 80.231, (real 83.71, fake 76.751 ) gen_loss 754.31\n",
            "iteration 4483, epoch 9, batch 435/481,disc_loss 82.979, (real 86.235, fake 79.723 ) gen_loss 740.39\n",
            "iteration 4484, epoch 9, batch 436/481,disc_loss 81.673, (real 85.147, fake 78.199 ) gen_loss 710.52\n",
            "iteration 4485, epoch 9, batch 437/481,disc_loss 80.301, (real 83.395, fake 77.207 ) gen_loss 663.53\n",
            "iteration 4486, epoch 9, batch 438/481,disc_loss 78.033, (real 80.876, fake 75.191 ) gen_loss 706.91\n",
            "iteration 4487, epoch 9, batch 439/481,disc_loss 76.844, (real 79.878, fake 73.811 ) gen_loss 723.74\n",
            "iteration 4488, epoch 9, batch 440/481,disc_loss 79.304, (real 83.433, fake 75.176 ) gen_loss 767.5\n",
            "iteration 4489, epoch 9, batch 441/481,disc_loss 75.85, (real 78.944, fake 72.756 ) gen_loss 773.9\n",
            "iteration 4490, epoch 9, batch 442/481,disc_loss 79.108, (real 82.301, fake 75.915 ) gen_loss 737.65\n",
            "iteration 4491, epoch 9, batch 443/481,disc_loss 82.696, (real 86.231, fake 79.161 ) gen_loss 735.29\n",
            "iteration 4492, epoch 9, batch 444/481,disc_loss 75.231, (real 77.482, fake 72.981 ) gen_loss 750.35\n",
            "iteration 4493, epoch 9, batch 445/481,disc_loss 76.568, (real 79.603, fake 73.533 ) gen_loss 757.96\n",
            "iteration 4494, epoch 9, batch 446/481,disc_loss 80.295, (real 82.564, fake 78.027 ) gen_loss 704.75\n",
            "iteration 4495, epoch 9, batch 447/481,disc_loss 79.522, (real 82.188, fake 76.856 ) gen_loss 713.34\n",
            "iteration 4496, epoch 9, batch 448/481,disc_loss 79.326, (real 82.894, fake 75.757 ) gen_loss 626.51\n",
            "iteration 4497, epoch 9, batch 449/481,disc_loss 76.886, (real 79.602, fake 74.171 ) gen_loss 650.42\n",
            "iteration 4498, epoch 9, batch 450/481,disc_loss 80.552, (real 82.83, fake 78.274 ) gen_loss 697.66\n",
            "iteration 4499, epoch 9, batch 451/481,disc_loss 77.211, (real 80.51, fake 73.913 ) gen_loss 682.63\n",
            "iteration 4500, epoch 9, batch 452/481,disc_loss 78.097, (real 81.413, fake 74.782 ) gen_loss 628.22\n",
            "iteration 4501, epoch 9, batch 453/481,disc_loss 77.541, (real 80.177, fake 74.906 ) gen_loss 651.43\n",
            "iteration 4502, epoch 9, batch 454/481,disc_loss 77.346, (real 79.38, fake 75.312 ) gen_loss 681.03\n",
            "iteration 4503, epoch 9, batch 455/481,disc_loss 80.807, (real 83.297, fake 78.317 ) gen_loss 766.05\n",
            "iteration 4504, epoch 9, batch 456/481,disc_loss 79.462, (real 81.687, fake 77.238 ) gen_loss 714.97\n",
            "iteration 4505, epoch 9, batch 457/481,disc_loss 75.636, (real 78.484, fake 72.787 ) gen_loss 681.61\n",
            "iteration 4506, epoch 9, batch 458/481,disc_loss 80.249, (real 82.485, fake 78.013 ) gen_loss 665.46\n",
            "iteration 4507, epoch 9, batch 459/481,disc_loss 80.947, (real 83.07, fake 78.825 ) gen_loss 704.06\n",
            "iteration 4508, epoch 9, batch 460/481,disc_loss 78.637, (real 81.563, fake 75.712 ) gen_loss 743.92\n",
            "iteration 4509, epoch 9, batch 461/481,disc_loss 73.799, (real 75.775, fake 71.823 ) gen_loss 704.74\n",
            "iteration 4510, epoch 9, batch 462/481,disc_loss 82.072, (real 85.44, fake 78.703 ) gen_loss 719.29\n",
            "iteration 4511, epoch 9, batch 463/481,disc_loss 75.806, (real 78.167, fake 73.446 ) gen_loss 675.46\n",
            "iteration 4512, epoch 9, batch 464/481,disc_loss 78.077, (real 81.73, fake 74.425 ) gen_loss 755.65\n",
            "iteration 4513, epoch 9, batch 465/481,disc_loss 75.967, (real 78.803, fake 73.131 ) gen_loss 637.14\n",
            "iteration 4514, epoch 9, batch 466/481,disc_loss 74.695, (real 78.03, fake 71.36 ) gen_loss 690.54\n",
            "iteration 4515, epoch 9, batch 467/481,disc_loss 76.876, (real 80.809, fake 72.943 ) gen_loss 648.58\n",
            "iteration 4516, epoch 9, batch 468/481,disc_loss 76.192, (real 78.972, fake 73.412 ) gen_loss 738.58\n",
            "iteration 4517, epoch 9, batch 469/481,disc_loss 81.126, (real 83.582, fake 78.671 ) gen_loss 771.78\n",
            "iteration 4518, epoch 9, batch 470/481,disc_loss 84.107, (real 87.777, fake 80.438 ) gen_loss 763.65\n",
            "iteration 4519, epoch 9, batch 471/481,disc_loss 82.999, (real 87.432, fake 78.566 ) gen_loss 690.23\n",
            "iteration 4520, epoch 9, batch 472/481,disc_loss 79.547, (real 82.453, fake 76.641 ) gen_loss 699.43\n",
            "iteration 4521, epoch 9, batch 473/481,disc_loss 78.789, (real 81.768, fake 75.81 ) gen_loss 707.51\n",
            "iteration 4522, epoch 9, batch 474/481,disc_loss 78.181, (real 81.83, fake 74.531 ) gen_loss 665.34\n",
            "iteration 4523, epoch 9, batch 475/481,disc_loss 78.824, (real 82.027, fake 75.621 ) gen_loss 712.21\n",
            "iteration 4524, epoch 9, batch 476/481,disc_loss 81.539, (real 85.408, fake 77.669 ) gen_loss 740.25\n",
            "iteration 4525, epoch 9, batch 477/481,disc_loss 73.074, (real 75.594, fake 70.553 ) gen_loss 734.07\n",
            "iteration 4526, epoch 9, batch 478/481,disc_loss 79.044, (real 82.269, fake 75.819 ) gen_loss 830.82\n",
            "iteration 4527, epoch 9, batch 479/481,disc_loss 76.648, (real 79.587, fake 73.71 ) gen_loss 836.71\n",
            "iteration 4528, epoch 9, batch 480/481,disc_loss 76.47, (real 79.409, fake 73.531 ) gen_loss 709.37\n",
            "iteration 4529, epoch 9, batch 481/481,disc_loss 79.834, (real 82.455, fake 77.213 ) gen_loss 811.63\n",
            "iteration 4530, epoch 10, batch 1/481,disc_loss 77.033, (real 79.455, fake 74.612 ) gen_loss 843.62\n",
            "iteration 4531, epoch 10, batch 2/481,disc_loss 75.192, (real 78.031, fake 72.353 ) gen_loss 618.42\n",
            "iteration 4532, epoch 10, batch 3/481,disc_loss 75.23, (real 77.626, fake 72.833 ) gen_loss 660.4\n",
            "iteration 4533, epoch 10, batch 4/481,disc_loss 76.915, (real 79.66, fake 74.17 ) gen_loss 707.92\n",
            "iteration 4534, epoch 10, batch 5/481,disc_loss 79.903, (real 83.887, fake 75.918 ) gen_loss 816.4\n",
            "iteration 4535, epoch 10, batch 6/481,disc_loss 81.92, (real 84.703, fake 79.137 ) gen_loss 744.51\n",
            "iteration 4536, epoch 10, batch 7/481,disc_loss 72.044, (real 75.123, fake 68.965 ) gen_loss 778.15\n",
            "iteration 4537, epoch 10, batch 8/481,disc_loss 74.418, (real 77.384, fake 71.453 ) gen_loss 685.77\n",
            "iteration 4538, epoch 10, batch 9/481,disc_loss 72.215, (real 75.305, fake 69.126 ) gen_loss 705.86\n",
            "iteration 4539, epoch 10, batch 10/481,disc_loss 76.439, (real 79.273, fake 73.605 ) gen_loss 725.67\n",
            "iteration 4540, epoch 10, batch 11/481,disc_loss 81.251, (real 82.826, fake 79.675 ) gen_loss 805.19\n",
            "iteration 4541, epoch 10, batch 12/481,disc_loss 78.021, (real 79.732, fake 76.31 ) gen_loss 669.48\n",
            "iteration 4542, epoch 10, batch 13/481,disc_loss 82.324, (real 84.299, fake 80.348 ) gen_loss 747.42\n",
            "iteration 4543, epoch 10, batch 14/481,disc_loss 76.609, (real 79.11, fake 74.108 ) gen_loss 768.69\n",
            "iteration 4544, epoch 10, batch 15/481,disc_loss 81.058, (real 83.418, fake 78.697 ) gen_loss 736.29\n",
            "iteration 4545, epoch 10, batch 16/481,disc_loss 78.611, (real 81.162, fake 76.061 ) gen_loss 726.5\n",
            "iteration 4546, epoch 10, batch 17/481,disc_loss 74.359, (real 77.385, fake 71.333 ) gen_loss 743.4\n",
            "iteration 4547, epoch 10, batch 18/481,disc_loss 76.258, (real 78.995, fake 73.521 ) gen_loss 822.75\n",
            "iteration 4548, epoch 10, batch 19/481,disc_loss 77.315, (real 79.969, fake 74.661 ) gen_loss 762.64\n",
            "iteration 4549, epoch 10, batch 20/481,disc_loss 80.119, (real 82.892, fake 77.346 ) gen_loss 682.15\n",
            "iteration 4550, epoch 10, batch 21/481,disc_loss 80.55, (real 83.241, fake 77.859 ) gen_loss 689.3\n",
            "iteration 4551, epoch 10, batch 22/481,disc_loss 77.462, (real 80.094, fake 74.83 ) gen_loss 769.91\n",
            "iteration 4552, epoch 10, batch 23/481,disc_loss 78.761, (real 80.73, fake 76.791 ) gen_loss 700.5\n",
            "iteration 4553, epoch 10, batch 24/481,disc_loss 74.122, (real 76.031, fake 72.214 ) gen_loss 680.23\n",
            "iteration 4554, epoch 10, batch 25/481,disc_loss 77.817, (real 80.818, fake 74.815 ) gen_loss 774.63\n",
            "iteration 4555, epoch 10, batch 26/481,disc_loss 76.96, (real 78.908, fake 75.012 ) gen_loss 784.72\n",
            "iteration 4556, epoch 10, batch 27/481,disc_loss 80.503, (real 84.083, fake 76.924 ) gen_loss 732.98\n",
            "iteration 4557, epoch 10, batch 28/481,disc_loss 72.493, (real 76.078, fake 68.908 ) gen_loss 718.52\n",
            "iteration 4558, epoch 10, batch 29/481,disc_loss 75.908, (real 78.738, fake 73.079 ) gen_loss 696.68\n",
            "iteration 4559, epoch 10, batch 30/481,disc_loss 81.901, (real 84.895, fake 78.907 ) gen_loss 711.72\n",
            "iteration 4560, epoch 10, batch 31/481,disc_loss 74.46, (real 77.207, fake 71.712 ) gen_loss 719.33\n",
            "iteration 4561, epoch 10, batch 32/481,disc_loss 76.452, (real 79.039, fake 73.865 ) gen_loss 682.26\n",
            "iteration 4562, epoch 10, batch 33/481,disc_loss 78.324, (real 79.51, fake 77.138 ) gen_loss 825.68\n",
            "iteration 4563, epoch 10, batch 34/481,disc_loss 76.114, (real 79.526, fake 72.702 ) gen_loss 813.09\n",
            "iteration 4564, epoch 10, batch 35/481,disc_loss 74.559, (real 76.754, fake 72.364 ) gen_loss 655.19\n",
            "iteration 4565, epoch 10, batch 36/481,disc_loss 79.417, (real 83.183, fake 75.651 ) gen_loss 771.08\n",
            "iteration 4566, epoch 10, batch 37/481,disc_loss 78.807, (real 80.873, fake 76.741 ) gen_loss 698.72\n",
            "iteration 4567, epoch 10, batch 38/481,disc_loss 79.895, (real 82.375, fake 77.414 ) gen_loss 697.65\n",
            "iteration 4568, epoch 10, batch 39/481,disc_loss 75.821, (real 78.187, fake 73.456 ) gen_loss 641.1\n",
            "iteration 4569, epoch 10, batch 40/481,disc_loss 75.599, (real 78.291, fake 72.907 ) gen_loss 670.69\n",
            "iteration 4570, epoch 10, batch 41/481,disc_loss 75.549, (real 78.428, fake 72.671 ) gen_loss 680.85\n",
            "iteration 4571, epoch 10, batch 42/481,disc_loss 76.126, (real 79.218, fake 73.034 ) gen_loss 709.36\n",
            "iteration 4572, epoch 10, batch 43/481,disc_loss 80.898, (real 83.918, fake 77.878 ) gen_loss 790.23\n",
            "iteration 4573, epoch 10, batch 44/481,disc_loss 80.55, (real 82.989, fake 78.111 ) gen_loss 750.44\n",
            "iteration 4574, epoch 10, batch 45/481,disc_loss 73.627, (real 76.627, fake 70.627 ) gen_loss 686.6\n",
            "iteration 4575, epoch 10, batch 46/481,disc_loss 75.253, (real 77.132, fake 73.374 ) gen_loss 680.66\n",
            "iteration 4576, epoch 10, batch 47/481,disc_loss 82.455, (real 85.201, fake 79.708 ) gen_loss 774.98\n",
            "iteration 4577, epoch 10, batch 48/481,disc_loss 78.885, (real 82.732, fake 75.039 ) gen_loss 722.98\n",
            "iteration 4578, epoch 10, batch 49/481,disc_loss 76.643, (real 79.206, fake 74.08 ) gen_loss 692.83\n",
            "iteration 4579, epoch 10, batch 50/481,disc_loss 82.646, (real 84.839, fake 80.453 ) gen_loss 752.6\n",
            "iteration 4580, epoch 10, batch 51/481,disc_loss 79.084, (real 81.534, fake 76.634 ) gen_loss 729.72\n",
            "iteration 4581, epoch 10, batch 52/481,disc_loss 75.379, (real 77.37, fake 73.389 ) gen_loss 791.5\n",
            "iteration 4582, epoch 10, batch 53/481,disc_loss 72.53, (real 74.82, fake 70.241 ) gen_loss 714.83\n",
            "iteration 4583, epoch 10, batch 54/481,disc_loss 75.316, (real 78.095, fake 72.537 ) gen_loss 769.94\n",
            "iteration 4584, epoch 10, batch 55/481,disc_loss 78.836, (real 82.064, fake 75.607 ) gen_loss 769.58\n",
            "iteration 4585, epoch 10, batch 56/481,disc_loss 79.442, (real 82.633, fake 76.252 ) gen_loss 711.75\n",
            "iteration 4586, epoch 10, batch 57/481,disc_loss 80.617, (real 83.833, fake 77.401 ) gen_loss 752.05\n",
            "iteration 4587, epoch 10, batch 58/481,disc_loss 76.608, (real 79.256, fake 73.96 ) gen_loss 670.33\n",
            "iteration 4588, epoch 10, batch 59/481,disc_loss 77.245, (real 79.844, fake 74.645 ) gen_loss 686.9\n",
            "iteration 4589, epoch 10, batch 60/481,disc_loss 75.921, (real 78.55, fake 73.292 ) gen_loss 774.51\n",
            "iteration 4590, epoch 10, batch 61/481,disc_loss 74.511, (real 77.257, fake 71.765 ) gen_loss 649.74\n",
            "iteration 4591, epoch 10, batch 62/481,disc_loss 77.293, (real 79.837, fake 74.748 ) gen_loss 831.31\n",
            "iteration 4592, epoch 10, batch 63/481,disc_loss 82.149, (real 84.235, fake 80.063 ) gen_loss 725.19\n",
            "iteration 4593, epoch 10, batch 64/481,disc_loss 78.913, (real 81.265, fake 76.561 ) gen_loss 789.65\n",
            "iteration 4594, epoch 10, batch 65/481,disc_loss 77.125, (real 80.123, fake 74.127 ) gen_loss 719.08\n",
            "iteration 4595, epoch 10, batch 66/481,disc_loss 77.337, (real 80.088, fake 74.586 ) gen_loss 700.19\n",
            "iteration 4596, epoch 10, batch 67/481,disc_loss 81.763, (real 84.7, fake 78.825 ) gen_loss 701.24\n",
            "iteration 4597, epoch 10, batch 68/481,disc_loss 75.336, (real 78.54, fake 72.132 ) gen_loss 708.06\n",
            "iteration 4598, epoch 10, batch 69/481,disc_loss 79.493, (real 81.302, fake 77.684 ) gen_loss 713.07\n",
            "iteration 4599, epoch 10, batch 70/481,disc_loss 76.686, (real 78.906, fake 74.466 ) gen_loss 786.59\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 4600, epoch 10, batch 71/481,disc_loss 78.399, (real 81.587, fake 75.212 ) gen_loss 793.29\n",
            "iteration 4601, epoch 10, batch 72/481,disc_loss 78.477, (real 81.597, fake 75.357 ) gen_loss 788.97\n",
            "iteration 4602, epoch 10, batch 73/481,disc_loss 76.259, (real 79.195, fake 73.323 ) gen_loss 714.2\n",
            "iteration 4603, epoch 10, batch 74/481,disc_loss 78.587, (real 81.909, fake 75.265 ) gen_loss 736.55\n",
            "iteration 4604, epoch 10, batch 75/481,disc_loss 77.627, (real 80.164, fake 75.089 ) gen_loss 771.61\n",
            "iteration 4605, epoch 10, batch 76/481,disc_loss 78.964, (real 81.354, fake 76.573 ) gen_loss 773.09\n",
            "iteration 4606, epoch 10, batch 77/481,disc_loss 72.998, (real 76.007, fake 69.989 ) gen_loss 754.55\n",
            "iteration 4607, epoch 10, batch 78/481,disc_loss 78.706, (real 81.784, fake 75.628 ) gen_loss 673.76\n",
            "iteration 4608, epoch 10, batch 79/481,disc_loss 78.74, (real 81.543, fake 75.937 ) gen_loss 784.34\n",
            "iteration 4609, epoch 10, batch 80/481,disc_loss 74.61, (real 77.351, fake 71.87 ) gen_loss 755.08\n",
            "iteration 4610, epoch 10, batch 81/481,disc_loss 74.575, (real 77.831, fake 71.319 ) gen_loss 677.93\n",
            "iteration 4611, epoch 10, batch 82/481,disc_loss 80.682, (real 83.372, fake 77.991 ) gen_loss 779.03\n",
            "iteration 4612, epoch 10, batch 83/481,disc_loss 83.789, (real 86.603, fake 80.974 ) gen_loss 762.98\n",
            "iteration 4613, epoch 10, batch 84/481,disc_loss 78.494, (real 81.112, fake 75.877 ) gen_loss 718.69\n",
            "iteration 4614, epoch 10, batch 85/481,disc_loss 79.863, (real 81.978, fake 77.747 ) gen_loss 747.5\n",
            "iteration 4615, epoch 10, batch 86/481,disc_loss 75.44, (real 78.656, fake 72.223 ) gen_loss 832.49\n",
            "iteration 4616, epoch 10, batch 87/481,disc_loss 78.934, (real 81.939, fake 75.929 ) gen_loss 754.65\n",
            "iteration 4617, epoch 10, batch 88/481,disc_loss 73.924, (real 76.555, fake 71.292 ) gen_loss 832.87\n",
            "iteration 4618, epoch 10, batch 89/481,disc_loss 74.04, (real 76.305, fake 71.775 ) gen_loss 695.09\n",
            "iteration 4619, epoch 10, batch 90/481,disc_loss 77.562, (real 80.549, fake 74.576 ) gen_loss 740.11\n",
            "iteration 4620, epoch 10, batch 91/481,disc_loss 75.808, (real 78.198, fake 73.417 ) gen_loss 755.9\n",
            "iteration 4621, epoch 10, batch 92/481,disc_loss 73.923, (real 76.683, fake 71.164 ) gen_loss 904.32\n",
            "iteration 4622, epoch 10, batch 93/481,disc_loss 75.712, (real 77.902, fake 73.522 ) gen_loss 704.9\n",
            "iteration 4623, epoch 10, batch 94/481,disc_loss 78.697, (real 81.396, fake 75.997 ) gen_loss 728.69\n",
            "iteration 4624, epoch 10, batch 95/481,disc_loss 78.772, (real 81.039, fake 76.506 ) gen_loss 626.33\n",
            "iteration 4625, epoch 10, batch 96/481,disc_loss 73.263, (real 76.393, fake 70.134 ) gen_loss 934.81\n",
            "iteration 4626, epoch 10, batch 97/481,disc_loss 78.465, (real 81.165, fake 75.766 ) gen_loss 739.32\n",
            "iteration 4627, epoch 10, batch 98/481,disc_loss 77.393, (real 80.277, fake 74.509 ) gen_loss 685.01\n",
            "iteration 4628, epoch 10, batch 99/481,disc_loss 79.611, (real 82.726, fake 76.497 ) gen_loss 703.31\n",
            "iteration 4629, epoch 10, batch 100/481,disc_loss 79.624, (real 82.74, fake 76.508 ) gen_loss 756.32\n",
            "iteration 4630, epoch 10, batch 101/481,disc_loss 77.736, (real 80.344, fake 75.127 ) gen_loss 772.55\n",
            "iteration 4631, epoch 10, batch 102/481,disc_loss 72.411, (real 75.065, fake 69.757 ) gen_loss 656.53\n",
            "iteration 4632, epoch 10, batch 103/481,disc_loss 78.251, (real 81.718, fake 74.785 ) gen_loss 773.66\n",
            "iteration 4633, epoch 10, batch 104/481,disc_loss 74.206, (real 76.962, fake 71.449 ) gen_loss 838.39\n",
            "iteration 4634, epoch 10, batch 105/481,disc_loss 78.233, (real 80.967, fake 75.499 ) gen_loss 721.12\n",
            "iteration 4635, epoch 10, batch 106/481,disc_loss 76.623, (real 79.638, fake 73.608 ) gen_loss 744.23\n",
            "iteration 4636, epoch 10, batch 107/481,disc_loss 77.217, (real 80.243, fake 74.191 ) gen_loss 677.88\n",
            "iteration 4637, epoch 10, batch 108/481,disc_loss 74.804, (real 78.946, fake 70.661 ) gen_loss 737.22\n",
            "iteration 4638, epoch 10, batch 109/481,disc_loss 78.961, (real 81.532, fake 76.389 ) gen_loss 733.59\n",
            "iteration 4639, epoch 10, batch 110/481,disc_loss 78.055, (real 80.834, fake 75.276 ) gen_loss 753.85\n",
            "iteration 4640, epoch 10, batch 111/481,disc_loss 76.753, (real 79.205, fake 74.301 ) gen_loss 880.59\n",
            "iteration 4641, epoch 10, batch 112/481,disc_loss 78.148, (real 81.848, fake 74.449 ) gen_loss 812.1\n",
            "iteration 4642, epoch 10, batch 113/481,disc_loss 81.287, (real 84.334, fake 78.239 ) gen_loss 853.14\n",
            "iteration 4643, epoch 10, batch 114/481,disc_loss 80.622, (real 83.179, fake 78.065 ) gen_loss 692.99\n",
            "iteration 4644, epoch 10, batch 115/481,disc_loss 78.745, (real 81.478, fake 76.013 ) gen_loss 794.26\n",
            "iteration 4645, epoch 10, batch 116/481,disc_loss 78.946, (real 81.472, fake 76.419 ) gen_loss 740.54\n",
            "iteration 4646, epoch 10, batch 117/481,disc_loss 82.085, (real 84.172, fake 79.999 ) gen_loss 703.61\n",
            "iteration 4647, epoch 10, batch 118/481,disc_loss 78.891, (real 82.648, fake 75.134 ) gen_loss 806.99\n",
            "iteration 4648, epoch 10, batch 119/481,disc_loss 77.827, (real 81.075, fake 74.579 ) gen_loss 698.93\n",
            "iteration 4649, epoch 10, batch 120/481,disc_loss 74.076, (real 76.762, fake 71.389 ) gen_loss 677.56\n",
            "iteration 4650, epoch 10, batch 121/481,disc_loss 80.535, (real 84.192, fake 76.878 ) gen_loss 794.18\n",
            "iteration 4651, epoch 10, batch 122/481,disc_loss 76.902, (real 80.587, fake 73.218 ) gen_loss 733.33\n",
            "iteration 4652, epoch 10, batch 123/481,disc_loss 74.334, (real 77.092, fake 71.576 ) gen_loss 753.79\n",
            "iteration 4653, epoch 10, batch 124/481,disc_loss 78.351, (real 80.847, fake 75.854 ) gen_loss 691.67\n",
            "iteration 4654, epoch 10, batch 125/481,disc_loss 81.048, (real 83.463, fake 78.633 ) gen_loss 662.12\n",
            "iteration 4655, epoch 10, batch 126/481,disc_loss 76.26, (real 79.321, fake 73.199 ) gen_loss 688.73\n",
            "iteration 4656, epoch 10, batch 127/481,disc_loss 80.66, (real 84.07, fake 77.25 ) gen_loss 744.15\n",
            "iteration 4657, epoch 10, batch 128/481,disc_loss 80.201, (real 82.649, fake 77.752 ) gen_loss 697.67\n",
            "iteration 4658, epoch 10, batch 129/481,disc_loss 80.218, (real 83.794, fake 76.642 ) gen_loss 765.21\n",
            "iteration 4659, epoch 10, batch 130/481,disc_loss 75.632, (real 77.517, fake 73.746 ) gen_loss 747.42\n",
            "iteration 4660, epoch 10, batch 131/481,disc_loss 79.682, (real 82.782, fake 76.582 ) gen_loss 978.19\n",
            "iteration 4661, epoch 10, batch 132/481,disc_loss 77.643, (real 80.268, fake 75.018 ) gen_loss 709.38\n",
            "iteration 4662, epoch 10, batch 133/481,disc_loss 78.989, (real 82.289, fake 75.69 ) gen_loss 768.47\n",
            "iteration 4663, epoch 10, batch 134/481,disc_loss 78.174, (real 81.337, fake 75.011 ) gen_loss 755.15\n",
            "iteration 4664, epoch 10, batch 135/481,disc_loss 81.291, (real 84.565, fake 78.016 ) gen_loss 762.37\n",
            "iteration 4665, epoch 10, batch 136/481,disc_loss 76.24, (real 78.212, fake 74.269 ) gen_loss 674.23\n",
            "iteration 4666, epoch 10, batch 137/481,disc_loss 79.194, (real 81.752, fake 76.635 ) gen_loss 712.57\n",
            "iteration 4667, epoch 10, batch 138/481,disc_loss 76.195, (real 78.542, fake 73.849 ) gen_loss 713.7\n",
            "iteration 4668, epoch 10, batch 139/481,disc_loss 80.824, (real 83.379, fake 78.268 ) gen_loss 703.63\n",
            "iteration 4669, epoch 10, batch 140/481,disc_loss 74.684, (real 78.434, fake 70.934 ) gen_loss 672.88\n",
            "iteration 4670, epoch 10, batch 141/481,disc_loss 80.625, (real 83.576, fake 77.674 ) gen_loss 749.59\n",
            "iteration 4671, epoch 10, batch 142/481,disc_loss 79.381, (real 81.897, fake 76.865 ) gen_loss 773.35\n",
            "iteration 4672, epoch 10, batch 143/481,disc_loss 80.634, (real 84.196, fake 77.071 ) gen_loss 745.57\n",
            "iteration 4673, epoch 10, batch 144/481,disc_loss 74.577, (real 77.923, fake 71.23 ) gen_loss 738.54\n",
            "iteration 4674, epoch 10, batch 145/481,disc_loss 77.528, (real 80.455, fake 74.6 ) gen_loss 749.76\n",
            "iteration 4675, epoch 10, batch 146/481,disc_loss 72.577, (real 75.41, fake 69.744 ) gen_loss 706.67\n",
            "iteration 4676, epoch 10, batch 147/481,disc_loss 78.222, (real 81.256, fake 75.188 ) gen_loss 753.22\n",
            "iteration 4677, epoch 10, batch 148/481,disc_loss 73.245, (real 75.405, fake 71.085 ) gen_loss 710.82\n",
            "iteration 4678, epoch 10, batch 149/481,disc_loss 78.282, (real 82.03, fake 74.534 ) gen_loss 712.08\n",
            "iteration 4679, epoch 10, batch 150/481,disc_loss 80.545, (real 84.014, fake 77.075 ) gen_loss 733.68\n",
            "iteration 4680, epoch 10, batch 151/481,disc_loss 76.965, (real 79.444, fake 74.485 ) gen_loss 777.11\n",
            "iteration 4681, epoch 10, batch 152/481,disc_loss 75.898, (real 78.086, fake 73.71 ) gen_loss 692.43\n",
            "iteration 4682, epoch 10, batch 153/481,disc_loss 74.616, (real 77.008, fake 72.223 ) gen_loss 750.79\n",
            "iteration 4683, epoch 10, batch 154/481,disc_loss 75.6, (real 77.687, fake 73.512 ) gen_loss 699.49\n",
            "iteration 4684, epoch 10, batch 155/481,disc_loss 78.964, (real 81.733, fake 76.195 ) gen_loss 658.37\n",
            "iteration 4685, epoch 10, batch 156/481,disc_loss 79.329, (real 82.181, fake 76.478 ) gen_loss 673.56\n",
            "iteration 4686, epoch 10, batch 157/481,disc_loss 80.183, (real 83.281, fake 77.086 ) gen_loss 734.22\n",
            "iteration 4687, epoch 10, batch 158/481,disc_loss 78.668, (real 81.827, fake 75.509 ) gen_loss 786.63\n",
            "iteration 4688, epoch 10, batch 159/481,disc_loss 75.617, (real 77.914, fake 73.32 ) gen_loss 790.41\n",
            "iteration 4689, epoch 10, batch 160/481,disc_loss 74.523, (real 76.304, fake 72.742 ) gen_loss 733.32\n",
            "iteration 4690, epoch 10, batch 161/481,disc_loss 78.639, (real 81.72, fake 75.557 ) gen_loss 673.69\n",
            "iteration 4691, epoch 10, batch 162/481,disc_loss 79.12, (real 82.106, fake 76.134 ) gen_loss 664.24\n",
            "iteration 4692, epoch 10, batch 163/481,disc_loss 79.328, (real 82.274, fake 76.382 ) gen_loss 741.96\n",
            "iteration 4693, epoch 10, batch 164/481,disc_loss 77.035, (real 80.358, fake 73.712 ) gen_loss 659.73\n",
            "iteration 4694, epoch 10, batch 165/481,disc_loss 77.797, (real 80.36, fake 75.235 ) gen_loss 674.9\n",
            "iteration 4695, epoch 10, batch 166/481,disc_loss 80.252, (real 82.812, fake 77.692 ) gen_loss 782.29\n",
            "iteration 4696, epoch 10, batch 167/481,disc_loss 77.843, (real 81.401, fake 74.285 ) gen_loss 687.94\n",
            "iteration 4697, epoch 10, batch 168/481,disc_loss 74.398, (real 77.26, fake 71.536 ) gen_loss 709.43\n",
            "iteration 4698, epoch 10, batch 169/481,disc_loss 73.949, (real 76.528, fake 71.37 ) gen_loss 712.86\n",
            "iteration 4699, epoch 10, batch 170/481,disc_loss 76.177, (real 78.549, fake 73.804 ) gen_loss 697.77\n",
            "iteration 4700, epoch 10, batch 171/481,disc_loss 81.219, (real 83.962, fake 78.476 ) gen_loss 748.92\n",
            "iteration 4701, epoch 10, batch 172/481,disc_loss 78.312, (real 81.952, fake 74.673 ) gen_loss 773.16\n",
            "iteration 4702, epoch 10, batch 173/481,disc_loss 77.634, (real 80.608, fake 74.66 ) gen_loss 634.69\n",
            "iteration 4703, epoch 10, batch 174/481,disc_loss 80.122, (real 82.426, fake 77.818 ) gen_loss 933.94\n",
            "iteration 4704, epoch 10, batch 175/481,disc_loss 84.765, (real 86.982, fake 82.548 ) gen_loss 713.86\n",
            "iteration 4705, epoch 10, batch 176/481,disc_loss 74.178, (real 77.273, fake 71.083 ) gen_loss 820.53\n",
            "iteration 4706, epoch 10, batch 177/481,disc_loss 80.345, (real 82.777, fake 77.913 ) gen_loss 645.78\n",
            "iteration 4707, epoch 10, batch 178/481,disc_loss 79.454, (real 82.601, fake 76.307 ) gen_loss 662.14\n",
            "iteration 4708, epoch 10, batch 179/481,disc_loss 74.939, (real 78.32, fake 71.558 ) gen_loss 736.5\n",
            "iteration 4709, epoch 10, batch 180/481,disc_loss 77.501, (real 80.208, fake 74.793 ) gen_loss 805.42\n",
            "iteration 4710, epoch 10, batch 181/481,disc_loss 81.713, (real 85.786, fake 77.64 ) gen_loss 689.35\n",
            "iteration 4711, epoch 10, batch 182/481,disc_loss 78.848, (real 81.743, fake 75.954 ) gen_loss 618.44\n",
            "iteration 4712, epoch 10, batch 183/481,disc_loss 79.121, (real 82.287, fake 75.955 ) gen_loss 711.39\n",
            "iteration 4713, epoch 10, batch 184/481,disc_loss 75.278, (real 78.531, fake 72.026 ) gen_loss 687.53\n",
            "iteration 4714, epoch 10, batch 185/481,disc_loss 76.496, (real 79.3, fake 73.691 ) gen_loss 699.61\n",
            "iteration 4715, epoch 10, batch 186/481,disc_loss 78.607, (real 81.317, fake 75.898 ) gen_loss 663.87\n",
            "iteration 4716, epoch 10, batch 187/481,disc_loss 81.05, (real 84.176, fake 77.924 ) gen_loss 775.92\n",
            "iteration 4717, epoch 10, batch 188/481,disc_loss 77.748, (real 80.864, fake 74.632 ) gen_loss 741.86\n",
            "iteration 4718, epoch 10, batch 189/481,disc_loss 75.888, (real 78.27, fake 73.506 ) gen_loss 630.06\n",
            "iteration 4719, epoch 10, batch 190/481,disc_loss 75.262, (real 77.483, fake 73.041 ) gen_loss 701.68\n",
            "iteration 4720, epoch 10, batch 191/481,disc_loss 74.9, (real 78.117, fake 71.683 ) gen_loss 717.53\n",
            "iteration 4721, epoch 10, batch 192/481,disc_loss 77.987, (real 80.364, fake 75.61 ) gen_loss 783.69\n",
            "iteration 4722, epoch 10, batch 193/481,disc_loss 76.921, (real 80.251, fake 73.591 ) gen_loss 755.24\n",
            "iteration 4723, epoch 10, batch 194/481,disc_loss 78.041, (real 80.848, fake 75.234 ) gen_loss 795.06\n",
            "iteration 4724, epoch 10, batch 195/481,disc_loss 80.876, (real 83.523, fake 78.228 ) gen_loss 777.47\n",
            "iteration 4725, epoch 10, batch 196/481,disc_loss 81.14, (real 83.87, fake 78.411 ) gen_loss 776.39\n",
            "iteration 4726, epoch 10, batch 197/481,disc_loss 74.121, (real 76.363, fake 71.879 ) gen_loss 786.75\n",
            "iteration 4727, epoch 10, batch 198/481,disc_loss 71.207, (real 73.635, fake 68.78 ) gen_loss 757.56\n",
            "iteration 4728, epoch 10, batch 199/481,disc_loss 80.815, (real 84.278, fake 77.352 ) gen_loss 708.32\n",
            "iteration 4729, epoch 10, batch 200/481,disc_loss 79.277, (real 82.112, fake 76.442 ) gen_loss 636.23\n",
            "iteration 4730, epoch 10, batch 201/481,disc_loss 78.551, (real 81.615, fake 75.486 ) gen_loss 640.61\n",
            "iteration 4731, epoch 10, batch 202/481,disc_loss 75.384, (real 77.881, fake 72.888 ) gen_loss 734.36\n",
            "iteration 4732, epoch 10, batch 203/481,disc_loss 83.305, (real 86.165, fake 80.444 ) gen_loss 767.67\n",
            "iteration 4733, epoch 10, batch 204/481,disc_loss 78.258, (real 80.381, fake 76.134 ) gen_loss 755.22\n",
            "iteration 4734, epoch 10, batch 205/481,disc_loss 77.902, (real 81.314, fake 74.49 ) gen_loss 730.69\n",
            "iteration 4735, epoch 10, batch 206/481,disc_loss 76.141, (real 77.984, fake 74.297 ) gen_loss 714.68\n",
            "iteration 4736, epoch 10, batch 207/481,disc_loss 79.501, (real 82.423, fake 76.578 ) gen_loss 648.15\n",
            "iteration 4737, epoch 10, batch 208/481,disc_loss 77.803, (real 80.709, fake 74.897 ) gen_loss 703.87\n",
            "iteration 4738, epoch 10, batch 209/481,disc_loss 83.806, (real 86.046, fake 81.566 ) gen_loss 666.18\n",
            "iteration 4739, epoch 10, batch 210/481,disc_loss 81.289, (real 85.283, fake 77.296 ) gen_loss 609.11\n",
            "iteration 4740, epoch 10, batch 211/481,disc_loss 74.333, (real 76.861, fake 71.804 ) gen_loss 727.34\n",
            "iteration 4741, epoch 10, batch 212/481,disc_loss 76.893, (real 79.121, fake 74.666 ) gen_loss 706.55\n",
            "iteration 4742, epoch 10, batch 213/481,disc_loss 78.571, (real 82.211, fake 74.932 ) gen_loss 684.64\n",
            "iteration 4743, epoch 10, batch 214/481,disc_loss 76.439, (real 79.302, fake 73.576 ) gen_loss 617.41\n",
            "iteration 4744, epoch 10, batch 215/481,disc_loss 80.76, (real 84.29, fake 77.231 ) gen_loss 734.91\n",
            "iteration 4745, epoch 10, batch 216/481,disc_loss 73.941, (real 77.285, fake 70.596 ) gen_loss 741.5\n",
            "iteration 4746, epoch 10, batch 217/481,disc_loss 74.442, (real 76.58, fake 72.304 ) gen_loss 717.09\n",
            "iteration 4747, epoch 10, batch 218/481,disc_loss 76.621, (real 79.464, fake 73.777 ) gen_loss 772.48\n",
            "iteration 4748, epoch 10, batch 219/481,disc_loss 78.905, (real 81.415, fake 76.395 ) gen_loss 745.31\n",
            "iteration 4749, epoch 10, batch 220/481,disc_loss 82.1, (real 84.831, fake 79.369 ) gen_loss 767.8\n",
            "iteration 4750, epoch 10, batch 221/481,disc_loss 79.312, (real 82.202, fake 76.422 ) gen_loss 821.22\n",
            "iteration 4751, epoch 10, batch 222/481,disc_loss 80.849, (real 83.12, fake 78.577 ) gen_loss 639.84\n",
            "iteration 4752, epoch 10, batch 223/481,disc_loss 75.669, (real 78.502, fake 72.836 ) gen_loss 813.28\n",
            "iteration 4753, epoch 10, batch 224/481,disc_loss 74.078, (real 77.126, fake 71.03 ) gen_loss 759.04\n",
            "iteration 4754, epoch 10, batch 225/481,disc_loss 79.289, (real 82.237, fake 76.342 ) gen_loss 786.73\n",
            "iteration 4755, epoch 10, batch 226/481,disc_loss 78.763, (real 81.581, fake 75.945 ) gen_loss 718.47\n",
            "iteration 4756, epoch 10, batch 227/481,disc_loss 77.358, (real 80.931, fake 73.785 ) gen_loss 928.38\n",
            "iteration 4757, epoch 10, batch 228/481,disc_loss 76.038, (real 79.059, fake 73.018 ) gen_loss 737.2\n",
            "iteration 4758, epoch 10, batch 229/481,disc_loss 77.083, (real 79.745, fake 74.422 ) gen_loss 759.56\n",
            "iteration 4759, epoch 10, batch 230/481,disc_loss 79.347, (real 83.874, fake 74.821 ) gen_loss 775.33\n",
            "iteration 4760, epoch 10, batch 231/481,disc_loss 74.551, (real 77.313, fake 71.788 ) gen_loss 732.41\n",
            "iteration 4761, epoch 10, batch 232/481,disc_loss 80.271, (real 83.51, fake 77.033 ) gen_loss 785.99\n",
            "iteration 4762, epoch 10, batch 233/481,disc_loss 81.858, (real 84.178, fake 79.538 ) gen_loss 789.1\n",
            "iteration 4763, epoch 10, batch 234/481,disc_loss 77.167, (real 79.55, fake 74.783 ) gen_loss 709.97\n",
            "iteration 4764, epoch 10, batch 235/481,disc_loss 80.193, (real 83.006, fake 77.379 ) gen_loss 826.08\n",
            "iteration 4765, epoch 10, batch 236/481,disc_loss 79.523, (real 83.944, fake 75.101 ) gen_loss 943.3\n",
            "iteration 4766, epoch 10, batch 237/481,disc_loss 77.802, (real 79.943, fake 75.662 ) gen_loss 766.46\n",
            "iteration 4767, epoch 10, batch 238/481,disc_loss 78.217, (real 80.537, fake 75.897 ) gen_loss 819.04\n",
            "iteration 4768, epoch 10, batch 239/481,disc_loss 75.153, (real 78.102, fake 72.205 ) gen_loss 845.12\n",
            "iteration 4769, epoch 10, batch 240/481,disc_loss 74.57, (real 76.951, fake 72.19 ) gen_loss 705.62\n",
            "iteration 4770, epoch 10, batch 241/481,disc_loss 74.643, (real 77.166, fake 72.121 ) gen_loss 891.84\n",
            "iteration 4771, epoch 10, batch 242/481,disc_loss 73.865, (real 77.151, fake 70.579 ) gen_loss 840.46\n",
            "iteration 4772, epoch 10, batch 243/481,disc_loss 77.711, (real 81.078, fake 74.343 ) gen_loss 831.39\n",
            "iteration 4773, epoch 10, batch 244/481,disc_loss 77.936, (real 79.765, fake 76.108 ) gen_loss 874.48\n",
            "iteration 4774, epoch 10, batch 245/481,disc_loss 77.129, (real 80.323, fake 73.936 ) gen_loss 761.12\n",
            "iteration 4775, epoch 10, batch 246/481,disc_loss 73.936, (real 76.953, fake 70.918 ) gen_loss 710.39\n",
            "iteration 4776, epoch 10, batch 247/481,disc_loss 76.083, (real 79.228, fake 72.939 ) gen_loss 721.96\n",
            "iteration 4777, epoch 10, batch 248/481,disc_loss 75.233, (real 77.728, fake 72.739 ) gen_loss 703.05\n",
            "iteration 4778, epoch 10, batch 249/481,disc_loss 73.245, (real 75.236, fake 71.253 ) gen_loss 720.14\n",
            "iteration 4779, epoch 10, batch 250/481,disc_loss 76.549, (real 79.355, fake 73.743 ) gen_loss 720.63\n",
            "iteration 4780, epoch 10, batch 251/481,disc_loss 80.634, (real 83.471, fake 77.797 ) gen_loss 761.63\n",
            "iteration 4781, epoch 10, batch 252/481,disc_loss 74.756, (real 78.726, fake 70.786 ) gen_loss 708.19\n",
            "iteration 4782, epoch 10, batch 253/481,disc_loss 74.168, (real 76.984, fake 71.351 ) gen_loss 734.9\n",
            "iteration 4783, epoch 10, batch 254/481,disc_loss 78.795, (real 81.439, fake 76.151 ) gen_loss 779.24\n",
            "iteration 4784, epoch 10, batch 255/481,disc_loss 76.009, (real 78.963, fake 73.056 ) gen_loss 724.36\n",
            "iteration 4785, epoch 10, batch 256/481,disc_loss 76.8, (real 79.956, fake 73.644 ) gen_loss 828.4\n",
            "iteration 4786, epoch 10, batch 257/481,disc_loss 75.009, (real 77.89, fake 72.129 ) gen_loss 685.14\n",
            "iteration 4787, epoch 10, batch 258/481,disc_loss 77.517, (real 80.391, fake 74.642 ) gen_loss 770.79\n",
            "iteration 4788, epoch 10, batch 259/481,disc_loss 76.888, (real 80.357, fake 73.418 ) gen_loss 714.29\n",
            "iteration 4789, epoch 10, batch 260/481,disc_loss 73.386, (real 76.133, fake 70.638 ) gen_loss 764.46\n",
            "iteration 4790, epoch 10, batch 261/481,disc_loss 76.107, (real 79.111, fake 73.104 ) gen_loss 802.58\n",
            "iteration 4791, epoch 10, batch 262/481,disc_loss 74.115, (real 76.836, fake 71.394 ) gen_loss 795.01\n",
            "iteration 4792, epoch 10, batch 263/481,disc_loss 79.604, (real 82.418, fake 76.789 ) gen_loss 738.22\n",
            "iteration 4793, epoch 10, batch 264/481,disc_loss 78.968, (real 82.31, fake 75.627 ) gen_loss 763.91\n",
            "iteration 4794, epoch 10, batch 265/481,disc_loss 77.691, (real 80.446, fake 74.935 ) gen_loss 780.85\n",
            "iteration 4795, epoch 10, batch 266/481,disc_loss 79.034, (real 83.061, fake 75.006 ) gen_loss 826.76\n",
            "iteration 4796, epoch 10, batch 267/481,disc_loss 82.418, (real 85.035, fake 79.8 ) gen_loss 804.0\n",
            "iteration 4797, epoch 10, batch 268/481,disc_loss 84.119, (real 87.534, fake 80.704 ) gen_loss 722.75\n",
            "iteration 4798, epoch 10, batch 269/481,disc_loss 82.242, (real 85.214, fake 79.271 ) gen_loss 719.3\n",
            "iteration 4799, epoch 10, batch 270/481,disc_loss 73.761, (real 77.26, fake 70.263 ) gen_loss 743.69\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 4800, epoch 10, batch 271/481,disc_loss 76.494, (real 79.124, fake 73.864 ) gen_loss 769.23\n",
            "iteration 4801, epoch 10, batch 272/481,disc_loss 79.487, (real 81.669, fake 77.305 ) gen_loss 806.68\n",
            "iteration 4802, epoch 10, batch 273/481,disc_loss 77.208, (real 79.426, fake 74.991 ) gen_loss 750.65\n",
            "iteration 4803, epoch 10, batch 274/481,disc_loss 80.862, (real 83.567, fake 78.157 ) gen_loss 690.02\n",
            "iteration 4804, epoch 10, batch 275/481,disc_loss 79.465, (real 81.573, fake 77.358 ) gen_loss 728.36\n",
            "iteration 4805, epoch 10, batch 276/481,disc_loss 81.517, (real 84.56, fake 78.474 ) gen_loss 867.35\n",
            "iteration 4806, epoch 10, batch 277/481,disc_loss 76.272, (real 78.636, fake 73.907 ) gen_loss 779.24\n",
            "iteration 4807, epoch 10, batch 278/481,disc_loss 79.007, (real 82.382, fake 75.632 ) gen_loss 806.62\n",
            "iteration 4808, epoch 10, batch 279/481,disc_loss 77.421, (real 80.531, fake 74.311 ) gen_loss 717.69\n",
            "iteration 4809, epoch 10, batch 280/481,disc_loss 81.448, (real 84.111, fake 78.786 ) gen_loss 700.3\n",
            "iteration 4810, epoch 10, batch 281/481,disc_loss 75.288, (real 78.471, fake 72.105 ) gen_loss 748.0\n",
            "iteration 4811, epoch 10, batch 282/481,disc_loss 83.407, (real 86.938, fake 79.876 ) gen_loss 767.63\n",
            "iteration 4812, epoch 10, batch 283/481,disc_loss 78.155, (real 82.094, fake 74.216 ) gen_loss 740.81\n",
            "iteration 4813, epoch 10, batch 284/481,disc_loss 78.964, (real 82.478, fake 75.45 ) gen_loss 833.43\n",
            "iteration 4814, epoch 10, batch 285/481,disc_loss 75.974, (real 78.123, fake 73.824 ) gen_loss 741.43\n",
            "iteration 4815, epoch 10, batch 286/481,disc_loss 77.164, (real 81.857, fake 72.47 ) gen_loss 758.48\n",
            "iteration 4816, epoch 10, batch 287/481,disc_loss 77.381, (real 79.726, fake 75.035 ) gen_loss 788.49\n",
            "iteration 4817, epoch 10, batch 288/481,disc_loss 77.179, (real 80.561, fake 73.798 ) gen_loss 808.4\n",
            "iteration 4818, epoch 10, batch 289/481,disc_loss 75.244, (real 78.221, fake 72.266 ) gen_loss 811.79\n",
            "iteration 4819, epoch 10, batch 290/481,disc_loss 74.965, (real 77.984, fake 71.946 ) gen_loss 733.23\n",
            "iteration 4820, epoch 10, batch 291/481,disc_loss 77.26, (real 80.601, fake 73.918 ) gen_loss 687.49\n",
            "iteration 4821, epoch 10, batch 292/481,disc_loss 76.883, (real 79.464, fake 74.301 ) gen_loss 808.68\n",
            "iteration 4822, epoch 10, batch 293/481,disc_loss 79.712, (real 84.151, fake 75.273 ) gen_loss 883.42\n",
            "iteration 4823, epoch 10, batch 294/481,disc_loss 85.266, (real 89.843, fake 80.69 ) gen_loss 746.53\n",
            "iteration 4824, epoch 10, batch 295/481,disc_loss 81.828, (real 84.038, fake 79.617 ) gen_loss 750.06\n",
            "iteration 4825, epoch 10, batch 296/481,disc_loss 79.713, (real 81.274, fake 78.153 ) gen_loss 665.18\n",
            "iteration 4826, epoch 10, batch 297/481,disc_loss 74.248, (real 77.365, fake 71.131 ) gen_loss 707.92\n",
            "iteration 4827, epoch 10, batch 298/481,disc_loss 78.38, (real 81.931, fake 74.828 ) gen_loss 763.23\n",
            "iteration 4828, epoch 10, batch 299/481,disc_loss 76.802, (real 79.885, fake 73.72 ) gen_loss 734.01\n",
            "iteration 4829, epoch 10, batch 300/481,disc_loss 79.356, (real 82.758, fake 75.953 ) gen_loss 825.7\n",
            "iteration 4830, epoch 10, batch 301/481,disc_loss 78.887, (real 82.969, fake 74.806 ) gen_loss 784.2\n",
            "iteration 4831, epoch 10, batch 302/481,disc_loss 77.175, (real 79.779, fake 74.571 ) gen_loss 745.18\n",
            "iteration 4832, epoch 10, batch 303/481,disc_loss 78.89, (real 82.044, fake 75.736 ) gen_loss 762.28\n",
            "iteration 4833, epoch 10, batch 304/481,disc_loss 76.671, (real 78.843, fake 74.499 ) gen_loss 784.58\n",
            "iteration 4834, epoch 10, batch 305/481,disc_loss 72.592, (real 75.549, fake 69.634 ) gen_loss 664.54\n",
            "iteration 4835, epoch 10, batch 306/481,disc_loss 80.083, (real 83.091, fake 77.076 ) gen_loss 724.64\n",
            "iteration 4836, epoch 10, batch 307/481,disc_loss 77.389, (real 79.921, fake 74.856 ) gen_loss 828.32\n",
            "iteration 4837, epoch 10, batch 308/481,disc_loss 78.324, (real 81.302, fake 75.346 ) gen_loss 682.43\n",
            "iteration 4838, epoch 10, batch 309/481,disc_loss 80.699, (real 83.805, fake 77.592 ) gen_loss 708.44\n",
            "iteration 4839, epoch 10, batch 310/481,disc_loss 76.938, (real 79.295, fake 74.581 ) gen_loss 700.19\n",
            "iteration 4840, epoch 10, batch 311/481,disc_loss 82.136, (real 85.03, fake 79.242 ) gen_loss 721.11\n",
            "iteration 4841, epoch 10, batch 312/481,disc_loss 83.39, (real 86.963, fake 79.818 ) gen_loss 752.51\n",
            "iteration 4842, epoch 10, batch 313/481,disc_loss 81.091, (real 84.378, fake 77.805 ) gen_loss 717.21\n",
            "iteration 4843, epoch 10, batch 314/481,disc_loss 79.514, (real 82.97, fake 76.057 ) gen_loss 735.06\n",
            "iteration 4844, epoch 10, batch 315/481,disc_loss 77.618, (real 81.384, fake 73.853 ) gen_loss 774.01\n",
            "iteration 4845, epoch 10, batch 316/481,disc_loss 77.291, (real 79.669, fake 74.913 ) gen_loss 758.67\n",
            "iteration 4846, epoch 10, batch 317/481,disc_loss 76.835, (real 80.473, fake 73.197 ) gen_loss 731.69\n",
            "iteration 4847, epoch 10, batch 318/481,disc_loss 80.935, (real 83.433, fake 78.436 ) gen_loss 808.85\n",
            "iteration 4848, epoch 10, batch 319/481,disc_loss 79.592, (real 83.631, fake 75.553 ) gen_loss 739.47\n",
            "iteration 4849, epoch 10, batch 320/481,disc_loss 76.355, (real 79.276, fake 73.433 ) gen_loss 762.83\n",
            "iteration 4850, epoch 10, batch 321/481,disc_loss 76.928, (real 79.344, fake 74.512 ) gen_loss 693.67\n",
            "iteration 4851, epoch 10, batch 322/481,disc_loss 84.639, (real 88.902, fake 80.375 ) gen_loss 752.67\n",
            "iteration 4852, epoch 10, batch 323/481,disc_loss 81.356, (real 85.153, fake 77.559 ) gen_loss 789.37\n",
            "iteration 4853, epoch 10, batch 324/481,disc_loss 77.416, (real 80.523, fake 74.309 ) gen_loss 699.03\n",
            "iteration 4854, epoch 10, batch 325/481,disc_loss 76.275, (real 78.626, fake 73.925 ) gen_loss 800.48\n",
            "iteration 4855, epoch 10, batch 326/481,disc_loss 76.815, (real 79.229, fake 74.402 ) gen_loss 827.41\n",
            "iteration 4856, epoch 10, batch 327/481,disc_loss 78.023, (real 80.98, fake 75.066 ) gen_loss 827.98\n",
            "iteration 4857, epoch 10, batch 328/481,disc_loss 77.998, (real 80.313, fake 75.683 ) gen_loss 702.47\n",
            "iteration 4858, epoch 10, batch 329/481,disc_loss 78.29, (real 80.278, fake 76.303 ) gen_loss 668.31\n",
            "iteration 4859, epoch 10, batch 330/481,disc_loss 78.603, (real 81.233, fake 75.973 ) gen_loss 737.94\n",
            "iteration 4860, epoch 10, batch 331/481,disc_loss 81.197, (real 84.935, fake 77.458 ) gen_loss 705.0\n",
            "iteration 4861, epoch 10, batch 332/481,disc_loss 78.874, (real 82.272, fake 75.475 ) gen_loss 769.78\n",
            "iteration 4862, epoch 10, batch 333/481,disc_loss 80.98, (real 84.23, fake 77.73 ) gen_loss 771.7\n",
            "iteration 4863, epoch 10, batch 334/481,disc_loss 79.033, (real 82.2, fake 75.866 ) gen_loss 782.19\n",
            "iteration 4864, epoch 10, batch 335/481,disc_loss 80.015, (real 83.105, fake 76.925 ) gen_loss 717.65\n",
            "iteration 4865, epoch 10, batch 336/481,disc_loss 81.415, (real 84.71, fake 78.121 ) gen_loss 668.38\n",
            "iteration 4866, epoch 10, batch 337/481,disc_loss 77.333, (real 80.005, fake 74.661 ) gen_loss 666.07\n",
            "iteration 4867, epoch 10, batch 338/481,disc_loss 77.776, (real 81.87, fake 73.681 ) gen_loss 731.84\n",
            "iteration 4868, epoch 10, batch 339/481,disc_loss 78.931, (real 82.987, fake 74.876 ) gen_loss 722.23\n",
            "iteration 4869, epoch 10, batch 340/481,disc_loss 77.267, (real 79.997, fake 74.537 ) gen_loss 797.81\n",
            "iteration 4870, epoch 10, batch 341/481,disc_loss 79.683, (real 82.349, fake 77.018 ) gen_loss 838.42\n",
            "iteration 4871, epoch 10, batch 342/481,disc_loss 76.398, (real 79.2, fake 73.597 ) gen_loss 900.96\n",
            "iteration 4872, epoch 10, batch 343/481,disc_loss 82.762, (real 86.628, fake 78.896 ) gen_loss 728.54\n",
            "iteration 4873, epoch 10, batch 344/481,disc_loss 74.483, (real 78.004, fake 70.963 ) gen_loss 756.96\n",
            "iteration 4874, epoch 10, batch 345/481,disc_loss 82.297, (real 84.892, fake 79.702 ) gen_loss 770.5\n",
            "iteration 4875, epoch 10, batch 346/481,disc_loss 79.892, (real 82.982, fake 76.801 ) gen_loss 760.39\n",
            "iteration 4876, epoch 10, batch 347/481,disc_loss 78.519, (real 81.348, fake 75.689 ) gen_loss 738.44\n",
            "iteration 4877, epoch 10, batch 348/481,disc_loss 74.279, (real 76.735, fake 71.824 ) gen_loss 742.07\n",
            "iteration 4878, epoch 10, batch 349/481,disc_loss 77.956, (real 80.587, fake 75.325 ) gen_loss 839.47\n",
            "iteration 4879, epoch 10, batch 350/481,disc_loss 77.582, (real 81.373, fake 73.792 ) gen_loss 825.99\n",
            "iteration 4880, epoch 10, batch 351/481,disc_loss 77.264, (real 80.279, fake 74.25 ) gen_loss 766.1\n",
            "iteration 4881, epoch 10, batch 352/481,disc_loss 76.438, (real 78.845, fake 74.032 ) gen_loss 732.96\n",
            "iteration 4882, epoch 10, batch 353/481,disc_loss 80.816, (real 83.237, fake 78.395 ) gen_loss 767.7\n",
            "iteration 4883, epoch 10, batch 354/481,disc_loss 75.794, (real 77.774, fake 73.814 ) gen_loss 707.3\n",
            "iteration 4884, epoch 10, batch 355/481,disc_loss 79.241, (real 81.468, fake 77.014 ) gen_loss 736.48\n",
            "iteration 4885, epoch 10, batch 356/481,disc_loss 78.865, (real 82.155, fake 75.574 ) gen_loss 742.14\n",
            "iteration 4886, epoch 10, batch 357/481,disc_loss 75.96, (real 78.032, fake 73.888 ) gen_loss 716.39\n",
            "iteration 4887, epoch 10, batch 358/481,disc_loss 75.415, (real 77.869, fake 72.961 ) gen_loss 776.04\n",
            "iteration 4888, epoch 10, batch 359/481,disc_loss 84.016, (real 87.374, fake 80.657 ) gen_loss 785.34\n",
            "iteration 4889, epoch 10, batch 360/481,disc_loss 81.301, (real 83.608, fake 78.994 ) gen_loss 804.5\n",
            "iteration 4890, epoch 10, batch 361/481,disc_loss 82.504, (real 85.507, fake 79.501 ) gen_loss 758.41\n",
            "iteration 4891, epoch 10, batch 362/481,disc_loss 84.696, (real 87.836, fake 81.556 ) gen_loss 688.18\n",
            "iteration 4892, epoch 10, batch 363/481,disc_loss 79.314, (real 82.049, fake 76.578 ) gen_loss 754.03\n",
            "iteration 4893, epoch 10, batch 364/481,disc_loss 73.574, (real 76.393, fake 70.754 ) gen_loss 826.56\n",
            "iteration 4894, epoch 10, batch 365/481,disc_loss 78.253, (real 80.987, fake 75.519 ) gen_loss 792.35\n",
            "iteration 4895, epoch 10, batch 366/481,disc_loss 84.872, (real 87.07, fake 82.675 ) gen_loss 777.42\n",
            "iteration 4896, epoch 10, batch 367/481,disc_loss 75.247, (real 79.05, fake 71.444 ) gen_loss 792.48\n",
            "iteration 4897, epoch 10, batch 368/481,disc_loss 73.861, (real 76.794, fake 70.927 ) gen_loss 724.07\n",
            "iteration 4898, epoch 10, batch 369/481,disc_loss 75.585, (real 78.26, fake 72.91 ) gen_loss 753.52\n",
            "iteration 4899, epoch 10, batch 370/481,disc_loss 75.253, (real 77.482, fake 73.024 ) gen_loss 814.79\n",
            "iteration 4900, epoch 10, batch 371/481,disc_loss 78.349, (real 80.692, fake 76.006 ) gen_loss 691.81\n",
            "iteration 4901, epoch 10, batch 372/481,disc_loss 77.611, (real 80.868, fake 74.354 ) gen_loss 757.11\n",
            "iteration 4902, epoch 10, batch 373/481,disc_loss 84.65, (real 87.543, fake 81.758 ) gen_loss 794.06\n",
            "iteration 4903, epoch 10, batch 374/481,disc_loss 76.683, (real 78.684, fake 74.682 ) gen_loss 767.86\n",
            "iteration 4904, epoch 10, batch 375/481,disc_loss 77.668, (real 81.279, fake 74.057 ) gen_loss 896.22\n",
            "iteration 4905, epoch 10, batch 376/481,disc_loss 76.752, (real 79.686, fake 73.818 ) gen_loss 828.04\n",
            "iteration 4906, epoch 10, batch 377/481,disc_loss 76.101, (real 78.967, fake 73.235 ) gen_loss 746.38\n",
            "iteration 4907, epoch 10, batch 378/481,disc_loss 82.154, (real 84.668, fake 79.64 ) gen_loss 830.34\n",
            "iteration 4908, epoch 10, batch 379/481,disc_loss 75.337, (real 77.013, fake 73.661 ) gen_loss 710.37\n",
            "iteration 4909, epoch 10, batch 380/481,disc_loss 77.816, (real 81.072, fake 74.56 ) gen_loss 747.77\n",
            "iteration 4910, epoch 10, batch 381/481,disc_loss 78.954, (real 81.936, fake 75.972 ) gen_loss 761.18\n",
            "iteration 4911, epoch 10, batch 382/481,disc_loss 74.501, (real 77.166, fake 71.836 ) gen_loss 764.13\n",
            "iteration 4912, epoch 10, batch 383/481,disc_loss 77.382, (real 81.113, fake 73.65 ) gen_loss 804.54\n",
            "iteration 4913, epoch 10, batch 384/481,disc_loss 76.854, (real 80.955, fake 72.754 ) gen_loss 750.89\n",
            "iteration 4914, epoch 10, batch 385/481,disc_loss 79.282, (real 82.205, fake 76.359 ) gen_loss 806.22\n",
            "iteration 4915, epoch 10, batch 386/481,disc_loss 75.028, (real 77.344, fake 72.712 ) gen_loss 810.11\n",
            "iteration 4916, epoch 10, batch 387/481,disc_loss 75.36, (real 78.904, fake 71.816 ) gen_loss 866.46\n",
            "iteration 4917, epoch 10, batch 388/481,disc_loss 78.022, (real 80.711, fake 75.333 ) gen_loss 916.74\n",
            "iteration 4918, epoch 10, batch 389/481,disc_loss 78.443, (real 80.235, fake 76.65 ) gen_loss 704.95\n",
            "iteration 4919, epoch 10, batch 390/481,disc_loss 80.875, (real 84.182, fake 77.568 ) gen_loss 948.93\n",
            "iteration 4920, epoch 10, batch 391/481,disc_loss 80.071, (real 83.651, fake 76.49 ) gen_loss 781.98\n",
            "iteration 4921, epoch 10, batch 392/481,disc_loss 83.241, (real 86.346, fake 80.136 ) gen_loss 759.07\n",
            "iteration 4922, epoch 10, batch 393/481,disc_loss 72.711, (real 75.142, fake 70.281 ) gen_loss 764.83\n",
            "iteration 4923, epoch 10, batch 394/481,disc_loss 76.312, (real 79.139, fake 73.486 ) gen_loss 794.15\n",
            "iteration 4924, epoch 10, batch 395/481,disc_loss 81.454, (real 84.832, fake 78.076 ) gen_loss 827.23\n",
            "iteration 4925, epoch 10, batch 396/481,disc_loss 74.954, (real 78.265, fake 71.644 ) gen_loss 801.7\n",
            "iteration 4926, epoch 10, batch 397/481,disc_loss 78.137, (real 81.433, fake 74.841 ) gen_loss 804.93\n",
            "iteration 4927, epoch 10, batch 398/481,disc_loss 80.377, (real 83.384, fake 77.37 ) gen_loss 781.38\n",
            "iteration 4928, epoch 10, batch 399/481,disc_loss 74.469, (real 77.478, fake 71.459 ) gen_loss 860.39\n",
            "iteration 4929, epoch 10, batch 400/481,disc_loss 79.663, (real 82.345, fake 76.981 ) gen_loss 821.02\n",
            "iteration 4930, epoch 10, batch 401/481,disc_loss 77.414, (real 80.631, fake 74.196 ) gen_loss 725.11\n",
            "iteration 4931, epoch 10, batch 402/481,disc_loss 83.68, (real 86.644, fake 80.715 ) gen_loss 714.02\n",
            "iteration 4932, epoch 10, batch 403/481,disc_loss 78.14, (real 81.105, fake 75.175 ) gen_loss 847.25\n",
            "iteration 4933, epoch 10, batch 404/481,disc_loss 76.362, (real 79.254, fake 73.469 ) gen_loss 697.54\n",
            "iteration 4934, epoch 10, batch 405/481,disc_loss 79.238, (real 82.195, fake 76.281 ) gen_loss 689.97\n",
            "iteration 4935, epoch 10, batch 406/481,disc_loss 76.978, (real 80.094, fake 73.862 ) gen_loss 885.28\n",
            "iteration 4936, epoch 10, batch 407/481,disc_loss 78.046, (real 81.195, fake 74.896 ) gen_loss 848.76\n",
            "iteration 4937, epoch 10, batch 408/481,disc_loss 76.694, (real 79.485, fake 73.903 ) gen_loss 777.61\n",
            "iteration 4938, epoch 10, batch 409/481,disc_loss 80.602, (real 82.881, fake 78.323 ) gen_loss 693.73\n",
            "iteration 4939, epoch 10, batch 410/481,disc_loss 78.214, (real 81.344, fake 75.084 ) gen_loss 752.93\n",
            "iteration 4940, epoch 10, batch 411/481,disc_loss 78.61, (real 81.546, fake 75.674 ) gen_loss 656.23\n",
            "iteration 4941, epoch 10, batch 412/481,disc_loss 76.481, (real 79.921, fake 73.042 ) gen_loss 724.58\n",
            "iteration 4942, epoch 10, batch 413/481,disc_loss 74.248, (real 76.472, fake 72.025 ) gen_loss 713.65\n",
            "iteration 4943, epoch 10, batch 414/481,disc_loss 76.647, (real 79.115, fake 74.179 ) gen_loss 837.66\n",
            "iteration 4944, epoch 10, batch 415/481,disc_loss 74.872, (real 77.92, fake 71.824 ) gen_loss 826.29\n",
            "iteration 4945, epoch 10, batch 416/481,disc_loss 74.476, (real 76.68, fake 72.272 ) gen_loss 770.06\n",
            "iteration 4946, epoch 10, batch 417/481,disc_loss 79.837, (real 83.015, fake 76.659 ) gen_loss 796.8\n",
            "iteration 4947, epoch 10, batch 418/481,disc_loss 79.82, (real 82.944, fake 76.696 ) gen_loss 771.34\n",
            "iteration 4948, epoch 10, batch 419/481,disc_loss 78.639, (real 81.755, fake 75.523 ) gen_loss 803.76\n",
            "iteration 4949, epoch 10, batch 420/481,disc_loss 80.42, (real 83.438, fake 77.402 ) gen_loss 803.16\n",
            "iteration 4950, epoch 10, batch 421/481,disc_loss 76.975, (real 79.985, fake 73.965 ) gen_loss 750.7\n",
            "iteration 4951, epoch 10, batch 422/481,disc_loss 79.218, (real 82.069, fake 76.368 ) gen_loss 749.93\n",
            "iteration 4952, epoch 10, batch 423/481,disc_loss 81.1, (real 83.5, fake 78.701 ) gen_loss 660.42\n",
            "iteration 4953, epoch 10, batch 424/481,disc_loss 77.546, (real 80.758, fake 74.333 ) gen_loss 747.12\n",
            "iteration 4954, epoch 10, batch 425/481,disc_loss 77.779, (real 80.565, fake 74.993 ) gen_loss 753.99\n",
            "iteration 4955, epoch 10, batch 426/481,disc_loss 83.27, (real 87.423, fake 79.118 ) gen_loss 723.21\n",
            "iteration 4956, epoch 10, batch 427/481,disc_loss 76.457, (real 80.96, fake 71.955 ) gen_loss 740.55\n",
            "iteration 4957, epoch 10, batch 428/481,disc_loss 78.11, (real 80.499, fake 75.72 ) gen_loss 742.63\n",
            "iteration 4958, epoch 10, batch 429/481,disc_loss 75.884, (real 78.81, fake 72.957 ) gen_loss 837.41\n",
            "iteration 4959, epoch 10, batch 430/481,disc_loss 80.064, (real 83.284, fake 76.843 ) gen_loss 772.02\n",
            "iteration 4960, epoch 10, batch 431/481,disc_loss 80.78, (real 84.228, fake 77.332 ) gen_loss 792.47\n",
            "iteration 4961, epoch 10, batch 432/481,disc_loss 78.562, (real 81.399, fake 75.725 ) gen_loss 719.53\n",
            "iteration 4962, epoch 10, batch 433/481,disc_loss 78.919, (real 81.761, fake 76.078 ) gen_loss 980.66\n",
            "iteration 4963, epoch 10, batch 434/481,disc_loss 78.348, (real 81.538, fake 75.158 ) gen_loss 803.81\n",
            "iteration 4964, epoch 10, batch 435/481,disc_loss 80.435, (real 83.011, fake 77.859 ) gen_loss 725.65\n",
            "iteration 4965, epoch 10, batch 436/481,disc_loss 75.016, (real 77.677, fake 72.356 ) gen_loss 705.25\n",
            "iteration 4966, epoch 10, batch 437/481,disc_loss 73.474, (real 76.867, fake 70.082 ) gen_loss 843.99\n",
            "iteration 4967, epoch 10, batch 438/481,disc_loss 76.442, (real 79.45, fake 73.434 ) gen_loss 799.87\n",
            "iteration 4968, epoch 10, batch 439/481,disc_loss 75.442, (real 79.209, fake 71.675 ) gen_loss 753.55\n",
            "iteration 4969, epoch 10, batch 440/481,disc_loss 78.086, (real 80.943, fake 75.229 ) gen_loss 705.98\n",
            "iteration 4970, epoch 10, batch 441/481,disc_loss 77.943, (real 80.823, fake 75.064 ) gen_loss 717.87\n",
            "iteration 4971, epoch 10, batch 442/481,disc_loss 82.169, (real 85.146, fake 79.192 ) gen_loss 739.16\n",
            "iteration 4972, epoch 10, batch 443/481,disc_loss 77.327, (real 79.718, fake 74.936 ) gen_loss 772.67\n",
            "iteration 4973, epoch 10, batch 444/481,disc_loss 76.893, (real 79.52, fake 74.266 ) gen_loss 802.31\n",
            "iteration 4974, epoch 10, batch 445/481,disc_loss 78.398, (real 80.824, fake 75.972 ) gen_loss 756.38\n",
            "iteration 4975, epoch 10, batch 446/481,disc_loss 82.345, (real 85.625, fake 79.066 ) gen_loss 731.17\n",
            "iteration 4976, epoch 10, batch 447/481,disc_loss 79.687, (real 82.138, fake 77.237 ) gen_loss 765.37\n",
            "iteration 4977, epoch 10, batch 448/481,disc_loss 79.039, (real 81.737, fake 76.34 ) gen_loss 740.79\n",
            "iteration 4978, epoch 10, batch 449/481,disc_loss 74.16, (real 77.159, fake 71.162 ) gen_loss 736.23\n",
            "iteration 4979, epoch 10, batch 450/481,disc_loss 75.21, (real 78.064, fake 72.357 ) gen_loss 829.75\n",
            "iteration 4980, epoch 10, batch 451/481,disc_loss 79.278, (real 81.783, fake 76.774 ) gen_loss 861.81\n",
            "iteration 4981, epoch 10, batch 452/481,disc_loss 78.49, (real 81.872, fake 75.108 ) gen_loss 781.31\n",
            "iteration 4982, epoch 10, batch 453/481,disc_loss 77.724, (real 81.094, fake 74.354 ) gen_loss 779.95\n",
            "iteration 4983, epoch 10, batch 454/481,disc_loss 79.621, (real 83.046, fake 76.197 ) gen_loss 786.44\n",
            "iteration 4984, epoch 10, batch 455/481,disc_loss 77.303, (real 79.874, fake 74.733 ) gen_loss 808.23\n",
            "iteration 4985, epoch 10, batch 456/481,disc_loss 80.458, (real 83.825, fake 77.09 ) gen_loss 818.28\n",
            "iteration 4986, epoch 10, batch 457/481,disc_loss 78.923, (real 81.9, fake 75.947 ) gen_loss 747.96\n",
            "iteration 4987, epoch 10, batch 458/481,disc_loss 79.779, (real 83.034, fake 76.525 ) gen_loss 906.99\n",
            "iteration 4988, epoch 10, batch 459/481,disc_loss 78.933, (real 80.961, fake 76.904 ) gen_loss 809.84\n",
            "iteration 4989, epoch 10, batch 460/481,disc_loss 77.677, (real 80.902, fake 74.451 ) gen_loss 782.53\n",
            "iteration 4990, epoch 10, batch 461/481,disc_loss 77.751, (real 80.031, fake 75.472 ) gen_loss 729.11\n",
            "iteration 4991, epoch 10, batch 462/481,disc_loss 77.207, (real 80.358, fake 74.057 ) gen_loss 807.08\n",
            "iteration 4992, epoch 10, batch 463/481,disc_loss 84.244, (real 87.859, fake 80.628 ) gen_loss 815.25\n",
            "iteration 4993, epoch 10, batch 464/481,disc_loss 74.533, (real 77.62, fake 71.446 ) gen_loss 714.62\n",
            "iteration 4994, epoch 10, batch 465/481,disc_loss 76.965, (real 80.418, fake 73.512 ) gen_loss 772.89\n",
            "iteration 4995, epoch 10, batch 466/481,disc_loss 79.348, (real 81.996, fake 76.701 ) gen_loss 753.39\n",
            "iteration 4996, epoch 10, batch 467/481,disc_loss 75.419, (real 78.835, fake 72.003 ) gen_loss 735.45\n",
            "iteration 4997, epoch 10, batch 468/481,disc_loss 76.249, (real 78.974, fake 73.524 ) gen_loss 848.22\n",
            "iteration 4998, epoch 10, batch 469/481,disc_loss 77.807, (real 80.626, fake 74.988 ) gen_loss 745.33\n",
            "iteration 4999, epoch 10, batch 470/481,disc_loss 78.277, (real 82.117, fake 74.438 ) gen_loss 828.34\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 5000, epoch 10, batch 471/481,disc_loss 75.012, (real 77.414, fake 72.609 ) gen_loss 758.79\n",
            "iteration 5001, epoch 10, batch 472/481,disc_loss 77.827, (real 79.74, fake 75.914 ) gen_loss 805.59\n",
            "iteration 5002, epoch 10, batch 473/481,disc_loss 78.03, (real 81.218, fake 74.842 ) gen_loss 768.2\n",
            "iteration 5003, epoch 10, batch 474/481,disc_loss 80.053, (real 81.826, fake 78.281 ) gen_loss 795.13\n",
            "iteration 5004, epoch 10, batch 475/481,disc_loss 76.688, (real 79.384, fake 73.991 ) gen_loss 801.71\n",
            "iteration 5005, epoch 10, batch 476/481,disc_loss 75.529, (real 78.446, fake 72.613 ) gen_loss 806.34\n",
            "iteration 5006, epoch 10, batch 477/481,disc_loss 77.038, (real 80.111, fake 73.966 ) gen_loss 762.4\n",
            "iteration 5007, epoch 10, batch 478/481,disc_loss 77.487, (real 81.009, fake 73.964 ) gen_loss 798.9\n",
            "iteration 5008, epoch 10, batch 479/481,disc_loss 75.176, (real 77.346, fake 73.006 ) gen_loss 811.9\n",
            "iteration 5009, epoch 10, batch 480/481,disc_loss 77.555, (real 80.721, fake 74.39 ) gen_loss 772.96\n",
            "iteration 5010, epoch 10, batch 481/481,disc_loss 78.601, (real 81.735, fake 75.466 ) gen_loss 832.01\n",
            "iteration 5011, epoch 11, batch 1/481,disc_loss 76.801, (real 78.861, fake 74.742 ) gen_loss 819.09\n",
            "iteration 5012, epoch 11, batch 2/481,disc_loss 77.73, (real 80.393, fake 75.068 ) gen_loss 813.75\n",
            "iteration 5013, epoch 11, batch 3/481,disc_loss 80.273, (real 83.43, fake 77.115 ) gen_loss 880.91\n",
            "iteration 5014, epoch 11, batch 4/481,disc_loss 78.934, (real 81.8, fake 76.067 ) gen_loss 734.06\n",
            "iteration 5015, epoch 11, batch 5/481,disc_loss 79.347, (real 81.977, fake 76.717 ) gen_loss 790.56\n",
            "iteration 5016, epoch 11, batch 6/481,disc_loss 79.515, (real 82.69, fake 76.341 ) gen_loss 807.14\n",
            "iteration 5017, epoch 11, batch 7/481,disc_loss 79.929, (real 81.729, fake 78.129 ) gen_loss 793.94\n",
            "iteration 5018, epoch 11, batch 8/481,disc_loss 78.134, (real 80.236, fake 76.032 ) gen_loss 871.76\n",
            "iteration 5019, epoch 11, batch 9/481,disc_loss 73.408, (real 76.646, fake 70.17 ) gen_loss 753.46\n",
            "iteration 5020, epoch 11, batch 10/481,disc_loss 73.151, (real 75.688, fake 70.613 ) gen_loss 744.75\n",
            "iteration 5021, epoch 11, batch 11/481,disc_loss 79.147, (real 81.792, fake 76.502 ) gen_loss 778.17\n",
            "iteration 5022, epoch 11, batch 12/481,disc_loss 81.796, (real 84.77, fake 78.823 ) gen_loss 770.96\n",
            "iteration 5023, epoch 11, batch 13/481,disc_loss 74.457, (real 77.428, fake 71.486 ) gen_loss 784.7\n",
            "iteration 5024, epoch 11, batch 14/481,disc_loss 79.176, (real 82.371, fake 75.981 ) gen_loss 772.84\n",
            "iteration 5025, epoch 11, batch 15/481,disc_loss 74.873, (real 77.948, fake 71.799 ) gen_loss 731.26\n",
            "iteration 5026, epoch 11, batch 16/481,disc_loss 76.929, (real 79.961, fake 73.897 ) gen_loss 722.99\n",
            "iteration 5027, epoch 11, batch 17/481,disc_loss 83.657, (real 86.508, fake 80.806 ) gen_loss 773.97\n",
            "iteration 5028, epoch 11, batch 18/481,disc_loss 79.328, (real 82.145, fake 76.511 ) gen_loss 768.74\n",
            "iteration 5029, epoch 11, batch 19/481,disc_loss 73.961, (real 76.685, fake 71.236 ) gen_loss 721.78\n",
            "iteration 5030, epoch 11, batch 20/481,disc_loss 76.605, (real 79.644, fake 73.566 ) gen_loss 805.94\n",
            "iteration 5031, epoch 11, batch 21/481,disc_loss 73.343, (real 75.598, fake 71.087 ) gen_loss 797.67\n",
            "iteration 5032, epoch 11, batch 22/481,disc_loss 79.189, (real 82.188, fake 76.19 ) gen_loss 917.8\n",
            "iteration 5033, epoch 11, batch 23/481,disc_loss 77.149, (real 80.389, fake 73.908 ) gen_loss 910.43\n",
            "iteration 5034, epoch 11, batch 24/481,disc_loss 74.696, (real 76.782, fake 72.611 ) gen_loss 790.72\n",
            "iteration 5035, epoch 11, batch 25/481,disc_loss 73.418, (real 75.549, fake 71.287 ) gen_loss 796.75\n",
            "iteration 5036, epoch 11, batch 26/481,disc_loss 78.988, (real 81.252, fake 76.724 ) gen_loss 687.14\n",
            "iteration 5037, epoch 11, batch 27/481,disc_loss 75.009, (real 78.255, fake 71.764 ) gen_loss 688.13\n",
            "iteration 5038, epoch 11, batch 28/481,disc_loss 74.464, (real 77.225, fake 71.702 ) gen_loss 806.71\n",
            "iteration 5039, epoch 11, batch 29/481,disc_loss 74.174, (real 76.819, fake 71.528 ) gen_loss 845.83\n",
            "iteration 5040, epoch 11, batch 30/481,disc_loss 78.836, (real 81.596, fake 76.077 ) gen_loss 912.39\n",
            "iteration 5041, epoch 11, batch 31/481,disc_loss 80.216, (real 82.066, fake 78.365 ) gen_loss 832.44\n",
            "iteration 5042, epoch 11, batch 32/481,disc_loss 72.357, (real 75.071, fake 69.644 ) gen_loss 809.83\n",
            "iteration 5043, epoch 11, batch 33/481,disc_loss 78.27, (real 79.854, fake 76.685 ) gen_loss 903.51\n",
            "iteration 5044, epoch 11, batch 34/481,disc_loss 80.929, (real 83.924, fake 77.933 ) gen_loss 805.79\n",
            "iteration 5045, epoch 11, batch 35/481,disc_loss 77.848, (real 80.489, fake 75.206 ) gen_loss 817.4\n",
            "iteration 5046, epoch 11, batch 36/481,disc_loss 79.692, (real 82.171, fake 77.213 ) gen_loss 952.54\n",
            "iteration 5047, epoch 11, batch 37/481,disc_loss 78.528, (real 81.828, fake 75.228 ) gen_loss 752.66\n",
            "iteration 5048, epoch 11, batch 38/481,disc_loss 80.014, (real 82.541, fake 77.487 ) gen_loss 767.45\n",
            "iteration 5049, epoch 11, batch 39/481,disc_loss 77.181, (real 79.144, fake 75.217 ) gen_loss 764.21\n",
            "iteration 5050, epoch 11, batch 40/481,disc_loss 75.329, (real 77.876, fake 72.782 ) gen_loss 835.84\n",
            "iteration 5051, epoch 11, batch 41/481,disc_loss 76.673, (real 79.767, fake 73.578 ) gen_loss 766.02\n",
            "iteration 5052, epoch 11, batch 42/481,disc_loss 77.026, (real 80.114, fake 73.938 ) gen_loss 867.62\n",
            "iteration 5053, epoch 11, batch 43/481,disc_loss 80.186, (real 82.531, fake 77.842 ) gen_loss 893.86\n",
            "iteration 5054, epoch 11, batch 44/481,disc_loss 79.449, (real 82.435, fake 76.464 ) gen_loss 840.91\n",
            "iteration 5055, epoch 11, batch 45/481,disc_loss 80.688, (real 82.694, fake 78.682 ) gen_loss 767.98\n",
            "iteration 5056, epoch 11, batch 46/481,disc_loss 75.417, (real 77.971, fake 72.863 ) gen_loss 805.84\n",
            "iteration 5057, epoch 11, batch 47/481,disc_loss 78.159, (real 80.474, fake 75.845 ) gen_loss 807.99\n",
            "iteration 5058, epoch 11, batch 48/481,disc_loss 80.161, (real 82.725, fake 77.597 ) gen_loss 867.78\n",
            "iteration 5059, epoch 11, batch 49/481,disc_loss 75.718, (real 78.182, fake 73.253 ) gen_loss 819.68\n",
            "iteration 5060, epoch 11, batch 50/481,disc_loss 79.526, (real 81.814, fake 77.238 ) gen_loss 784.84\n",
            "iteration 5061, epoch 11, batch 51/481,disc_loss 79.371, (real 82.004, fake 76.738 ) gen_loss 785.11\n",
            "iteration 5062, epoch 11, batch 52/481,disc_loss 78.852, (real 81.818, fake 75.887 ) gen_loss 757.01\n",
            "iteration 5063, epoch 11, batch 53/481,disc_loss 79.251, (real 82.712, fake 75.789 ) gen_loss 831.2\n",
            "iteration 5064, epoch 11, batch 54/481,disc_loss 80.124, (real 82.987, fake 77.262 ) gen_loss 782.04\n",
            "iteration 5065, epoch 11, batch 55/481,disc_loss 75.888, (real 78.73, fake 73.045 ) gen_loss 784.64\n",
            "iteration 5066, epoch 11, batch 56/481,disc_loss 78.213, (real 81.284, fake 75.141 ) gen_loss 750.23\n",
            "iteration 5067, epoch 11, batch 57/481,disc_loss 76.305, (real 79.175, fake 73.434 ) gen_loss 787.65\n",
            "iteration 5068, epoch 11, batch 58/481,disc_loss 76.357, (real 79.494, fake 73.22 ) gen_loss 742.52\n",
            "iteration 5069, epoch 11, batch 59/481,disc_loss 75.918, (real 78.654, fake 73.182 ) gen_loss 792.73\n",
            "iteration 5070, epoch 11, batch 60/481,disc_loss 76.324, (real 79.132, fake 73.516 ) gen_loss 829.6\n",
            "iteration 5071, epoch 11, batch 61/481,disc_loss 79.319, (real 82.443, fake 76.195 ) gen_loss 804.17\n",
            "iteration 5072, epoch 11, batch 62/481,disc_loss 74.264, (real 76.931, fake 71.597 ) gen_loss 746.15\n",
            "iteration 5073, epoch 11, batch 63/481,disc_loss 77.505, (real 79.536, fake 75.474 ) gen_loss 842.06\n",
            "iteration 5074, epoch 11, batch 64/481,disc_loss 76.741, (real 80.053, fake 73.429 ) gen_loss 790.09\n",
            "iteration 5075, epoch 11, batch 65/481,disc_loss 74.095, (real 76.985, fake 71.205 ) gen_loss 826.96\n",
            "iteration 5076, epoch 11, batch 66/481,disc_loss 77.001, (real 79.487, fake 74.516 ) gen_loss 733.98\n",
            "iteration 5077, epoch 11, batch 67/481,disc_loss 77.099, (real 79.437, fake 74.761 ) gen_loss 748.53\n",
            "iteration 5078, epoch 11, batch 68/481,disc_loss 79.365, (real 80.606, fake 78.124 ) gen_loss 919.41\n",
            "iteration 5079, epoch 11, batch 69/481,disc_loss 76.592, (real 78.74, fake 74.445 ) gen_loss 847.77\n",
            "iteration 5080, epoch 11, batch 70/481,disc_loss 79.467, (real 82.397, fake 76.537 ) gen_loss 769.9\n",
            "iteration 5081, epoch 11, batch 71/481,disc_loss 78.986, (real 81.806, fake 76.167 ) gen_loss 795.03\n",
            "iteration 5082, epoch 11, batch 72/481,disc_loss 78.976, (real 81.265, fake 76.687 ) gen_loss 872.41\n",
            "iteration 5083, epoch 11, batch 73/481,disc_loss 79.2, (real 81.59, fake 76.81 ) gen_loss 705.04\n",
            "iteration 5084, epoch 11, batch 74/481,disc_loss 78.208, (real 81.222, fake 75.195 ) gen_loss 790.95\n",
            "iteration 5085, epoch 11, batch 75/481,disc_loss 77.93, (real 80.678, fake 75.182 ) gen_loss 833.74\n",
            "iteration 5086, epoch 11, batch 76/481,disc_loss 74.853, (real 77.375, fake 72.332 ) gen_loss 817.43\n",
            "iteration 5087, epoch 11, batch 77/481,disc_loss 80.74, (real 84.025, fake 77.456 ) gen_loss 903.93\n",
            "iteration 5088, epoch 11, batch 78/481,disc_loss 73.867, (real 76.847, fake 70.887 ) gen_loss 796.85\n",
            "iteration 5089, epoch 11, batch 79/481,disc_loss 82.229, (real 84.609, fake 79.849 ) gen_loss 854.95\n",
            "iteration 5090, epoch 11, batch 80/481,disc_loss 79.654, (real 82.607, fake 76.702 ) gen_loss 832.39\n",
            "iteration 5091, epoch 11, batch 81/481,disc_loss 78.764, (real 81.653, fake 75.876 ) gen_loss 747.8\n",
            "iteration 5092, epoch 11, batch 82/481,disc_loss 79.952, (real 82.057, fake 77.847 ) gen_loss 814.2\n",
            "iteration 5093, epoch 11, batch 83/481,disc_loss 75.636, (real 77.896, fake 73.375 ) gen_loss 872.82\n",
            "iteration 5094, epoch 11, batch 84/481,disc_loss 73.854, (real 76.205, fake 71.502 ) gen_loss 855.0\n",
            "iteration 5095, epoch 11, batch 85/481,disc_loss 79.562, (real 81.915, fake 77.209 ) gen_loss 738.89\n",
            "iteration 5096, epoch 11, batch 86/481,disc_loss 75.482, (real 77.814, fake 73.15 ) gen_loss 797.3\n",
            "iteration 5097, epoch 11, batch 87/481,disc_loss 76.472, (real 79.511, fake 73.434 ) gen_loss 826.98\n",
            "iteration 5098, epoch 11, batch 88/481,disc_loss 74.841, (real 77.644, fake 72.039 ) gen_loss 863.7\n",
            "iteration 5099, epoch 11, batch 89/481,disc_loss 76.857, (real 79.231, fake 74.482 ) gen_loss 845.41\n",
            "iteration 5100, epoch 11, batch 90/481,disc_loss 75.47, (real 77.614, fake 73.327 ) gen_loss 822.81\n",
            "iteration 5101, epoch 11, batch 91/481,disc_loss 77.251, (real 79.54, fake 74.961 ) gen_loss 810.33\n",
            "iteration 5102, epoch 11, batch 92/481,disc_loss 72.576, (real 75.957, fake 69.195 ) gen_loss 841.8\n",
            "iteration 5103, epoch 11, batch 93/481,disc_loss 80.302, (real 83.02, fake 77.585 ) gen_loss 861.92\n",
            "iteration 5104, epoch 11, batch 94/481,disc_loss 79.138, (real 81.483, fake 76.794 ) gen_loss 1055.3\n",
            "iteration 5105, epoch 11, batch 95/481,disc_loss 78.09, (real 81.054, fake 75.126 ) gen_loss 894.34\n",
            "iteration 5106, epoch 11, batch 96/481,disc_loss 77.477, (real 79.819, fake 75.136 ) gen_loss 806.44\n",
            "iteration 5107, epoch 11, batch 97/481,disc_loss 78.757, (real 80.292, fake 77.222 ) gen_loss 802.76\n",
            "iteration 5108, epoch 11, batch 98/481,disc_loss 77.816, (real 81.447, fake 74.185 ) gen_loss 819.67\n",
            "iteration 5109, epoch 11, batch 99/481,disc_loss 78.583, (real 81.619, fake 75.547 ) gen_loss 793.17\n",
            "iteration 5110, epoch 11, batch 100/481,disc_loss 74.841, (real 78.214, fake 71.469 ) gen_loss 769.92\n",
            "iteration 5111, epoch 11, batch 101/481,disc_loss 78.445, (real 80.44, fake 76.45 ) gen_loss 819.79\n",
            "iteration 5112, epoch 11, batch 102/481,disc_loss 80.829, (real 83.676, fake 77.982 ) gen_loss 870.02\n",
            "iteration 5113, epoch 11, batch 103/481,disc_loss 76.851, (real 79.749, fake 73.953 ) gen_loss 736.25\n",
            "iteration 5114, epoch 11, batch 104/481,disc_loss 77.409, (real 79.676, fake 75.142 ) gen_loss 735.69\n",
            "iteration 5115, epoch 11, batch 105/481,disc_loss 79.071, (real 82.067, fake 76.075 ) gen_loss 776.14\n",
            "iteration 5116, epoch 11, batch 106/481,disc_loss 75.453, (real 78.204, fake 72.703 ) gen_loss 759.74\n",
            "iteration 5117, epoch 11, batch 107/481,disc_loss 78.398, (real 81.251, fake 75.545 ) gen_loss 891.78\n",
            "iteration 5118, epoch 11, batch 108/481,disc_loss 76.801, (real 80.193, fake 73.409 ) gen_loss 894.32\n",
            "iteration 5119, epoch 11, batch 109/481,disc_loss 74.747, (real 77.523, fake 71.97 ) gen_loss 825.28\n",
            "iteration 5120, epoch 11, batch 110/481,disc_loss 78.157, (real 81.229, fake 75.084 ) gen_loss 778.28\n",
            "iteration 5121, epoch 11, batch 111/481,disc_loss 79.268, (real 82.088, fake 76.448 ) gen_loss 842.94\n",
            "iteration 5122, epoch 11, batch 112/481,disc_loss 80.327, (real 82.473, fake 78.18 ) gen_loss 806.18\n",
            "iteration 5123, epoch 11, batch 113/481,disc_loss 73.867, (real 76.497, fake 71.237 ) gen_loss 777.5\n",
            "iteration 5124, epoch 11, batch 114/481,disc_loss 82.654, (real 85.16, fake 80.149 ) gen_loss 740.89\n",
            "iteration 5125, epoch 11, batch 115/481,disc_loss 79.084, (real 81.015, fake 77.152 ) gen_loss 1015.7\n",
            "iteration 5126, epoch 11, batch 116/481,disc_loss 75.225, (real 76.887, fake 73.564 ) gen_loss 830.34\n",
            "iteration 5127, epoch 11, batch 117/481,disc_loss 79.936, (real 82.09, fake 77.782 ) gen_loss 834.05\n",
            "iteration 5128, epoch 11, batch 118/481,disc_loss 78.282, (real 80.429, fake 76.135 ) gen_loss 797.92\n",
            "iteration 5129, epoch 11, batch 119/481,disc_loss 75.731, (real 78.962, fake 72.5 ) gen_loss 785.12\n",
            "iteration 5130, epoch 11, batch 120/481,disc_loss 77.767, (real 80.969, fake 74.564 ) gen_loss 825.37\n",
            "iteration 5131, epoch 11, batch 121/481,disc_loss 79.494, (real 82.568, fake 76.419 ) gen_loss 805.77\n",
            "iteration 5132, epoch 11, batch 122/481,disc_loss 76.704, (real 80.117, fake 73.291 ) gen_loss 896.53\n",
            "iteration 5133, epoch 11, batch 123/481,disc_loss 76.735, (real 79.37, fake 74.099 ) gen_loss 869.89\n",
            "iteration 5134, epoch 11, batch 124/481,disc_loss 82.455, (real 85.891, fake 79.019 ) gen_loss 786.56\n",
            "iteration 5135, epoch 11, batch 125/481,disc_loss 82.49, (real 85.616, fake 79.363 ) gen_loss 765.56\n",
            "iteration 5136, epoch 11, batch 126/481,disc_loss 76.245, (real 78.31, fake 74.18 ) gen_loss 800.51\n",
            "iteration 5137, epoch 11, batch 127/481,disc_loss 76.869, (real 79.517, fake 74.222 ) gen_loss 876.29\n",
            "iteration 5138, epoch 11, batch 128/481,disc_loss 74.008, (real 76.655, fake 71.361 ) gen_loss 770.72\n",
            "iteration 5139, epoch 11, batch 129/481,disc_loss 76.525, (real 79.226, fake 73.823 ) gen_loss 745.69\n",
            "iteration 5140, epoch 11, batch 130/481,disc_loss 79.797, (real 84.209, fake 75.384 ) gen_loss 1066.5\n",
            "iteration 5141, epoch 11, batch 131/481,disc_loss 79.404, (real 81.455, fake 77.353 ) gen_loss 857.74\n",
            "iteration 5142, epoch 11, batch 132/481,disc_loss 79.598, (real 82.559, fake 76.637 ) gen_loss 717.64\n",
            "iteration 5143, epoch 11, batch 133/481,disc_loss 74.942, (real 77.639, fake 72.245 ) gen_loss 738.51\n",
            "iteration 5144, epoch 11, batch 134/481,disc_loss 78.318, (real 81.724, fake 74.913 ) gen_loss 878.42\n",
            "iteration 5145, epoch 11, batch 135/481,disc_loss 82.384, (real 85.275, fake 79.492 ) gen_loss 782.73\n",
            "iteration 5146, epoch 11, batch 136/481,disc_loss 78.102, (real 81.386, fake 74.818 ) gen_loss 784.66\n",
            "iteration 5147, epoch 11, batch 137/481,disc_loss 75.905, (real 78.696, fake 73.114 ) gen_loss 775.16\n",
            "iteration 5148, epoch 11, batch 138/481,disc_loss 75.246, (real 78.266, fake 72.226 ) gen_loss 792.75\n",
            "iteration 5149, epoch 11, batch 139/481,disc_loss 77.446, (real 79.913, fake 74.979 ) gen_loss 750.11\n",
            "iteration 5150, epoch 11, batch 140/481,disc_loss 78.848, (real 82.594, fake 75.103 ) gen_loss 837.47\n",
            "iteration 5151, epoch 11, batch 141/481,disc_loss 73.081, (real 76.266, fake 69.896 ) gen_loss 811.23\n",
            "iteration 5152, epoch 11, batch 142/481,disc_loss 76.05, (real 79.157, fake 72.944 ) gen_loss 843.15\n",
            "iteration 5153, epoch 11, batch 143/481,disc_loss 76.838, (real 79.122, fake 74.555 ) gen_loss 861.32\n",
            "iteration 5154, epoch 11, batch 144/481,disc_loss 80.545, (real 83.062, fake 78.029 ) gen_loss 870.48\n",
            "iteration 5155, epoch 11, batch 145/481,disc_loss 80.095, (real 82.623, fake 77.567 ) gen_loss 743.94\n",
            "iteration 5156, epoch 11, batch 146/481,disc_loss 78.165, (real 81.966, fake 74.365 ) gen_loss 847.95\n",
            "iteration 5157, epoch 11, batch 147/481,disc_loss 76.488, (real 78.859, fake 74.117 ) gen_loss 807.56\n",
            "iteration 5158, epoch 11, batch 148/481,disc_loss 77.843, (real 79.918, fake 75.767 ) gen_loss 909.45\n",
            "iteration 5159, epoch 11, batch 149/481,disc_loss 76.112, (real 78.167, fake 74.056 ) gen_loss 820.09\n",
            "iteration 5160, epoch 11, batch 150/481,disc_loss 82.118, (real 85.846, fake 78.389 ) gen_loss 940.53\n",
            "iteration 5161, epoch 11, batch 151/481,disc_loss 79.485, (real 82.072, fake 76.898 ) gen_loss 850.26\n",
            "iteration 5162, epoch 11, batch 152/481,disc_loss 80.018, (real 83.089, fake 76.946 ) gen_loss 828.88\n",
            "iteration 5163, epoch 11, batch 153/481,disc_loss 76.977, (real 79.937, fake 74.016 ) gen_loss 801.0\n",
            "iteration 5164, epoch 11, batch 154/481,disc_loss 79.476, (real 82.228, fake 76.723 ) gen_loss 882.61\n",
            "iteration 5165, epoch 11, batch 155/481,disc_loss 74.911, (real 77.271, fake 72.55 ) gen_loss 907.2\n",
            "iteration 5166, epoch 11, batch 156/481,disc_loss 72.126, (real 74.799, fake 69.454 ) gen_loss 897.85\n",
            "iteration 5167, epoch 11, batch 157/481,disc_loss 70.874, (real 73.486, fake 68.263 ) gen_loss 1100.2\n",
            "iteration 5168, epoch 11, batch 158/481,disc_loss 81.538, (real 84.566, fake 78.51 ) gen_loss 977.87\n",
            "iteration 5169, epoch 11, batch 159/481,disc_loss 68.997, (real 71.488, fake 66.506 ) gen_loss 858.85\n",
            "iteration 5170, epoch 11, batch 160/481,disc_loss 76.394, (real 79.005, fake 73.783 ) gen_loss 848.58\n",
            "iteration 5171, epoch 11, batch 161/481,disc_loss 77.514, (real 80.356, fake 74.672 ) gen_loss 762.05\n",
            "iteration 5172, epoch 11, batch 162/481,disc_loss 78.596, (real 80.656, fake 76.536 ) gen_loss 854.43\n",
            "iteration 5173, epoch 11, batch 163/481,disc_loss 76.37, (real 79.347, fake 73.393 ) gen_loss 893.67\n",
            "iteration 5174, epoch 11, batch 164/481,disc_loss 74.369, (real 77.154, fake 71.583 ) gen_loss 847.9\n",
            "iteration 5175, epoch 11, batch 165/481,disc_loss 76.574, (real 79.753, fake 73.395 ) gen_loss 840.07\n",
            "iteration 5176, epoch 11, batch 166/481,disc_loss 77.245, (real 80.661, fake 73.828 ) gen_loss 842.34\n",
            "iteration 5177, epoch 11, batch 167/481,disc_loss 75.99, (real 78.417, fake 73.563 ) gen_loss 868.17\n",
            "iteration 5178, epoch 11, batch 168/481,disc_loss 80.948, (real 84.087, fake 77.81 ) gen_loss 828.04\n",
            "iteration 5179, epoch 11, batch 169/481,disc_loss 77.614, (real 80.858, fake 74.371 ) gen_loss 897.84\n",
            "iteration 5180, epoch 11, batch 170/481,disc_loss 77.002, (real 79.112, fake 74.891 ) gen_loss 795.77\n",
            "iteration 5181, epoch 11, batch 171/481,disc_loss 81.225, (real 84.01, fake 78.44 ) gen_loss 798.64\n",
            "iteration 5182, epoch 11, batch 172/481,disc_loss 77.232, (real 79.612, fake 74.852 ) gen_loss 815.12\n",
            "iteration 5183, epoch 11, batch 173/481,disc_loss 77.116, (real 79.384, fake 74.848 ) gen_loss 933.26\n",
            "iteration 5184, epoch 11, batch 174/481,disc_loss 77.272, (real 80.048, fake 74.497 ) gen_loss 851.25\n",
            "iteration 5185, epoch 11, batch 175/481,disc_loss 76.519, (real 79.918, fake 73.119 ) gen_loss 896.39\n",
            "iteration 5186, epoch 11, batch 176/481,disc_loss 83.31, (real 85.644, fake 80.976 ) gen_loss 831.46\n",
            "iteration 5187, epoch 11, batch 177/481,disc_loss 77.564, (real 80.273, fake 74.856 ) gen_loss 810.57\n",
            "iteration 5188, epoch 11, batch 178/481,disc_loss 78.798, (real 81.237, fake 76.358 ) gen_loss 915.23\n",
            "iteration 5189, epoch 11, batch 179/481,disc_loss 83.722, (real 86.881, fake 80.564 ) gen_loss 902.61\n",
            "iteration 5190, epoch 11, batch 180/481,disc_loss 76.05, (real 78.25, fake 73.85 ) gen_loss 826.99\n",
            "iteration 5191, epoch 11, batch 181/481,disc_loss 76.085, (real 78.197, fake 73.973 ) gen_loss 784.79\n",
            "iteration 5192, epoch 11, batch 182/481,disc_loss 76.516, (real 79.296, fake 73.735 ) gen_loss 751.22\n",
            "iteration 5193, epoch 11, batch 183/481,disc_loss 80.26, (real 82.639, fake 77.882 ) gen_loss 883.22\n",
            "iteration 5194, epoch 11, batch 184/481,disc_loss 80.214, (real 82.968, fake 77.461 ) gen_loss 826.34\n",
            "iteration 5195, epoch 11, batch 185/481,disc_loss 76.705, (real 79.512, fake 73.897 ) gen_loss 729.76\n",
            "iteration 5196, epoch 11, batch 186/481,disc_loss 75.255, (real 78.28, fake 72.23 ) gen_loss 818.49\n",
            "iteration 5197, epoch 11, batch 187/481,disc_loss 80.436, (real 82.914, fake 77.959 ) gen_loss 740.88\n",
            "iteration 5198, epoch 11, batch 188/481,disc_loss 80.787, (real 84.256, fake 77.317 ) gen_loss 752.0\n",
            "iteration 5199, epoch 11, batch 189/481,disc_loss 77.299, (real 80.699, fake 73.899 ) gen_loss 829.02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 5200, epoch 11, batch 190/481,disc_loss 75.81, (real 78.743, fake 72.878 ) gen_loss 846.72\n",
            "iteration 5201, epoch 11, batch 191/481,disc_loss 75.614, (real 78.34, fake 72.888 ) gen_loss 820.13\n",
            "iteration 5202, epoch 11, batch 192/481,disc_loss 73.822, (real 76.424, fake 71.22 ) gen_loss 893.35\n",
            "iteration 5203, epoch 11, batch 193/481,disc_loss 76.117, (real 78.084, fake 74.149 ) gen_loss 874.92\n",
            "iteration 5204, epoch 11, batch 194/481,disc_loss 77.608, (real 79.715, fake 75.502 ) gen_loss 846.4\n",
            "iteration 5205, epoch 11, batch 195/481,disc_loss 75.287, (real 77.768, fake 72.807 ) gen_loss 876.79\n",
            "iteration 5206, epoch 11, batch 196/481,disc_loss 80.189, (real 83.007, fake 77.37 ) gen_loss 867.91\n",
            "iteration 5207, epoch 11, batch 197/481,disc_loss 79.62, (real 82.465, fake 76.776 ) gen_loss 754.17\n",
            "iteration 5208, epoch 11, batch 198/481,disc_loss 78.782, (real 81.723, fake 75.842 ) gen_loss 824.51\n",
            "iteration 5209, epoch 11, batch 199/481,disc_loss 78.014, (real 81.19, fake 74.838 ) gen_loss 780.64\n",
            "iteration 5210, epoch 11, batch 200/481,disc_loss 80.612, (real 82.93, fake 78.294 ) gen_loss 933.35\n",
            "iteration 5211, epoch 11, batch 201/481,disc_loss 76.869, (real 79.747, fake 73.99 ) gen_loss 868.81\n",
            "iteration 5212, epoch 11, batch 202/481,disc_loss 75.079, (real 77.984, fake 72.174 ) gen_loss 854.53\n",
            "iteration 5213, epoch 11, batch 203/481,disc_loss 78.052, (real 80.368, fake 75.736 ) gen_loss 798.48\n",
            "iteration 5214, epoch 11, batch 204/481,disc_loss 77.521, (real 80.777, fake 74.265 ) gen_loss 819.7\n",
            "iteration 5215, epoch 11, batch 205/481,disc_loss 74.054, (real 76.55, fake 71.558 ) gen_loss 861.75\n",
            "iteration 5216, epoch 11, batch 206/481,disc_loss 76.964, (real 79.806, fake 74.123 ) gen_loss 775.7\n",
            "iteration 5217, epoch 11, batch 207/481,disc_loss 80.266, (real 82.005, fake 78.528 ) gen_loss 804.63\n",
            "iteration 5218, epoch 11, batch 208/481,disc_loss 79.226, (real 81.989, fake 76.464 ) gen_loss 790.87\n",
            "iteration 5219, epoch 11, batch 209/481,disc_loss 76.096, (real 78.512, fake 73.68 ) gen_loss 1047.5\n",
            "iteration 5220, epoch 11, batch 210/481,disc_loss 81.425, (real 84.622, fake 78.227 ) gen_loss 986.95\n",
            "iteration 5221, epoch 11, batch 211/481,disc_loss 78.153, (real 80.97, fake 75.337 ) gen_loss 793.77\n",
            "iteration 5222, epoch 11, batch 212/481,disc_loss 76.43, (real 79.205, fake 73.656 ) gen_loss 983.26\n",
            "iteration 5223, epoch 11, batch 213/481,disc_loss 79.555, (real 81.867, fake 77.244 ) gen_loss 884.64\n",
            "iteration 5224, epoch 11, batch 214/481,disc_loss 69.9, (real 72.582, fake 67.218 ) gen_loss 999.5\n",
            "iteration 5225, epoch 11, batch 215/481,disc_loss 76.106, (real 78.314, fake 73.898 ) gen_loss 787.91\n",
            "iteration 5226, epoch 11, batch 216/481,disc_loss 75.995, (real 78.936, fake 73.053 ) gen_loss 840.49\n",
            "iteration 5227, epoch 11, batch 217/481,disc_loss 74.771, (real 77.268, fake 72.273 ) gen_loss 771.25\n",
            "iteration 5228, epoch 11, batch 218/481,disc_loss 75.458, (real 77.981, fake 72.935 ) gen_loss 780.31\n",
            "iteration 5229, epoch 11, batch 219/481,disc_loss 75.466, (real 78.346, fake 72.587 ) gen_loss 711.66\n",
            "iteration 5230, epoch 11, batch 220/481,disc_loss 77.543, (real 80.378, fake 74.707 ) gen_loss 896.95\n",
            "iteration 5231, epoch 11, batch 221/481,disc_loss 80.234, (real 82.985, fake 77.483 ) gen_loss 796.33\n",
            "iteration 5232, epoch 11, batch 222/481,disc_loss 78.034, (real 80.994, fake 75.074 ) gen_loss 898.48\n",
            "iteration 5233, epoch 11, batch 223/481,disc_loss 76.529, (real 78.875, fake 74.183 ) gen_loss 856.3\n",
            "iteration 5234, epoch 11, batch 224/481,disc_loss 80.971, (real 84.18, fake 77.762 ) gen_loss 874.66\n",
            "iteration 5235, epoch 11, batch 225/481,disc_loss 75.949, (real 78.666, fake 73.233 ) gen_loss 884.07\n",
            "iteration 5236, epoch 11, batch 226/481,disc_loss 80.893, (real 84.011, fake 77.775 ) gen_loss 819.57\n",
            "iteration 5237, epoch 11, batch 227/481,disc_loss 74.902, (real 78.063, fake 71.742 ) gen_loss 836.67\n",
            "iteration 5238, epoch 11, batch 228/481,disc_loss 77.386, (real 79.531, fake 75.242 ) gen_loss 767.36\n",
            "iteration 5239, epoch 11, batch 229/481,disc_loss 78.331, (real 81.309, fake 75.354 ) gen_loss 786.66\n",
            "iteration 5240, epoch 11, batch 230/481,disc_loss 79.019, (real 82.176, fake 75.861 ) gen_loss 911.31\n",
            "iteration 5241, epoch 11, batch 231/481,disc_loss 75.621, (real 78.965, fake 72.276 ) gen_loss 786.05\n",
            "iteration 5242, epoch 11, batch 232/481,disc_loss 78.287, (real 80.363, fake 76.211 ) gen_loss 852.9\n",
            "iteration 5243, epoch 11, batch 233/481,disc_loss 80.495, (real 82.924, fake 78.067 ) gen_loss 809.26\n",
            "iteration 5244, epoch 11, batch 234/481,disc_loss 75.767, (real 78.026, fake 73.509 ) gen_loss 839.67\n",
            "iteration 5245, epoch 11, batch 235/481,disc_loss 84.0, (real 87.998, fake 80.002 ) gen_loss 803.35\n",
            "iteration 5246, epoch 11, batch 236/481,disc_loss 74.941, (real 77.97, fake 71.912 ) gen_loss 809.19\n",
            "iteration 5247, epoch 11, batch 237/481,disc_loss 78.789, (real 81.889, fake 75.689 ) gen_loss 857.4\n",
            "iteration 5248, epoch 11, batch 238/481,disc_loss 82.435, (real 83.964, fake 80.906 ) gen_loss 801.92\n",
            "iteration 5249, epoch 11, batch 239/481,disc_loss 74.609, (real 77.191, fake 72.026 ) gen_loss 806.19\n",
            "iteration 5250, epoch 11, batch 240/481,disc_loss 74.925, (real 77.734, fake 72.116 ) gen_loss 885.74\n",
            "iteration 5251, epoch 11, batch 241/481,disc_loss 74.486, (real 77.714, fake 71.258 ) gen_loss 873.03\n",
            "iteration 5252, epoch 11, batch 242/481,disc_loss 76.011, (real 78.526, fake 73.496 ) gen_loss 818.24\n",
            "iteration 5253, epoch 11, batch 243/481,disc_loss 79.49, (real 82.37, fake 76.61 ) gen_loss 806.53\n",
            "iteration 5254, epoch 11, batch 244/481,disc_loss 81.776, (real 83.92, fake 79.633 ) gen_loss 811.21\n",
            "iteration 5255, epoch 11, batch 245/481,disc_loss 78.158, (real 80.913, fake 75.403 ) gen_loss 816.77\n",
            "iteration 5256, epoch 11, batch 246/481,disc_loss 81.997, (real 85.26, fake 78.734 ) gen_loss 768.3\n",
            "iteration 5257, epoch 11, batch 247/481,disc_loss 78.564, (real 80.705, fake 76.424 ) gen_loss 860.87\n",
            "iteration 5258, epoch 11, batch 248/481,disc_loss 75.389, (real 77.968, fake 72.809 ) gen_loss 826.41\n",
            "iteration 5259, epoch 11, batch 249/481,disc_loss 77.897, (real 80.84, fake 74.955 ) gen_loss 850.32\n",
            "iteration 5260, epoch 11, batch 250/481,disc_loss 76.533, (real 80.046, fake 73.021 ) gen_loss 757.72\n",
            "iteration 5261, epoch 11, batch 251/481,disc_loss 79.868, (real 82.778, fake 76.957 ) gen_loss 895.37\n",
            "iteration 5262, epoch 11, batch 252/481,disc_loss 74.654, (real 77.167, fake 72.142 ) gen_loss 836.18\n",
            "iteration 5263, epoch 11, batch 253/481,disc_loss 76.552, (real 79.106, fake 73.997 ) gen_loss 933.83\n",
            "iteration 5264, epoch 11, batch 254/481,disc_loss 81.623, (real 83.227, fake 80.02 ) gen_loss 748.24\n",
            "iteration 5265, epoch 11, batch 255/481,disc_loss 78.768, (real 79.674, fake 77.863 ) gen_loss 880.3\n",
            "iteration 5266, epoch 11, batch 256/481,disc_loss 81.774, (real 84.249, fake 79.298 ) gen_loss 912.88\n",
            "iteration 5267, epoch 11, batch 257/481,disc_loss 77.295, (real 80.322, fake 74.268 ) gen_loss 777.09\n",
            "iteration 5268, epoch 11, batch 258/481,disc_loss 77.132, (real 79.43, fake 74.835 ) gen_loss 764.92\n",
            "iteration 5269, epoch 11, batch 259/481,disc_loss 78.912, (real 82.023, fake 75.801 ) gen_loss 809.14\n",
            "iteration 5270, epoch 11, batch 260/481,disc_loss 77.027, (real 79.904, fake 74.151 ) gen_loss 802.73\n",
            "iteration 5271, epoch 11, batch 261/481,disc_loss 75.928, (real 78.777, fake 73.079 ) gen_loss 828.4\n",
            "iteration 5272, epoch 11, batch 262/481,disc_loss 80.781, (real 83.597, fake 77.965 ) gen_loss 872.61\n",
            "iteration 5273, epoch 11, batch 263/481,disc_loss 77.825, (real 80.513, fake 75.138 ) gen_loss 801.52\n",
            "iteration 5274, epoch 11, batch 264/481,disc_loss 83.002, (real 85.17, fake 80.834 ) gen_loss 748.07\n",
            "iteration 5275, epoch 11, batch 265/481,disc_loss 76.968, (real 79.516, fake 74.419 ) gen_loss 844.46\n",
            "iteration 5276, epoch 11, batch 266/481,disc_loss 79.513, (real 82.278, fake 76.748 ) gen_loss 714.1\n",
            "iteration 5277, epoch 11, batch 267/481,disc_loss 80.69, (real 83.319, fake 78.061 ) gen_loss 756.36\n",
            "iteration 5278, epoch 11, batch 268/481,disc_loss 79.578, (real 82.224, fake 76.932 ) gen_loss 709.3\n",
            "iteration 5279, epoch 11, batch 269/481,disc_loss 72.824, (real 75.594, fake 70.053 ) gen_loss 810.39\n",
            "iteration 5280, epoch 11, batch 270/481,disc_loss 77.523, (real 80.245, fake 74.801 ) gen_loss 815.46\n",
            "iteration 5281, epoch 11, batch 271/481,disc_loss 78.297, (real 80.528, fake 76.066 ) gen_loss 708.3\n",
            "iteration 5282, epoch 11, batch 272/481,disc_loss 76.525, (real 79.975, fake 73.076 ) gen_loss 783.22\n",
            "iteration 5283, epoch 11, batch 273/481,disc_loss 76.765, (real 79.527, fake 74.003 ) gen_loss 774.08\n",
            "iteration 5284, epoch 11, batch 274/481,disc_loss 78.094, (real 81.102, fake 75.086 ) gen_loss 800.55\n",
            "iteration 5285, epoch 11, batch 275/481,disc_loss 77.463, (real 80.457, fake 74.469 ) gen_loss 802.43\n",
            "iteration 5286, epoch 11, batch 276/481,disc_loss 77.306, (real 80.518, fake 74.094 ) gen_loss 759.27\n",
            "iteration 5287, epoch 11, batch 277/481,disc_loss 81.269, (real 83.741, fake 78.796 ) gen_loss 967.21\n",
            "iteration 5288, epoch 11, batch 278/481,disc_loss 78.235, (real 81.135, fake 75.335 ) gen_loss 804.17\n",
            "iteration 5289, epoch 11, batch 279/481,disc_loss 75.17, (real 77.713, fake 72.627 ) gen_loss 839.48\n",
            "iteration 5290, epoch 11, batch 280/481,disc_loss 81.067, (real 83.59, fake 78.544 ) gen_loss 857.37\n",
            "iteration 5291, epoch 11, batch 281/481,disc_loss 79.497, (real 83.873, fake 75.121 ) gen_loss 872.88\n",
            "iteration 5292, epoch 11, batch 282/481,disc_loss 77.888, (real 80.907, fake 74.87 ) gen_loss 898.45\n",
            "iteration 5293, epoch 11, batch 283/481,disc_loss 77.222, (real 79.994, fake 74.45 ) gen_loss 861.07\n",
            "iteration 5294, epoch 11, batch 284/481,disc_loss 80.887, (real 83.28, fake 78.495 ) gen_loss 884.17\n",
            "iteration 5295, epoch 11, batch 285/481,disc_loss 73.312, (real 75.702, fake 70.922 ) gen_loss 830.55\n",
            "iteration 5296, epoch 11, batch 286/481,disc_loss 74.905, (real 77.526, fake 72.284 ) gen_loss 855.85\n",
            "iteration 5297, epoch 11, batch 287/481,disc_loss 79.046, (real 80.826, fake 77.267 ) gen_loss 868.61\n",
            "iteration 5298, epoch 11, batch 288/481,disc_loss 79.129, (real 81.007, fake 77.25 ) gen_loss 799.19\n",
            "iteration 5299, epoch 11, batch 289/481,disc_loss 71.151, (real 74.283, fake 68.019 ) gen_loss 786.94\n",
            "iteration 5300, epoch 11, batch 290/481,disc_loss 77.144, (real 80.209, fake 74.078 ) gen_loss 868.25\n",
            "iteration 5301, epoch 11, batch 291/481,disc_loss 76.39, (real 79.684, fake 73.097 ) gen_loss 841.58\n",
            "iteration 5302, epoch 11, batch 292/481,disc_loss 79.055, (real 81.086, fake 77.024 ) gen_loss 880.16\n",
            "iteration 5303, epoch 11, batch 293/481,disc_loss 76.399, (real 79.322, fake 73.475 ) gen_loss 764.73\n",
            "iteration 5304, epoch 11, batch 294/481,disc_loss 74.495, (real 76.945, fake 72.045 ) gen_loss 703.98\n",
            "iteration 5305, epoch 11, batch 295/481,disc_loss 73.719, (real 75.815, fake 71.623 ) gen_loss 718.67\n",
            "iteration 5306, epoch 11, batch 296/481,disc_loss 76.559, (real 79.403, fake 73.715 ) gen_loss 715.55\n",
            "iteration 5307, epoch 11, batch 297/481,disc_loss 78.522, (real 81.17, fake 75.874 ) gen_loss 667.4\n",
            "iteration 5308, epoch 11, batch 298/481,disc_loss 79.973, (real 83.244, fake 76.703 ) gen_loss 716.6\n",
            "iteration 5309, epoch 11, batch 299/481,disc_loss 80.412, (real 83.151, fake 77.673 ) gen_loss 760.42\n",
            "iteration 5310, epoch 11, batch 300/481,disc_loss 75.987, (real 79.694, fake 72.28 ) gen_loss 748.73\n",
            "iteration 5311, epoch 11, batch 301/481,disc_loss 80.246, (real 83.963, fake 76.53 ) gen_loss 766.99\n",
            "iteration 5312, epoch 11, batch 302/481,disc_loss 80.886, (real 83.21, fake 78.563 ) gen_loss 716.23\n",
            "iteration 5313, epoch 11, batch 303/481,disc_loss 77.162, (real 79.948, fake 74.376 ) gen_loss 844.83\n",
            "iteration 5314, epoch 11, batch 304/481,disc_loss 79.829, (real 82.397, fake 77.261 ) gen_loss 873.59\n",
            "iteration 5315, epoch 11, batch 305/481,disc_loss 81.572, (real 84.062, fake 79.083 ) gen_loss 728.5\n",
            "iteration 5316, epoch 11, batch 306/481,disc_loss 76.781, (real 79.319, fake 74.244 ) gen_loss 786.78\n",
            "iteration 5317, epoch 11, batch 307/481,disc_loss 79.525, (real 82.077, fake 76.973 ) gen_loss 801.36\n",
            "iteration 5318, epoch 11, batch 308/481,disc_loss 75.618, (real 78.15, fake 73.086 ) gen_loss 798.67\n",
            "iteration 5319, epoch 11, batch 309/481,disc_loss 77.665, (real 80.898, fake 74.433 ) gen_loss 872.61\n",
            "iteration 5320, epoch 11, batch 310/481,disc_loss 81.112, (real 84.634, fake 77.59 ) gen_loss 860.86\n",
            "iteration 5321, epoch 11, batch 311/481,disc_loss 78.484, (real 81.758, fake 75.21 ) gen_loss 949.56\n",
            "iteration 5322, epoch 11, batch 312/481,disc_loss 75.845, (real 78.599, fake 73.091 ) gen_loss 885.34\n",
            "iteration 5323, epoch 11, batch 313/481,disc_loss 77.961, (real 80.457, fake 75.464 ) gen_loss 800.74\n",
            "iteration 5324, epoch 11, batch 314/481,disc_loss 77.518, (real 79.447, fake 75.59 ) gen_loss 950.05\n",
            "iteration 5325, epoch 11, batch 315/481,disc_loss 77.082, (real 79.697, fake 74.467 ) gen_loss 715.17\n",
            "iteration 5326, epoch 11, batch 316/481,disc_loss 77.652, (real 80.708, fake 74.597 ) gen_loss 773.75\n",
            "iteration 5327, epoch 11, batch 317/481,disc_loss 73.702, (real 76.039, fake 71.366 ) gen_loss 733.26\n",
            "iteration 5328, epoch 11, batch 318/481,disc_loss 71.967, (real 75.058, fake 68.877 ) gen_loss 772.21\n",
            "iteration 5329, epoch 11, batch 319/481,disc_loss 75.801, (real 79.039, fake 72.563 ) gen_loss 798.48\n",
            "iteration 5330, epoch 11, batch 320/481,disc_loss 77.656, (real 80.561, fake 74.751 ) gen_loss 768.28\n",
            "iteration 5331, epoch 11, batch 321/481,disc_loss 79.935, (real 83.684, fake 76.185 ) gen_loss 838.61\n",
            "iteration 5332, epoch 11, batch 322/481,disc_loss 77.813, (real 80.704, fake 74.922 ) gen_loss 862.72\n",
            "iteration 5333, epoch 11, batch 323/481,disc_loss 80.057, (real 83.318, fake 76.795 ) gen_loss 911.83\n",
            "iteration 5334, epoch 11, batch 324/481,disc_loss 73.911, (real 76.785, fake 71.038 ) gen_loss 1008.5\n",
            "iteration 5335, epoch 11, batch 325/481,disc_loss 79.172, (real 82.049, fake 76.295 ) gen_loss 919.53\n",
            "iteration 5336, epoch 11, batch 326/481,disc_loss 80.967, (real 83.091, fake 78.843 ) gen_loss 851.74\n",
            "iteration 5337, epoch 11, batch 327/481,disc_loss 79.199, (real 82.315, fake 76.084 ) gen_loss 854.88\n",
            "iteration 5338, epoch 11, batch 328/481,disc_loss 79.345, (real 81.981, fake 76.708 ) gen_loss 814.04\n",
            "iteration 5339, epoch 11, batch 329/481,disc_loss 74.994, (real 77.596, fake 72.391 ) gen_loss 740.87\n",
            "iteration 5340, epoch 11, batch 330/481,disc_loss 80.5, (real 83.045, fake 77.955 ) gen_loss 799.63\n",
            "iteration 5341, epoch 11, batch 331/481,disc_loss 80.375, (real 83.273, fake 77.477 ) gen_loss 756.23\n",
            "iteration 5342, epoch 11, batch 332/481,disc_loss 81.589, (real 84.014, fake 79.164 ) gen_loss 852.19\n",
            "iteration 5343, epoch 11, batch 333/481,disc_loss 79.04, (real 81.85, fake 76.229 ) gen_loss 771.31\n",
            "iteration 5344, epoch 11, batch 334/481,disc_loss 78.263, (real 81.319, fake 75.207 ) gen_loss 815.56\n",
            "iteration 5345, epoch 11, batch 335/481,disc_loss 78.501, (real 81.229, fake 75.773 ) gen_loss 789.46\n",
            "iteration 5346, epoch 11, batch 336/481,disc_loss 76.563, (real 79.209, fake 73.917 ) gen_loss 903.7\n",
            "iteration 5347, epoch 11, batch 337/481,disc_loss 75.631, (real 78.196, fake 73.066 ) gen_loss 926.65\n",
            "iteration 5348, epoch 11, batch 338/481,disc_loss 74.429, (real 76.724, fake 72.135 ) gen_loss 825.53\n",
            "iteration 5349, epoch 11, batch 339/481,disc_loss 74.751, (real 77.612, fake 71.89 ) gen_loss 782.78\n",
            "iteration 5350, epoch 11, batch 340/481,disc_loss 76.316, (real 79.908, fake 72.724 ) gen_loss 842.07\n",
            "iteration 5351, epoch 11, batch 341/481,disc_loss 76.527, (real 80.08, fake 72.974 ) gen_loss 864.32\n",
            "iteration 5352, epoch 11, batch 342/481,disc_loss 72.988, (real 76.153, fake 69.823 ) gen_loss 801.82\n",
            "iteration 5353, epoch 11, batch 343/481,disc_loss 73.751, (real 76.448, fake 71.054 ) gen_loss 773.2\n",
            "iteration 5354, epoch 11, batch 344/481,disc_loss 79.11, (real 82.943, fake 75.278 ) gen_loss 818.62\n",
            "iteration 5355, epoch 11, batch 345/481,disc_loss 80.868, (real 83.153, fake 78.583 ) gen_loss 867.48\n",
            "iteration 5356, epoch 11, batch 346/481,disc_loss 78.717, (real 80.843, fake 76.591 ) gen_loss 735.86\n",
            "iteration 5357, epoch 11, batch 347/481,disc_loss 80.37, (real 83.495, fake 77.244 ) gen_loss 906.44\n",
            "iteration 5358, epoch 11, batch 348/481,disc_loss 75.521, (real 78.628, fake 72.415 ) gen_loss 920.22\n",
            "iteration 5359, epoch 11, batch 349/481,disc_loss 77.464, (real 81.123, fake 73.805 ) gen_loss 909.67\n",
            "iteration 5360, epoch 11, batch 350/481,disc_loss 76.428, (real 79.049, fake 73.808 ) gen_loss 879.72\n",
            "iteration 5361, epoch 11, batch 351/481,disc_loss 75.825, (real 78.079, fake 73.571 ) gen_loss 766.81\n",
            "iteration 5362, epoch 11, batch 352/481,disc_loss 78.15, (real 80.507, fake 75.793 ) gen_loss 823.34\n",
            "iteration 5363, epoch 11, batch 353/481,disc_loss 79.485, (real 81.795, fake 77.176 ) gen_loss 835.46\n",
            "iteration 5364, epoch 11, batch 354/481,disc_loss 76.361, (real 79.115, fake 73.607 ) gen_loss 856.46\n",
            "iteration 5365, epoch 11, batch 355/481,disc_loss 78.629, (real 81.23, fake 76.029 ) gen_loss 848.01\n",
            "iteration 5366, epoch 11, batch 356/481,disc_loss 80.455, (real 84.047, fake 76.864 ) gen_loss 868.09\n",
            "iteration 5367, epoch 11, batch 357/481,disc_loss 77.155, (real 79.463, fake 74.847 ) gen_loss 813.25\n",
            "iteration 5368, epoch 11, batch 358/481,disc_loss 77.818, (real 80.342, fake 75.294 ) gen_loss 864.42\n",
            "iteration 5369, epoch 11, batch 359/481,disc_loss 77.83, (real 81.576, fake 74.083 ) gen_loss 916.08\n",
            "iteration 5370, epoch 11, batch 360/481,disc_loss 79.761, (real 84.005, fake 75.516 ) gen_loss 818.07\n",
            "iteration 5371, epoch 11, batch 361/481,disc_loss 78.597, (real 81.992, fake 75.203 ) gen_loss 898.89\n",
            "iteration 5372, epoch 11, batch 362/481,disc_loss 77.341, (real 80.072, fake 74.61 ) gen_loss 920.94\n",
            "iteration 5373, epoch 11, batch 363/481,disc_loss 78.191, (real 80.207, fake 76.174 ) gen_loss 819.26\n",
            "iteration 5374, epoch 11, batch 364/481,disc_loss 75.785, (real 77.599, fake 73.971 ) gen_loss 872.2\n",
            "iteration 5375, epoch 11, batch 365/481,disc_loss 80.654, (real 83.549, fake 77.759 ) gen_loss 887.63\n",
            "iteration 5376, epoch 11, batch 366/481,disc_loss 76.904, (real 79.049, fake 74.758 ) gen_loss 824.86\n",
            "iteration 5377, epoch 11, batch 367/481,disc_loss 80.496, (real 83.792, fake 77.199 ) gen_loss 840.66\n",
            "iteration 5378, epoch 11, batch 368/481,disc_loss 76.317, (real 78.456, fake 74.179 ) gen_loss 813.79\n",
            "iteration 5379, epoch 11, batch 369/481,disc_loss 76.349, (real 78.164, fake 74.534 ) gen_loss 782.64\n",
            "iteration 5380, epoch 11, batch 370/481,disc_loss 81.341, (real 84.551, fake 78.131 ) gen_loss 812.61\n",
            "iteration 5381, epoch 11, batch 371/481,disc_loss 73.585, (real 76.604, fake 70.567 ) gen_loss 784.7\n",
            "iteration 5382, epoch 11, batch 372/481,disc_loss 74.898, (real 76.933, fake 72.862 ) gen_loss 804.47\n",
            "iteration 5383, epoch 11, batch 373/481,disc_loss 80.829, (real 84.502, fake 77.157 ) gen_loss 813.05\n",
            "iteration 5384, epoch 11, batch 374/481,disc_loss 75.417, (real 78.748, fake 72.087 ) gen_loss 848.15\n",
            "iteration 5385, epoch 11, batch 375/481,disc_loss 79.618, (real 82.602, fake 76.634 ) gen_loss 807.88\n",
            "iteration 5386, epoch 11, batch 376/481,disc_loss 79.862, (real 82.599, fake 77.124 ) gen_loss 813.12\n",
            "iteration 5387, epoch 11, batch 377/481,disc_loss 77.609, (real 80.421, fake 74.796 ) gen_loss 820.05\n",
            "iteration 5388, epoch 11, batch 378/481,disc_loss 78.073, (real 81.138, fake 75.009 ) gen_loss 801.24\n",
            "iteration 5389, epoch 11, batch 379/481,disc_loss 74.818, (real 76.884, fake 72.753 ) gen_loss 739.29\n",
            "iteration 5390, epoch 11, batch 380/481,disc_loss 75.905, (real 79.142, fake 72.668 ) gen_loss 759.75\n",
            "iteration 5391, epoch 11, batch 381/481,disc_loss 73.09, (real 75.676, fake 70.505 ) gen_loss 762.39\n",
            "iteration 5392, epoch 11, batch 382/481,disc_loss 78.326, (real 81.482, fake 75.17 ) gen_loss 759.75\n",
            "iteration 5393, epoch 11, batch 383/481,disc_loss 79.55, (real 83.172, fake 75.929 ) gen_loss 820.51\n",
            "iteration 5394, epoch 11, batch 384/481,disc_loss 81.008, (real 84.302, fake 77.714 ) gen_loss 900.23\n",
            "iteration 5395, epoch 11, batch 385/481,disc_loss 74.327, (real 76.482, fake 72.172 ) gen_loss 815.22\n",
            "iteration 5396, epoch 11, batch 386/481,disc_loss 80.94, (real 84.491, fake 77.39 ) gen_loss 853.07\n",
            "iteration 5397, epoch 11, batch 387/481,disc_loss 81.68, (real 86.3, fake 77.06 ) gen_loss 797.43\n",
            "iteration 5398, epoch 11, batch 388/481,disc_loss 80.517, (real 83.149, fake 77.885 ) gen_loss 839.41\n",
            "iteration 5399, epoch 11, batch 389/481,disc_loss 80.867, (real 83.311, fake 78.423 ) gen_loss 884.65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 5400, epoch 11, batch 390/481,disc_loss 77.429, (real 79.702, fake 75.157 ) gen_loss 850.31\n",
            "iteration 5401, epoch 11, batch 391/481,disc_loss 78.761, (real 81.94, fake 75.582 ) gen_loss 790.45\n",
            "iteration 5402, epoch 11, batch 392/481,disc_loss 72.387, (real 74.47, fake 70.303 ) gen_loss 963.79\n",
            "iteration 5403, epoch 11, batch 393/481,disc_loss 84.63, (real 88.276, fake 80.984 ) gen_loss 860.9\n",
            "iteration 5404, epoch 11, batch 394/481,disc_loss 74.098, (real 77.664, fake 70.531 ) gen_loss 841.05\n",
            "iteration 5405, epoch 11, batch 395/481,disc_loss 74.251, (real 76.462, fake 72.04 ) gen_loss 767.62\n",
            "iteration 5406, epoch 11, batch 396/481,disc_loss 81.69, (real 84.2, fake 79.181 ) gen_loss 946.46\n",
            "iteration 5407, epoch 11, batch 397/481,disc_loss 80.836, (real 84.537, fake 77.134 ) gen_loss 833.48\n",
            "iteration 5408, epoch 11, batch 398/481,disc_loss 79.862, (real 82.372, fake 77.351 ) gen_loss 825.34\n",
            "iteration 5409, epoch 11, batch 399/481,disc_loss 79.635, (real 82.326, fake 76.944 ) gen_loss 913.45\n",
            "iteration 5410, epoch 11, batch 400/481,disc_loss 80.433, (real 84.268, fake 76.599 ) gen_loss 784.45\n",
            "iteration 5411, epoch 11, batch 401/481,disc_loss 79.513, (real 82.757, fake 76.269 ) gen_loss 753.71\n",
            "iteration 5412, epoch 11, batch 402/481,disc_loss 75.827, (real 78.949, fake 72.705 ) gen_loss 787.05\n",
            "iteration 5413, epoch 11, batch 403/481,disc_loss 80.046, (real 83.196, fake 76.897 ) gen_loss 729.43\n",
            "iteration 5414, epoch 11, batch 404/481,disc_loss 78.542, (real 80.719, fake 76.366 ) gen_loss 955.99\n",
            "iteration 5415, epoch 11, batch 405/481,disc_loss 78.826, (real 82.22, fake 75.432 ) gen_loss 781.7\n",
            "iteration 5416, epoch 11, batch 406/481,disc_loss 79.221, (real 81.422, fake 77.02 ) gen_loss 773.43\n",
            "iteration 5417, epoch 11, batch 407/481,disc_loss 76.172, (real 78.969, fake 73.375 ) gen_loss 833.62\n",
            "iteration 5418, epoch 11, batch 408/481,disc_loss 78.743, (real 80.867, fake 76.619 ) gen_loss 797.31\n",
            "iteration 5419, epoch 11, batch 409/481,disc_loss 79.299, (real 82.3, fake 76.299 ) gen_loss 883.07\n",
            "iteration 5420, epoch 11, batch 410/481,disc_loss 74.913, (real 77.452, fake 72.374 ) gen_loss 777.12\n",
            "iteration 5421, epoch 11, batch 411/481,disc_loss 76.785, (real 80.071, fake 73.5 ) gen_loss 754.07\n",
            "iteration 5422, epoch 11, batch 412/481,disc_loss 71.549, (real 74.049, fake 69.049 ) gen_loss 798.33\n",
            "iteration 5423, epoch 11, batch 413/481,disc_loss 73.978, (real 77.675, fake 70.28 ) gen_loss 771.77\n",
            "iteration 5424, epoch 11, batch 414/481,disc_loss 76.735, (real 79.603, fake 73.867 ) gen_loss 856.53\n",
            "iteration 5425, epoch 11, batch 415/481,disc_loss 76.267, (real 79.287, fake 73.246 ) gen_loss 804.35\n",
            "iteration 5426, epoch 11, batch 416/481,disc_loss 74.751, (real 77.393, fake 72.108 ) gen_loss 822.03\n",
            "iteration 5427, epoch 11, batch 417/481,disc_loss 75.104, (real 77.983, fake 72.224 ) gen_loss 849.25\n",
            "iteration 5428, epoch 11, batch 418/481,disc_loss 77.816, (real 80.713, fake 74.919 ) gen_loss 870.93\n",
            "iteration 5429, epoch 11, batch 419/481,disc_loss 80.651, (real 83.338, fake 77.964 ) gen_loss 831.63\n",
            "iteration 5430, epoch 11, batch 420/481,disc_loss 79.771, (real 82.777, fake 76.765 ) gen_loss 784.69\n",
            "iteration 5431, epoch 11, batch 421/481,disc_loss 77.947, (real 81.293, fake 74.602 ) gen_loss 756.17\n",
            "iteration 5432, epoch 11, batch 422/481,disc_loss 77.272, (real 80.68, fake 73.864 ) gen_loss 782.31\n",
            "iteration 5433, epoch 11, batch 423/481,disc_loss 79.887, (real 82.58, fake 77.195 ) gen_loss 805.54\n",
            "iteration 5434, epoch 11, batch 424/481,disc_loss 76.99, (real 79.837, fake 74.143 ) gen_loss 960.84\n",
            "iteration 5435, epoch 11, batch 425/481,disc_loss 75.911, (real 78.264, fake 73.557 ) gen_loss 757.45\n",
            "iteration 5436, epoch 11, batch 426/481,disc_loss 79.475, (real 82.348, fake 76.603 ) gen_loss 787.23\n",
            "iteration 5437, epoch 11, batch 427/481,disc_loss 76.581, (real 79.647, fake 73.516 ) gen_loss 839.15\n",
            "iteration 5438, epoch 11, batch 428/481,disc_loss 79.543, (real 82.994, fake 76.092 ) gen_loss 913.39\n",
            "iteration 5439, epoch 11, batch 429/481,disc_loss 77.935, (real 81.533, fake 74.337 ) gen_loss 922.04\n",
            "iteration 5440, epoch 11, batch 430/481,disc_loss 73.064, (real 76.048, fake 70.079 ) gen_loss 864.74\n",
            "iteration 5441, epoch 11, batch 431/481,disc_loss 77.754, (real 81.148, fake 74.36 ) gen_loss 1033.0\n",
            "iteration 5442, epoch 11, batch 432/481,disc_loss 79.59, (real 82.405, fake 76.775 ) gen_loss 800.44\n",
            "iteration 5443, epoch 11, batch 433/481,disc_loss 78.193, (real 80.501, fake 75.886 ) gen_loss 695.89\n",
            "iteration 5444, epoch 11, batch 434/481,disc_loss 78.112, (real 79.99, fake 76.234 ) gen_loss 912.35\n",
            "iteration 5445, epoch 11, batch 435/481,disc_loss 71.368, (real 73.594, fake 69.142 ) gen_loss 844.17\n",
            "iteration 5446, epoch 11, batch 436/481,disc_loss 80.252, (real 82.809, fake 77.696 ) gen_loss 828.85\n",
            "iteration 5447, epoch 11, batch 437/481,disc_loss 77.094, (real 79.891, fake 74.298 ) gen_loss 923.33\n",
            "iteration 5448, epoch 11, batch 438/481,disc_loss 82.33, (real 85.89, fake 78.77 ) gen_loss 951.26\n",
            "iteration 5449, epoch 11, batch 439/481,disc_loss 75.735, (real 78.799, fake 72.671 ) gen_loss 785.47\n",
            "iteration 5450, epoch 11, batch 440/481,disc_loss 75.821, (real 77.942, fake 73.7 ) gen_loss 883.31\n",
            "iteration 5451, epoch 11, batch 441/481,disc_loss 78.313, (real 80.429, fake 76.197 ) gen_loss 933.18\n",
            "iteration 5452, epoch 11, batch 442/481,disc_loss 75.993, (real 78.474, fake 73.513 ) gen_loss 1000.1\n",
            "iteration 5453, epoch 11, batch 443/481,disc_loss 76.548, (real 79.395, fake 73.702 ) gen_loss 888.63\n",
            "iteration 5454, epoch 11, batch 444/481,disc_loss 75.826, (real 78.669, fake 72.983 ) gen_loss 878.22\n",
            "iteration 5455, epoch 11, batch 445/481,disc_loss 77.209, (real 80.659, fake 73.76 ) gen_loss 928.13\n",
            "iteration 5456, epoch 11, batch 446/481,disc_loss 79.388, (real 82.098, fake 76.679 ) gen_loss 1052.0\n",
            "iteration 5457, epoch 11, batch 447/481,disc_loss 77.82, (real 80.946, fake 74.694 ) gen_loss 795.46\n",
            "iteration 5458, epoch 11, batch 448/481,disc_loss 79.388, (real 82.581, fake 76.195 ) gen_loss 856.5\n",
            "iteration 5459, epoch 11, batch 449/481,disc_loss 78.434, (real 81.546, fake 75.321 ) gen_loss 854.36\n",
            "iteration 5460, epoch 11, batch 450/481,disc_loss 80.307, (real 83.113, fake 77.501 ) gen_loss 846.91\n",
            "iteration 5461, epoch 11, batch 451/481,disc_loss 77.403, (real 79.79, fake 75.015 ) gen_loss 810.74\n",
            "iteration 5462, epoch 11, batch 452/481,disc_loss 77.912, (real 81.81, fake 74.013 ) gen_loss 863.92\n",
            "iteration 5463, epoch 11, batch 453/481,disc_loss 80.169, (real 82.896, fake 77.442 ) gen_loss 818.6\n",
            "iteration 5464, epoch 11, batch 454/481,disc_loss 79.92, (real 83.361, fake 76.479 ) gen_loss 900.19\n",
            "iteration 5465, epoch 11, batch 455/481,disc_loss 87.225, (real 89.14, fake 85.31 ) gen_loss 872.4\n",
            "iteration 5466, epoch 11, batch 456/481,disc_loss 81.388, (real 84.268, fake 78.509 ) gen_loss 720.38\n",
            "iteration 5467, epoch 11, batch 457/481,disc_loss 80.34, (real 83.535, fake 77.145 ) gen_loss 888.73\n",
            "iteration 5468, epoch 11, batch 458/481,disc_loss 78.3, (real 82.352, fake 74.249 ) gen_loss 812.25\n",
            "iteration 5469, epoch 11, batch 459/481,disc_loss 79.417, (real 82.237, fake 76.597 ) gen_loss 809.28\n",
            "iteration 5470, epoch 11, batch 460/481,disc_loss 80.497, (real 82.64, fake 78.353 ) gen_loss 893.33\n",
            "iteration 5471, epoch 11, batch 461/481,disc_loss 73.13, (real 75.549, fake 70.711 ) gen_loss 852.97\n",
            "iteration 5472, epoch 11, batch 462/481,disc_loss 80.15, (real 83.483, fake 76.818 ) gen_loss 748.5\n",
            "iteration 5473, epoch 11, batch 463/481,disc_loss 74.454, (real 77.351, fake 71.556 ) gen_loss 698.06\n",
            "iteration 5474, epoch 11, batch 464/481,disc_loss 75.589, (real 78.265, fake 72.913 ) gen_loss 789.43\n",
            "iteration 5475, epoch 11, batch 465/481,disc_loss 77.544, (real 80.092, fake 74.997 ) gen_loss 836.63\n",
            "iteration 5476, epoch 11, batch 466/481,disc_loss 81.793, (real 85.302, fake 78.284 ) gen_loss 827.05\n",
            "iteration 5477, epoch 11, batch 467/481,disc_loss 73.539, (real 75.827, fake 71.252 ) gen_loss 792.78\n",
            "iteration 5478, epoch 11, batch 468/481,disc_loss 78.241, (real 81.612, fake 74.871 ) gen_loss 768.08\n",
            "iteration 5479, epoch 11, batch 469/481,disc_loss 80.644, (real 83.497, fake 77.791 ) gen_loss 776.01\n",
            "iteration 5480, epoch 11, batch 470/481,disc_loss 74.612, (real 77.175, fake 72.049 ) gen_loss 768.81\n",
            "iteration 5481, epoch 11, batch 471/481,disc_loss 78.947, (real 81.62, fake 76.274 ) gen_loss 791.94\n",
            "iteration 5482, epoch 11, batch 472/481,disc_loss 75.949, (real 79.514, fake 72.384 ) gen_loss 873.17\n",
            "iteration 5483, epoch 11, batch 473/481,disc_loss 79.357, (real 82.149, fake 76.564 ) gen_loss 845.88\n",
            "iteration 5484, epoch 11, batch 474/481,disc_loss 79.265, (real 82.054, fake 76.477 ) gen_loss 780.18\n",
            "iteration 5485, epoch 11, batch 475/481,disc_loss 76.94, (real 80.162, fake 73.718 ) gen_loss 716.84\n",
            "iteration 5486, epoch 11, batch 476/481,disc_loss 77.68, (real 80.19, fake 75.169 ) gen_loss 842.24\n",
            "iteration 5487, epoch 11, batch 477/481,disc_loss 77.284, (real 80.715, fake 73.854 ) gen_loss 736.68\n",
            "iteration 5488, epoch 11, batch 478/481,disc_loss 79.726, (real 82.378, fake 77.075 ) gen_loss 867.1\n",
            "iteration 5489, epoch 11, batch 479/481,disc_loss 76.505, (real 80.474, fake 72.535 ) gen_loss 897.22\n",
            "iteration 5490, epoch 11, batch 480/481,disc_loss 80.39, (real 83.823, fake 76.956 ) gen_loss 897.71\n",
            "iteration 5491, epoch 11, batch 481/481,disc_loss 77.091, (real 79.888, fake 74.293 ) gen_loss 866.38\n",
            "iteration 5492, epoch 12, batch 1/481,disc_loss 82.658, (real 85.083, fake 80.232 ) gen_loss 854.15\n",
            "iteration 5493, epoch 12, batch 2/481,disc_loss 77.533, (real 79.223, fake 75.843 ) gen_loss 818.98\n",
            "iteration 5494, epoch 12, batch 3/481,disc_loss 77.961, (real 80.178, fake 75.745 ) gen_loss 782.34\n",
            "iteration 5495, epoch 12, batch 4/481,disc_loss 77.037, (real 79.253, fake 74.821 ) gen_loss 869.56\n",
            "iteration 5496, epoch 12, batch 5/481,disc_loss 71.974, (real 74.694, fake 69.254 ) gen_loss 839.26\n",
            "iteration 5497, epoch 12, batch 6/481,disc_loss 78.09, (real 79.969, fake 76.211 ) gen_loss 813.88\n",
            "iteration 5498, epoch 12, batch 7/481,disc_loss 78.83, (real 81.465, fake 76.194 ) gen_loss 786.5\n",
            "iteration 5499, epoch 12, batch 8/481,disc_loss 77.358, (real 79.782, fake 74.934 ) gen_loss 759.43\n",
            "iteration 5500, epoch 12, batch 9/481,disc_loss 76.115, (real 79.087, fake 73.143 ) gen_loss 755.35\n",
            "iteration 5501, epoch 12, batch 10/481,disc_loss 76.087, (real 79.063, fake 73.111 ) gen_loss 726.15\n",
            "iteration 5502, epoch 12, batch 11/481,disc_loss 76.206, (real 78.581, fake 73.831 ) gen_loss 836.12\n",
            "iteration 5503, epoch 12, batch 12/481,disc_loss 77.977, (real 80.614, fake 75.34 ) gen_loss 884.09\n",
            "iteration 5504, epoch 12, batch 13/481,disc_loss 75.537, (real 77.949, fake 73.125 ) gen_loss 818.9\n",
            "iteration 5505, epoch 12, batch 14/481,disc_loss 76.447, (real 78.261, fake 74.633 ) gen_loss 855.62\n",
            "iteration 5506, epoch 12, batch 15/481,disc_loss 80.565, (real 83.563, fake 77.568 ) gen_loss 815.91\n",
            "iteration 5507, epoch 12, batch 16/481,disc_loss 75.726, (real 77.766, fake 73.686 ) gen_loss 788.49\n",
            "iteration 5508, epoch 12, batch 17/481,disc_loss 78.943, (real 81.259, fake 76.626 ) gen_loss 834.42\n",
            "iteration 5509, epoch 12, batch 18/481,disc_loss 81.3, (real 84.525, fake 78.076 ) gen_loss 779.15\n",
            "iteration 5510, epoch 12, batch 19/481,disc_loss 79.309, (real 82.746, fake 75.871 ) gen_loss 863.59\n",
            "iteration 5511, epoch 12, batch 20/481,disc_loss 77.408, (real 79.934, fake 74.882 ) gen_loss 748.93\n",
            "iteration 5512, epoch 12, batch 21/481,disc_loss 77.978, (real 80.456, fake 75.501 ) gen_loss 780.88\n",
            "iteration 5513, epoch 12, batch 22/481,disc_loss 77.604, (real 80.384, fake 74.824 ) gen_loss 785.68\n",
            "iteration 5514, epoch 12, batch 23/481,disc_loss 77.114, (real 79.547, fake 74.681 ) gen_loss 782.98\n",
            "iteration 5515, epoch 12, batch 24/481,disc_loss 77.454, (real 80.862, fake 74.045 ) gen_loss 858.14\n",
            "iteration 5516, epoch 12, batch 25/481,disc_loss 79.576, (real 81.787, fake 77.365 ) gen_loss 837.76\n",
            "iteration 5517, epoch 12, batch 26/481,disc_loss 72.137, (real 74.952, fake 69.322 ) gen_loss 945.32\n",
            "iteration 5518, epoch 12, batch 27/481,disc_loss 77.818, (real 80.035, fake 75.602 ) gen_loss 815.03\n",
            "iteration 5519, epoch 12, batch 28/481,disc_loss 78.334, (real 80.588, fake 76.081 ) gen_loss 824.91\n",
            "iteration 5520, epoch 12, batch 29/481,disc_loss 76.67, (real 79.495, fake 73.844 ) gen_loss 796.74\n",
            "iteration 5521, epoch 12, batch 30/481,disc_loss 74.491, (real 76.726, fake 72.256 ) gen_loss 898.03\n",
            "iteration 5522, epoch 12, batch 31/481,disc_loss 75.163, (real 78.049, fake 72.277 ) gen_loss 784.92\n",
            "iteration 5523, epoch 12, batch 32/481,disc_loss 75.808, (real 78.137, fake 73.478 ) gen_loss 875.42\n",
            "iteration 5524, epoch 12, batch 33/481,disc_loss 79.034, (real 81.647, fake 76.421 ) gen_loss 844.35\n",
            "iteration 5525, epoch 12, batch 34/481,disc_loss 70.788, (real 73.361, fake 68.216 ) gen_loss 835.38\n",
            "iteration 5526, epoch 12, batch 35/481,disc_loss 77.632, (real 80.056, fake 75.207 ) gen_loss 801.17\n",
            "iteration 5527, epoch 12, batch 36/481,disc_loss 78.838, (real 81.073, fake 76.603 ) gen_loss 822.08\n",
            "iteration 5528, epoch 12, batch 37/481,disc_loss 77.601, (real 80.146, fake 75.056 ) gen_loss 831.77\n",
            "iteration 5529, epoch 12, batch 38/481,disc_loss 76.272, (real 78.465, fake 74.078 ) gen_loss 726.97\n",
            "iteration 5530, epoch 12, batch 39/481,disc_loss 73.823, (real 76.779, fake 70.866 ) gen_loss 827.93\n",
            "iteration 5531, epoch 12, batch 40/481,disc_loss 79.432, (real 82.078, fake 76.786 ) gen_loss 788.04\n",
            "iteration 5532, epoch 12, batch 41/481,disc_loss 80.171, (real 82.033, fake 78.308 ) gen_loss 789.96\n",
            "iteration 5533, epoch 12, batch 42/481,disc_loss 82.746, (real 84.903, fake 80.588 ) gen_loss 767.31\n",
            "iteration 5534, epoch 12, batch 43/481,disc_loss 76.697, (real 79.144, fake 74.25 ) gen_loss 802.28\n",
            "iteration 5535, epoch 12, batch 44/481,disc_loss 76.332, (real 78.374, fake 74.289 ) gen_loss 795.19\n",
            "iteration 5536, epoch 12, batch 45/481,disc_loss 75.791, (real 78.196, fake 73.386 ) gen_loss 816.47\n",
            "iteration 5537, epoch 12, batch 46/481,disc_loss 73.371, (real 75.41, fake 71.332 ) gen_loss 813.64\n",
            "iteration 5538, epoch 12, batch 47/481,disc_loss 74.082, (real 75.771, fake 72.393 ) gen_loss 823.45\n",
            "iteration 5539, epoch 12, batch 48/481,disc_loss 77.017, (real 80.151, fake 73.883 ) gen_loss 919.29\n",
            "iteration 5540, epoch 12, batch 49/481,disc_loss 76.655, (real 79.348, fake 73.961 ) gen_loss 978.0\n",
            "iteration 5541, epoch 12, batch 50/481,disc_loss 76.265, (real 79.748, fake 72.781 ) gen_loss 880.33\n",
            "iteration 5542, epoch 12, batch 51/481,disc_loss 76.163, (real 79.118, fake 73.207 ) gen_loss 935.79\n",
            "iteration 5543, epoch 12, batch 52/481,disc_loss 79.271, (real 81.821, fake 76.721 ) gen_loss 826.01\n",
            "iteration 5544, epoch 12, batch 53/481,disc_loss 77.284, (real 79.672, fake 74.896 ) gen_loss 795.51\n",
            "iteration 5545, epoch 12, batch 54/481,disc_loss 77.809, (real 79.996, fake 75.622 ) gen_loss 904.29\n",
            "iteration 5546, epoch 12, batch 55/481,disc_loss 76.918, (real 79.641, fake 74.194 ) gen_loss 763.73\n",
            "iteration 5547, epoch 12, batch 56/481,disc_loss 74.104, (real 76.365, fake 71.844 ) gen_loss 788.18\n",
            "iteration 5548, epoch 12, batch 57/481,disc_loss 74.206, (real 76.509, fake 71.904 ) gen_loss 833.95\n",
            "iteration 5549, epoch 12, batch 58/481,disc_loss 75.086, (real 78.341, fake 71.832 ) gen_loss 878.89\n",
            "iteration 5550, epoch 12, batch 59/481,disc_loss 78.354, (real 81.175, fake 75.534 ) gen_loss 861.27\n",
            "iteration 5551, epoch 12, batch 60/481,disc_loss 77.319, (real 80.485, fake 74.153 ) gen_loss 825.83\n",
            "iteration 5552, epoch 12, batch 61/481,disc_loss 81.257, (real 83.812, fake 78.701 ) gen_loss 809.01\n",
            "iteration 5553, epoch 12, batch 62/481,disc_loss 77.877, (real 81.051, fake 74.704 ) gen_loss 941.41\n",
            "iteration 5554, epoch 12, batch 63/481,disc_loss 77.058, (real 80.448, fake 73.669 ) gen_loss 941.55\n",
            "iteration 5555, epoch 12, batch 64/481,disc_loss 75.636, (real 78.447, fake 72.825 ) gen_loss 966.14\n",
            "iteration 5556, epoch 12, batch 65/481,disc_loss 79.053, (real 81.473, fake 76.633 ) gen_loss 834.39\n",
            "iteration 5557, epoch 12, batch 66/481,disc_loss 76.146, (real 78.229, fake 74.063 ) gen_loss 862.56\n",
            "iteration 5558, epoch 12, batch 67/481,disc_loss 76.705, (real 79.485, fake 73.925 ) gen_loss 893.03\n",
            "iteration 5559, epoch 12, batch 68/481,disc_loss 78.808, (real 81.07, fake 76.546 ) gen_loss 855.44\n",
            "iteration 5560, epoch 12, batch 69/481,disc_loss 76.627, (real 79.52, fake 73.735 ) gen_loss 883.71\n",
            "iteration 5561, epoch 12, batch 70/481,disc_loss 79.338, (real 82.189, fake 76.486 ) gen_loss 886.97\n",
            "iteration 5562, epoch 12, batch 71/481,disc_loss 73.323, (real 75.832, fake 70.814 ) gen_loss 752.98\n",
            "iteration 5563, epoch 12, batch 72/481,disc_loss 76.787, (real 78.6, fake 74.975 ) gen_loss 780.36\n",
            "iteration 5564, epoch 12, batch 73/481,disc_loss 75.009, (real 76.856, fake 73.161 ) gen_loss 839.35\n",
            "iteration 5565, epoch 12, batch 74/481,disc_loss 79.387, (real 81.688, fake 77.086 ) gen_loss 879.39\n",
            "iteration 5566, epoch 12, batch 75/481,disc_loss 79.115, (real 81.741, fake 76.49 ) gen_loss 806.71\n",
            "iteration 5567, epoch 12, batch 76/481,disc_loss 75.877, (real 80.268, fake 71.486 ) gen_loss 762.9\n",
            "iteration 5568, epoch 12, batch 77/481,disc_loss 76.835, (real 80.005, fake 73.664 ) gen_loss 846.85\n",
            "iteration 5569, epoch 12, batch 78/481,disc_loss 79.677, (real 81.717, fake 77.638 ) gen_loss 878.9\n",
            "iteration 5570, epoch 12, batch 79/481,disc_loss 81.662, (real 83.923, fake 79.401 ) gen_loss 819.43\n",
            "iteration 5571, epoch 12, batch 80/481,disc_loss 69.405, (real 71.909, fake 66.902 ) gen_loss 854.91\n",
            "iteration 5572, epoch 12, batch 81/481,disc_loss 75.232, (real 77.258, fake 73.206 ) gen_loss 880.9\n",
            "iteration 5573, epoch 12, batch 82/481,disc_loss 76.519, (real 79.11, fake 73.929 ) gen_loss 861.62\n",
            "iteration 5574, epoch 12, batch 83/481,disc_loss 75.893, (real 78.052, fake 73.734 ) gen_loss 790.27\n",
            "iteration 5575, epoch 12, batch 84/481,disc_loss 76.865, (real 79.313, fake 74.417 ) gen_loss 751.15\n",
            "iteration 5576, epoch 12, batch 85/481,disc_loss 80.066, (real 82.868, fake 77.264 ) gen_loss 791.01\n",
            "iteration 5577, epoch 12, batch 86/481,disc_loss 77.916, (real 80.465, fake 75.367 ) gen_loss 846.16\n",
            "iteration 5578, epoch 12, batch 87/481,disc_loss 80.734, (real 83.929, fake 77.54 ) gen_loss 854.83\n",
            "iteration 5579, epoch 12, batch 88/481,disc_loss 75.246, (real 78.083, fake 72.409 ) gen_loss 869.64\n",
            "iteration 5580, epoch 12, batch 89/481,disc_loss 73.702, (real 77.77, fake 69.634 ) gen_loss 848.79\n",
            "iteration 5581, epoch 12, batch 90/481,disc_loss 76.797, (real 78.829, fake 74.765 ) gen_loss 890.05\n",
            "iteration 5582, epoch 12, batch 91/481,disc_loss 79.542, (real 81.782, fake 77.301 ) gen_loss 856.9\n",
            "iteration 5583, epoch 12, batch 92/481,disc_loss 79.991, (real 82.103, fake 77.879 ) gen_loss 821.34\n",
            "iteration 5584, epoch 12, batch 93/481,disc_loss 80.966, (real 83.499, fake 78.433 ) gen_loss 951.08\n",
            "iteration 5585, epoch 12, batch 94/481,disc_loss 75.066, (real 77.267, fake 72.865 ) gen_loss 901.06\n",
            "iteration 5586, epoch 12, batch 95/481,disc_loss 72.292, (real 75.157, fake 69.426 ) gen_loss 883.34\n",
            "iteration 5587, epoch 12, batch 96/481,disc_loss 75.482, (real 77.913, fake 73.05 ) gen_loss 939.91\n",
            "iteration 5588, epoch 12, batch 97/481,disc_loss 75.606, (real 77.312, fake 73.899 ) gen_loss 895.98\n",
            "iteration 5589, epoch 12, batch 98/481,disc_loss 74.346, (real 77.297, fake 71.395 ) gen_loss 808.58\n",
            "iteration 5590, epoch 12, batch 99/481,disc_loss 80.254, (real 82.873, fake 77.635 ) gen_loss 875.29\n",
            "iteration 5591, epoch 12, batch 100/481,disc_loss 79.214, (real 81.693, fake 76.735 ) gen_loss 812.87\n",
            "iteration 5592, epoch 12, batch 101/481,disc_loss 74.974, (real 77.016, fake 72.933 ) gen_loss 872.78\n",
            "iteration 5593, epoch 12, batch 102/481,disc_loss 78.196, (real 79.644, fake 76.748 ) gen_loss 884.54\n",
            "iteration 5594, epoch 12, batch 103/481,disc_loss 76.183, (real 78.427, fake 73.939 ) gen_loss 829.21\n",
            "iteration 5595, epoch 12, batch 104/481,disc_loss 83.563, (real 86.629, fake 80.497 ) gen_loss 809.97\n",
            "iteration 5596, epoch 12, batch 105/481,disc_loss 70.429, (real 73.223, fake 67.636 ) gen_loss 839.12\n",
            "iteration 5597, epoch 12, batch 106/481,disc_loss 75.303, (real 77.365, fake 73.241 ) gen_loss 1019.8\n",
            "iteration 5598, epoch 12, batch 107/481,disc_loss 78.599, (real 81.15, fake 76.049 ) gen_loss 817.1\n",
            "iteration 5599, epoch 12, batch 108/481,disc_loss 73.578, (real 76.123, fake 71.033 ) gen_loss 826.18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 5600, epoch 12, batch 109/481,disc_loss 79.385, (real 81.752, fake 77.017 ) gen_loss 746.9\n",
            "iteration 5601, epoch 12, batch 110/481,disc_loss 75.917, (real 78.936, fake 72.898 ) gen_loss 787.59\n",
            "iteration 5602, epoch 12, batch 111/481,disc_loss 72.608, (real 75.397, fake 69.819 ) gen_loss 819.32\n",
            "iteration 5603, epoch 12, batch 112/481,disc_loss 77.511, (real 80.451, fake 74.571 ) gen_loss 812.09\n",
            "iteration 5604, epoch 12, batch 113/481,disc_loss 73.815, (real 75.388, fake 72.242 ) gen_loss 776.99\n",
            "iteration 5605, epoch 12, batch 114/481,disc_loss 74.452, (real 76.88, fake 72.024 ) gen_loss 742.69\n",
            "iteration 5606, epoch 12, batch 115/481,disc_loss 71.944, (real 74.587, fake 69.301 ) gen_loss 810.47\n",
            "iteration 5607, epoch 12, batch 116/481,disc_loss 78.565, (real 81.256, fake 75.874 ) gen_loss 878.34\n",
            "iteration 5608, epoch 12, batch 117/481,disc_loss 81.832, (real 84.853, fake 78.811 ) gen_loss 921.6\n",
            "iteration 5609, epoch 12, batch 118/481,disc_loss 74.644, (real 77.125, fake 72.163 ) gen_loss 1018.5\n",
            "iteration 5610, epoch 12, batch 119/481,disc_loss 77.011, (real 79.735, fake 74.288 ) gen_loss 936.46\n",
            "iteration 5611, epoch 12, batch 120/481,disc_loss 77.387, (real 79.405, fake 75.368 ) gen_loss 920.59\n",
            "iteration 5612, epoch 12, batch 121/481,disc_loss 78.503, (real 81.815, fake 75.192 ) gen_loss 844.15\n",
            "iteration 5613, epoch 12, batch 122/481,disc_loss 76.725, (real 78.963, fake 74.487 ) gen_loss 862.0\n",
            "iteration 5614, epoch 12, batch 123/481,disc_loss 78.945, (real 81.746, fake 76.145 ) gen_loss 956.75\n",
            "iteration 5615, epoch 12, batch 124/481,disc_loss 80.939, (real 84.719, fake 77.159 ) gen_loss 816.03\n",
            "iteration 5616, epoch 12, batch 125/481,disc_loss 81.973, (real 84.362, fake 79.585 ) gen_loss 735.2\n",
            "iteration 5617, epoch 12, batch 126/481,disc_loss 76.586, (real 79.984, fake 73.188 ) gen_loss 795.33\n",
            "iteration 5618, epoch 12, batch 127/481,disc_loss 82.668, (real 85.106, fake 80.229 ) gen_loss 808.9\n",
            "iteration 5619, epoch 12, batch 128/481,disc_loss 79.067, (real 81.757, fake 76.378 ) gen_loss 896.03\n",
            "iteration 5620, epoch 12, batch 129/481,disc_loss 79.279, (real 81.789, fake 76.77 ) gen_loss 853.48\n",
            "iteration 5621, epoch 12, batch 130/481,disc_loss 78.578, (real 81.344, fake 75.812 ) gen_loss 875.24\n",
            "iteration 5622, epoch 12, batch 131/481,disc_loss 79.479, (real 81.659, fake 77.3 ) gen_loss 855.51\n",
            "iteration 5623, epoch 12, batch 132/481,disc_loss 78.461, (real 80.662, fake 76.259 ) gen_loss 833.18\n",
            "iteration 5624, epoch 12, batch 133/481,disc_loss 74.726, (real 77.126, fake 72.326 ) gen_loss 770.66\n",
            "iteration 5625, epoch 12, batch 134/481,disc_loss 80.619, (real 83.26, fake 77.978 ) gen_loss 941.92\n",
            "iteration 5626, epoch 12, batch 135/481,disc_loss 72.578, (real 74.661, fake 70.495 ) gen_loss 832.33\n",
            "iteration 5627, epoch 12, batch 136/481,disc_loss 74.774, (real 77.675, fake 71.873 ) gen_loss 795.41\n",
            "iteration 5628, epoch 12, batch 137/481,disc_loss 76.951, (real 78.982, fake 74.92 ) gen_loss 850.1\n",
            "iteration 5629, epoch 12, batch 138/481,disc_loss 75.186, (real 77.611, fake 72.76 ) gen_loss 806.7\n",
            "iteration 5630, epoch 12, batch 139/481,disc_loss 78.235, (real 80.92, fake 75.551 ) gen_loss 802.81\n",
            "iteration 5631, epoch 12, batch 140/481,disc_loss 75.96, (real 78.756, fake 73.164 ) gen_loss 715.94\n",
            "iteration 5632, epoch 12, batch 141/481,disc_loss 79.703, (real 81.473, fake 77.934 ) gen_loss 893.94\n",
            "iteration 5633, epoch 12, batch 142/481,disc_loss 78.081, (real 81.868, fake 74.294 ) gen_loss 782.55\n",
            "iteration 5634, epoch 12, batch 143/481,disc_loss 74.922, (real 77.46, fake 72.384 ) gen_loss 850.35\n",
            "iteration 5635, epoch 12, batch 144/481,disc_loss 78.521, (real 80.883, fake 76.16 ) gen_loss 850.69\n",
            "iteration 5636, epoch 12, batch 145/481,disc_loss 79.399, (real 82.552, fake 76.247 ) gen_loss 931.74\n",
            "iteration 5637, epoch 12, batch 146/481,disc_loss 74.482, (real 76.642, fake 72.323 ) gen_loss 828.34\n",
            "iteration 5638, epoch 12, batch 147/481,disc_loss 79.926, (real 83.401, fake 76.451 ) gen_loss 853.12\n",
            "iteration 5639, epoch 12, batch 148/481,disc_loss 78.445, (real 81.244, fake 75.646 ) gen_loss 891.02\n",
            "iteration 5640, epoch 12, batch 149/481,disc_loss 76.911, (real 79.231, fake 74.591 ) gen_loss 790.08\n",
            "iteration 5641, epoch 12, batch 150/481,disc_loss 74.568, (real 76.453, fake 72.683 ) gen_loss 776.3\n",
            "iteration 5642, epoch 12, batch 151/481,disc_loss 73.067, (real 75.361, fake 70.772 ) gen_loss 854.99\n",
            "iteration 5643, epoch 12, batch 152/481,disc_loss 77.444, (real 80.97, fake 73.918 ) gen_loss 895.21\n",
            "iteration 5644, epoch 12, batch 153/481,disc_loss 78.452, (real 81.784, fake 75.121 ) gen_loss 828.52\n",
            "iteration 5645, epoch 12, batch 154/481,disc_loss 76.207, (real 78.302, fake 74.112 ) gen_loss 896.12\n",
            "iteration 5646, epoch 12, batch 155/481,disc_loss 76.955, (real 79.617, fake 74.293 ) gen_loss 818.77\n",
            "iteration 5647, epoch 12, batch 156/481,disc_loss 75.208, (real 78.191, fake 72.225 ) gen_loss 847.42\n",
            "iteration 5648, epoch 12, batch 157/481,disc_loss 79.963, (real 84.335, fake 75.591 ) gen_loss 890.83\n",
            "iteration 5649, epoch 12, batch 158/481,disc_loss 80.852, (real 83.684, fake 78.02 ) gen_loss 732.98\n",
            "iteration 5650, epoch 12, batch 159/481,disc_loss 80.114, (real 82.807, fake 77.422 ) gen_loss 774.89\n",
            "iteration 5651, epoch 12, batch 160/481,disc_loss 79.063, (real 81.858, fake 76.268 ) gen_loss 765.71\n",
            "iteration 5652, epoch 12, batch 161/481,disc_loss 78.674, (real 82.966, fake 74.383 ) gen_loss 899.39\n",
            "iteration 5653, epoch 12, batch 162/481,disc_loss 75.227, (real 77.761, fake 72.694 ) gen_loss 850.49\n",
            "iteration 5654, epoch 12, batch 163/481,disc_loss 79.355, (real 82.482, fake 76.228 ) gen_loss 857.28\n",
            "iteration 5655, epoch 12, batch 164/481,disc_loss 78.918, (real 81.225, fake 76.611 ) gen_loss 875.16\n",
            "iteration 5656, epoch 12, batch 165/481,disc_loss 78.938, (real 81.845, fake 76.031 ) gen_loss 890.36\n",
            "iteration 5657, epoch 12, batch 166/481,disc_loss 78.202, (real 81.137, fake 75.266 ) gen_loss 868.1\n",
            "iteration 5658, epoch 12, batch 167/481,disc_loss 77.455, (real 79.928, fake 74.981 ) gen_loss 821.43\n",
            "iteration 5659, epoch 12, batch 168/481,disc_loss 81.985, (real 85.321, fake 78.649 ) gen_loss 775.86\n",
            "iteration 5660, epoch 12, batch 169/481,disc_loss 76.847, (real 79.367, fake 74.327 ) gen_loss 831.05\n",
            "iteration 5661, epoch 12, batch 170/481,disc_loss 78.25, (real 80.099, fake 76.402 ) gen_loss 758.44\n",
            "iteration 5662, epoch 12, batch 171/481,disc_loss 76.477, (real 78.768, fake 74.185 ) gen_loss 848.77\n",
            "iteration 5663, epoch 12, batch 172/481,disc_loss 76.732, (real 79.069, fake 74.395 ) gen_loss 993.44\n",
            "iteration 5664, epoch 12, batch 173/481,disc_loss 78.189, (real 80.702, fake 75.676 ) gen_loss 838.4\n",
            "iteration 5665, epoch 12, batch 174/481,disc_loss 74.885, (real 77.745, fake 72.025 ) gen_loss 855.74\n",
            "iteration 5666, epoch 12, batch 175/481,disc_loss 76.949, (real 79.192, fake 74.707 ) gen_loss 1066.8\n",
            "iteration 5667, epoch 12, batch 176/481,disc_loss 77.856, (real 80.815, fake 74.897 ) gen_loss 828.47\n",
            "iteration 5668, epoch 12, batch 177/481,disc_loss 80.107, (real 82.861, fake 77.352 ) gen_loss 896.61\n",
            "iteration 5669, epoch 12, batch 178/481,disc_loss 76.406, (real 78.905, fake 73.907 ) gen_loss 875.56\n",
            "iteration 5670, epoch 12, batch 179/481,disc_loss 84.136, (real 87.606, fake 80.666 ) gen_loss 907.05\n",
            "iteration 5671, epoch 12, batch 180/481,disc_loss 76.311, (real 78.287, fake 74.336 ) gen_loss 852.18\n",
            "iteration 5672, epoch 12, batch 181/481,disc_loss 80.185, (real 82.452, fake 77.918 ) gen_loss 768.1\n",
            "iteration 5673, epoch 12, batch 182/481,disc_loss 77.038, (real 79.887, fake 74.189 ) gen_loss 900.04\n",
            "iteration 5674, epoch 12, batch 183/481,disc_loss 77.985, (real 80.871, fake 75.099 ) gen_loss 741.72\n",
            "iteration 5675, epoch 12, batch 184/481,disc_loss 76.777, (real 79.477, fake 74.076 ) gen_loss 806.84\n",
            "iteration 5676, epoch 12, batch 185/481,disc_loss 80.084, (real 84.089, fake 76.078 ) gen_loss 830.72\n",
            "iteration 5677, epoch 12, batch 186/481,disc_loss 78.238, (real 81.349, fake 75.127 ) gen_loss 770.29\n",
            "iteration 5678, epoch 12, batch 187/481,disc_loss 77.266, (real 79.314, fake 75.218 ) gen_loss 862.9\n",
            "iteration 5679, epoch 12, batch 188/481,disc_loss 78.434, (real 81.055, fake 75.813 ) gen_loss 810.4\n",
            "iteration 5680, epoch 12, batch 189/481,disc_loss 77.854, (real 80.539, fake 75.17 ) gen_loss 850.94\n",
            "iteration 5681, epoch 12, batch 190/481,disc_loss 79.028, (real 82.202, fake 75.853 ) gen_loss 875.57\n",
            "iteration 5682, epoch 12, batch 191/481,disc_loss 77.428, (real 79.864, fake 74.991 ) gen_loss 776.08\n",
            "iteration 5683, epoch 12, batch 192/481,disc_loss 78.307, (real 81.459, fake 75.154 ) gen_loss 881.11\n",
            "iteration 5684, epoch 12, batch 193/481,disc_loss 79.006, (real 80.944, fake 77.068 ) gen_loss 846.16\n",
            "iteration 5685, epoch 12, batch 194/481,disc_loss 81.815, (real 84.643, fake 78.987 ) gen_loss 844.02\n",
            "iteration 5686, epoch 12, batch 195/481,disc_loss 79.951, (real 83.316, fake 76.586 ) gen_loss 807.34\n",
            "iteration 5687, epoch 12, batch 196/481,disc_loss 74.643, (real 76.691, fake 72.594 ) gen_loss 763.76\n",
            "iteration 5688, epoch 12, batch 197/481,disc_loss 74.375, (real 76.894, fake 71.855 ) gen_loss 849.55\n",
            "iteration 5689, epoch 12, batch 198/481,disc_loss 82.219, (real 84.843, fake 79.595 ) gen_loss 862.14\n",
            "iteration 5690, epoch 12, batch 199/481,disc_loss 76.862, (real 79.885, fake 73.839 ) gen_loss 798.79\n",
            "iteration 5691, epoch 12, batch 200/481,disc_loss 77.307, (real 80.008, fake 74.606 ) gen_loss 834.55\n",
            "iteration 5692, epoch 12, batch 201/481,disc_loss 74.472, (real 77.321, fake 71.623 ) gen_loss 926.89\n",
            "iteration 5693, epoch 12, batch 202/481,disc_loss 75.183, (real 77.474, fake 72.891 ) gen_loss 819.09\n",
            "iteration 5694, epoch 12, batch 203/481,disc_loss 80.37, (real 83.682, fake 77.058 ) gen_loss 891.74\n",
            "iteration 5695, epoch 12, batch 204/481,disc_loss 78.037, (real 80.182, fake 75.892 ) gen_loss 866.45\n",
            "iteration 5696, epoch 12, batch 205/481,disc_loss 80.145, (real 82.97, fake 77.32 ) gen_loss 843.0\n",
            "iteration 5697, epoch 12, batch 206/481,disc_loss 75.908, (real 78.557, fake 73.258 ) gen_loss 853.64\n",
            "iteration 5698, epoch 12, batch 207/481,disc_loss 74.588, (real 77.208, fake 71.969 ) gen_loss 886.45\n",
            "iteration 5699, epoch 12, batch 208/481,disc_loss 78.823, (real 81.267, fake 76.378 ) gen_loss 859.27\n",
            "iteration 5700, epoch 12, batch 209/481,disc_loss 79.783, (real 82.497, fake 77.069 ) gen_loss 856.75\n",
            "iteration 5701, epoch 12, batch 210/481,disc_loss 78.608, (real 82.731, fake 74.485 ) gen_loss 870.63\n",
            "iteration 5702, epoch 12, batch 211/481,disc_loss 76.396, (real 79.806, fake 72.986 ) gen_loss 760.96\n",
            "iteration 5703, epoch 12, batch 212/481,disc_loss 80.836, (real 83.662, fake 78.009 ) gen_loss 874.92\n",
            "iteration 5704, epoch 12, batch 213/481,disc_loss 74.325, (real 76.019, fake 72.63 ) gen_loss 851.69\n",
            "iteration 5705, epoch 12, batch 214/481,disc_loss 75.36, (real 78.809, fake 71.911 ) gen_loss 896.87\n",
            "iteration 5706, epoch 12, batch 215/481,disc_loss 79.201, (real 81.981, fake 76.42 ) gen_loss 919.86\n",
            "iteration 5707, epoch 12, batch 216/481,disc_loss 77.088, (real 79.733, fake 74.443 ) gen_loss 934.91\n",
            "iteration 5708, epoch 12, batch 217/481,disc_loss 80.075, (real 83.23, fake 76.92 ) gen_loss 800.07\n",
            "iteration 5709, epoch 12, batch 218/481,disc_loss 78.006, (real 80.619, fake 75.393 ) gen_loss 832.36\n",
            "iteration 5710, epoch 12, batch 219/481,disc_loss 78.782, (real 81.361, fake 76.203 ) gen_loss 783.64\n",
            "iteration 5711, epoch 12, batch 220/481,disc_loss 79.723, (real 82.841, fake 76.606 ) gen_loss 794.86\n",
            "iteration 5712, epoch 12, batch 221/481,disc_loss 77.747, (real 81.649, fake 73.845 ) gen_loss 832.71\n",
            "iteration 5713, epoch 12, batch 222/481,disc_loss 76.606, (real 79.324, fake 73.889 ) gen_loss 892.58\n",
            "iteration 5714, epoch 12, batch 223/481,disc_loss 78.063, (real 81.654, fake 74.471 ) gen_loss 895.41\n",
            "iteration 5715, epoch 12, batch 224/481,disc_loss 78.99, (real 82.064, fake 75.917 ) gen_loss 884.81\n",
            "iteration 5716, epoch 12, batch 225/481,disc_loss 82.129, (real 84.451, fake 79.807 ) gen_loss 814.12\n",
            "iteration 5717, epoch 12, batch 226/481,disc_loss 74.737, (real 77.893, fake 71.582 ) gen_loss 809.82\n",
            "iteration 5718, epoch 12, batch 227/481,disc_loss 77.051, (real 79.313, fake 74.789 ) gen_loss 866.46\n",
            "iteration 5719, epoch 12, batch 228/481,disc_loss 78.823, (real 82.006, fake 75.64 ) gen_loss 917.57\n",
            "iteration 5720, epoch 12, batch 229/481,disc_loss 74.144, (real 76.667, fake 71.621 ) gen_loss 897.6\n",
            "iteration 5721, epoch 12, batch 230/481,disc_loss 78.19, (real 80.761, fake 75.62 ) gen_loss 754.69\n",
            "iteration 5722, epoch 12, batch 231/481,disc_loss 73.842, (real 77.063, fake 70.621 ) gen_loss 801.26\n",
            "iteration 5723, epoch 12, batch 232/481,disc_loss 81.799, (real 84.116, fake 79.482 ) gen_loss 877.22\n",
            "iteration 5724, epoch 12, batch 233/481,disc_loss 83.742, (real 86.613, fake 80.87 ) gen_loss 863.83\n",
            "iteration 5725, epoch 12, batch 234/481,disc_loss 78.521, (real 82.045, fake 74.998 ) gen_loss 820.73\n",
            "iteration 5726, epoch 12, batch 235/481,disc_loss 75.165, (real 77.504, fake 72.825 ) gen_loss 907.69\n",
            "iteration 5727, epoch 12, batch 236/481,disc_loss 82.732, (real 85.02, fake 80.444 ) gen_loss 824.62\n",
            "iteration 5728, epoch 12, batch 237/481,disc_loss 77.739, (real 78.917, fake 76.561 ) gen_loss 713.18\n",
            "iteration 5729, epoch 12, batch 238/481,disc_loss 80.828, (real 83.133, fake 78.523 ) gen_loss 761.94\n",
            "iteration 5730, epoch 12, batch 239/481,disc_loss 76.917, (real 79.501, fake 74.334 ) gen_loss 819.38\n",
            "iteration 5731, epoch 12, batch 240/481,disc_loss 79.883, (real 82.799, fake 76.968 ) gen_loss 849.14\n",
            "iteration 5732, epoch 12, batch 241/481,disc_loss 78.198, (real 81.989, fake 74.407 ) gen_loss 796.56\n",
            "iteration 5733, epoch 12, batch 242/481,disc_loss 78.767, (real 81.868, fake 75.666 ) gen_loss 809.3\n",
            "iteration 5734, epoch 12, batch 243/481,disc_loss 75.437, (real 77.579, fake 73.294 ) gen_loss 805.18\n",
            "iteration 5735, epoch 12, batch 244/481,disc_loss 82.473, (real 85.047, fake 79.9 ) gen_loss 855.4\n",
            "iteration 5736, epoch 12, batch 245/481,disc_loss 72.105, (real 75.333, fake 68.877 ) gen_loss 990.79\n",
            "iteration 5737, epoch 12, batch 246/481,disc_loss 79.159, (real 81.992, fake 76.326 ) gen_loss 826.31\n",
            "iteration 5738, epoch 12, batch 247/481,disc_loss 76.848, (real 80.178, fake 73.518 ) gen_loss 791.27\n",
            "iteration 5739, epoch 12, batch 248/481,disc_loss 79.374, (real 83.572, fake 75.177 ) gen_loss 847.11\n",
            "iteration 5740, epoch 12, batch 249/481,disc_loss 78.634, (real 80.973, fake 76.295 ) gen_loss 730.84\n",
            "iteration 5741, epoch 12, batch 250/481,disc_loss 77.674, (real 80.995, fake 74.353 ) gen_loss 829.59\n",
            "iteration 5742, epoch 12, batch 251/481,disc_loss 78.039, (real 81.246, fake 74.833 ) gen_loss 792.39\n",
            "iteration 5743, epoch 12, batch 252/481,disc_loss 72.949, (real 76.318, fake 69.58 ) gen_loss 893.38\n",
            "iteration 5744, epoch 12, batch 253/481,disc_loss 78.43, (real 82.068, fake 74.792 ) gen_loss 841.61\n",
            "iteration 5745, epoch 12, batch 254/481,disc_loss 79.373, (real 81.517, fake 77.228 ) gen_loss 849.41\n",
            "iteration 5746, epoch 12, batch 255/481,disc_loss 75.118, (real 78.11, fake 72.127 ) gen_loss 761.96\n",
            "iteration 5747, epoch 12, batch 256/481,disc_loss 76.628, (real 79.16, fake 74.096 ) gen_loss 764.49\n",
            "iteration 5748, epoch 12, batch 257/481,disc_loss 79.409, (real 82.652, fake 76.166 ) gen_loss 904.47\n",
            "iteration 5749, epoch 12, batch 258/481,disc_loss 78.971, (real 81.353, fake 76.589 ) gen_loss 869.04\n",
            "iteration 5750, epoch 12, batch 259/481,disc_loss 75.889, (real 78.272, fake 73.505 ) gen_loss 953.08\n",
            "iteration 5751, epoch 12, batch 260/481,disc_loss 73.224, (real 75.993, fake 70.454 ) gen_loss 838.37\n",
            "iteration 5752, epoch 12, batch 261/481,disc_loss 80.188, (real 82.89, fake 77.486 ) gen_loss 778.2\n",
            "iteration 5753, epoch 12, batch 262/481,disc_loss 77.286, (real 80.142, fake 74.429 ) gen_loss 785.18\n",
            "iteration 5754, epoch 12, batch 263/481,disc_loss 75.706, (real 78.669, fake 72.743 ) gen_loss 903.82\n",
            "iteration 5755, epoch 12, batch 264/481,disc_loss 80.186, (real 83.108, fake 77.264 ) gen_loss 909.24\n",
            "iteration 5756, epoch 12, batch 265/481,disc_loss 80.391, (real 82.862, fake 77.921 ) gen_loss 780.96\n",
            "iteration 5757, epoch 12, batch 266/481,disc_loss 78.092, (real 80.069, fake 76.114 ) gen_loss 941.99\n",
            "iteration 5758, epoch 12, batch 267/481,disc_loss 76.114, (real 78.585, fake 73.643 ) gen_loss 809.6\n",
            "iteration 5759, epoch 12, batch 268/481,disc_loss 72.543, (real 75.44, fake 69.646 ) gen_loss 972.34\n",
            "iteration 5760, epoch 12, batch 269/481,disc_loss 73.908, (real 76.655, fake 71.16 ) gen_loss 885.37\n",
            "iteration 5761, epoch 12, batch 270/481,disc_loss 71.729, (real 74.155, fake 69.303 ) gen_loss 971.93\n",
            "iteration 5762, epoch 12, batch 271/481,disc_loss 75.184, (real 77.545, fake 72.824 ) gen_loss 921.53\n",
            "iteration 5763, epoch 12, batch 272/481,disc_loss 77.527, (real 80.967, fake 74.088 ) gen_loss 911.84\n",
            "iteration 5764, epoch 12, batch 273/481,disc_loss 77.41, (real 79.825, fake 74.996 ) gen_loss 860.49\n",
            "iteration 5765, epoch 12, batch 274/481,disc_loss 80.141, (real 83.441, fake 76.841 ) gen_loss 806.93\n",
            "iteration 5766, epoch 12, batch 275/481,disc_loss 79.618, (real 82.963, fake 76.273 ) gen_loss 809.99\n",
            "iteration 5767, epoch 12, batch 276/481,disc_loss 76.289, (real 79.147, fake 73.43 ) gen_loss 943.57\n",
            "iteration 5768, epoch 12, batch 277/481,disc_loss 76.84, (real 79.684, fake 73.995 ) gen_loss 861.85\n",
            "iteration 5769, epoch 12, batch 278/481,disc_loss 78.475, (real 80.717, fake 76.233 ) gen_loss 897.3\n",
            "iteration 5770, epoch 12, batch 279/481,disc_loss 75.427, (real 78.02, fake 72.833 ) gen_loss 858.69\n",
            "iteration 5771, epoch 12, batch 280/481,disc_loss 79.884, (real 82.715, fake 77.052 ) gen_loss 781.22\n",
            "iteration 5772, epoch 12, batch 281/481,disc_loss 77.802, (real 82.11, fake 73.495 ) gen_loss 915.01\n",
            "iteration 5773, epoch 12, batch 282/481,disc_loss 75.6, (real 77.903, fake 73.298 ) gen_loss 839.77\n",
            "iteration 5774, epoch 12, batch 283/481,disc_loss 76.917, (real 79.728, fake 74.106 ) gen_loss 896.78\n",
            "iteration 5775, epoch 12, batch 284/481,disc_loss 79.698, (real 82.638, fake 76.758 ) gen_loss 953.02\n",
            "iteration 5776, epoch 12, batch 285/481,disc_loss 80.385, (real 83.315, fake 77.454 ) gen_loss 813.16\n",
            "iteration 5777, epoch 12, batch 286/481,disc_loss 81.308, (real 85.113, fake 77.503 ) gen_loss 871.52\n",
            "iteration 5778, epoch 12, batch 287/481,disc_loss 78.255, (real 80.722, fake 75.789 ) gen_loss 934.3\n",
            "iteration 5779, epoch 12, batch 288/481,disc_loss 83.264, (real 86.387, fake 80.141 ) gen_loss 933.97\n",
            "iteration 5780, epoch 12, batch 289/481,disc_loss 77.626, (real 80.983, fake 74.27 ) gen_loss 893.03\n",
            "iteration 5781, epoch 12, batch 290/481,disc_loss 76.434, (real 79.517, fake 73.351 ) gen_loss 875.78\n",
            "iteration 5782, epoch 12, batch 291/481,disc_loss 77.719, (real 81.079, fake 74.359 ) gen_loss 900.45\n",
            "iteration 5783, epoch 12, batch 292/481,disc_loss 73.03, (real 75.567, fake 70.492 ) gen_loss 898.76\n",
            "iteration 5784, epoch 12, batch 293/481,disc_loss 76.536, (real 79.159, fake 73.912 ) gen_loss 832.39\n",
            "iteration 5785, epoch 12, batch 294/481,disc_loss 77.348, (real 80.435, fake 74.26 ) gen_loss 850.02\n",
            "iteration 5786, epoch 12, batch 295/481,disc_loss 79.747, (real 83.194, fake 76.3 ) gen_loss 927.32\n",
            "iteration 5787, epoch 12, batch 296/481,disc_loss 78.809, (real 81.395, fake 76.223 ) gen_loss 884.88\n",
            "iteration 5788, epoch 12, batch 297/481,disc_loss 76.965, (real 80.388, fake 73.543 ) gen_loss 891.01\n",
            "iteration 5789, epoch 12, batch 298/481,disc_loss 78.993, (real 83.151, fake 74.835 ) gen_loss 895.92\n",
            "iteration 5790, epoch 12, batch 299/481,disc_loss 76.266, (real 79.596, fake 72.937 ) gen_loss 923.69\n",
            "iteration 5791, epoch 12, batch 300/481,disc_loss 75.701, (real 78.962, fake 72.44 ) gen_loss 827.66\n",
            "iteration 5792, epoch 12, batch 301/481,disc_loss 79.513, (real 82.851, fake 76.174 ) gen_loss 826.67\n",
            "iteration 5793, epoch 12, batch 302/481,disc_loss 79.766, (real 82.623, fake 76.91 ) gen_loss 760.41\n",
            "iteration 5794, epoch 12, batch 303/481,disc_loss 74.482, (real 76.89, fake 72.075 ) gen_loss 755.75\n",
            "iteration 5795, epoch 12, batch 304/481,disc_loss 78.397, (real 81.246, fake 75.549 ) gen_loss 835.68\n",
            "iteration 5796, epoch 12, batch 305/481,disc_loss 77.396, (real 80.028, fake 74.764 ) gen_loss 813.96\n",
            "iteration 5797, epoch 12, batch 306/481,disc_loss 74.933, (real 77.36, fake 72.505 ) gen_loss 839.18\n",
            "iteration 5798, epoch 12, batch 307/481,disc_loss 73.621, (real 77.192, fake 70.05 ) gen_loss 924.73\n",
            "iteration 5799, epoch 12, batch 308/481,disc_loss 82.013, (real 84.943, fake 79.084 ) gen_loss 904.04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 5800, epoch 12, batch 309/481,disc_loss 75.494, (real 77.822, fake 73.167 ) gen_loss 766.7\n",
            "iteration 5801, epoch 12, batch 310/481,disc_loss 72.094, (real 74.579, fake 69.61 ) gen_loss 864.01\n",
            "iteration 5802, epoch 12, batch 311/481,disc_loss 76.532, (real 79.333, fake 73.731 ) gen_loss 934.76\n",
            "iteration 5803, epoch 12, batch 312/481,disc_loss 74.977, (real 77.619, fake 72.335 ) gen_loss 878.41\n",
            "iteration 5804, epoch 12, batch 313/481,disc_loss 79.247, (real 82.232, fake 76.262 ) gen_loss 969.88\n",
            "iteration 5805, epoch 12, batch 314/481,disc_loss 80.32, (real 82.76, fake 77.88 ) gen_loss 953.47\n",
            "iteration 5806, epoch 12, batch 315/481,disc_loss 78.136, (real 81.588, fake 74.684 ) gen_loss 925.41\n",
            "iteration 5807, epoch 12, batch 316/481,disc_loss 73.13, (real 75.418, fake 70.842 ) gen_loss 881.22\n",
            "iteration 5808, epoch 12, batch 317/481,disc_loss 79.512, (real 82.737, fake 76.287 ) gen_loss 912.1\n",
            "iteration 5809, epoch 12, batch 318/481,disc_loss 80.099, (real 82.311, fake 77.887 ) gen_loss 827.05\n",
            "iteration 5810, epoch 12, batch 319/481,disc_loss 78.699, (real 80.915, fake 76.483 ) gen_loss 755.38\n",
            "iteration 5811, epoch 12, batch 320/481,disc_loss 78.544, (real 81.37, fake 75.718 ) gen_loss 911.44\n",
            "iteration 5812, epoch 12, batch 321/481,disc_loss 81.839, (real 84.57, fake 79.108 ) gen_loss 988.91\n",
            "iteration 5813, epoch 12, batch 322/481,disc_loss 78.816, (real 81.059, fake 76.574 ) gen_loss 785.19\n",
            "iteration 5814, epoch 12, batch 323/481,disc_loss 77.743, (real 81.168, fake 74.318 ) gen_loss 860.16\n",
            "iteration 5815, epoch 12, batch 324/481,disc_loss 80.208, (real 82.788, fake 77.629 ) gen_loss 914.24\n",
            "iteration 5816, epoch 12, batch 325/481,disc_loss 77.525, (real 80.01, fake 75.041 ) gen_loss 894.0\n",
            "iteration 5817, epoch 12, batch 326/481,disc_loss 79.077, (real 81.794, fake 76.359 ) gen_loss 801.08\n",
            "iteration 5818, epoch 12, batch 327/481,disc_loss 75.586, (real 77.98, fake 73.193 ) gen_loss 838.51\n",
            "iteration 5819, epoch 12, batch 328/481,disc_loss 76.367, (real 78.875, fake 73.859 ) gen_loss 890.26\n",
            "iteration 5820, epoch 12, batch 329/481,disc_loss 75.842, (real 79.473, fake 72.211 ) gen_loss 899.16\n",
            "iteration 5821, epoch 12, batch 330/481,disc_loss 73.571, (real 77.777, fake 69.366 ) gen_loss 830.92\n",
            "iteration 5822, epoch 12, batch 331/481,disc_loss 80.458, (real 83.842, fake 77.073 ) gen_loss 778.17\n",
            "iteration 5823, epoch 12, batch 332/481,disc_loss 72.73, (real 75.185, fake 70.275 ) gen_loss 832.06\n",
            "iteration 5824, epoch 12, batch 333/481,disc_loss 79.554, (real 82.994, fake 76.113 ) gen_loss 858.98\n",
            "iteration 5825, epoch 12, batch 334/481,disc_loss 80.517, (real 83.554, fake 77.479 ) gen_loss 919.85\n",
            "iteration 5826, epoch 12, batch 335/481,disc_loss 76.772, (real 79.174, fake 74.371 ) gen_loss 883.94\n",
            "iteration 5827, epoch 12, batch 336/481,disc_loss 76.458, (real 79.885, fake 73.031 ) gen_loss 1045.3\n",
            "iteration 5828, epoch 12, batch 337/481,disc_loss 76.558, (real 79.694, fake 73.422 ) gen_loss 887.35\n",
            "iteration 5829, epoch 12, batch 338/481,disc_loss 80.571, (real 84.1, fake 77.042 ) gen_loss 1077.9\n",
            "iteration 5830, epoch 12, batch 339/481,disc_loss 80.903, (real 84.367, fake 77.439 ) gen_loss 1022.8\n",
            "iteration 5831, epoch 12, batch 340/481,disc_loss 77.114, (real 79.386, fake 74.843 ) gen_loss 863.59\n",
            "iteration 5832, epoch 12, batch 341/481,disc_loss 72.671, (real 74.907, fake 70.436 ) gen_loss 805.14\n",
            "iteration 5833, epoch 12, batch 342/481,disc_loss 81.846, (real 84.549, fake 79.142 ) gen_loss 830.78\n",
            "iteration 5834, epoch 12, batch 343/481,disc_loss 78.355, (real 80.827, fake 75.884 ) gen_loss 822.18\n",
            "iteration 5835, epoch 12, batch 344/481,disc_loss 77.824, (real 80.516, fake 75.132 ) gen_loss 892.11\n",
            "iteration 5836, epoch 12, batch 345/481,disc_loss 76.028, (real 78.747, fake 73.308 ) gen_loss 901.46\n",
            "iteration 5837, epoch 12, batch 346/481,disc_loss 76.389, (real 78.33, fake 74.447 ) gen_loss 851.19\n",
            "iteration 5838, epoch 12, batch 347/481,disc_loss 75.405, (real 78.364, fake 72.445 ) gen_loss 861.41\n",
            "iteration 5839, epoch 12, batch 348/481,disc_loss 76.596, (real 79.245, fake 73.946 ) gen_loss 893.55\n",
            "iteration 5840, epoch 12, batch 349/481,disc_loss 80.056, (real 82.817, fake 77.295 ) gen_loss 862.71\n",
            "iteration 5841, epoch 12, batch 350/481,disc_loss 79.247, (real 81.766, fake 76.729 ) gen_loss 788.26\n",
            "iteration 5842, epoch 12, batch 351/481,disc_loss 75.272, (real 78.133, fake 72.412 ) gen_loss 805.56\n",
            "iteration 5843, epoch 12, batch 352/481,disc_loss 77.484, (real 80.581, fake 74.388 ) gen_loss 749.04\n",
            "iteration 5844, epoch 12, batch 353/481,disc_loss 77.169, (real 80.253, fake 74.085 ) gen_loss 866.03\n",
            "iteration 5845, epoch 12, batch 354/481,disc_loss 76.0, (real 79.06, fake 72.939 ) gen_loss 797.36\n",
            "iteration 5846, epoch 12, batch 355/481,disc_loss 77.905, (real 80.398, fake 75.413 ) gen_loss 853.53\n",
            "iteration 5847, epoch 12, batch 356/481,disc_loss 76.968, (real 79.921, fake 74.014 ) gen_loss 888.3\n",
            "iteration 5848, epoch 12, batch 357/481,disc_loss 79.834, (real 81.595, fake 78.073 ) gen_loss 812.47\n",
            "iteration 5849, epoch 12, batch 358/481,disc_loss 78.85, (real 81.902, fake 75.799 ) gen_loss 890.56\n",
            "iteration 5850, epoch 12, batch 359/481,disc_loss 75.883, (real 79.202, fake 72.563 ) gen_loss 887.63\n",
            "iteration 5851, epoch 12, batch 360/481,disc_loss 80.942, (real 83.089, fake 78.795 ) gen_loss 943.59\n",
            "iteration 5852, epoch 12, batch 361/481,disc_loss 79.209, (real 81.948, fake 76.47 ) gen_loss 791.53\n",
            "iteration 5853, epoch 12, batch 362/481,disc_loss 78.298, (real 80.679, fake 75.917 ) gen_loss 838.36\n",
            "iteration 5854, epoch 12, batch 363/481,disc_loss 79.903, (real 82.736, fake 77.069 ) gen_loss 869.28\n",
            "iteration 5855, epoch 12, batch 364/481,disc_loss 78.473, (real 81.021, fake 75.926 ) gen_loss 892.87\n",
            "iteration 5856, epoch 12, batch 365/481,disc_loss 76.086, (real 78.205, fake 73.968 ) gen_loss 853.73\n",
            "iteration 5857, epoch 12, batch 366/481,disc_loss 73.347, (real 75.938, fake 70.756 ) gen_loss 917.99\n",
            "iteration 5858, epoch 12, batch 367/481,disc_loss 78.276, (real 80.791, fake 75.761 ) gen_loss 860.88\n",
            "iteration 5859, epoch 12, batch 368/481,disc_loss 75.012, (real 78.36, fake 71.664 ) gen_loss 949.73\n",
            "iteration 5860, epoch 12, batch 369/481,disc_loss 77.904, (real 80.757, fake 75.05 ) gen_loss 864.18\n",
            "iteration 5861, epoch 12, batch 370/481,disc_loss 79.683, (real 83.256, fake 76.11 ) gen_loss 964.25\n",
            "iteration 5862, epoch 12, batch 371/481,disc_loss 76.498, (real 78.123, fake 74.874 ) gen_loss 778.34\n",
            "iteration 5863, epoch 12, batch 372/481,disc_loss 78.474, (real 80.97, fake 75.978 ) gen_loss 767.92\n",
            "iteration 5864, epoch 12, batch 373/481,disc_loss 77.829, (real 80.944, fake 74.715 ) gen_loss 732.19\n",
            "iteration 5865, epoch 12, batch 374/481,disc_loss 77.925, (real 80.784, fake 75.066 ) gen_loss 837.71\n",
            "iteration 5866, epoch 12, batch 375/481,disc_loss 76.87, (real 79.438, fake 74.301 ) gen_loss 832.5\n",
            "iteration 5867, epoch 12, batch 376/481,disc_loss 75.249, (real 77.541, fake 72.956 ) gen_loss 828.14\n",
            "iteration 5868, epoch 12, batch 377/481,disc_loss 75.096, (real 77.762, fake 72.43 ) gen_loss 841.21\n",
            "iteration 5869, epoch 12, batch 378/481,disc_loss 74.503, (real 78.044, fake 70.962 ) gen_loss 942.59\n",
            "iteration 5870, epoch 12, batch 379/481,disc_loss 80.041, (real 82.032, fake 78.051 ) gen_loss 988.73\n",
            "iteration 5871, epoch 12, batch 380/481,disc_loss 77.214, (real 77.934, fake 76.494 ) gen_loss 865.69\n",
            "iteration 5872, epoch 12, batch 381/481,disc_loss 76.501, (real 79.651, fake 73.352 ) gen_loss 922.15\n",
            "iteration 5873, epoch 12, batch 382/481,disc_loss 72.628, (real 75.444, fake 69.812 ) gen_loss 838.45\n",
            "iteration 5874, epoch 12, batch 383/481,disc_loss 78.494, (real 80.915, fake 76.072 ) gen_loss 883.97\n",
            "iteration 5875, epoch 12, batch 384/481,disc_loss 78.257, (real 80.416, fake 76.098 ) gen_loss 952.03\n",
            "iteration 5876, epoch 12, batch 385/481,disc_loss 78.508, (real 81.257, fake 75.759 ) gen_loss 810.21\n",
            "iteration 5877, epoch 12, batch 386/481,disc_loss 79.0, (real 81.908, fake 76.092 ) gen_loss 817.12\n",
            "iteration 5878, epoch 12, batch 387/481,disc_loss 79.197, (real 82.492, fake 75.901 ) gen_loss 974.88\n",
            "iteration 5879, epoch 12, batch 388/481,disc_loss 76.924, (real 79.484, fake 74.364 ) gen_loss 990.7\n",
            "iteration 5880, epoch 12, batch 389/481,disc_loss 77.006, (real 79.422, fake 74.589 ) gen_loss 868.67\n",
            "iteration 5881, epoch 12, batch 390/481,disc_loss 76.68, (real 79.629, fake 73.731 ) gen_loss 925.67\n",
            "iteration 5882, epoch 12, batch 391/481,disc_loss 77.997, (real 80.87, fake 75.125 ) gen_loss 918.7\n",
            "iteration 5883, epoch 12, batch 392/481,disc_loss 78.752, (real 81.369, fake 76.135 ) gen_loss 1192.3\n",
            "iteration 5884, epoch 12, batch 393/481,disc_loss 75.589, (real 78.241, fake 72.937 ) gen_loss 910.15\n",
            "iteration 5885, epoch 12, batch 394/481,disc_loss 77.465, (real 80.604, fake 74.327 ) gen_loss 872.36\n",
            "iteration 5886, epoch 12, batch 395/481,disc_loss 76.776, (real 80.205, fake 73.347 ) gen_loss 768.44\n",
            "iteration 5887, epoch 12, batch 396/481,disc_loss 73.929, (real 76.612, fake 71.245 ) gen_loss 841.26\n",
            "iteration 5888, epoch 12, batch 397/481,disc_loss 80.16, (real 82.522, fake 77.798 ) gen_loss 1064.7\n",
            "iteration 5889, epoch 12, batch 398/481,disc_loss 79.599, (real 82.312, fake 76.886 ) gen_loss 842.01\n",
            "iteration 5890, epoch 12, batch 399/481,disc_loss 81.805, (real 84.351, fake 79.26 ) gen_loss 777.19\n",
            "iteration 5891, epoch 12, batch 400/481,disc_loss 78.721, (real 81.808, fake 75.633 ) gen_loss 780.07\n",
            "iteration 5892, epoch 12, batch 401/481,disc_loss 76.364, (real 79.454, fake 73.274 ) gen_loss 859.76\n",
            "iteration 5893, epoch 12, batch 402/481,disc_loss 77.175, (real 79.972, fake 74.377 ) gen_loss 850.45\n",
            "iteration 5894, epoch 12, batch 403/481,disc_loss 79.464, (real 83.424, fake 75.504 ) gen_loss 989.85\n",
            "iteration 5895, epoch 12, batch 404/481,disc_loss 80.829, (real 84.136, fake 77.522 ) gen_loss 963.91\n",
            "iteration 5896, epoch 12, batch 405/481,disc_loss 76.907, (real 81.101, fake 72.714 ) gen_loss 803.28\n",
            "iteration 5897, epoch 12, batch 406/481,disc_loss 77.286, (real 79.849, fake 74.724 ) gen_loss 851.62\n",
            "iteration 5898, epoch 12, batch 407/481,disc_loss 76.48, (real 79.845, fake 73.115 ) gen_loss 791.18\n",
            "iteration 5899, epoch 12, batch 408/481,disc_loss 78.475, (real 80.583, fake 76.366 ) gen_loss 784.06\n",
            "iteration 5900, epoch 12, batch 409/481,disc_loss 73.875, (real 76.977, fake 70.774 ) gen_loss 822.49\n",
            "iteration 5901, epoch 12, batch 410/481,disc_loss 81.263, (real 83.671, fake 78.855 ) gen_loss 872.7\n",
            "iteration 5902, epoch 12, batch 411/481,disc_loss 80.471, (real 83.678, fake 77.264 ) gen_loss 886.74\n",
            "iteration 5903, epoch 12, batch 412/481,disc_loss 81.454, (real 84.095, fake 78.812 ) gen_loss 816.29\n",
            "iteration 5904, epoch 12, batch 413/481,disc_loss 78.765, (real 81.044, fake 76.486 ) gen_loss 921.98\n",
            "iteration 5905, epoch 12, batch 414/481,disc_loss 76.174, (real 79.217, fake 73.13 ) gen_loss 872.95\n",
            "iteration 5906, epoch 12, batch 415/481,disc_loss 77.799, (real 80.566, fake 75.032 ) gen_loss 839.27\n",
            "iteration 5907, epoch 12, batch 416/481,disc_loss 77.356, (real 80.696, fake 74.017 ) gen_loss 870.57\n",
            "iteration 5908, epoch 12, batch 417/481,disc_loss 75.39, (real 77.973, fake 72.806 ) gen_loss 817.18\n",
            "iteration 5909, epoch 12, batch 418/481,disc_loss 78.779, (real 82.01, fake 75.547 ) gen_loss 879.83\n",
            "iteration 5910, epoch 12, batch 419/481,disc_loss 76.158, (real 78.896, fake 73.421 ) gen_loss 788.74\n",
            "iteration 5911, epoch 12, batch 420/481,disc_loss 78.152, (real 80.078, fake 76.225 ) gen_loss 910.23\n",
            "iteration 5912, epoch 12, batch 421/481,disc_loss 77.688, (real 81.126, fake 74.25 ) gen_loss 887.12\n",
            "iteration 5913, epoch 12, batch 422/481,disc_loss 75.148, (real 77.738, fake 72.558 ) gen_loss 826.62\n",
            "iteration 5914, epoch 12, batch 423/481,disc_loss 79.258, (real 82.333, fake 76.182 ) gen_loss 843.06\n",
            "iteration 5915, epoch 12, batch 424/481,disc_loss 73.111, (real 75.447, fake 70.774 ) gen_loss 854.94\n",
            "iteration 5916, epoch 12, batch 425/481,disc_loss 73.326, (real 76.632, fake 70.02 ) gen_loss 977.39\n",
            "iteration 5917, epoch 12, batch 426/481,disc_loss 74.536, (real 77.12, fake 71.952 ) gen_loss 897.61\n",
            "iteration 5918, epoch 12, batch 427/481,disc_loss 78.868, (real 81.132, fake 76.604 ) gen_loss 939.16\n",
            "iteration 5919, epoch 12, batch 428/481,disc_loss 83.536, (real 85.971, fake 81.101 ) gen_loss 793.12\n",
            "iteration 5920, epoch 12, batch 429/481,disc_loss 79.917, (real 83.172, fake 76.661 ) gen_loss 834.44\n",
            "iteration 5921, epoch 12, batch 430/481,disc_loss 75.493, (real 78.462, fake 72.523 ) gen_loss 810.71\n",
            "iteration 5922, epoch 12, batch 431/481,disc_loss 77.688, (real 80.481, fake 74.894 ) gen_loss 873.32\n",
            "iteration 5923, epoch 12, batch 432/481,disc_loss 79.573, (real 82.421, fake 76.725 ) gen_loss 880.07\n",
            "iteration 5924, epoch 12, batch 433/481,disc_loss 77.252, (real 80.073, fake 74.43 ) gen_loss 777.16\n",
            "iteration 5925, epoch 12, batch 434/481,disc_loss 79.702, (real 83.481, fake 75.922 ) gen_loss 795.31\n",
            "iteration 5926, epoch 12, batch 435/481,disc_loss 75.968, (real 79.915, fake 72.021 ) gen_loss 818.32\n",
            "iteration 5927, epoch 12, batch 436/481,disc_loss 76.139, (real 78.664, fake 73.614 ) gen_loss 799.12\n",
            "iteration 5928, epoch 12, batch 437/481,disc_loss 77.758, (real 79.856, fake 75.661 ) gen_loss 980.18\n",
            "iteration 5929, epoch 12, batch 438/481,disc_loss 79.846, (real 82.791, fake 76.9 ) gen_loss 1050.6\n",
            "iteration 5930, epoch 12, batch 439/481,disc_loss 73.987, (real 76.365, fake 71.609 ) gen_loss 886.51\n",
            "iteration 5931, epoch 12, batch 440/481,disc_loss 73.315, (real 75.841, fake 70.79 ) gen_loss 822.62\n",
            "iteration 5932, epoch 12, batch 441/481,disc_loss 77.168, (real 80.186, fake 74.15 ) gen_loss 852.65\n",
            "iteration 5933, epoch 12, batch 442/481,disc_loss 72.781, (real 76.167, fake 69.396 ) gen_loss 798.47\n",
            "iteration 5934, epoch 12, batch 443/481,disc_loss 79.571, (real 82.846, fake 76.296 ) gen_loss 855.86\n",
            "iteration 5935, epoch 12, batch 444/481,disc_loss 78.021, (real 82.026, fake 74.016 ) gen_loss 882.51\n",
            "iteration 5936, epoch 12, batch 445/481,disc_loss 78.666, (real 80.967, fake 76.366 ) gen_loss 827.32\n",
            "iteration 5937, epoch 12, batch 446/481,disc_loss 77.278, (real 81.046, fake 73.509 ) gen_loss 889.14\n",
            "iteration 5938, epoch 12, batch 447/481,disc_loss 81.635, (real 84.127, fake 79.143 ) gen_loss 810.35\n",
            "iteration 5939, epoch 12, batch 448/481,disc_loss 78.244, (real 81.411, fake 75.077 ) gen_loss 839.98\n",
            "iteration 5940, epoch 12, batch 449/481,disc_loss 76.213, (real 79.401, fake 73.025 ) gen_loss 823.36\n",
            "iteration 5941, epoch 12, batch 450/481,disc_loss 76.973, (real 79.304, fake 74.641 ) gen_loss 1011.8\n",
            "iteration 5942, epoch 12, batch 451/481,disc_loss 72.293, (real 74.431, fake 70.155 ) gen_loss 923.89\n",
            "iteration 5943, epoch 12, batch 452/481,disc_loss 74.994, (real 77.582, fake 72.407 ) gen_loss 909.96\n",
            "iteration 5944, epoch 12, batch 453/481,disc_loss 73.356, (real 75.693, fake 71.019 ) gen_loss 941.63\n",
            "iteration 5945, epoch 12, batch 454/481,disc_loss 82.262, (real 84.776, fake 79.748 ) gen_loss 910.32\n",
            "iteration 5946, epoch 12, batch 455/481,disc_loss 74.193, (real 77.896, fake 70.489 ) gen_loss 958.95\n",
            "iteration 5947, epoch 12, batch 456/481,disc_loss 75.378, (real 78.024, fake 72.731 ) gen_loss 856.96\n",
            "iteration 5948, epoch 12, batch 457/481,disc_loss 81.683, (real 84.928, fake 78.439 ) gen_loss 841.58\n",
            "iteration 5949, epoch 12, batch 458/481,disc_loss 75.628, (real 77.919, fake 73.337 ) gen_loss 808.53\n",
            "iteration 5950, epoch 12, batch 459/481,disc_loss 75.442, (real 78.483, fake 72.4 ) gen_loss 792.47\n",
            "iteration 5951, epoch 12, batch 460/481,disc_loss 75.676, (real 78.882, fake 72.471 ) gen_loss 771.49\n",
            "iteration 5952, epoch 12, batch 461/481,disc_loss 79.369, (real 81.96, fake 76.778 ) gen_loss 883.33\n",
            "iteration 5953, epoch 12, batch 462/481,disc_loss 76.137, (real 78.335, fake 73.94 ) gen_loss 862.33\n",
            "iteration 5954, epoch 12, batch 463/481,disc_loss 76.654, (real 80.186, fake 73.121 ) gen_loss 913.5\n",
            "iteration 5955, epoch 12, batch 464/481,disc_loss 74.363, (real 78.055, fake 70.67 ) gen_loss 962.09\n",
            "iteration 5956, epoch 12, batch 465/481,disc_loss 75.643, (real 78.656, fake 72.631 ) gen_loss 930.63\n",
            "iteration 5957, epoch 12, batch 466/481,disc_loss 73.98, (real 77.986, fake 69.973 ) gen_loss 865.79\n",
            "iteration 5958, epoch 12, batch 467/481,disc_loss 77.413, (real 80.537, fake 74.289 ) gen_loss 932.25\n",
            "iteration 5959, epoch 12, batch 468/481,disc_loss 77.815, (real 80.71, fake 74.919 ) gen_loss 849.33\n",
            "iteration 5960, epoch 12, batch 469/481,disc_loss 73.618, (real 76.132, fake 71.104 ) gen_loss 803.78\n",
            "iteration 5961, epoch 12, batch 470/481,disc_loss 80.651, (real 82.919, fake 78.383 ) gen_loss 817.91\n",
            "iteration 5962, epoch 12, batch 471/481,disc_loss 75.037, (real 77.499, fake 72.575 ) gen_loss 839.79\n",
            "iteration 5963, epoch 12, batch 472/481,disc_loss 76.358, (real 79.374, fake 73.341 ) gen_loss 962.96\n",
            "iteration 5964, epoch 12, batch 473/481,disc_loss 74.107, (real 76.949, fake 71.266 ) gen_loss 813.55\n",
            "iteration 5965, epoch 12, batch 474/481,disc_loss 79.148, (real 82.17, fake 76.126 ) gen_loss 849.79\n",
            "iteration 5966, epoch 12, batch 475/481,disc_loss 77.627, (real 80.548, fake 74.707 ) gen_loss 809.42\n",
            "iteration 5967, epoch 12, batch 476/481,disc_loss 75.193, (real 78.125, fake 72.261 ) gen_loss 882.79\n",
            "iteration 5968, epoch 12, batch 477/481,disc_loss 79.967, (real 82.46, fake 77.475 ) gen_loss 868.64\n",
            "iteration 5969, epoch 12, batch 478/481,disc_loss 76.773, (real 80.553, fake 72.993 ) gen_loss 958.28\n",
            "iteration 5970, epoch 12, batch 479/481,disc_loss 76.583, (real 79.119, fake 74.046 ) gen_loss 825.84\n",
            "iteration 5971, epoch 12, batch 480/481,disc_loss 78.07, (real 81.304, fake 74.835 ) gen_loss 997.54\n",
            "iteration 5972, epoch 12, batch 481/481,disc_loss 78.968, (real 81.89, fake 76.047 ) gen_loss 889.44\n",
            "iteration 5973, epoch 13, batch 1/481,disc_loss 79.7, (real 82.027, fake 77.373 ) gen_loss 881.47\n",
            "iteration 5974, epoch 13, batch 2/481,disc_loss 79.609, (real 82.163, fake 77.055 ) gen_loss 823.07\n",
            "iteration 5975, epoch 13, batch 3/481,disc_loss 78.604, (real 82.913, fake 74.294 ) gen_loss 898.87\n",
            "iteration 5976, epoch 13, batch 4/481,disc_loss 78.172, (real 80.422, fake 75.922 ) gen_loss 895.53\n",
            "iteration 5977, epoch 13, batch 5/481,disc_loss 76.442, (real 79.019, fake 73.865 ) gen_loss 858.11\n",
            "iteration 5978, epoch 13, batch 6/481,disc_loss 78.811, (real 81.255, fake 76.368 ) gen_loss 805.66\n",
            "iteration 5979, epoch 13, batch 7/481,disc_loss 77.824, (real 80.585, fake 75.063 ) gen_loss 847.54\n",
            "iteration 5980, epoch 13, batch 8/481,disc_loss 76.147, (real 78.528, fake 73.766 ) gen_loss 816.3\n",
            "iteration 5981, epoch 13, batch 9/481,disc_loss 76.974, (real 79.829, fake 74.118 ) gen_loss 886.32\n",
            "iteration 5982, epoch 13, batch 10/481,disc_loss 76.091, (real 78.708, fake 73.474 ) gen_loss 875.32\n",
            "iteration 5983, epoch 13, batch 11/481,disc_loss 74.869, (real 77.196, fake 72.543 ) gen_loss 948.79\n",
            "iteration 5984, epoch 13, batch 12/481,disc_loss 78.297, (real 81.323, fake 75.271 ) gen_loss 820.09\n",
            "iteration 5985, epoch 13, batch 13/481,disc_loss 78.549, (real 81.137, fake 75.961 ) gen_loss 931.86\n",
            "iteration 5986, epoch 13, batch 14/481,disc_loss 74.221, (real 76.504, fake 71.939 ) gen_loss 833.55\n",
            "iteration 5987, epoch 13, batch 15/481,disc_loss 77.802, (real 80.658, fake 74.945 ) gen_loss 853.46\n",
            "iteration 5988, epoch 13, batch 16/481,disc_loss 76.524, (real 79.679, fake 73.37 ) gen_loss 771.14\n",
            "iteration 5989, epoch 13, batch 17/481,disc_loss 75.208, (real 77.98, fake 72.436 ) gen_loss 801.49\n",
            "iteration 5990, epoch 13, batch 18/481,disc_loss 73.853, (real 76.154, fake 71.551 ) gen_loss 924.14\n",
            "iteration 5991, epoch 13, batch 19/481,disc_loss 75.479, (real 77.633, fake 73.324 ) gen_loss 851.4\n",
            "iteration 5992, epoch 13, batch 20/481,disc_loss 80.427, (real 82.655, fake 78.198 ) gen_loss 868.33\n",
            "iteration 5993, epoch 13, batch 21/481,disc_loss 74.594, (real 77.456, fake 71.731 ) gen_loss 880.25\n",
            "iteration 5994, epoch 13, batch 22/481,disc_loss 73.036, (real 75.407, fake 70.666 ) gen_loss 915.51\n",
            "iteration 5995, epoch 13, batch 23/481,disc_loss 79.805, (real 83.201, fake 76.408 ) gen_loss 881.35\n",
            "iteration 5996, epoch 13, batch 24/481,disc_loss 76.711, (real 79.413, fake 74.008 ) gen_loss 810.03\n",
            "iteration 5997, epoch 13, batch 25/481,disc_loss 79.277, (real 81.786, fake 76.768 ) gen_loss 786.86\n",
            "iteration 5998, epoch 13, batch 26/481,disc_loss 76.657, (real 78.509, fake 74.804 ) gen_loss 984.93\n",
            "iteration 5999, epoch 13, batch 27/481,disc_loss 78.167, (real 80.917, fake 75.417 ) gen_loss 1092.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 6000, epoch 13, batch 28/481,disc_loss 78.149, (real 80.844, fake 75.454 ) gen_loss 847.04\n",
            "iteration 6001, epoch 13, batch 29/481,disc_loss 73.815, (real 75.93, fake 71.701 ) gen_loss 844.78\n",
            "iteration 6002, epoch 13, batch 30/481,disc_loss 81.235, (real 83.135, fake 79.335 ) gen_loss 784.04\n",
            "iteration 6003, epoch 13, batch 31/481,disc_loss 75.714, (real 78.233, fake 73.194 ) gen_loss 843.06\n",
            "iteration 6004, epoch 13, batch 32/481,disc_loss 74.241, (real 76.469, fake 72.013 ) gen_loss 839.47\n",
            "iteration 6005, epoch 13, batch 33/481,disc_loss 73.141, (real 74.957, fake 71.325 ) gen_loss 895.31\n",
            "iteration 6006, epoch 13, batch 34/481,disc_loss 79.467, (real 82.122, fake 76.812 ) gen_loss 974.57\n",
            "iteration 6007, epoch 13, batch 35/481,disc_loss 77.849, (real 80.223, fake 75.476 ) gen_loss 851.39\n",
            "iteration 6008, epoch 13, batch 36/481,disc_loss 79.4, (real 82.041, fake 76.759 ) gen_loss 798.9\n",
            "iteration 6009, epoch 13, batch 37/481,disc_loss 76.943, (real 79.877, fake 74.01 ) gen_loss 776.79\n",
            "iteration 6010, epoch 13, batch 38/481,disc_loss 78.007, (real 81.019, fake 74.995 ) gen_loss 932.51\n",
            "iteration 6011, epoch 13, batch 39/481,disc_loss 75.667, (real 78.703, fake 72.631 ) gen_loss 817.89\n",
            "iteration 6012, epoch 13, batch 40/481,disc_loss 78.757, (real 82.411, fake 75.104 ) gen_loss 969.93\n",
            "iteration 6013, epoch 13, batch 41/481,disc_loss 75.054, (real 77.253, fake 72.854 ) gen_loss 1127.8\n",
            "iteration 6014, epoch 13, batch 42/481,disc_loss 75.346, (real 77.981, fake 72.711 ) gen_loss 865.03\n",
            "iteration 6015, epoch 13, batch 43/481,disc_loss 79.57, (real 82.28, fake 76.861 ) gen_loss 1115.6\n",
            "iteration 6016, epoch 13, batch 44/481,disc_loss 76.39, (real 79.081, fake 73.698 ) gen_loss 962.09\n",
            "iteration 6017, epoch 13, batch 45/481,disc_loss 78.825, (real 80.749, fake 76.9 ) gen_loss 940.53\n",
            "iteration 6018, epoch 13, batch 46/481,disc_loss 73.704, (real 76.013, fake 71.395 ) gen_loss 951.29\n",
            "iteration 6019, epoch 13, batch 47/481,disc_loss 78.39, (real 80.58, fake 76.2 ) gen_loss 867.06\n",
            "iteration 6020, epoch 13, batch 48/481,disc_loss 76.322, (real 79.533, fake 73.112 ) gen_loss 939.62\n",
            "iteration 6021, epoch 13, batch 49/481,disc_loss 78.725, (real 81.459, fake 75.991 ) gen_loss 1021.2\n",
            "iteration 6022, epoch 13, batch 50/481,disc_loss 78.318, (real 80.935, fake 75.7 ) gen_loss 1030.7\n",
            "iteration 6023, epoch 13, batch 51/481,disc_loss 71.243, (real 73.969, fake 68.516 ) gen_loss 968.74\n",
            "iteration 6024, epoch 13, batch 52/481,disc_loss 78.521, (real 80.686, fake 76.356 ) gen_loss 943.91\n",
            "iteration 6025, epoch 13, batch 53/481,disc_loss 76.103, (real 79.52, fake 72.686 ) gen_loss 939.81\n",
            "iteration 6026, epoch 13, batch 54/481,disc_loss 79.033, (real 81.613, fake 76.452 ) gen_loss 849.09\n",
            "iteration 6027, epoch 13, batch 55/481,disc_loss 71.953, (real 74.051, fake 69.855 ) gen_loss 840.36\n",
            "iteration 6028, epoch 13, batch 56/481,disc_loss 79.509, (real 81.667, fake 77.351 ) gen_loss 823.37\n",
            "iteration 6029, epoch 13, batch 57/481,disc_loss 76.514, (real 78.476, fake 74.553 ) gen_loss 957.66\n",
            "iteration 6030, epoch 13, batch 58/481,disc_loss 78.37, (real 79.835, fake 76.906 ) gen_loss 894.74\n",
            "iteration 6031, epoch 13, batch 59/481,disc_loss 76.302, (real 78.302, fake 74.302 ) gen_loss 890.12\n",
            "iteration 6032, epoch 13, batch 60/481,disc_loss 76.513, (real 79.09, fake 73.936 ) gen_loss 818.49\n",
            "iteration 6033, epoch 13, batch 61/481,disc_loss 77.064, (real 79.92, fake 74.209 ) gen_loss 1001.8\n",
            "iteration 6034, epoch 13, batch 62/481,disc_loss 74.443, (real 77.552, fake 71.333 ) gen_loss 829.7\n",
            "iteration 6035, epoch 13, batch 63/481,disc_loss 73.994, (real 76.022, fake 71.965 ) gen_loss 856.59\n",
            "iteration 6036, epoch 13, batch 64/481,disc_loss 76.372, (real 79.072, fake 73.672 ) gen_loss 1016.1\n",
            "iteration 6037, epoch 13, batch 65/481,disc_loss 79.601, (real 82.197, fake 77.004 ) gen_loss 824.67\n",
            "iteration 6038, epoch 13, batch 66/481,disc_loss 78.788, (real 81.42, fake 76.156 ) gen_loss 720.31\n",
            "iteration 6039, epoch 13, batch 67/481,disc_loss 78.394, (real 81.074, fake 75.715 ) gen_loss 916.72\n",
            "iteration 6040, epoch 13, batch 68/481,disc_loss 75.659, (real 77.997, fake 73.321 ) gen_loss 947.0\n",
            "iteration 6041, epoch 13, batch 69/481,disc_loss 72.904, (real 75.024, fake 70.783 ) gen_loss 923.83\n",
            "iteration 6042, epoch 13, batch 70/481,disc_loss 76.966, (real 79.314, fake 74.618 ) gen_loss 941.07\n",
            "iteration 6043, epoch 13, batch 71/481,disc_loss 75.632, (real 78.003, fake 73.261 ) gen_loss 864.03\n",
            "iteration 6044, epoch 13, batch 72/481,disc_loss 79.946, (real 82.452, fake 77.44 ) gen_loss 860.92\n",
            "iteration 6045, epoch 13, batch 73/481,disc_loss 79.741, (real 82.053, fake 77.428 ) gen_loss 785.31\n",
            "iteration 6046, epoch 13, batch 74/481,disc_loss 78.217, (real 80.772, fake 75.662 ) gen_loss 824.43\n",
            "iteration 6047, epoch 13, batch 75/481,disc_loss 78.525, (real 81.472, fake 75.577 ) gen_loss 854.24\n",
            "iteration 6048, epoch 13, batch 76/481,disc_loss 82.792, (real 85.6, fake 79.983 ) gen_loss 1036.3\n",
            "iteration 6049, epoch 13, batch 77/481,disc_loss 80.036, (real 82.793, fake 77.28 ) gen_loss 1105.4\n",
            "iteration 6050, epoch 13, batch 78/481,disc_loss 82.279, (real 85.352, fake 79.205 ) gen_loss 856.47\n",
            "iteration 6051, epoch 13, batch 79/481,disc_loss 73.087, (real 75.609, fake 70.565 ) gen_loss 934.24\n",
            "iteration 6052, epoch 13, batch 80/481,disc_loss 80.96, (real 83.972, fake 77.948 ) gen_loss 908.24\n",
            "iteration 6053, epoch 13, batch 81/481,disc_loss 75.174, (real 77.297, fake 73.05 ) gen_loss 924.33\n",
            "iteration 6054, epoch 13, batch 82/481,disc_loss 71.245, (real 72.532, fake 69.958 ) gen_loss 896.73\n",
            "iteration 6055, epoch 13, batch 83/481,disc_loss 79.376, (real 81.624, fake 77.129 ) gen_loss 848.15\n",
            "iteration 6056, epoch 13, batch 84/481,disc_loss 81.025, (real 84.382, fake 77.669 ) gen_loss 848.33\n",
            "iteration 6057, epoch 13, batch 85/481,disc_loss 82.7, (real 84.953, fake 80.448 ) gen_loss 810.73\n",
            "iteration 6058, epoch 13, batch 86/481,disc_loss 76.64, (real 80.229, fake 73.052 ) gen_loss 798.06\n",
            "iteration 6059, epoch 13, batch 87/481,disc_loss 76.272, (real 78.626, fake 73.919 ) gen_loss 873.07\n",
            "iteration 6060, epoch 13, batch 88/481,disc_loss 74.436, (real 76.763, fake 72.109 ) gen_loss 920.15\n",
            "iteration 6061, epoch 13, batch 89/481,disc_loss 73.596, (real 76.263, fake 70.928 ) gen_loss 823.88\n",
            "iteration 6062, epoch 13, batch 90/481,disc_loss 81.448, (real 84.344, fake 78.553 ) gen_loss 860.16\n",
            "iteration 6063, epoch 13, batch 91/481,disc_loss 75.904, (real 77.841, fake 73.967 ) gen_loss 785.72\n",
            "iteration 6064, epoch 13, batch 92/481,disc_loss 80.098, (real 83.289, fake 76.906 ) gen_loss 880.49\n",
            "iteration 6065, epoch 13, batch 93/481,disc_loss 75.624, (real 78.137, fake 73.112 ) gen_loss 881.36\n",
            "iteration 6066, epoch 13, batch 94/481,disc_loss 75.108, (real 77.765, fake 72.451 ) gen_loss 850.98\n",
            "iteration 6067, epoch 13, batch 95/481,disc_loss 73.332, (real 76.356, fake 70.308 ) gen_loss 836.95\n",
            "iteration 6068, epoch 13, batch 96/481,disc_loss 76.0, (real 79.819, fake 72.182 ) gen_loss 922.76\n",
            "iteration 6069, epoch 13, batch 97/481,disc_loss 72.727, (real 74.633, fake 70.821 ) gen_loss 854.24\n",
            "iteration 6070, epoch 13, batch 98/481,disc_loss 74.61, (real 77.053, fake 72.166 ) gen_loss 913.93\n",
            "iteration 6071, epoch 13, batch 99/481,disc_loss 79.06, (real 81.619, fake 76.502 ) gen_loss 797.86\n",
            "iteration 6072, epoch 13, batch 100/481,disc_loss 73.367, (real 75.944, fake 70.789 ) gen_loss 853.34\n",
            "iteration 6073, epoch 13, batch 101/481,disc_loss 79.061, (real 81.676, fake 76.446 ) gen_loss 993.67\n",
            "iteration 6074, epoch 13, batch 102/481,disc_loss 79.679, (real 81.815, fake 77.543 ) gen_loss 906.44\n",
            "iteration 6075, epoch 13, batch 103/481,disc_loss 80.523, (real 83.507, fake 77.539 ) gen_loss 834.57\n",
            "iteration 6076, epoch 13, batch 104/481,disc_loss 74.964, (real 77.153, fake 72.776 ) gen_loss 801.32\n",
            "iteration 6077, epoch 13, batch 105/481,disc_loss 77.74, (real 80.66, fake 74.821 ) gen_loss 970.87\n",
            "iteration 6078, epoch 13, batch 106/481,disc_loss 74.426, (real 77.157, fake 71.694 ) gen_loss 801.16\n",
            "iteration 6079, epoch 13, batch 107/481,disc_loss 80.727, (real 83.683, fake 77.77 ) gen_loss 845.47\n",
            "iteration 6080, epoch 13, batch 108/481,disc_loss 76.401, (real 79.165, fake 73.637 ) gen_loss 1027.9\n",
            "iteration 6081, epoch 13, batch 109/481,disc_loss 75.603, (real 77.806, fake 73.401 ) gen_loss 832.59\n",
            "iteration 6082, epoch 13, batch 110/481,disc_loss 73.817, (real 76.057, fake 71.578 ) gen_loss 860.49\n",
            "iteration 6083, epoch 13, batch 111/481,disc_loss 72.542, (real 75.888, fake 69.196 ) gen_loss 847.08\n",
            "iteration 6084, epoch 13, batch 112/481,disc_loss 79.243, (real 81.894, fake 76.592 ) gen_loss 1116.1\n",
            "iteration 6085, epoch 13, batch 113/481,disc_loss 81.865, (real 83.547, fake 80.183 ) gen_loss 784.25\n",
            "iteration 6086, epoch 13, batch 114/481,disc_loss 78.367, (real 80.883, fake 75.852 ) gen_loss 745.15\n",
            "iteration 6087, epoch 13, batch 115/481,disc_loss 74.008, (real 76.385, fake 71.632 ) gen_loss 929.85\n",
            "iteration 6088, epoch 13, batch 116/481,disc_loss 80.363, (real 83.382, fake 77.343 ) gen_loss 886.67\n",
            "iteration 6089, epoch 13, batch 117/481,disc_loss 72.75, (real 74.969, fake 70.53 ) gen_loss 810.52\n",
            "iteration 6090, epoch 13, batch 118/481,disc_loss 72.948, (real 75.381, fake 70.515 ) gen_loss 927.47\n",
            "iteration 6091, epoch 13, batch 119/481,disc_loss 77.724, (real 80.131, fake 75.317 ) gen_loss 863.1\n",
            "iteration 6092, epoch 13, batch 120/481,disc_loss 86.353, (real 88.519, fake 84.188 ) gen_loss 920.71\n",
            "iteration 6093, epoch 13, batch 121/481,disc_loss 79.498, (real 83.18, fake 75.816 ) gen_loss 917.48\n",
            "iteration 6094, epoch 13, batch 122/481,disc_loss 75.907, (real 78.073, fake 73.741 ) gen_loss 874.01\n",
            "iteration 6095, epoch 13, batch 123/481,disc_loss 73.183, (real 76.546, fake 69.82 ) gen_loss 943.26\n",
            "iteration 6096, epoch 13, batch 124/481,disc_loss 73.175, (real 75.049, fake 71.301 ) gen_loss 803.13\n",
            "iteration 6097, epoch 13, batch 125/481,disc_loss 75.316, (real 77.766, fake 72.866 ) gen_loss 823.64\n",
            "iteration 6098, epoch 13, batch 126/481,disc_loss 79.081, (real 81.312, fake 76.85 ) gen_loss 753.02\n",
            "iteration 6099, epoch 13, batch 127/481,disc_loss 80.711, (real 83.434, fake 77.987 ) gen_loss 826.68\n",
            "iteration 6100, epoch 13, batch 128/481,disc_loss 79.619, (real 81.87, fake 77.368 ) gen_loss 789.12\n",
            "iteration 6101, epoch 13, batch 129/481,disc_loss 75.679, (real 78.03, fake 73.328 ) gen_loss 830.82\n",
            "iteration 6102, epoch 13, batch 130/481,disc_loss 81.153, (real 83.686, fake 78.621 ) gen_loss 777.3\n",
            "iteration 6103, epoch 13, batch 131/481,disc_loss 76.105, (real 78.975, fake 73.236 ) gen_loss 851.87\n",
            "iteration 6104, epoch 13, batch 132/481,disc_loss 75.864, (real 78.187, fake 73.54 ) gen_loss 742.99\n",
            "iteration 6105, epoch 13, batch 133/481,disc_loss 80.416, (real 82.759, fake 78.072 ) gen_loss 984.19\n",
            "iteration 6106, epoch 13, batch 134/481,disc_loss 79.726, (real 81.981, fake 77.471 ) gen_loss 973.17\n",
            "iteration 6107, epoch 13, batch 135/481,disc_loss 76.982, (real 79.328, fake 74.636 ) gen_loss 867.51\n",
            "iteration 6108, epoch 13, batch 136/481,disc_loss 72.355, (real 74.607, fake 70.103 ) gen_loss 874.0\n",
            "iteration 6109, epoch 13, batch 137/481,disc_loss 73.021, (real 75.574, fake 70.468 ) gen_loss 880.33\n",
            "iteration 6110, epoch 13, batch 138/481,disc_loss 74.887, (real 78.09, fake 71.684 ) gen_loss 793.36\n",
            "iteration 6111, epoch 13, batch 139/481,disc_loss 77.995, (real 80.658, fake 75.331 ) gen_loss 800.98\n",
            "iteration 6112, epoch 13, batch 140/481,disc_loss 78.709, (real 80.964, fake 76.455 ) gen_loss 836.06\n",
            "iteration 6113, epoch 13, batch 141/481,disc_loss 77.264, (real 79.825, fake 74.702 ) gen_loss 879.62\n",
            "iteration 6114, epoch 13, batch 142/481,disc_loss 73.9, (real 76.654, fake 71.146 ) gen_loss 904.71\n",
            "iteration 6115, epoch 13, batch 143/481,disc_loss 77.648, (real 80.57, fake 74.726 ) gen_loss 922.2\n",
            "iteration 6116, epoch 13, batch 144/481,disc_loss 75.172, (real 79.397, fake 70.947 ) gen_loss 927.9\n",
            "iteration 6117, epoch 13, batch 145/481,disc_loss 76.765, (real 79.28, fake 74.251 ) gen_loss 929.95\n",
            "iteration 6118, epoch 13, batch 146/481,disc_loss 78.299, (real 81.634, fake 74.964 ) gen_loss 806.36\n",
            "iteration 6119, epoch 13, batch 147/481,disc_loss 80.63, (real 82.938, fake 78.321 ) gen_loss 886.61\n",
            "iteration 6120, epoch 13, batch 148/481,disc_loss 74.395, (real 76.1, fake 72.69 ) gen_loss 891.5\n",
            "iteration 6121, epoch 13, batch 149/481,disc_loss 76.911, (real 79.699, fake 74.124 ) gen_loss 806.18\n",
            "iteration 6122, epoch 13, batch 150/481,disc_loss 71.571, (real 74.003, fake 69.138 ) gen_loss 876.0\n",
            "iteration 6123, epoch 13, batch 151/481,disc_loss 79.496, (real 81.967, fake 77.025 ) gen_loss 856.54\n",
            "iteration 6124, epoch 13, batch 152/481,disc_loss 78.263, (real 81.075, fake 75.451 ) gen_loss 832.41\n",
            "iteration 6125, epoch 13, batch 153/481,disc_loss 82.062, (real 84.379, fake 79.744 ) gen_loss 875.37\n",
            "iteration 6126, epoch 13, batch 154/481,disc_loss 76.893, (real 79.382, fake 74.404 ) gen_loss 857.73\n",
            "iteration 6127, epoch 13, batch 155/481,disc_loss 75.71, (real 78.055, fake 73.364 ) gen_loss 838.25\n",
            "iteration 6128, epoch 13, batch 156/481,disc_loss 80.615, (real 82.979, fake 78.25 ) gen_loss 785.07\n",
            "iteration 6129, epoch 13, batch 157/481,disc_loss 80.976, (real 83.322, fake 78.631 ) gen_loss 1068.5\n",
            "iteration 6130, epoch 13, batch 158/481,disc_loss 76.224, (real 78.039, fake 74.41 ) gen_loss 764.62\n",
            "iteration 6131, epoch 13, batch 159/481,disc_loss 75.452, (real 78.798, fake 72.106 ) gen_loss 944.14\n",
            "iteration 6132, epoch 13, batch 160/481,disc_loss 77.184, (real 79.209, fake 75.159 ) gen_loss 830.72\n",
            "iteration 6133, epoch 13, batch 161/481,disc_loss 77.958, (real 80.144, fake 75.773 ) gen_loss 814.81\n",
            "iteration 6134, epoch 13, batch 162/481,disc_loss 76.486, (real 79.245, fake 73.727 ) gen_loss 768.66\n",
            "iteration 6135, epoch 13, batch 163/481,disc_loss 75.521, (real 78.483, fake 72.56 ) gen_loss 795.96\n",
            "iteration 6136, epoch 13, batch 164/481,disc_loss 77.724, (real 80.182, fake 75.266 ) gen_loss 917.77\n",
            "iteration 6137, epoch 13, batch 165/481,disc_loss 78.053, (real 80.706, fake 75.4 ) gen_loss 922.9\n",
            "iteration 6138, epoch 13, batch 166/481,disc_loss 78.185, (real 80.409, fake 75.961 ) gen_loss 810.35\n",
            "iteration 6139, epoch 13, batch 167/481,disc_loss 78.282, (real 80.798, fake 75.767 ) gen_loss 916.54\n",
            "iteration 6140, epoch 13, batch 168/481,disc_loss 77.841, (real 80.796, fake 74.886 ) gen_loss 871.48\n",
            "iteration 6141, epoch 13, batch 169/481,disc_loss 72.939, (real 75.214, fake 70.665 ) gen_loss 806.87\n",
            "iteration 6142, epoch 13, batch 170/481,disc_loss 78.952, (real 81.997, fake 75.906 ) gen_loss 789.33\n",
            "iteration 6143, epoch 13, batch 171/481,disc_loss 73.222, (real 75.415, fake 71.029 ) gen_loss 893.23\n",
            "iteration 6144, epoch 13, batch 172/481,disc_loss 75.2, (real 77.734, fake 72.665 ) gen_loss 873.23\n",
            "iteration 6145, epoch 13, batch 173/481,disc_loss 77.533, (real 79.794, fake 75.272 ) gen_loss 838.76\n",
            "iteration 6146, epoch 13, batch 174/481,disc_loss 75.713, (real 77.974, fake 73.452 ) gen_loss 915.59\n",
            "iteration 6147, epoch 13, batch 175/481,disc_loss 77.415, (real 79.495, fake 75.335 ) gen_loss 869.22\n",
            "iteration 6148, epoch 13, batch 176/481,disc_loss 80.372, (real 83.303, fake 77.441 ) gen_loss 837.68\n",
            "iteration 6149, epoch 13, batch 177/481,disc_loss 81.997, (real 86.151, fake 77.842 ) gen_loss 948.88\n",
            "iteration 6150, epoch 13, batch 178/481,disc_loss 72.894, (real 76.03, fake 69.759 ) gen_loss 910.0\n",
            "iteration 6151, epoch 13, batch 179/481,disc_loss 78.949, (real 81.332, fake 76.566 ) gen_loss 907.8\n",
            "iteration 6152, epoch 13, batch 180/481,disc_loss 77.103, (real 80.414, fake 73.793 ) gen_loss 852.03\n",
            "iteration 6153, epoch 13, batch 181/481,disc_loss 74.645, (real 76.566, fake 72.724 ) gen_loss 864.87\n",
            "iteration 6154, epoch 13, batch 182/481,disc_loss 81.084, (real 83.439, fake 78.729 ) gen_loss 888.23\n",
            "iteration 6155, epoch 13, batch 183/481,disc_loss 78.674, (real 80.77, fake 76.579 ) gen_loss 943.01\n",
            "iteration 6156, epoch 13, batch 184/481,disc_loss 78.459, (real 81.581, fake 75.337 ) gen_loss 956.25\n",
            "iteration 6157, epoch 13, batch 185/481,disc_loss 83.957, (real 86.773, fake 81.141 ) gen_loss 750.21\n",
            "iteration 6158, epoch 13, batch 186/481,disc_loss 76.622, (real 79.113, fake 74.132 ) gen_loss 830.4\n",
            "iteration 6159, epoch 13, batch 187/481,disc_loss 80.763, (real 84.136, fake 77.391 ) gen_loss 789.88\n",
            "iteration 6160, epoch 13, batch 188/481,disc_loss 77.773, (real 80.425, fake 75.121 ) gen_loss 848.37\n",
            "iteration 6161, epoch 13, batch 189/481,disc_loss 77.737, (real 80.132, fake 75.342 ) gen_loss 885.78\n",
            "iteration 6162, epoch 13, batch 190/481,disc_loss 79.904, (real 82.882, fake 76.926 ) gen_loss 977.57\n",
            "iteration 6163, epoch 13, batch 191/481,disc_loss 76.557, (real 79.49, fake 73.625 ) gen_loss 957.69\n",
            "iteration 6164, epoch 13, batch 192/481,disc_loss 76.557, (real 79.405, fake 73.71 ) gen_loss 865.81\n",
            "iteration 6165, epoch 13, batch 193/481,disc_loss 76.958, (real 79.893, fake 74.024 ) gen_loss 887.07\n",
            "iteration 6166, epoch 13, batch 194/481,disc_loss 76.588, (real 79.489, fake 73.687 ) gen_loss 822.03\n",
            "iteration 6167, epoch 13, batch 195/481,disc_loss 81.31, (real 83.351, fake 79.27 ) gen_loss 878.63\n",
            "iteration 6168, epoch 13, batch 196/481,disc_loss 75.546, (real 78.501, fake 72.59 ) gen_loss 891.06\n",
            "iteration 6169, epoch 13, batch 197/481,disc_loss 77.44, (real 78.905, fake 75.976 ) gen_loss 942.47\n",
            "iteration 6170, epoch 13, batch 198/481,disc_loss 78.856, (real 81.016, fake 76.696 ) gen_loss 893.77\n",
            "iteration 6171, epoch 13, batch 199/481,disc_loss 80.77, (real 83.541, fake 77.998 ) gen_loss 878.78\n",
            "iteration 6172, epoch 13, batch 200/481,disc_loss 80.778, (real 84.977, fake 76.579 ) gen_loss 909.28\n",
            "iteration 6173, epoch 13, batch 201/481,disc_loss 74.788, (real 77.611, fake 71.965 ) gen_loss 935.4\n",
            "iteration 6174, epoch 13, batch 202/481,disc_loss 73.49, (real 76.068, fake 70.913 ) gen_loss 853.01\n",
            "iteration 6175, epoch 13, batch 203/481,disc_loss 74.99, (real 77.47, fake 72.51 ) gen_loss 943.41\n",
            "iteration 6176, epoch 13, batch 204/481,disc_loss 76.313, (real 78.205, fake 74.421 ) gen_loss 852.28\n",
            "iteration 6177, epoch 13, batch 205/481,disc_loss 77.662, (real 80.106, fake 75.217 ) gen_loss 959.45\n",
            "iteration 6178, epoch 13, batch 206/481,disc_loss 78.021, (real 80.553, fake 75.489 ) gen_loss 923.8\n",
            "iteration 6179, epoch 13, batch 207/481,disc_loss 77.599, (real 79.946, fake 75.251 ) gen_loss 842.08\n",
            "iteration 6180, epoch 13, batch 208/481,disc_loss 75.171, (real 78.438, fake 71.904 ) gen_loss 847.0\n",
            "iteration 6181, epoch 13, batch 209/481,disc_loss 82.166, (real 84.453, fake 79.88 ) gen_loss 923.2\n",
            "iteration 6182, epoch 13, batch 210/481,disc_loss 72.154, (real 74.813, fake 69.496 ) gen_loss 856.62\n",
            "iteration 6183, epoch 13, batch 211/481,disc_loss 76.911, (real 79.205, fake 74.617 ) gen_loss 869.89\n",
            "iteration 6184, epoch 13, batch 212/481,disc_loss 82.457, (real 84.557, fake 80.358 ) gen_loss 945.14\n",
            "iteration 6185, epoch 13, batch 213/481,disc_loss 77.053, (real 79.262, fake 74.844 ) gen_loss 873.27\n",
            "iteration 6186, epoch 13, batch 214/481,disc_loss 75.98, (real 78.724, fake 73.236 ) gen_loss 914.11\n",
            "iteration 6187, epoch 13, batch 215/481,disc_loss 76.543, (real 79.458, fake 73.628 ) gen_loss 863.63\n",
            "iteration 6188, epoch 13, batch 216/481,disc_loss 76.447, (real 79.921, fake 72.974 ) gen_loss 861.42\n",
            "iteration 6189, epoch 13, batch 217/481,disc_loss 75.288, (real 77.679, fake 72.896 ) gen_loss 860.45\n",
            "iteration 6190, epoch 13, batch 218/481,disc_loss 76.895, (real 80.017, fake 73.772 ) gen_loss 755.5\n",
            "iteration 6191, epoch 13, batch 219/481,disc_loss 76.024, (real 78.399, fake 73.649 ) gen_loss 1138.4\n",
            "iteration 6192, epoch 13, batch 220/481,disc_loss 79.262, (real 82.356, fake 76.169 ) gen_loss 1057.0\n",
            "iteration 6193, epoch 13, batch 221/481,disc_loss 80.532, (real 82.052, fake 79.013 ) gen_loss 1100.0\n",
            "iteration 6194, epoch 13, batch 222/481,disc_loss 78.321, (real 80.515, fake 76.126 ) gen_loss 865.5\n",
            "iteration 6195, epoch 13, batch 223/481,disc_loss 81.613, (real 84.378, fake 78.847 ) gen_loss 937.9\n",
            "iteration 6196, epoch 13, batch 224/481,disc_loss 78.47, (real 80.62, fake 76.32 ) gen_loss 864.37\n",
            "iteration 6197, epoch 13, batch 225/481,disc_loss 73.465, (real 76.166, fake 70.764 ) gen_loss 763.42\n",
            "iteration 6198, epoch 13, batch 226/481,disc_loss 78.129, (real 80.467, fake 75.791 ) gen_loss 885.55\n",
            "iteration 6199, epoch 13, batch 227/481,disc_loss 79.351, (real 82.268, fake 76.434 ) gen_loss 929.99\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 6200, epoch 13, batch 228/481,disc_loss 75.299, (real 77.596, fake 73.003 ) gen_loss 760.69\n",
            "iteration 6201, epoch 13, batch 229/481,disc_loss 80.575, (real 83.137, fake 78.013 ) gen_loss 760.38\n",
            "iteration 6202, epoch 13, batch 230/481,disc_loss 80.64, (real 83.202, fake 78.079 ) gen_loss 772.13\n",
            "iteration 6203, epoch 13, batch 231/481,disc_loss 79.151, (real 82.07, fake 76.233 ) gen_loss 859.24\n",
            "iteration 6204, epoch 13, batch 232/481,disc_loss 76.499, (real 78.882, fake 74.115 ) gen_loss 944.16\n",
            "iteration 6205, epoch 13, batch 233/481,disc_loss 72.503, (real 75.085, fake 69.92 ) gen_loss 841.73\n",
            "iteration 6206, epoch 13, batch 234/481,disc_loss 78.075, (real 80.208, fake 75.942 ) gen_loss 777.05\n",
            "iteration 6207, epoch 13, batch 235/481,disc_loss 76.998, (real 79.332, fake 74.664 ) gen_loss 940.25\n",
            "iteration 6208, epoch 13, batch 236/481,disc_loss 76.65, (real 78.83, fake 74.47 ) gen_loss 946.3\n",
            "iteration 6209, epoch 13, batch 237/481,disc_loss 75.609, (real 77.553, fake 73.664 ) gen_loss 934.74\n",
            "iteration 6210, epoch 13, batch 238/481,disc_loss 77.169, (real 80.936, fake 73.402 ) gen_loss 876.99\n",
            "iteration 6211, epoch 13, batch 239/481,disc_loss 78.31, (real 81.301, fake 75.319 ) gen_loss 826.52\n",
            "iteration 6212, epoch 13, batch 240/481,disc_loss 81.058, (real 83.398, fake 78.717 ) gen_loss 836.38\n",
            "iteration 6213, epoch 13, batch 241/481,disc_loss 76.846, (real 79.397, fake 74.296 ) gen_loss 760.76\n",
            "iteration 6214, epoch 13, batch 242/481,disc_loss 74.601, (real 77.461, fake 71.741 ) gen_loss 818.3\n",
            "iteration 6215, epoch 13, batch 243/481,disc_loss 72.996, (real 76.523, fake 69.47 ) gen_loss 839.63\n",
            "iteration 6216, epoch 13, batch 244/481,disc_loss 73.623, (real 76.417, fake 70.829 ) gen_loss 976.22\n",
            "iteration 6217, epoch 13, batch 245/481,disc_loss 77.181, (real 79.275, fake 75.087 ) gen_loss 991.72\n",
            "iteration 6218, epoch 13, batch 246/481,disc_loss 76.147, (real 78.257, fake 74.036 ) gen_loss 951.18\n",
            "iteration 6219, epoch 13, batch 247/481,disc_loss 75.83, (real 78.55, fake 73.109 ) gen_loss 847.31\n",
            "iteration 6220, epoch 13, batch 248/481,disc_loss 80.891, (real 83.514, fake 78.269 ) gen_loss 770.42\n",
            "iteration 6221, epoch 13, batch 249/481,disc_loss 78.712, (real 80.983, fake 76.441 ) gen_loss 743.79\n",
            "iteration 6222, epoch 13, batch 250/481,disc_loss 77.329, (real 80.67, fake 73.987 ) gen_loss 799.79\n",
            "iteration 6223, epoch 13, batch 251/481,disc_loss 74.502, (real 77.529, fake 71.474 ) gen_loss 871.61\n",
            "iteration 6224, epoch 13, batch 252/481,disc_loss 80.994, (real 83.452, fake 78.536 ) gen_loss 658.65\n",
            "iteration 6225, epoch 13, batch 253/481,disc_loss 77.577, (real 79.387, fake 75.766 ) gen_loss 786.62\n",
            "iteration 6226, epoch 13, batch 254/481,disc_loss 81.166, (real 83.282, fake 79.051 ) gen_loss 820.71\n",
            "iteration 6227, epoch 13, batch 255/481,disc_loss 75.936, (real 78.812, fake 73.06 ) gen_loss 803.85\n",
            "iteration 6228, epoch 13, batch 256/481,disc_loss 74.163, (real 77.301, fake 71.025 ) gen_loss 870.48\n",
            "iteration 6229, epoch 13, batch 257/481,disc_loss 75.964, (real 79.978, fake 71.95 ) gen_loss 824.49\n",
            "iteration 6230, epoch 13, batch 258/481,disc_loss 75.701, (real 78.586, fake 72.816 ) gen_loss 818.82\n",
            "iteration 6231, epoch 13, batch 259/481,disc_loss 78.398, (real 80.863, fake 75.933 ) gen_loss 784.98\n",
            "iteration 6232, epoch 13, batch 260/481,disc_loss 81.191, (real 83.783, fake 78.599 ) gen_loss 807.02\n",
            "iteration 6233, epoch 13, batch 261/481,disc_loss 73.252, (real 75.416, fake 71.088 ) gen_loss 890.48\n",
            "iteration 6234, epoch 13, batch 262/481,disc_loss 79.924, (real 82.664, fake 77.184 ) gen_loss 936.24\n",
            "iteration 6235, epoch 13, batch 263/481,disc_loss 81.795, (real 84.622, fake 78.968 ) gen_loss 838.81\n",
            "iteration 6236, epoch 13, batch 264/481,disc_loss 76.452, (real 79.539, fake 73.365 ) gen_loss 899.89\n",
            "iteration 6237, epoch 13, batch 265/481,disc_loss 78.376, (real 81.403, fake 75.35 ) gen_loss 806.74\n",
            "iteration 6238, epoch 13, batch 266/481,disc_loss 76.596, (real 79.289, fake 73.904 ) gen_loss 878.47\n",
            "iteration 6239, epoch 13, batch 267/481,disc_loss 74.986, (real 77.2, fake 72.772 ) gen_loss 903.84\n",
            "iteration 6240, epoch 13, batch 268/481,disc_loss 78.732, (real 82.195, fake 75.27 ) gen_loss 856.89\n",
            "iteration 6241, epoch 13, batch 269/481,disc_loss 78.068, (real 81.726, fake 74.41 ) gen_loss 887.98\n",
            "iteration 6242, epoch 13, batch 270/481,disc_loss 77.147, (real 79.238, fake 75.055 ) gen_loss 992.45\n",
            "iteration 6243, epoch 13, batch 271/481,disc_loss 81.325, (real 84.062, fake 78.588 ) gen_loss 913.71\n",
            "iteration 6244, epoch 13, batch 272/481,disc_loss 76.112, (real 79.273, fake 72.95 ) gen_loss 756.5\n",
            "iteration 6245, epoch 13, batch 273/481,disc_loss 78.691, (real 81.726, fake 75.655 ) gen_loss 803.73\n",
            "iteration 6246, epoch 13, batch 274/481,disc_loss 77.979, (real 80.773, fake 75.185 ) gen_loss 803.63\n",
            "iteration 6247, epoch 13, batch 275/481,disc_loss 76.027, (real 78.244, fake 73.811 ) gen_loss 899.99\n",
            "iteration 6248, epoch 13, batch 276/481,disc_loss 77.12, (real 80.265, fake 73.974 ) gen_loss 995.44\n",
            "iteration 6249, epoch 13, batch 277/481,disc_loss 79.075, (real 81.691, fake 76.458 ) gen_loss 1055.2\n",
            "iteration 6250, epoch 13, batch 278/481,disc_loss 72.174, (real 74.657, fake 69.692 ) gen_loss 965.97\n",
            "iteration 6251, epoch 13, batch 279/481,disc_loss 78.574, (real 80.595, fake 76.552 ) gen_loss 1050.0\n",
            "iteration 6252, epoch 13, batch 280/481,disc_loss 79.522, (real 82.317, fake 76.726 ) gen_loss 994.13\n",
            "iteration 6253, epoch 13, batch 281/481,disc_loss 77.154, (real 79.729, fake 74.579 ) gen_loss 858.7\n",
            "iteration 6254, epoch 13, batch 282/481,disc_loss 80.95, (real 83.796, fake 78.104 ) gen_loss 799.63\n",
            "iteration 6255, epoch 13, batch 283/481,disc_loss 79.561, (real 82.223, fake 76.899 ) gen_loss 772.48\n",
            "iteration 6256, epoch 13, batch 284/481,disc_loss 74.622, (real 76.912, fake 72.332 ) gen_loss 859.65\n",
            "iteration 6257, epoch 13, batch 285/481,disc_loss 79.668, (real 82.614, fake 76.722 ) gen_loss 875.94\n",
            "iteration 6258, epoch 13, batch 286/481,disc_loss 73.021, (real 75.937, fake 70.105 ) gen_loss 892.96\n",
            "iteration 6259, epoch 13, batch 287/481,disc_loss 76.095, (real 78.822, fake 73.368 ) gen_loss 805.52\n",
            "iteration 6260, epoch 13, batch 288/481,disc_loss 73.842, (real 76.235, fake 71.449 ) gen_loss 825.13\n",
            "iteration 6261, epoch 13, batch 289/481,disc_loss 75.368, (real 78.581, fake 72.155 ) gen_loss 830.0\n",
            "iteration 6262, epoch 13, batch 290/481,disc_loss 76.287, (real 78.539, fake 74.036 ) gen_loss 866.09\n",
            "iteration 6263, epoch 13, batch 291/481,disc_loss 76.749, (real 78.769, fake 74.729 ) gen_loss 974.05\n",
            "iteration 6264, epoch 13, batch 292/481,disc_loss 78.955, (real 82.301, fake 75.609 ) gen_loss 875.32\n",
            "iteration 6265, epoch 13, batch 293/481,disc_loss 76.943, (real 79.687, fake 74.198 ) gen_loss 920.58\n",
            "iteration 6266, epoch 13, batch 294/481,disc_loss 75.782, (real 79.405, fake 72.16 ) gen_loss 865.82\n",
            "iteration 6267, epoch 13, batch 295/481,disc_loss 75.899, (real 79.246, fake 72.553 ) gen_loss 839.39\n",
            "iteration 6268, epoch 13, batch 296/481,disc_loss 75.249, (real 77.492, fake 73.006 ) gen_loss 871.22\n",
            "iteration 6269, epoch 13, batch 297/481,disc_loss 78.917, (real 82.24, fake 75.595 ) gen_loss 890.73\n",
            "iteration 6270, epoch 13, batch 298/481,disc_loss 77.699, (real 80.51, fake 74.887 ) gen_loss 898.49\n",
            "iteration 6271, epoch 13, batch 299/481,disc_loss 77.571, (real 79.659, fake 75.483 ) gen_loss 885.18\n",
            "iteration 6272, epoch 13, batch 300/481,disc_loss 82.176, (real 86.095, fake 78.257 ) gen_loss 938.64\n",
            "iteration 6273, epoch 13, batch 301/481,disc_loss 77.663, (real 80.355, fake 74.97 ) gen_loss 973.75\n",
            "iteration 6274, epoch 13, batch 302/481,disc_loss 76.843, (real 80.022, fake 73.664 ) gen_loss 840.2\n",
            "iteration 6275, epoch 13, batch 303/481,disc_loss 75.807, (real 78.156, fake 73.458 ) gen_loss 858.24\n",
            "iteration 6276, epoch 13, batch 304/481,disc_loss 80.283, (real 82.655, fake 77.91 ) gen_loss 853.19\n",
            "iteration 6277, epoch 13, batch 305/481,disc_loss 77.277, (real 79.844, fake 74.71 ) gen_loss 811.82\n",
            "iteration 6278, epoch 13, batch 306/481,disc_loss 76.505, (real 78.446, fake 74.564 ) gen_loss 982.8\n",
            "iteration 6279, epoch 13, batch 307/481,disc_loss 77.617, (real 80.293, fake 74.941 ) gen_loss 901.19\n",
            "iteration 6280, epoch 13, batch 308/481,disc_loss 76.627, (real 78.957, fake 74.297 ) gen_loss 869.98\n",
            "iteration 6281, epoch 13, batch 309/481,disc_loss 79.11, (real 81.645, fake 76.575 ) gen_loss 915.33\n",
            "iteration 6282, epoch 13, batch 310/481,disc_loss 79.072, (real 82.239, fake 75.906 ) gen_loss 891.24\n",
            "iteration 6283, epoch 13, batch 311/481,disc_loss 74.618, (real 77.288, fake 71.949 ) gen_loss 874.3\n",
            "iteration 6284, epoch 13, batch 312/481,disc_loss 77.674, (real 80.065, fake 75.282 ) gen_loss 786.38\n",
            "iteration 6285, epoch 13, batch 313/481,disc_loss 75.661, (real 78.449, fake 72.872 ) gen_loss 814.77\n",
            "iteration 6286, epoch 13, batch 314/481,disc_loss 78.516, (real 80.736, fake 76.295 ) gen_loss 934.37\n",
            "iteration 6287, epoch 13, batch 315/481,disc_loss 82.292, (real 84.42, fake 80.163 ) gen_loss 928.44\n",
            "iteration 6288, epoch 13, batch 316/481,disc_loss 74.874, (real 77.467, fake 72.28 ) gen_loss 941.98\n",
            "iteration 6289, epoch 13, batch 317/481,disc_loss 78.345, (real 81.432, fake 75.258 ) gen_loss 890.88\n",
            "iteration 6290, epoch 13, batch 318/481,disc_loss 81.91, (real 85.438, fake 78.383 ) gen_loss 911.79\n",
            "iteration 6291, epoch 13, batch 319/481,disc_loss 72.34, (real 75.05, fake 69.63 ) gen_loss 855.65\n",
            "iteration 6292, epoch 13, batch 320/481,disc_loss 78.55, (real 81.281, fake 75.818 ) gen_loss 989.22\n",
            "iteration 6293, epoch 13, batch 321/481,disc_loss 79.815, (real 82.366, fake 77.264 ) gen_loss 898.01\n",
            "iteration 6294, epoch 13, batch 322/481,disc_loss 80.006, (real 83.513, fake 76.5 ) gen_loss 856.58\n",
            "iteration 6295, epoch 13, batch 323/481,disc_loss 80.75, (real 83.343, fake 78.157 ) gen_loss 888.0\n",
            "iteration 6296, epoch 13, batch 324/481,disc_loss 82.026, (real 84.912, fake 79.139 ) gen_loss 906.78\n",
            "iteration 6297, epoch 13, batch 325/481,disc_loss 75.664, (real 78.152, fake 73.176 ) gen_loss 908.22\n",
            "iteration 6298, epoch 13, batch 326/481,disc_loss 78.251, (real 80.4, fake 76.103 ) gen_loss 865.55\n",
            "iteration 6299, epoch 13, batch 327/481,disc_loss 77.147, (real 80.445, fake 73.849 ) gen_loss 821.4\n",
            "iteration 6300, epoch 13, batch 328/481,disc_loss 76.492, (real 79.123, fake 73.861 ) gen_loss 915.28\n",
            "iteration 6301, epoch 13, batch 329/481,disc_loss 78.266, (real 81.042, fake 75.491 ) gen_loss 955.03\n",
            "iteration 6302, epoch 13, batch 330/481,disc_loss 75.546, (real 77.842, fake 73.25 ) gen_loss 841.78\n",
            "iteration 6303, epoch 13, batch 331/481,disc_loss 78.236, (real 80.868, fake 75.604 ) gen_loss 864.68\n",
            "iteration 6304, epoch 13, batch 332/481,disc_loss 76.982, (real 78.941, fake 75.023 ) gen_loss 905.33\n",
            "iteration 6305, epoch 13, batch 333/481,disc_loss 75.333, (real 78.038, fake 72.628 ) gen_loss 832.95\n",
            "iteration 6306, epoch 13, batch 334/481,disc_loss 81.632, (real 84.476, fake 78.787 ) gen_loss 831.63\n",
            "iteration 6307, epoch 13, batch 335/481,disc_loss 79.808, (real 82.439, fake 77.176 ) gen_loss 813.84\n",
            "iteration 6308, epoch 13, batch 336/481,disc_loss 81.134, (real 84.256, fake 78.013 ) gen_loss 924.19\n",
            "iteration 6309, epoch 13, batch 337/481,disc_loss 79.057, (real 82.095, fake 76.018 ) gen_loss 829.64\n",
            "iteration 6310, epoch 13, batch 338/481,disc_loss 77.136, (real 79.545, fake 74.726 ) gen_loss 847.7\n",
            "iteration 6311, epoch 13, batch 339/481,disc_loss 75.244, (real 78.209, fake 72.279 ) gen_loss 866.54\n",
            "iteration 6312, epoch 13, batch 340/481,disc_loss 77.648, (real 80.684, fake 74.611 ) gen_loss 859.02\n",
            "iteration 6313, epoch 13, batch 341/481,disc_loss 75.105, (real 77.583, fake 72.627 ) gen_loss 875.76\n",
            "iteration 6314, epoch 13, batch 342/481,disc_loss 76.074, (real 78.543, fake 73.605 ) gen_loss 889.05\n",
            "iteration 6315, epoch 13, batch 343/481,disc_loss 78.798, (real 81.402, fake 76.194 ) gen_loss 878.28\n",
            "iteration 6316, epoch 13, batch 344/481,disc_loss 76.661, (real 79.0, fake 74.321 ) gen_loss 897.75\n",
            "iteration 6317, epoch 13, batch 345/481,disc_loss 77.623, (real 79.895, fake 75.351 ) gen_loss 828.16\n",
            "iteration 6318, epoch 13, batch 346/481,disc_loss 73.765, (real 76.543, fake 70.987 ) gen_loss 926.94\n",
            "iteration 6319, epoch 13, batch 347/481,disc_loss 75.684, (real 78.215, fake 73.153 ) gen_loss 851.4\n",
            "iteration 6320, epoch 13, batch 348/481,disc_loss 79.033, (real 81.9, fake 76.165 ) gen_loss 860.51\n",
            "iteration 6321, epoch 13, batch 349/481,disc_loss 75.965, (real 78.801, fake 73.129 ) gen_loss 845.18\n",
            "iteration 6322, epoch 13, batch 350/481,disc_loss 77.873, (real 80.815, fake 74.931 ) gen_loss 866.59\n",
            "iteration 6323, epoch 13, batch 351/481,disc_loss 79.647, (real 82.133, fake 77.161 ) gen_loss 815.69\n",
            "iteration 6324, epoch 13, batch 352/481,disc_loss 77.166, (real 79.317, fake 75.014 ) gen_loss 884.39\n",
            "iteration 6325, epoch 13, batch 353/481,disc_loss 80.152, (real 82.393, fake 77.91 ) gen_loss 908.62\n",
            "iteration 6326, epoch 13, batch 354/481,disc_loss 77.532, (real 80.409, fake 74.656 ) gen_loss 887.28\n",
            "iteration 6327, epoch 13, batch 355/481,disc_loss 78.802, (real 81.7, fake 75.904 ) gen_loss 918.27\n",
            "iteration 6328, epoch 13, batch 356/481,disc_loss 76.383, (real 79.457, fake 73.309 ) gen_loss 1003.9\n",
            "iteration 6329, epoch 13, batch 357/481,disc_loss 76.7, (real 79.308, fake 74.092 ) gen_loss 975.66\n",
            "iteration 6330, epoch 13, batch 358/481,disc_loss 72.227, (real 74.971, fake 69.484 ) gen_loss 912.36\n",
            "iteration 6331, epoch 13, batch 359/481,disc_loss 76.753, (real 78.784, fake 74.723 ) gen_loss 833.81\n",
            "iteration 6332, epoch 13, batch 360/481,disc_loss 79.203, (real 81.4, fake 77.005 ) gen_loss 904.71\n",
            "iteration 6333, epoch 13, batch 361/481,disc_loss 75.252, (real 78.315, fake 72.189 ) gen_loss 878.35\n",
            "iteration 6334, epoch 13, batch 362/481,disc_loss 77.846, (real 81.286, fake 74.406 ) gen_loss 857.53\n",
            "iteration 6335, epoch 13, batch 363/481,disc_loss 76.986, (real 79.256, fake 74.716 ) gen_loss 777.68\n",
            "iteration 6336, epoch 13, batch 364/481,disc_loss 80.261, (real 83.546, fake 76.976 ) gen_loss 880.78\n",
            "iteration 6337, epoch 13, batch 365/481,disc_loss 77.938, (real 80.666, fake 75.211 ) gen_loss 794.19\n",
            "iteration 6338, epoch 13, batch 366/481,disc_loss 74.642, (real 77.684, fake 71.6 ) gen_loss 760.63\n",
            "iteration 6339, epoch 13, batch 367/481,disc_loss 81.144, (real 83.695, fake 78.594 ) gen_loss 916.02\n",
            "iteration 6340, epoch 13, batch 368/481,disc_loss 72.38, (real 75.508, fake 69.251 ) gen_loss 1019.5\n",
            "iteration 6341, epoch 13, batch 369/481,disc_loss 74.499, (real 77.057, fake 71.941 ) gen_loss 939.79\n",
            "iteration 6342, epoch 13, batch 370/481,disc_loss 77.347, (real 79.867, fake 74.826 ) gen_loss 980.89\n",
            "iteration 6343, epoch 13, batch 371/481,disc_loss 78.022, (real 81.075, fake 74.969 ) gen_loss 981.68\n",
            "iteration 6344, epoch 13, batch 372/481,disc_loss 77.968, (real 80.399, fake 75.537 ) gen_loss 883.22\n",
            "iteration 6345, epoch 13, batch 373/481,disc_loss 74.186, (real 76.105, fake 72.268 ) gen_loss 844.97\n",
            "iteration 6346, epoch 13, batch 374/481,disc_loss 76.786, (real 79.412, fake 74.159 ) gen_loss 887.48\n",
            "iteration 6347, epoch 13, batch 375/481,disc_loss 71.731, (real 73.893, fake 69.57 ) gen_loss 990.28\n",
            "iteration 6348, epoch 13, batch 376/481,disc_loss 76.421, (real 79.93, fake 72.912 ) gen_loss 823.08\n",
            "iteration 6349, epoch 13, batch 377/481,disc_loss 73.653, (real 76.78, fake 70.525 ) gen_loss 961.48\n",
            "iteration 6350, epoch 13, batch 378/481,disc_loss 77.295, (real 79.834, fake 74.756 ) gen_loss 854.88\n",
            "iteration 6351, epoch 13, batch 379/481,disc_loss 77.109, (real 79.388, fake 74.83 ) gen_loss 865.32\n",
            "iteration 6352, epoch 13, batch 380/481,disc_loss 75.097, (real 77.905, fake 72.288 ) gen_loss 894.43\n",
            "iteration 6353, epoch 13, batch 381/481,disc_loss 74.401, (real 77.573, fake 71.228 ) gen_loss 777.05\n",
            "iteration 6354, epoch 13, batch 382/481,disc_loss 72.674, (real 75.022, fake 70.327 ) gen_loss 834.87\n",
            "iteration 6355, epoch 13, batch 383/481,disc_loss 79.75, (real 82.761, fake 76.738 ) gen_loss 779.65\n",
            "iteration 6356, epoch 13, batch 384/481,disc_loss 78.403, (real 81.637, fake 75.169 ) gen_loss 749.0\n",
            "iteration 6357, epoch 13, batch 385/481,disc_loss 75.492, (real 78.279, fake 72.705 ) gen_loss 850.77\n",
            "iteration 6358, epoch 13, batch 386/481,disc_loss 77.367, (real 79.622, fake 75.112 ) gen_loss 729.66\n",
            "iteration 6359, epoch 13, batch 387/481,disc_loss 79.52, (real 81.943, fake 77.098 ) gen_loss 777.01\n",
            "iteration 6360, epoch 13, batch 388/481,disc_loss 76.444, (real 78.931, fake 73.956 ) gen_loss 932.37\n",
            "iteration 6361, epoch 13, batch 389/481,disc_loss 76.509, (real 79.428, fake 73.589 ) gen_loss 813.02\n",
            "iteration 6362, epoch 13, batch 390/481,disc_loss 76.875, (real 78.257, fake 75.492 ) gen_loss 890.72\n",
            "iteration 6363, epoch 13, batch 391/481,disc_loss 76.049, (real 78.519, fake 73.578 ) gen_loss 925.94\n",
            "iteration 6364, epoch 13, batch 392/481,disc_loss 80.854, (real 84.221, fake 77.487 ) gen_loss 921.93\n",
            "iteration 6365, epoch 13, batch 393/481,disc_loss 80.287, (real 82.788, fake 77.785 ) gen_loss 840.41\n",
            "iteration 6366, epoch 13, batch 394/481,disc_loss 78.485, (real 81.032, fake 75.938 ) gen_loss 742.67\n",
            "iteration 6367, epoch 13, batch 395/481,disc_loss 77.945, (real 79.978, fake 75.912 ) gen_loss 890.31\n",
            "iteration 6368, epoch 13, batch 396/481,disc_loss 80.299, (real 82.003, fake 78.595 ) gen_loss 861.78\n",
            "iteration 6369, epoch 13, batch 397/481,disc_loss 73.723, (real 76.375, fake 71.072 ) gen_loss 818.33\n",
            "iteration 6370, epoch 13, batch 398/481,disc_loss 81.789, (real 84.233, fake 79.344 ) gen_loss 847.45\n",
            "iteration 6371, epoch 13, batch 399/481,disc_loss 78.229, (real 81.007, fake 75.452 ) gen_loss 920.65\n",
            "iteration 6372, epoch 13, batch 400/481,disc_loss 76.06, (real 79.317, fake 72.804 ) gen_loss 893.12\n",
            "iteration 6373, epoch 13, batch 401/481,disc_loss 75.224, (real 77.713, fake 72.735 ) gen_loss 873.7\n",
            "iteration 6374, epoch 13, batch 402/481,disc_loss 79.448, (real 81.83, fake 77.067 ) gen_loss 875.71\n",
            "iteration 6375, epoch 13, batch 403/481,disc_loss 80.775, (real 83.147, fake 78.404 ) gen_loss 973.11\n",
            "iteration 6376, epoch 13, batch 404/481,disc_loss 76.14, (real 79.294, fake 72.987 ) gen_loss 863.22\n",
            "iteration 6377, epoch 13, batch 405/481,disc_loss 78.692, (real 81.027, fake 76.358 ) gen_loss 1008.0\n",
            "iteration 6378, epoch 13, batch 406/481,disc_loss 76.557, (real 79.949, fake 73.164 ) gen_loss 1007.5\n",
            "iteration 6379, epoch 13, batch 407/481,disc_loss 72.969, (real 76.534, fake 69.404 ) gen_loss 855.25\n",
            "iteration 6380, epoch 13, batch 408/481,disc_loss 78.789, (real 80.715, fake 76.863 ) gen_loss 755.57\n",
            "iteration 6381, epoch 13, batch 409/481,disc_loss 71.414, (real 73.546, fake 69.282 ) gen_loss 758.17\n",
            "iteration 6382, epoch 13, batch 410/481,disc_loss 76.915, (real 79.689, fake 74.141 ) gen_loss 860.6\n",
            "iteration 6383, epoch 13, batch 411/481,disc_loss 78.303, (real 81.526, fake 75.08 ) gen_loss 990.27\n",
            "iteration 6384, epoch 13, batch 412/481,disc_loss 77.877, (real 80.377, fake 75.377 ) gen_loss 902.28\n",
            "iteration 6385, epoch 13, batch 413/481,disc_loss 81.058, (real 84.58, fake 77.536 ) gen_loss 856.2\n",
            "iteration 6386, epoch 13, batch 414/481,disc_loss 75.097, (real 77.691, fake 72.504 ) gen_loss 810.05\n",
            "iteration 6387, epoch 13, batch 415/481,disc_loss 76.54, (real 79.076, fake 74.004 ) gen_loss 807.9\n",
            "iteration 6388, epoch 13, batch 416/481,disc_loss 75.627, (real 78.444, fake 72.809 ) gen_loss 856.26\n",
            "iteration 6389, epoch 13, batch 417/481,disc_loss 78.628, (real 81.084, fake 76.172 ) gen_loss 772.27\n",
            "iteration 6390, epoch 13, batch 418/481,disc_loss 74.388, (real 76.349, fake 72.427 ) gen_loss 854.7\n",
            "iteration 6391, epoch 13, batch 419/481,disc_loss 79.065, (real 80.611, fake 77.518 ) gen_loss 778.74\n",
            "iteration 6392, epoch 13, batch 420/481,disc_loss 77.068, (real 79.521, fake 74.616 ) gen_loss 817.08\n",
            "iteration 6393, epoch 13, batch 421/481,disc_loss 78.838, (real 81.215, fake 76.462 ) gen_loss 924.05\n",
            "iteration 6394, epoch 13, batch 422/481,disc_loss 77.109, (real 79.875, fake 74.343 ) gen_loss 944.98\n",
            "iteration 6395, epoch 13, batch 423/481,disc_loss 78.409, (real 81.595, fake 75.223 ) gen_loss 898.66\n",
            "iteration 6396, epoch 13, batch 424/481,disc_loss 78.309, (real 80.838, fake 75.781 ) gen_loss 919.66\n",
            "iteration 6397, epoch 13, batch 425/481,disc_loss 75.586, (real 78.217, fake 72.954 ) gen_loss 1043.0\n",
            "iteration 6398, epoch 13, batch 426/481,disc_loss 78.265, (real 80.511, fake 76.018 ) gen_loss 929.24\n",
            "iteration 6399, epoch 13, batch 427/481,disc_loss 73.241, (real 75.718, fake 70.764 ) gen_loss 902.47\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 6400, epoch 13, batch 428/481,disc_loss 75.055, (real 77.323, fake 72.786 ) gen_loss 980.55\n",
            "iteration 6401, epoch 13, batch 429/481,disc_loss 78.095, (real 80.42, fake 75.77 ) gen_loss 773.21\n",
            "iteration 6402, epoch 13, batch 430/481,disc_loss 75.202, (real 77.165, fake 73.239 ) gen_loss 755.49\n",
            "iteration 6403, epoch 13, batch 431/481,disc_loss 80.858, (real 83.91, fake 77.805 ) gen_loss 806.76\n",
            "iteration 6404, epoch 13, batch 432/481,disc_loss 77.786, (real 80.958, fake 74.615 ) gen_loss 874.57\n",
            "iteration 6405, epoch 13, batch 433/481,disc_loss 72.976, (real 75.777, fake 70.175 ) gen_loss 852.57\n",
            "iteration 6406, epoch 13, batch 434/481,disc_loss 74.32, (real 77.732, fake 70.908 ) gen_loss 1010.7\n",
            "iteration 6407, epoch 13, batch 435/481,disc_loss 81.09, (real 83.477, fake 78.704 ) gen_loss 1026.9\n",
            "iteration 6408, epoch 13, batch 436/481,disc_loss 75.058, (real 77.56, fake 72.556 ) gen_loss 915.24\n",
            "iteration 6409, epoch 13, batch 437/481,disc_loss 77.796, (real 80.55, fake 75.042 ) gen_loss 927.68\n",
            "iteration 6410, epoch 13, batch 438/481,disc_loss 78.601, (real 81.008, fake 76.194 ) gen_loss 924.57\n",
            "iteration 6411, epoch 13, batch 439/481,disc_loss 77.061, (real 79.331, fake 74.79 ) gen_loss 884.14\n",
            "iteration 6412, epoch 13, batch 440/481,disc_loss 77.199, (real 79.558, fake 74.839 ) gen_loss 866.9\n",
            "iteration 6413, epoch 13, batch 441/481,disc_loss 81.088, (real 83.411, fake 78.766 ) gen_loss 836.92\n",
            "iteration 6414, epoch 13, batch 442/481,disc_loss 80.471, (real 83.391, fake 77.551 ) gen_loss 819.77\n",
            "iteration 6415, epoch 13, batch 443/481,disc_loss 75.891, (real 78.7, fake 73.081 ) gen_loss 833.19\n",
            "iteration 6416, epoch 13, batch 444/481,disc_loss 78.821, (real 80.951, fake 76.691 ) gen_loss 882.06\n",
            "iteration 6417, epoch 13, batch 445/481,disc_loss 77.827, (real 80.176, fake 75.478 ) gen_loss 881.87\n",
            "iteration 6418, epoch 13, batch 446/481,disc_loss 78.149, (real 80.516, fake 75.782 ) gen_loss 885.95\n",
            "iteration 6419, epoch 13, batch 447/481,disc_loss 72.972, (real 75.54, fake 70.403 ) gen_loss 995.92\n",
            "iteration 6420, epoch 13, batch 448/481,disc_loss 81.761, (real 84.593, fake 78.929 ) gen_loss 736.76\n",
            "iteration 6421, epoch 13, batch 449/481,disc_loss 80.904, (real 84.395, fake 77.413 ) gen_loss 771.21\n",
            "iteration 6422, epoch 13, batch 450/481,disc_loss 76.618, (real 79.493, fake 73.743 ) gen_loss 927.93\n",
            "iteration 6423, epoch 13, batch 451/481,disc_loss 79.844, (real 82.755, fake 76.932 ) gen_loss 873.15\n",
            "iteration 6424, epoch 13, batch 452/481,disc_loss 82.105, (real 84.309, fake 79.901 ) gen_loss 737.86\n",
            "iteration 6425, epoch 13, batch 453/481,disc_loss 74.244, (real 77.14, fake 71.348 ) gen_loss 826.66\n",
            "iteration 6426, epoch 13, batch 454/481,disc_loss 74.68, (real 76.948, fake 72.413 ) gen_loss 828.86\n",
            "iteration 6427, epoch 13, batch 455/481,disc_loss 73.222, (real 75.78, fake 70.664 ) gen_loss 781.7\n",
            "iteration 6428, epoch 13, batch 456/481,disc_loss 76.854, (real 79.235, fake 74.474 ) gen_loss 895.05\n",
            "iteration 6429, epoch 13, batch 457/481,disc_loss 75.156, (real 78.131, fake 72.182 ) gen_loss 864.63\n",
            "iteration 6430, epoch 13, batch 458/481,disc_loss 80.251, (real 82.657, fake 77.845 ) gen_loss 738.69\n",
            "iteration 6431, epoch 13, batch 459/481,disc_loss 76.944, (real 79.625, fake 74.263 ) gen_loss 815.23\n",
            "iteration 6432, epoch 13, batch 460/481,disc_loss 77.833, (real 80.334, fake 75.331 ) gen_loss 937.1\n",
            "iteration 6433, epoch 13, batch 461/481,disc_loss 76.023, (real 78.503, fake 73.543 ) gen_loss 825.11\n",
            "iteration 6434, epoch 13, batch 462/481,disc_loss 77.223, (real 79.202, fake 75.245 ) gen_loss 881.61\n",
            "iteration 6435, epoch 13, batch 463/481,disc_loss 76.48, (real 79.396, fake 73.564 ) gen_loss 809.99\n",
            "iteration 6436, epoch 13, batch 464/481,disc_loss 73.322, (real 75.963, fake 70.68 ) gen_loss 877.96\n",
            "iteration 6437, epoch 13, batch 465/481,disc_loss 77.442, (real 80.395, fake 74.49 ) gen_loss 883.27\n",
            "iteration 6438, epoch 13, batch 466/481,disc_loss 78.184, (real 80.643, fake 75.726 ) gen_loss 942.97\n",
            "iteration 6439, epoch 13, batch 467/481,disc_loss 76.825, (real 79.379, fake 74.271 ) gen_loss 798.79\n",
            "iteration 6440, epoch 13, batch 468/481,disc_loss 74.8, (real 78.207, fake 71.394 ) gen_loss 815.58\n",
            "iteration 6441, epoch 13, batch 469/481,disc_loss 74.924, (real 77.084, fake 72.764 ) gen_loss 812.65\n",
            "iteration 6442, epoch 13, batch 470/481,disc_loss 74.534, (real 77.487, fake 71.58 ) gen_loss 806.37\n",
            "iteration 6443, epoch 13, batch 471/481,disc_loss 78.184, (real 80.957, fake 75.41 ) gen_loss 817.96\n",
            "iteration 6444, epoch 13, batch 472/481,disc_loss 77.207, (real 79.507, fake 74.908 ) gen_loss 812.09\n",
            "iteration 6445, epoch 13, batch 473/481,disc_loss 77.732, (real 80.25, fake 75.214 ) gen_loss 930.36\n",
            "iteration 6446, epoch 13, batch 474/481,disc_loss 77.877, (real 80.11, fake 75.643 ) gen_loss 886.17\n",
            "iteration 6447, epoch 13, batch 475/481,disc_loss 79.114, (real 82.123, fake 76.106 ) gen_loss 759.0\n",
            "iteration 6448, epoch 13, batch 476/481,disc_loss 78.215, (real 80.94, fake 75.49 ) gen_loss 733.87\n",
            "iteration 6449, epoch 13, batch 477/481,disc_loss 80.165, (real 82.631, fake 77.699 ) gen_loss 788.48\n",
            "iteration 6450, epoch 13, batch 478/481,disc_loss 80.585, (real 83.298, fake 77.871 ) gen_loss 929.13\n",
            "iteration 6451, epoch 13, batch 479/481,disc_loss 76.721, (real 79.186, fake 74.257 ) gen_loss 899.51\n",
            "iteration 6452, epoch 13, batch 480/481,disc_loss 76.932, (real 79.065, fake 74.799 ) gen_loss 871.31\n",
            "iteration 6453, epoch 13, batch 481/481,disc_loss 78.932, (real 81.34, fake 76.523 ) gen_loss 919.34\n",
            "iteration 6454, epoch 14, batch 1/481,disc_loss 82.732, (real 85.202, fake 80.261 ) gen_loss 912.07\n",
            "iteration 6455, epoch 14, batch 2/481,disc_loss 77.346, (real 79.218, fake 75.474 ) gen_loss 853.93\n",
            "iteration 6456, epoch 14, batch 3/481,disc_loss 77.485, (real 79.656, fake 75.313 ) gen_loss 832.79\n",
            "iteration 6457, epoch 14, batch 4/481,disc_loss 76.686, (real 78.715, fake 74.656 ) gen_loss 856.92\n",
            "iteration 6458, epoch 14, batch 5/481,disc_loss 76.68, (real 79.353, fake 74.006 ) gen_loss 892.18\n",
            "iteration 6459, epoch 14, batch 6/481,disc_loss 76.405, (real 79.217, fake 73.592 ) gen_loss 815.28\n",
            "iteration 6460, epoch 14, batch 7/481,disc_loss 73.998, (real 75.906, fake 72.09 ) gen_loss 856.4\n",
            "iteration 6461, epoch 14, batch 8/481,disc_loss 79.899, (real 81.784, fake 78.015 ) gen_loss 847.08\n",
            "iteration 6462, epoch 14, batch 9/481,disc_loss 75.255, (real 77.05, fake 73.46 ) gen_loss 881.38\n",
            "iteration 6463, epoch 14, batch 10/481,disc_loss 75.207, (real 77.638, fake 72.776 ) gen_loss 833.75\n",
            "iteration 6464, epoch 14, batch 11/481,disc_loss 75.589, (real 78.023, fake 73.155 ) gen_loss 794.37\n",
            "iteration 6465, epoch 14, batch 12/481,disc_loss 79.169, (real 81.344, fake 76.994 ) gen_loss 932.88\n",
            "iteration 6466, epoch 14, batch 13/481,disc_loss 79.382, (real 81.854, fake 76.911 ) gen_loss 855.73\n",
            "iteration 6467, epoch 14, batch 14/481,disc_loss 77.451, (real 79.33, fake 75.573 ) gen_loss 948.81\n",
            "iteration 6468, epoch 14, batch 15/481,disc_loss 72.333, (real 74.738, fake 69.927 ) gen_loss 798.97\n",
            "iteration 6469, epoch 14, batch 16/481,disc_loss 79.805, (real 82.112, fake 77.499 ) gen_loss 714.24\n",
            "iteration 6470, epoch 14, batch 17/481,disc_loss 81.045, (real 82.75, fake 79.341 ) gen_loss 876.95\n",
            "iteration 6471, epoch 14, batch 18/481,disc_loss 76.988, (real 79.366, fake 74.61 ) gen_loss 837.08\n",
            "iteration 6472, epoch 14, batch 19/481,disc_loss 75.833, (real 78.891, fake 72.775 ) gen_loss 764.9\n",
            "iteration 6473, epoch 14, batch 20/481,disc_loss 76.621, (real 78.624, fake 74.619 ) gen_loss 817.82\n",
            "iteration 6474, epoch 14, batch 21/481,disc_loss 72.798, (real 74.655, fake 70.941 ) gen_loss 839.64\n",
            "iteration 6475, epoch 14, batch 22/481,disc_loss 79.458, (real 82.173, fake 76.743 ) gen_loss 758.96\n",
            "iteration 6476, epoch 14, batch 23/481,disc_loss 78.758, (real 81.417, fake 76.1 ) gen_loss 875.5\n",
            "iteration 6477, epoch 14, batch 24/481,disc_loss 73.7, (real 76.532, fake 70.869 ) gen_loss 889.01\n",
            "iteration 6478, epoch 14, batch 25/481,disc_loss 78.546, (real 81.005, fake 76.088 ) gen_loss 899.79\n",
            "iteration 6479, epoch 14, batch 26/481,disc_loss 76.205, (real 78.825, fake 73.584 ) gen_loss 941.51\n",
            "iteration 6480, epoch 14, batch 27/481,disc_loss 74.775, (real 76.949, fake 72.602 ) gen_loss 913.24\n",
            "iteration 6481, epoch 14, batch 28/481,disc_loss 76.006, (real 77.68, fake 74.332 ) gen_loss 888.56\n",
            "iteration 6482, epoch 14, batch 29/481,disc_loss 78.888, (real 82.137, fake 75.639 ) gen_loss 827.21\n",
            "iteration 6483, epoch 14, batch 30/481,disc_loss 76.896, (real 79.035, fake 74.757 ) gen_loss 881.66\n",
            "iteration 6484, epoch 14, batch 31/481,disc_loss 73.475, (real 75.163, fake 71.786 ) gen_loss 879.63\n",
            "iteration 6485, epoch 14, batch 32/481,disc_loss 76.831, (real 78.769, fake 74.893 ) gen_loss 1019.9\n",
            "iteration 6486, epoch 14, batch 33/481,disc_loss 78.212, (real 80.391, fake 76.033 ) gen_loss 954.89\n",
            "iteration 6487, epoch 14, batch 34/481,disc_loss 76.391, (real 78.552, fake 74.231 ) gen_loss 905.97\n",
            "iteration 6488, epoch 14, batch 35/481,disc_loss 75.09, (real 77.02, fake 73.159 ) gen_loss 978.71\n",
            "iteration 6489, epoch 14, batch 36/481,disc_loss 79.2, (real 82.032, fake 76.367 ) gen_loss 861.14\n",
            "iteration 6490, epoch 14, batch 37/481,disc_loss 77.616, (real 80.204, fake 75.028 ) gen_loss 830.88\n",
            "iteration 6491, epoch 14, batch 38/481,disc_loss 78.184, (real 80.398, fake 75.97 ) gen_loss 838.89\n",
            "iteration 6492, epoch 14, batch 39/481,disc_loss 74.461, (real 77.278, fake 71.644 ) gen_loss 943.45\n",
            "iteration 6493, epoch 14, batch 40/481,disc_loss 77.187, (real 78.918, fake 75.456 ) gen_loss 842.39\n",
            "iteration 6494, epoch 14, batch 41/481,disc_loss 76.441, (real 78.578, fake 74.304 ) gen_loss 810.47\n",
            "iteration 6495, epoch 14, batch 42/481,disc_loss 78.755, (real 81.483, fake 76.027 ) gen_loss 810.64\n",
            "iteration 6496, epoch 14, batch 43/481,disc_loss 77.437, (real 80.314, fake 74.56 ) gen_loss 920.61\n",
            "iteration 6497, epoch 14, batch 44/481,disc_loss 75.649, (real 78.442, fake 72.855 ) gen_loss 869.92\n",
            "iteration 6498, epoch 14, batch 45/481,disc_loss 76.942, (real 78.911, fake 74.973 ) gen_loss 939.04\n",
            "iteration 6499, epoch 14, batch 46/481,disc_loss 75.672, (real 78.399, fake 72.945 ) gen_loss 878.67\n",
            "iteration 6500, epoch 14, batch 47/481,disc_loss 79.688, (real 82.563, fake 76.813 ) gen_loss 885.86\n",
            "iteration 6501, epoch 14, batch 48/481,disc_loss 73.961, (real 75.931, fake 71.991 ) gen_loss 837.79\n",
            "iteration 6502, epoch 14, batch 49/481,disc_loss 81.35, (real 83.429, fake 79.271 ) gen_loss 927.01\n",
            "iteration 6503, epoch 14, batch 50/481,disc_loss 77.969, (real 80.318, fake 75.621 ) gen_loss 802.5\n",
            "iteration 6504, epoch 14, batch 51/481,disc_loss 74.093, (real 76.578, fake 71.608 ) gen_loss 819.76\n",
            "iteration 6505, epoch 14, batch 52/481,disc_loss 76.891, (real 79.29, fake 74.492 ) gen_loss 861.02\n",
            "iteration 6506, epoch 14, batch 53/481,disc_loss 74.09, (real 77.004, fake 71.176 ) gen_loss 738.49\n",
            "iteration 6507, epoch 14, batch 54/481,disc_loss 77.756, (real 80.254, fake 75.258 ) gen_loss 758.66\n",
            "iteration 6508, epoch 14, batch 55/481,disc_loss 73.787, (real 76.664, fake 70.909 ) gen_loss 717.44\n",
            "iteration 6509, epoch 14, batch 56/481,disc_loss 75.096, (real 77.466, fake 72.727 ) gen_loss 867.35\n",
            "iteration 6510, epoch 14, batch 57/481,disc_loss 78.144, (real 80.58, fake 75.707 ) gen_loss 921.02\n",
            "iteration 6511, epoch 14, batch 58/481,disc_loss 80.45, (real 83.775, fake 77.125 ) gen_loss 875.59\n",
            "iteration 6512, epoch 14, batch 59/481,disc_loss 74.858, (real 77.349, fake 72.367 ) gen_loss 875.18\n",
            "iteration 6513, epoch 14, batch 60/481,disc_loss 78.476, (real 80.645, fake 76.308 ) gen_loss 772.16\n",
            "iteration 6514, epoch 14, batch 61/481,disc_loss 76.166, (real 77.784, fake 74.547 ) gen_loss 902.12\n",
            "iteration 6515, epoch 14, batch 62/481,disc_loss 79.408, (real 81.688, fake 77.128 ) gen_loss 789.55\n",
            "iteration 6516, epoch 14, batch 63/481,disc_loss 79.146, (real 81.813, fake 76.479 ) gen_loss 887.95\n",
            "iteration 6517, epoch 14, batch 64/481,disc_loss 78.755, (real 81.257, fake 76.253 ) gen_loss 858.54\n",
            "iteration 6518, epoch 14, batch 65/481,disc_loss 79.21, (real 81.084, fake 77.336 ) gen_loss 807.54\n",
            "iteration 6519, epoch 14, batch 66/481,disc_loss 77.75, (real 80.812, fake 74.689 ) gen_loss 844.97\n",
            "iteration 6520, epoch 14, batch 67/481,disc_loss 72.176, (real 74.982, fake 69.37 ) gen_loss 849.43\n",
            "iteration 6521, epoch 14, batch 68/481,disc_loss 79.027, (real 81.219, fake 76.835 ) gen_loss 896.23\n",
            "iteration 6522, epoch 14, batch 69/481,disc_loss 81.291, (real 83.684, fake 78.898 ) gen_loss 908.3\n",
            "iteration 6523, epoch 14, batch 70/481,disc_loss 78.486, (real 80.956, fake 76.017 ) gen_loss 868.63\n",
            "iteration 6524, epoch 14, batch 71/481,disc_loss 76.671, (real 78.585, fake 74.757 ) gen_loss 908.04\n",
            "iteration 6525, epoch 14, batch 72/481,disc_loss 76.99, (real 78.919, fake 75.061 ) gen_loss 787.71\n",
            "iteration 6526, epoch 14, batch 73/481,disc_loss 77.508, (real 79.479, fake 75.536 ) gen_loss 822.93\n",
            "iteration 6527, epoch 14, batch 74/481,disc_loss 78.87, (real 81.192, fake 76.547 ) gen_loss 747.37\n",
            "iteration 6528, epoch 14, batch 75/481,disc_loss 72.738, (real 75.721, fake 69.755 ) gen_loss 823.63\n",
            "iteration 6529, epoch 14, batch 76/481,disc_loss 76.429, (real 77.918, fake 74.94 ) gen_loss 867.62\n",
            "iteration 6530, epoch 14, batch 77/481,disc_loss 74.744, (real 76.948, fake 72.54 ) gen_loss 849.86\n",
            "iteration 6531, epoch 14, batch 78/481,disc_loss 75.484, (real 77.796, fake 73.173 ) gen_loss 871.54\n",
            "iteration 6532, epoch 14, batch 79/481,disc_loss 74.927, (real 77.614, fake 72.24 ) gen_loss 883.28\n",
            "iteration 6533, epoch 14, batch 80/481,disc_loss 78.59, (real 80.526, fake 76.654 ) gen_loss 835.16\n",
            "iteration 6534, epoch 14, batch 81/481,disc_loss 77.056, (real 79.476, fake 74.635 ) gen_loss 829.26\n",
            "iteration 6535, epoch 14, batch 82/481,disc_loss 79.074, (real 81.027, fake 77.12 ) gen_loss 901.56\n",
            "iteration 6536, epoch 14, batch 83/481,disc_loss 75.759, (real 78.221, fake 73.298 ) gen_loss 910.86\n",
            "iteration 6537, epoch 14, batch 84/481,disc_loss 76.726, (real 79.492, fake 73.959 ) gen_loss 1019.1\n",
            "iteration 6538, epoch 14, batch 85/481,disc_loss 75.214, (real 77.276, fake 73.153 ) gen_loss 879.78\n",
            "iteration 6539, epoch 14, batch 86/481,disc_loss 81.14, (real 83.538, fake 78.742 ) gen_loss 891.38\n",
            "iteration 6540, epoch 14, batch 87/481,disc_loss 75.74, (real 78.299, fake 73.181 ) gen_loss 864.66\n",
            "iteration 6541, epoch 14, batch 88/481,disc_loss 80.21, (real 83.171, fake 77.248 ) gen_loss 807.93\n",
            "iteration 6542, epoch 14, batch 89/481,disc_loss 76.023, (real 78.911, fake 73.134 ) gen_loss 830.61\n",
            "iteration 6543, epoch 14, batch 90/481,disc_loss 75.378, (real 77.529, fake 73.226 ) gen_loss 836.83\n",
            "iteration 6544, epoch 14, batch 91/481,disc_loss 78.053, (real 80.396, fake 75.71 ) gen_loss 899.58\n",
            "iteration 6545, epoch 14, batch 92/481,disc_loss 75.049, (real 77.421, fake 72.678 ) gen_loss 861.03\n",
            "iteration 6546, epoch 14, batch 93/481,disc_loss 75.24, (real 77.316, fake 73.164 ) gen_loss 929.12\n",
            "iteration 6547, epoch 14, batch 94/481,disc_loss 77.264, (real 79.464, fake 75.065 ) gen_loss 868.91\n",
            "iteration 6548, epoch 14, batch 95/481,disc_loss 77.917, (real 79.995, fake 75.839 ) gen_loss 912.74\n",
            "iteration 6549, epoch 14, batch 96/481,disc_loss 77.557, (real 80.496, fake 74.618 ) gen_loss 840.48\n",
            "iteration 6550, epoch 14, batch 97/481,disc_loss 76.925, (real 79.699, fake 74.151 ) gen_loss 860.89\n",
            "iteration 6551, epoch 14, batch 98/481,disc_loss 78.071, (real 80.056, fake 76.085 ) gen_loss 916.63\n",
            "iteration 6552, epoch 14, batch 99/481,disc_loss 76.318, (real 78.926, fake 73.71 ) gen_loss 905.39\n",
            "iteration 6553, epoch 14, batch 100/481,disc_loss 78.357, (real 81.148, fake 75.567 ) gen_loss 1056.4\n",
            "iteration 6554, epoch 14, batch 101/481,disc_loss 77.035, (real 79.974, fake 74.096 ) gen_loss 855.28\n",
            "iteration 6555, epoch 14, batch 102/481,disc_loss 79.449, (real 81.782, fake 77.116 ) gen_loss 796.31\n",
            "iteration 6556, epoch 14, batch 103/481,disc_loss 78.432, (real 81.812, fake 75.052 ) gen_loss 792.75\n",
            "iteration 6557, epoch 14, batch 104/481,disc_loss 82.104, (real 84.652, fake 79.557 ) gen_loss 825.53\n",
            "iteration 6558, epoch 14, batch 105/481,disc_loss 75.669, (real 77.725, fake 73.613 ) gen_loss 848.1\n",
            "iteration 6559, epoch 14, batch 106/481,disc_loss 77.833, (real 79.604, fake 76.061 ) gen_loss 913.36\n",
            "iteration 6560, epoch 14, batch 107/481,disc_loss 80.096, (real 84.344, fake 75.848 ) gen_loss 778.14\n",
            "iteration 6561, epoch 14, batch 108/481,disc_loss 75.952, (real 79.739, fake 72.165 ) gen_loss 861.01\n",
            "iteration 6562, epoch 14, batch 109/481,disc_loss 74.085, (real 76.241, fake 71.929 ) gen_loss 798.75\n",
            "iteration 6563, epoch 14, batch 110/481,disc_loss 80.131, (real 81.73, fake 78.532 ) gen_loss 869.99\n",
            "iteration 6564, epoch 14, batch 111/481,disc_loss 78.551, (real 81.379, fake 75.722 ) gen_loss 875.57\n",
            "iteration 6565, epoch 14, batch 112/481,disc_loss 76.106, (real 78.852, fake 73.361 ) gen_loss 987.96\n",
            "iteration 6566, epoch 14, batch 113/481,disc_loss 75.761, (real 78.169, fake 73.353 ) gen_loss 953.59\n",
            "iteration 6567, epoch 14, batch 114/481,disc_loss 78.361, (real 80.077, fake 76.645 ) gen_loss 932.27\n",
            "iteration 6568, epoch 14, batch 115/481,disc_loss 75.583, (real 77.513, fake 73.652 ) gen_loss 855.62\n",
            "iteration 6569, epoch 14, batch 116/481,disc_loss 73.114, (real 74.943, fake 71.286 ) gen_loss 966.26\n",
            "iteration 6570, epoch 14, batch 117/481,disc_loss 82.572, (real 84.916, fake 80.227 ) gen_loss 848.12\n",
            "iteration 6571, epoch 14, batch 118/481,disc_loss 74.399, (real 76.41, fake 72.388 ) gen_loss 869.4\n",
            "iteration 6572, epoch 14, batch 119/481,disc_loss 79.172, (real 81.664, fake 76.68 ) gen_loss 700.93\n",
            "iteration 6573, epoch 14, batch 120/481,disc_loss 75.953, (real 77.725, fake 74.181 ) gen_loss 801.98\n",
            "iteration 6574, epoch 14, batch 121/481,disc_loss 73.21, (real 75.932, fake 70.487 ) gen_loss 862.87\n",
            "iteration 6575, epoch 14, batch 122/481,disc_loss 77.216, (real 79.625, fake 74.808 ) gen_loss 891.86\n",
            "iteration 6576, epoch 14, batch 123/481,disc_loss 74.258, (real 76.69, fake 71.826 ) gen_loss 912.54\n",
            "iteration 6577, epoch 14, batch 124/481,disc_loss 76.397, (real 78.623, fake 74.171 ) gen_loss 912.38\n",
            "iteration 6578, epoch 14, batch 125/481,disc_loss 76.855, (real 79.253, fake 74.457 ) gen_loss 806.04\n",
            "iteration 6579, epoch 14, batch 126/481,disc_loss 79.32, (real 81.517, fake 77.124 ) gen_loss 833.6\n",
            "iteration 6580, epoch 14, batch 127/481,disc_loss 78.486, (real 80.248, fake 76.724 ) gen_loss 797.24\n",
            "iteration 6581, epoch 14, batch 128/481,disc_loss 76.39, (real 79.469, fake 73.311 ) gen_loss 859.12\n",
            "iteration 6582, epoch 14, batch 129/481,disc_loss 78.035, (real 80.236, fake 75.833 ) gen_loss 809.1\n",
            "iteration 6583, epoch 14, batch 130/481,disc_loss 73.599, (real 75.998, fake 71.199 ) gen_loss 936.86\n",
            "iteration 6584, epoch 14, batch 131/481,disc_loss 76.927, (real 79.901, fake 73.953 ) gen_loss 868.65\n",
            "iteration 6585, epoch 14, batch 132/481,disc_loss 79.433, (real 82.335, fake 76.53 ) gen_loss 944.4\n",
            "iteration 6586, epoch 14, batch 133/481,disc_loss 80.228, (real 82.301, fake 78.155 ) gen_loss 892.47\n",
            "iteration 6587, epoch 14, batch 134/481,disc_loss 74.086, (real 77.294, fake 70.877 ) gen_loss 855.33\n",
            "iteration 6588, epoch 14, batch 135/481,disc_loss 78.604, (real 82.171, fake 75.037 ) gen_loss 834.9\n",
            "iteration 6589, epoch 14, batch 136/481,disc_loss 78.13, (real 80.289, fake 75.972 ) gen_loss 812.51\n",
            "iteration 6590, epoch 14, batch 137/481,disc_loss 76.674, (real 79.209, fake 74.138 ) gen_loss 851.45\n",
            "iteration 6591, epoch 14, batch 138/481,disc_loss 75.427, (real 77.319, fake 73.536 ) gen_loss 866.57\n",
            "iteration 6592, epoch 14, batch 139/481,disc_loss 78.31, (real 80.864, fake 75.756 ) gen_loss 880.74\n",
            "iteration 6593, epoch 14, batch 140/481,disc_loss 76.607, (real 79.044, fake 74.171 ) gen_loss 948.19\n",
            "iteration 6594, epoch 14, batch 141/481,disc_loss 74.193, (real 76.792, fake 71.593 ) gen_loss 921.58\n",
            "iteration 6595, epoch 14, batch 142/481,disc_loss 74.283, (real 76.812, fake 71.754 ) gen_loss 900.36\n",
            "iteration 6596, epoch 14, batch 143/481,disc_loss 74.18, (real 76.215, fake 72.145 ) gen_loss 936.39\n",
            "iteration 6597, epoch 14, batch 144/481,disc_loss 73.213, (real 75.386, fake 71.04 ) gen_loss 839.2\n",
            "iteration 6598, epoch 14, batch 145/481,disc_loss 79.082, (real 80.893, fake 77.272 ) gen_loss 934.78\n",
            "iteration 6599, epoch 14, batch 146/481,disc_loss 73.98, (real 75.937, fake 72.023 ) gen_loss 895.35\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 6600, epoch 14, batch 147/481,disc_loss 75.92, (real 78.282, fake 73.559 ) gen_loss 834.9\n",
            "iteration 6601, epoch 14, batch 148/481,disc_loss 79.189, (real 81.222, fake 77.156 ) gen_loss 746.09\n",
            "iteration 6602, epoch 14, batch 149/481,disc_loss 80.964, (real 82.965, fake 78.963 ) gen_loss 810.17\n",
            "iteration 6603, epoch 14, batch 150/481,disc_loss 79.878, (real 82.435, fake 77.321 ) gen_loss 798.5\n",
            "iteration 6604, epoch 14, batch 151/481,disc_loss 74.787, (real 77.331, fake 72.244 ) gen_loss 859.49\n",
            "iteration 6605, epoch 14, batch 152/481,disc_loss 79.112, (real 81.883, fake 76.341 ) gen_loss 968.99\n",
            "iteration 6606, epoch 14, batch 153/481,disc_loss 80.52, (real 82.673, fake 78.366 ) gen_loss 915.68\n",
            "iteration 6607, epoch 14, batch 154/481,disc_loss 76.717, (real 79.88, fake 73.553 ) gen_loss 819.74\n",
            "iteration 6608, epoch 14, batch 155/481,disc_loss 79.592, (real 82.281, fake 76.903 ) gen_loss 896.29\n",
            "iteration 6609, epoch 14, batch 156/481,disc_loss 75.096, (real 77.564, fake 72.628 ) gen_loss 855.55\n",
            "iteration 6610, epoch 14, batch 157/481,disc_loss 78.295, (real 82.198, fake 74.392 ) gen_loss 905.91\n",
            "iteration 6611, epoch 14, batch 158/481,disc_loss 76.752, (real 79.284, fake 74.221 ) gen_loss 860.78\n",
            "iteration 6612, epoch 14, batch 159/481,disc_loss 74.599, (real 76.774, fake 72.424 ) gen_loss 921.54\n",
            "iteration 6613, epoch 14, batch 160/481,disc_loss 73.108, (real 74.955, fake 71.261 ) gen_loss 809.91\n",
            "iteration 6614, epoch 14, batch 161/481,disc_loss 76.781, (real 78.621, fake 74.942 ) gen_loss 956.3\n",
            "iteration 6615, epoch 14, batch 162/481,disc_loss 79.948, (real 82.977, fake 76.919 ) gen_loss 918.94\n",
            "iteration 6616, epoch 14, batch 163/481,disc_loss 78.1, (real 80.933, fake 75.266 ) gen_loss 807.77\n",
            "iteration 6617, epoch 14, batch 164/481,disc_loss 77.09, (real 79.944, fake 74.236 ) gen_loss 846.84\n",
            "iteration 6618, epoch 14, batch 165/481,disc_loss 80.763, (real 84.356, fake 77.171 ) gen_loss 882.13\n",
            "iteration 6619, epoch 14, batch 166/481,disc_loss 77.778, (real 81.086, fake 74.47 ) gen_loss 908.31\n",
            "iteration 6620, epoch 14, batch 167/481,disc_loss 79.79, (real 82.318, fake 77.263 ) gen_loss 781.56\n",
            "iteration 6621, epoch 14, batch 168/481,disc_loss 78.322, (real 81.166, fake 75.479 ) gen_loss 864.79\n",
            "iteration 6622, epoch 14, batch 169/481,disc_loss 77.307, (real 79.766, fake 74.848 ) gen_loss 889.09\n",
            "iteration 6623, epoch 14, batch 170/481,disc_loss 81.225, (real 83.962, fake 78.488 ) gen_loss 1086.5\n",
            "iteration 6624, epoch 14, batch 171/481,disc_loss 77.147, (real 80.181, fake 74.114 ) gen_loss 926.07\n",
            "iteration 6625, epoch 14, batch 172/481,disc_loss 77.474, (real 79.494, fake 75.454 ) gen_loss 813.26\n",
            "iteration 6626, epoch 14, batch 173/481,disc_loss 78.815, (real 81.094, fake 76.537 ) gen_loss 834.59\n",
            "iteration 6627, epoch 14, batch 174/481,disc_loss 73.878, (real 76.096, fake 71.659 ) gen_loss 927.72\n",
            "iteration 6628, epoch 14, batch 175/481,disc_loss 77.27, (real 79.521, fake 75.019 ) gen_loss 846.45\n",
            "iteration 6629, epoch 14, batch 176/481,disc_loss 77.676, (real 80.179, fake 75.174 ) gen_loss 873.32\n",
            "iteration 6630, epoch 14, batch 177/481,disc_loss 73.627, (real 75.591, fake 71.663 ) gen_loss 921.3\n",
            "iteration 6631, epoch 14, batch 178/481,disc_loss 74.659, (real 76.667, fake 72.65 ) gen_loss 945.34\n",
            "iteration 6632, epoch 14, batch 179/481,disc_loss 74.182, (real 76.257, fake 72.106 ) gen_loss 879.39\n",
            "iteration 6633, epoch 14, batch 180/481,disc_loss 74.273, (real 76.95, fake 71.596 ) gen_loss 865.84\n",
            "iteration 6634, epoch 14, batch 181/481,disc_loss 75.999, (real 78.608, fake 73.39 ) gen_loss 803.72\n",
            "iteration 6635, epoch 14, batch 182/481,disc_loss 77.281, (real 80.062, fake 74.5 ) gen_loss 824.77\n",
            "iteration 6636, epoch 14, batch 183/481,disc_loss 79.858, (real 81.918, fake 77.798 ) gen_loss 778.57\n",
            "iteration 6637, epoch 14, batch 184/481,disc_loss 78.426, (real 81.498, fake 75.354 ) gen_loss 840.47\n",
            "iteration 6638, epoch 14, batch 185/481,disc_loss 75.742, (real 77.86, fake 73.625 ) gen_loss 859.19\n",
            "iteration 6639, epoch 14, batch 186/481,disc_loss 78.105, (real 80.239, fake 75.972 ) gen_loss 904.88\n",
            "iteration 6640, epoch 14, batch 187/481,disc_loss 70.451, (real 72.966, fake 67.936 ) gen_loss 902.6\n",
            "iteration 6641, epoch 14, batch 188/481,disc_loss 74.26, (real 76.152, fake 72.369 ) gen_loss 814.01\n",
            "iteration 6642, epoch 14, batch 189/481,disc_loss 69.363, (real 71.505, fake 67.22 ) gen_loss 883.63\n",
            "iteration 6643, epoch 14, batch 190/481,disc_loss 82.547, (real 85.449, fake 79.645 ) gen_loss 816.94\n",
            "iteration 6644, epoch 14, batch 191/481,disc_loss 75.589, (real 77.403, fake 73.776 ) gen_loss 799.46\n",
            "iteration 6645, epoch 14, batch 192/481,disc_loss 73.922, (real 76.992, fake 70.853 ) gen_loss 849.93\n",
            "iteration 6646, epoch 14, batch 193/481,disc_loss 75.946, (real 78.375, fake 73.516 ) gen_loss 892.78\n",
            "iteration 6647, epoch 14, batch 194/481,disc_loss 77.335, (real 78.913, fake 75.756 ) gen_loss 832.61\n",
            "iteration 6648, epoch 14, batch 195/481,disc_loss 81.321, (real 83.333, fake 79.309 ) gen_loss 812.02\n",
            "iteration 6649, epoch 14, batch 196/481,disc_loss 81.56, (real 83.859, fake 79.26 ) gen_loss 787.05\n",
            "iteration 6650, epoch 14, batch 197/481,disc_loss 78.493, (real 80.896, fake 76.09 ) gen_loss 814.56\n",
            "iteration 6651, epoch 14, batch 198/481,disc_loss 77.641, (real 80.282, fake 75.0 ) gen_loss 818.73\n",
            "iteration 6652, epoch 14, batch 199/481,disc_loss 78.014, (real 80.349, fake 75.678 ) gen_loss 999.77\n",
            "iteration 6653, epoch 14, batch 200/481,disc_loss 74.526, (real 76.407, fake 72.645 ) gen_loss 832.25\n",
            "iteration 6654, epoch 14, batch 201/481,disc_loss 79.421, (real 81.754, fake 77.087 ) gen_loss 765.22\n",
            "iteration 6655, epoch 14, batch 202/481,disc_loss 79.247, (real 81.883, fake 76.611 ) gen_loss 782.91\n",
            "iteration 6656, epoch 14, batch 203/481,disc_loss 80.287, (real 82.84, fake 77.735 ) gen_loss 799.01\n",
            "iteration 6657, epoch 14, batch 204/481,disc_loss 76.945, (real 78.96, fake 74.93 ) gen_loss 917.35\n",
            "iteration 6658, epoch 14, batch 205/481,disc_loss 79.346, (real 81.603, fake 77.088 ) gen_loss 828.47\n",
            "iteration 6659, epoch 14, batch 206/481,disc_loss 74.895, (real 76.925, fake 72.865 ) gen_loss 875.17\n",
            "iteration 6660, epoch 14, batch 207/481,disc_loss 74.928, (real 77.325, fake 72.531 ) gen_loss 891.44\n",
            "iteration 6661, epoch 14, batch 208/481,disc_loss 81.951, (real 84.188, fake 79.714 ) gen_loss 865.68\n",
            "iteration 6662, epoch 14, batch 209/481,disc_loss 75.144, (real 77.298, fake 72.991 ) gen_loss 913.31\n",
            "iteration 6663, epoch 14, batch 210/481,disc_loss 76.273, (real 78.578, fake 73.967 ) gen_loss 920.39\n",
            "iteration 6664, epoch 14, batch 211/481,disc_loss 71.745, (real 74.147, fake 69.344 ) gen_loss 847.47\n",
            "iteration 6665, epoch 14, batch 212/481,disc_loss 79.39, (real 81.623, fake 77.158 ) gen_loss 893.03\n",
            "iteration 6666, epoch 14, batch 213/481,disc_loss 73.862, (real 75.541, fake 72.182 ) gen_loss 949.76\n",
            "iteration 6667, epoch 14, batch 214/481,disc_loss 75.371, (real 77.301, fake 73.442 ) gen_loss 956.23\n",
            "iteration 6668, epoch 14, batch 215/481,disc_loss 82.233, (real 85.411, fake 79.056 ) gen_loss 845.03\n",
            "iteration 6669, epoch 14, batch 216/481,disc_loss 77.165, (real 79.882, fake 74.448 ) gen_loss 798.73\n",
            "iteration 6670, epoch 14, batch 217/481,disc_loss 74.835, (real 78.278, fake 71.392 ) gen_loss 823.98\n",
            "iteration 6671, epoch 14, batch 218/481,disc_loss 76.518, (real 79.318, fake 73.717 ) gen_loss 938.49\n",
            "iteration 6672, epoch 14, batch 219/481,disc_loss 79.032, (real 81.448, fake 76.617 ) gen_loss 1000.9\n",
            "iteration 6673, epoch 14, batch 220/481,disc_loss 75.119, (real 77.313, fake 72.926 ) gen_loss 885.43\n",
            "iteration 6674, epoch 14, batch 221/481,disc_loss 78.997, (real 81.145, fake 76.849 ) gen_loss 908.41\n",
            "iteration 6675, epoch 14, batch 222/481,disc_loss 78.537, (real 80.826, fake 76.249 ) gen_loss 914.2\n",
            "iteration 6676, epoch 14, batch 223/481,disc_loss 74.337, (real 77.443, fake 71.232 ) gen_loss 910.82\n",
            "iteration 6677, epoch 14, batch 224/481,disc_loss 75.433, (real 77.601, fake 73.265 ) gen_loss 901.73\n",
            "iteration 6678, epoch 14, batch 225/481,disc_loss 74.308, (real 76.839, fake 71.777 ) gen_loss 868.91\n",
            "iteration 6679, epoch 14, batch 226/481,disc_loss 78.998, (real 81.161, fake 76.835 ) gen_loss 850.52\n",
            "iteration 6680, epoch 14, batch 227/481,disc_loss 73.005, (real 75.647, fake 70.363 ) gen_loss 964.32\n",
            "iteration 6681, epoch 14, batch 228/481,disc_loss 77.346, (real 79.497, fake 75.194 ) gen_loss 902.26\n",
            "iteration 6682, epoch 14, batch 229/481,disc_loss 74.819, (real 76.477, fake 73.162 ) gen_loss 824.53\n",
            "iteration 6683, epoch 14, batch 230/481,disc_loss 76.633, (real 78.824, fake 74.441 ) gen_loss 874.5\n",
            "iteration 6684, epoch 14, batch 231/481,disc_loss 78.58, (real 81.211, fake 75.948 ) gen_loss 895.08\n",
            "iteration 6685, epoch 14, batch 232/481,disc_loss 74.845, (real 76.618, fake 73.071 ) gen_loss 845.66\n",
            "iteration 6686, epoch 14, batch 233/481,disc_loss 79.801, (real 81.588, fake 78.013 ) gen_loss 824.75\n",
            "iteration 6687, epoch 14, batch 234/481,disc_loss 75.237, (real 77.815, fake 72.659 ) gen_loss 856.06\n",
            "iteration 6688, epoch 14, batch 235/481,disc_loss 75.942, (real 78.192, fake 73.692 ) gen_loss 941.7\n",
            "iteration 6689, epoch 14, batch 236/481,disc_loss 80.273, (real 82.287, fake 78.26 ) gen_loss 870.73\n",
            "iteration 6690, epoch 14, batch 237/481,disc_loss 76.713, (real 79.026, fake 74.399 ) gen_loss 834.79\n",
            "iteration 6691, epoch 14, batch 238/481,disc_loss 76.216, (real 78.728, fake 73.704 ) gen_loss 969.03\n",
            "iteration 6692, epoch 14, batch 239/481,disc_loss 78.715, (real 82.343, fake 75.087 ) gen_loss 873.6\n",
            "iteration 6693, epoch 14, batch 240/481,disc_loss 74.124, (real 76.347, fake 71.901 ) gen_loss 850.89\n",
            "iteration 6694, epoch 14, batch 241/481,disc_loss 78.039, (real 80.552, fake 75.526 ) gen_loss 822.72\n",
            "iteration 6695, epoch 14, batch 242/481,disc_loss 78.623, (real 80.838, fake 76.409 ) gen_loss 789.85\n",
            "iteration 6696, epoch 14, batch 243/481,disc_loss 74.704, (real 77.053, fake 72.355 ) gen_loss 844.47\n",
            "iteration 6697, epoch 14, batch 244/481,disc_loss 77.59, (real 79.509, fake 75.671 ) gen_loss 985.03\n",
            "iteration 6698, epoch 14, batch 245/481,disc_loss 78.353, (real 81.29, fake 75.415 ) gen_loss 847.52\n",
            "iteration 6699, epoch 14, batch 246/481,disc_loss 74.837, (real 77.393, fake 72.282 ) gen_loss 878.55\n",
            "iteration 6700, epoch 14, batch 247/481,disc_loss 77.436, (real 80.11, fake 74.762 ) gen_loss 843.21\n",
            "iteration 6701, epoch 14, batch 248/481,disc_loss 73.543, (real 75.795, fake 71.291 ) gen_loss 913.32\n",
            "iteration 6702, epoch 14, batch 249/481,disc_loss 78.876, (real 81.082, fake 76.67 ) gen_loss 915.69\n",
            "iteration 6703, epoch 14, batch 250/481,disc_loss 77.589, (real 79.42, fake 75.758 ) gen_loss 932.14\n",
            "iteration 6704, epoch 14, batch 251/481,disc_loss 78.365, (real 80.344, fake 76.386 ) gen_loss 950.09\n",
            "iteration 6705, epoch 14, batch 252/481,disc_loss 78.818, (real 80.832, fake 76.804 ) gen_loss 892.24\n",
            "iteration 6706, epoch 14, batch 253/481,disc_loss 75.82, (real 78.328, fake 73.313 ) gen_loss 907.72\n",
            "iteration 6707, epoch 14, batch 254/481,disc_loss 77.275, (real 80.18, fake 74.37 ) gen_loss 869.34\n",
            "iteration 6708, epoch 14, batch 255/481,disc_loss 72.523, (real 74.308, fake 70.737 ) gen_loss 875.31\n",
            "iteration 6709, epoch 14, batch 256/481,disc_loss 77.999, (real 80.536, fake 75.462 ) gen_loss 856.3\n",
            "iteration 6710, epoch 14, batch 257/481,disc_loss 76.302, (real 78.808, fake 73.795 ) gen_loss 792.3\n",
            "iteration 6711, epoch 14, batch 258/481,disc_loss 78.264, (real 80.757, fake 75.771 ) gen_loss 786.84\n",
            "iteration 6712, epoch 14, batch 259/481,disc_loss 75.058, (real 77.448, fake 72.668 ) gen_loss 867.27\n",
            "iteration 6713, epoch 14, batch 260/481,disc_loss 79.973, (real 83.116, fake 76.831 ) gen_loss 818.64\n",
            "iteration 6714, epoch 14, batch 261/481,disc_loss 76.017, (real 77.823, fake 74.21 ) gen_loss 802.49\n",
            "iteration 6715, epoch 14, batch 262/481,disc_loss 77.343, (real 80.491, fake 74.195 ) gen_loss 870.11\n",
            "iteration 6716, epoch 14, batch 263/481,disc_loss 73.596, (real 76.033, fake 71.16 ) gen_loss 820.18\n",
            "iteration 6717, epoch 14, batch 264/481,disc_loss 76.555, (real 79.529, fake 73.58 ) gen_loss 849.28\n",
            "iteration 6718, epoch 14, batch 265/481,disc_loss 78.117, (real 80.865, fake 75.37 ) gen_loss 861.95\n",
            "iteration 6719, epoch 14, batch 266/481,disc_loss 71.903, (real 75.002, fake 68.805 ) gen_loss 886.39\n",
            "iteration 6720, epoch 14, batch 267/481,disc_loss 73.931, (real 76.397, fake 71.464 ) gen_loss 892.04\n",
            "iteration 6721, epoch 14, batch 268/481,disc_loss 79.601, (real 81.75, fake 77.452 ) gen_loss 989.0\n",
            "iteration 6722, epoch 14, batch 269/481,disc_loss 77.433, (real 79.876, fake 74.99 ) gen_loss 880.83\n",
            "iteration 6723, epoch 14, batch 270/481,disc_loss 77.959, (real 79.605, fake 76.313 ) gen_loss 742.42\n",
            "iteration 6724, epoch 14, batch 271/481,disc_loss 76.711, (real 78.793, fake 74.628 ) gen_loss 757.68\n",
            "iteration 6725, epoch 14, batch 272/481,disc_loss 80.379, (real 83.152, fake 77.607 ) gen_loss 953.77\n",
            "iteration 6726, epoch 14, batch 273/481,disc_loss 77.089, (real 78.603, fake 75.575 ) gen_loss 976.62\n",
            "iteration 6727, epoch 14, batch 274/481,disc_loss 77.68, (real 81.108, fake 74.253 ) gen_loss 865.68\n",
            "iteration 6728, epoch 14, batch 275/481,disc_loss 78.981, (real 81.746, fake 76.216 ) gen_loss 929.44\n",
            "iteration 6729, epoch 14, batch 276/481,disc_loss 77.635, (real 79.223, fake 76.047 ) gen_loss 872.65\n",
            "iteration 6730, epoch 14, batch 277/481,disc_loss 76.846, (real 79.396, fake 74.295 ) gen_loss 956.21\n",
            "iteration 6731, epoch 14, batch 278/481,disc_loss 78.383, (real 80.704, fake 76.061 ) gen_loss 890.45\n",
            "iteration 6732, epoch 14, batch 279/481,disc_loss 74.847, (real 77.328, fake 72.367 ) gen_loss 901.21\n",
            "iteration 6733, epoch 14, batch 280/481,disc_loss 72.647, (real 74.452, fake 70.842 ) gen_loss 834.92\n",
            "iteration 6734, epoch 14, batch 281/481,disc_loss 72.506, (real 76.186, fake 68.826 ) gen_loss 848.26\n",
            "iteration 6735, epoch 14, batch 282/481,disc_loss 74.702, (real 76.953, fake 72.451 ) gen_loss 846.96\n",
            "iteration 6736, epoch 14, batch 283/481,disc_loss 80.385, (real 82.813, fake 77.957 ) gen_loss 828.38\n",
            "iteration 6737, epoch 14, batch 284/481,disc_loss 78.269, (real 80.31, fake 76.228 ) gen_loss 816.69\n",
            "iteration 6738, epoch 14, batch 285/481,disc_loss 76.358, (real 78.371, fake 74.344 ) gen_loss 778.37\n",
            "iteration 6739, epoch 14, batch 286/481,disc_loss 76.132, (real 78.643, fake 73.622 ) gen_loss 922.38\n",
            "iteration 6740, epoch 14, batch 287/481,disc_loss 78.487, (real 81.196, fake 75.777 ) gen_loss 893.07\n",
            "iteration 6741, epoch 14, batch 288/481,disc_loss 76.861, (real 79.146, fake 74.575 ) gen_loss 893.38\n",
            "iteration 6742, epoch 14, batch 289/481,disc_loss 77.682, (real 80.192, fake 75.173 ) gen_loss 917.98\n",
            "iteration 6743, epoch 14, batch 290/481,disc_loss 72.265, (real 75.474, fake 69.055 ) gen_loss 965.43\n",
            "iteration 6744, epoch 14, batch 291/481,disc_loss 73.603, (real 75.969, fake 71.238 ) gen_loss 883.86\n",
            "iteration 6745, epoch 14, batch 292/481,disc_loss 79.625, (real 84.025, fake 75.225 ) gen_loss 762.49\n",
            "iteration 6746, epoch 14, batch 293/481,disc_loss 78.6, (real 81.16, fake 76.04 ) gen_loss 809.76\n",
            "iteration 6747, epoch 14, batch 294/481,disc_loss 75.29, (real 78.073, fake 72.507 ) gen_loss 883.65\n",
            "iteration 6748, epoch 14, batch 295/481,disc_loss 79.767, (real 82.249, fake 77.284 ) gen_loss 931.3\n",
            "iteration 6749, epoch 14, batch 296/481,disc_loss 80.373, (real 82.632, fake 78.115 ) gen_loss 922.35\n",
            "iteration 6750, epoch 14, batch 297/481,disc_loss 77.897, (real 80.345, fake 75.448 ) gen_loss 961.02\n",
            "iteration 6751, epoch 14, batch 298/481,disc_loss 79.078, (real 80.893, fake 77.264 ) gen_loss 901.01\n",
            "iteration 6752, epoch 14, batch 299/481,disc_loss 72.774, (real 75.096, fake 70.453 ) gen_loss 868.0\n",
            "iteration 6753, epoch 14, batch 300/481,disc_loss 77.969, (real 80.495, fake 75.443 ) gen_loss 856.51\n",
            "iteration 6754, epoch 14, batch 301/481,disc_loss 79.369, (real 81.318, fake 77.42 ) gen_loss 858.93\n",
            "iteration 6755, epoch 14, batch 302/481,disc_loss 87.468, (real 89.509, fake 85.427 ) gen_loss 876.76\n",
            "iteration 6756, epoch 14, batch 303/481,disc_loss 73.765, (real 76.651, fake 70.879 ) gen_loss 817.16\n",
            "iteration 6757, epoch 14, batch 304/481,disc_loss 76.653, (real 78.581, fake 74.725 ) gen_loss 969.66\n",
            "iteration 6758, epoch 14, batch 305/481,disc_loss 77.654, (real 80.426, fake 74.882 ) gen_loss 807.01\n",
            "iteration 6759, epoch 14, batch 306/481,disc_loss 76.481, (real 79.254, fake 73.708 ) gen_loss 859.82\n",
            "iteration 6760, epoch 14, batch 307/481,disc_loss 74.454, (real 77.278, fake 71.631 ) gen_loss 823.28\n",
            "iteration 6761, epoch 14, batch 308/481,disc_loss 79.298, (real 82.606, fake 75.99 ) gen_loss 879.02\n",
            "iteration 6762, epoch 14, batch 309/481,disc_loss 76.919, (real 79.518, fake 74.32 ) gen_loss 809.57\n",
            "iteration 6763, epoch 14, batch 310/481,disc_loss 75.826, (real 79.019, fake 72.633 ) gen_loss 877.31\n",
            "iteration 6764, epoch 14, batch 311/481,disc_loss 75.908, (real 78.522, fake 73.294 ) gen_loss 915.77\n",
            "iteration 6765, epoch 14, batch 312/481,disc_loss 71.827, (real 75.102, fake 68.552 ) gen_loss 811.84\n",
            "iteration 6766, epoch 14, batch 313/481,disc_loss 74.434, (real 76.804, fake 72.064 ) gen_loss 925.04\n",
            "iteration 6767, epoch 14, batch 314/481,disc_loss 75.197, (real 77.182, fake 73.213 ) gen_loss 893.51\n",
            "iteration 6768, epoch 14, batch 315/481,disc_loss 74.311, (real 76.501, fake 72.122 ) gen_loss 848.23\n",
            "iteration 6769, epoch 14, batch 316/481,disc_loss 78.206, (real 80.714, fake 75.698 ) gen_loss 857.08\n",
            "iteration 6770, epoch 14, batch 317/481,disc_loss 73.095, (real 75.841, fake 70.35 ) gen_loss 903.33\n",
            "iteration 6771, epoch 14, batch 318/481,disc_loss 75.327, (real 77.804, fake 72.85 ) gen_loss 951.96\n",
            "iteration 6772, epoch 14, batch 319/481,disc_loss 78.214, (real 80.551, fake 75.878 ) gen_loss 875.21\n",
            "iteration 6773, epoch 14, batch 320/481,disc_loss 79.18, (real 82.97, fake 75.39 ) gen_loss 769.11\n",
            "iteration 6774, epoch 14, batch 321/481,disc_loss 77.458, (real 79.981, fake 74.936 ) gen_loss 917.73\n",
            "iteration 6775, epoch 14, batch 322/481,disc_loss 77.713, (real 80.423, fake 75.003 ) gen_loss 889.84\n",
            "iteration 6776, epoch 14, batch 323/481,disc_loss 76.357, (real 78.557, fake 74.157 ) gen_loss 863.63\n",
            "iteration 6777, epoch 14, batch 324/481,disc_loss 78.996, (real 81.09, fake 76.901 ) gen_loss 925.5\n",
            "iteration 6778, epoch 14, batch 325/481,disc_loss 76.987, (real 79.455, fake 74.519 ) gen_loss 1000.3\n",
            "iteration 6779, epoch 14, batch 326/481,disc_loss 77.789, (real 80.348, fake 75.229 ) gen_loss 908.98\n",
            "iteration 6780, epoch 14, batch 327/481,disc_loss 76.38, (real 78.313, fake 74.448 ) gen_loss 960.39\n",
            "iteration 6781, epoch 14, batch 328/481,disc_loss 73.364, (real 76.33, fake 70.399 ) gen_loss 931.76\n",
            "iteration 6782, epoch 14, batch 329/481,disc_loss 77.223, (real 79.493, fake 74.954 ) gen_loss 854.09\n",
            "iteration 6783, epoch 14, batch 330/481,disc_loss 77.961, (real 80.599, fake 75.323 ) gen_loss 830.94\n",
            "iteration 6784, epoch 14, batch 331/481,disc_loss 75.66, (real 78.131, fake 73.19 ) gen_loss 855.04\n",
            "iteration 6785, epoch 14, batch 332/481,disc_loss 74.897, (real 76.851, fake 72.943 ) gen_loss 842.98\n",
            "iteration 6786, epoch 14, batch 333/481,disc_loss 79.238, (real 81.633, fake 76.843 ) gen_loss 865.5\n",
            "iteration 6787, epoch 14, batch 334/481,disc_loss 76.968, (real 79.455, fake 74.481 ) gen_loss 937.34\n",
            "iteration 6788, epoch 14, batch 335/481,disc_loss 74.154, (real 77.004, fake 71.304 ) gen_loss 1002.1\n",
            "iteration 6789, epoch 14, batch 336/481,disc_loss 75.811, (real 78.636, fake 72.986 ) gen_loss 936.03\n",
            "iteration 6790, epoch 14, batch 337/481,disc_loss 72.832, (real 75.623, fake 70.04 ) gen_loss 906.22\n",
            "iteration 6791, epoch 14, batch 338/481,disc_loss 75.104, (real 77.427, fake 72.782 ) gen_loss 932.62\n",
            "iteration 6792, epoch 14, batch 339/481,disc_loss 74.624, (real 77.729, fake 71.52 ) gen_loss 848.08\n",
            "iteration 6793, epoch 14, batch 340/481,disc_loss 78.313, (real 80.279, fake 76.348 ) gen_loss 914.97\n",
            "iteration 6794, epoch 14, batch 341/481,disc_loss 79.942, (real 82.342, fake 77.542 ) gen_loss 912.66\n",
            "iteration 6795, epoch 14, batch 342/481,disc_loss 74.114, (real 76.723, fake 71.505 ) gen_loss 885.71\n",
            "iteration 6796, epoch 14, batch 343/481,disc_loss 75.686, (real 78.239, fake 73.134 ) gen_loss 983.25\n",
            "iteration 6797, epoch 14, batch 344/481,disc_loss 76.98, (real 80.416, fake 73.545 ) gen_loss 794.33\n",
            "iteration 6798, epoch 14, batch 345/481,disc_loss 80.639, (real 83.542, fake 77.737 ) gen_loss 842.89\n",
            "iteration 6799, epoch 14, batch 346/481,disc_loss 75.346, (real 78.551, fake 72.14 ) gen_loss 854.48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 6800, epoch 14, batch 347/481,disc_loss 82.555, (real 85.445, fake 79.665 ) gen_loss 910.2\n",
            "iteration 6801, epoch 14, batch 348/481,disc_loss 78.213, (real 80.476, fake 75.95 ) gen_loss 1063.7\n",
            "iteration 6802, epoch 14, batch 349/481,disc_loss 74.73, (real 76.872, fake 72.589 ) gen_loss 970.1\n",
            "iteration 6803, epoch 14, batch 350/481,disc_loss 73.849, (real 76.602, fake 71.097 ) gen_loss 877.13\n",
            "iteration 6804, epoch 14, batch 351/481,disc_loss 74.844, (real 76.875, fake 72.814 ) gen_loss 836.82\n",
            "iteration 6805, epoch 14, batch 352/481,disc_loss 77.531, (real 79.777, fake 75.284 ) gen_loss 1006.3\n",
            "iteration 6806, epoch 14, batch 353/481,disc_loss 74.893, (real 77.36, fake 72.425 ) gen_loss 814.97\n",
            "iteration 6807, epoch 14, batch 354/481,disc_loss 76.997, (real 79.361, fake 74.632 ) gen_loss 970.26\n",
            "iteration 6808, epoch 14, batch 355/481,disc_loss 78.627, (real 81.205, fake 76.049 ) gen_loss 860.33\n",
            "iteration 6809, epoch 14, batch 356/481,disc_loss 76.683, (real 78.791, fake 74.574 ) gen_loss 823.29\n",
            "iteration 6810, epoch 14, batch 357/481,disc_loss 77.522, (real 79.821, fake 75.223 ) gen_loss 908.75\n",
            "iteration 6811, epoch 14, batch 358/481,disc_loss 75.3, (real 78.733, fake 71.867 ) gen_loss 814.27\n",
            "iteration 6812, epoch 14, batch 359/481,disc_loss 81.204, (real 82.711, fake 79.698 ) gen_loss 818.22\n",
            "iteration 6813, epoch 14, batch 360/481,disc_loss 80.335, (real 83.643, fake 77.028 ) gen_loss 1070.5\n",
            "iteration 6814, epoch 14, batch 361/481,disc_loss 76.962, (real 79.782, fake 74.143 ) gen_loss 793.44\n",
            "iteration 6815, epoch 14, batch 362/481,disc_loss 74.747, (real 76.594, fake 72.9 ) gen_loss 745.0\n",
            "iteration 6816, epoch 14, batch 363/481,disc_loss 73.34, (real 76.056, fake 70.624 ) gen_loss 885.82\n",
            "iteration 6817, epoch 14, batch 364/481,disc_loss 79.377, (real 81.923, fake 76.832 ) gen_loss 816.98\n",
            "iteration 6818, epoch 14, batch 365/481,disc_loss 80.322, (real 84.072, fake 76.572 ) gen_loss 777.72\n",
            "iteration 6819, epoch 14, batch 366/481,disc_loss 77.969, (real 79.774, fake 76.163 ) gen_loss 826.11\n",
            "iteration 6820, epoch 14, batch 367/481,disc_loss 77.769, (real 80.595, fake 74.943 ) gen_loss 846.1\n",
            "iteration 6821, epoch 14, batch 368/481,disc_loss 75.182, (real 77.949, fake 72.415 ) gen_loss 940.93\n",
            "iteration 6822, epoch 14, batch 369/481,disc_loss 76.002, (real 78.968, fake 73.036 ) gen_loss 893.92\n",
            "iteration 6823, epoch 14, batch 370/481,disc_loss 78.563, (real 80.954, fake 76.171 ) gen_loss 1054.1\n",
            "iteration 6824, epoch 14, batch 371/481,disc_loss 77.165, (real 79.308, fake 75.022 ) gen_loss 983.75\n",
            "iteration 6825, epoch 14, batch 372/481,disc_loss 76.296, (real 78.899, fake 73.692 ) gen_loss 805.7\n",
            "iteration 6826, epoch 14, batch 373/481,disc_loss 78.564, (real 81.346, fake 75.783 ) gen_loss 1054.8\n",
            "iteration 6827, epoch 14, batch 374/481,disc_loss 76.07, (real 78.346, fake 73.794 ) gen_loss 863.56\n",
            "iteration 6828, epoch 14, batch 375/481,disc_loss 79.208, (real 81.949, fake 76.467 ) gen_loss 830.78\n",
            "iteration 6829, epoch 14, batch 376/481,disc_loss 81.105, (real 84.298, fake 77.912 ) gen_loss 878.32\n",
            "iteration 6830, epoch 14, batch 377/481,disc_loss 74.459, (real 77.103, fake 71.815 ) gen_loss 919.14\n",
            "iteration 6831, epoch 14, batch 378/481,disc_loss 81.184, (real 83.919, fake 78.449 ) gen_loss 907.46\n",
            "iteration 6832, epoch 14, batch 379/481,disc_loss 76.868, (real 79.499, fake 74.237 ) gen_loss 774.41\n",
            "iteration 6833, epoch 14, batch 380/481,disc_loss 75.951, (real 78.635, fake 73.267 ) gen_loss 903.67\n",
            "iteration 6834, epoch 14, batch 381/481,disc_loss 75.693, (real 78.03, fake 73.357 ) gen_loss 850.06\n",
            "iteration 6835, epoch 14, batch 382/481,disc_loss 78.444, (real 80.876, fake 76.012 ) gen_loss 1015.5\n",
            "iteration 6836, epoch 14, batch 383/481,disc_loss 80.884, (real 84.397, fake 77.37 ) gen_loss 947.59\n",
            "iteration 6837, epoch 14, batch 384/481,disc_loss 80.704, (real 83.297, fake 78.111 ) gen_loss 823.66\n",
            "iteration 6838, epoch 14, batch 385/481,disc_loss 77.378, (real 79.434, fake 75.322 ) gen_loss 863.44\n",
            "iteration 6839, epoch 14, batch 386/481,disc_loss 76.305, (real 78.5, fake 74.111 ) gen_loss 913.43\n",
            "iteration 6840, epoch 14, batch 387/481,disc_loss 79.299, (real 82.417, fake 76.182 ) gen_loss 946.38\n",
            "iteration 6841, epoch 14, batch 388/481,disc_loss 76.593, (real 79.022, fake 74.163 ) gen_loss 907.94\n",
            "iteration 6842, epoch 14, batch 389/481,disc_loss 78.631, (real 81.3, fake 75.963 ) gen_loss 913.88\n",
            "iteration 6843, epoch 14, batch 390/481,disc_loss 78.23, (real 81.096, fake 75.364 ) gen_loss 907.63\n",
            "iteration 6844, epoch 14, batch 391/481,disc_loss 77.526, (real 80.613, fake 74.439 ) gen_loss 833.71\n",
            "iteration 6845, epoch 14, batch 392/481,disc_loss 76.68, (real 79.36, fake 73.999 ) gen_loss 913.66\n",
            "iteration 6846, epoch 14, batch 393/481,disc_loss 75.239, (real 78.25, fake 72.227 ) gen_loss 953.91\n",
            "iteration 6847, epoch 14, batch 394/481,disc_loss 80.677, (real 83.158, fake 78.196 ) gen_loss 950.57\n",
            "iteration 6848, epoch 14, batch 395/481,disc_loss 75.995, (real 78.135, fake 73.855 ) gen_loss 801.57\n",
            "iteration 6849, epoch 14, batch 396/481,disc_loss 74.94, (real 76.681, fake 73.198 ) gen_loss 850.84\n",
            "iteration 6850, epoch 14, batch 397/481,disc_loss 79.724, (real 82.007, fake 77.441 ) gen_loss 911.48\n",
            "iteration 6851, epoch 14, batch 398/481,disc_loss 77.488, (real 80.499, fake 74.477 ) gen_loss 992.61\n",
            "iteration 6852, epoch 14, batch 399/481,disc_loss 77.869, (real 80.328, fake 75.41 ) gen_loss 851.33\n",
            "iteration 6853, epoch 14, batch 400/481,disc_loss 78.294, (real 81.48, fake 75.108 ) gen_loss 910.42\n",
            "iteration 6854, epoch 14, batch 401/481,disc_loss 77.442, (real 79.598, fake 75.287 ) gen_loss 885.13\n",
            "iteration 6855, epoch 14, batch 402/481,disc_loss 78.34, (real 79.582, fake 77.098 ) gen_loss 912.09\n",
            "iteration 6856, epoch 14, batch 403/481,disc_loss 75.777, (real 78.271, fake 73.283 ) gen_loss 830.46\n",
            "iteration 6857, epoch 14, batch 404/481,disc_loss 75.327, (real 77.975, fake 72.678 ) gen_loss 810.11\n",
            "iteration 6858, epoch 14, batch 405/481,disc_loss 76.162, (real 78.196, fake 74.129 ) gen_loss 914.6\n",
            "iteration 6859, epoch 14, batch 406/481,disc_loss 76.719, (real 79.69, fake 73.748 ) gen_loss 956.23\n",
            "iteration 6860, epoch 14, batch 407/481,disc_loss 75.876, (real 78.023, fake 73.73 ) gen_loss 857.8\n",
            "iteration 6861, epoch 14, batch 408/481,disc_loss 78.313, (real 81.276, fake 75.35 ) gen_loss 821.49\n",
            "iteration 6862, epoch 14, batch 409/481,disc_loss 79.014, (real 81.253, fake 76.775 ) gen_loss 856.73\n",
            "iteration 6863, epoch 14, batch 410/481,disc_loss 74.761, (real 77.422, fake 72.101 ) gen_loss 831.57\n",
            "iteration 6864, epoch 14, batch 411/481,disc_loss 74.846, (real 77.112, fake 72.58 ) gen_loss 848.04\n",
            "iteration 6865, epoch 14, batch 412/481,disc_loss 78.496, (real 80.958, fake 76.034 ) gen_loss 797.29\n",
            "iteration 6866, epoch 14, batch 413/481,disc_loss 76.223, (real 79.151, fake 73.295 ) gen_loss 806.2\n",
            "iteration 6867, epoch 14, batch 414/481,disc_loss 80.94, (real 83.549, fake 78.332 ) gen_loss 830.25\n",
            "iteration 6868, epoch 14, batch 415/481,disc_loss 85.438, (real 87.7, fake 83.175 ) gen_loss 867.76\n",
            "iteration 6869, epoch 14, batch 416/481,disc_loss 82.369, (real 84.349, fake 80.39 ) gen_loss 864.34\n",
            "iteration 6870, epoch 14, batch 417/481,disc_loss 75.422, (real 77.653, fake 73.19 ) gen_loss 820.32\n",
            "iteration 6871, epoch 14, batch 418/481,disc_loss 79.57, (real 81.299, fake 77.842 ) gen_loss 879.43\n",
            "iteration 6872, epoch 14, batch 419/481,disc_loss 77.205, (real 79.073, fake 75.336 ) gen_loss 865.57\n",
            "iteration 6873, epoch 14, batch 420/481,disc_loss 80.143, (real 82.895, fake 77.391 ) gen_loss 897.14\n",
            "iteration 6874, epoch 14, batch 421/481,disc_loss 76.691, (real 79.13, fake 74.252 ) gen_loss 919.53\n",
            "iteration 6875, epoch 14, batch 422/481,disc_loss 74.692, (real 77.324, fake 72.061 ) gen_loss 1000.5\n",
            "iteration 6876, epoch 14, batch 423/481,disc_loss 77.155, (real 79.423, fake 74.886 ) gen_loss 895.96\n",
            "iteration 6877, epoch 14, batch 424/481,disc_loss 77.809, (real 80.526, fake 75.092 ) gen_loss 862.51\n",
            "iteration 6878, epoch 14, batch 425/481,disc_loss 80.43, (real 82.473, fake 78.387 ) gen_loss 790.24\n",
            "iteration 6879, epoch 14, batch 426/481,disc_loss 82.03, (real 84.577, fake 79.482 ) gen_loss 907.86\n",
            "iteration 6880, epoch 14, batch 427/481,disc_loss 73.13, (real 75.928, fake 70.332 ) gen_loss 935.9\n",
            "iteration 6881, epoch 14, batch 428/481,disc_loss 75.044, (real 77.562, fake 72.525 ) gen_loss 848.19\n",
            "iteration 6882, epoch 14, batch 429/481,disc_loss 75.238, (real 78.364, fake 72.112 ) gen_loss 1221.6\n",
            "iteration 6883, epoch 14, batch 430/481,disc_loss 76.455, (real 78.854, fake 74.057 ) gen_loss 873.04\n",
            "iteration 6884, epoch 14, batch 431/481,disc_loss 78.413, (real 81.048, fake 75.777 ) gen_loss 909.72\n",
            "iteration 6885, epoch 14, batch 432/481,disc_loss 82.849, (real 86.091, fake 79.606 ) gen_loss 973.47\n",
            "iteration 6886, epoch 14, batch 433/481,disc_loss 78.082, (real 80.883, fake 75.281 ) gen_loss 886.85\n",
            "iteration 6887, epoch 14, batch 434/481,disc_loss 76.214, (real 78.095, fake 74.332 ) gen_loss 839.47\n",
            "iteration 6888, epoch 14, batch 435/481,disc_loss 74.957, (real 77.344, fake 72.57 ) gen_loss 858.9\n",
            "iteration 6889, epoch 14, batch 436/481,disc_loss 75.881, (real 78.792, fake 72.97 ) gen_loss 862.15\n",
            "iteration 6890, epoch 14, batch 437/481,disc_loss 79.964, (real 82.637, fake 77.292 ) gen_loss 914.53\n",
            "iteration 6891, epoch 14, batch 438/481,disc_loss 77.849, (real 80.823, fake 74.874 ) gen_loss 889.62\n",
            "iteration 6892, epoch 14, batch 439/481,disc_loss 80.478, (real 83.102, fake 77.854 ) gen_loss 925.55\n",
            "iteration 6893, epoch 14, batch 440/481,disc_loss 75.335, (real 77.855, fake 72.816 ) gen_loss 837.56\n",
            "iteration 6894, epoch 14, batch 441/481,disc_loss 78.431, (real 80.859, fake 76.002 ) gen_loss 1109.6\n",
            "iteration 6895, epoch 14, batch 442/481,disc_loss 78.194, (real 80.829, fake 75.559 ) gen_loss 1038.0\n",
            "iteration 6896, epoch 14, batch 443/481,disc_loss 81.7, (real 84.554, fake 78.846 ) gen_loss 913.95\n",
            "iteration 6897, epoch 14, batch 444/481,disc_loss 79.196, (real 82.394, fake 75.998 ) gen_loss 795.02\n",
            "iteration 6898, epoch 14, batch 445/481,disc_loss 77.693, (real 80.768, fake 74.618 ) gen_loss 763.88\n",
            "iteration 6899, epoch 14, batch 446/481,disc_loss 78.782, (real 81.296, fake 76.269 ) gen_loss 858.88\n",
            "iteration 6900, epoch 14, batch 447/481,disc_loss 82.07, (real 85.379, fake 78.76 ) gen_loss 887.42\n",
            "iteration 6901, epoch 14, batch 448/481,disc_loss 75.098, (real 78.127, fake 72.069 ) gen_loss 860.79\n",
            "iteration 6902, epoch 14, batch 449/481,disc_loss 79.717, (real 82.242, fake 77.191 ) gen_loss 927.59\n",
            "iteration 6903, epoch 14, batch 450/481,disc_loss 74.87, (real 77.596, fake 72.145 ) gen_loss 939.79\n",
            "iteration 6904, epoch 14, batch 451/481,disc_loss 79.47, (real 82.826, fake 76.114 ) gen_loss 929.67\n",
            "iteration 6905, epoch 14, batch 452/481,disc_loss 77.437, (real 80.246, fake 74.628 ) gen_loss 824.13\n",
            "iteration 6906, epoch 14, batch 453/481,disc_loss 76.926, (real 79.046, fake 74.806 ) gen_loss 922.75\n",
            "iteration 6907, epoch 14, batch 454/481,disc_loss 75.302, (real 78.356, fake 72.248 ) gen_loss 896.65\n",
            "iteration 6908, epoch 14, batch 455/481,disc_loss 77.662, (real 79.998, fake 75.327 ) gen_loss 965.19\n",
            "iteration 6909, epoch 14, batch 456/481,disc_loss 79.058, (real 81.514, fake 76.601 ) gen_loss 917.33\n",
            "iteration 6910, epoch 14, batch 457/481,disc_loss 81.179, (real 84.2, fake 78.157 ) gen_loss 865.51\n",
            "iteration 6911, epoch 14, batch 458/481,disc_loss 72.926, (real 76.294, fake 69.557 ) gen_loss 941.28\n",
            "iteration 6912, epoch 14, batch 459/481,disc_loss 76.458, (real 81.115, fake 71.802 ) gen_loss 962.39\n",
            "iteration 6913, epoch 14, batch 460/481,disc_loss 76.498, (real 78.865, fake 74.131 ) gen_loss 885.88\n",
            "iteration 6914, epoch 14, batch 461/481,disc_loss 75.548, (real 77.865, fake 73.231 ) gen_loss 867.12\n",
            "iteration 6915, epoch 14, batch 462/481,disc_loss 70.103, (real 72.417, fake 67.788 ) gen_loss 972.85\n",
            "iteration 6916, epoch 14, batch 463/481,disc_loss 76.198, (real 78.922, fake 73.475 ) gen_loss 883.01\n",
            "iteration 6917, epoch 14, batch 464/481,disc_loss 79.23, (real 82.086, fake 76.375 ) gen_loss 888.24\n",
            "iteration 6918, epoch 14, batch 465/481,disc_loss 78.928, (real 81.692, fake 76.163 ) gen_loss 924.18\n",
            "iteration 6919, epoch 14, batch 466/481,disc_loss 76.531, (real 79.076, fake 73.985 ) gen_loss 933.59\n",
            "iteration 6920, epoch 14, batch 467/481,disc_loss 77.714, (real 79.613, fake 75.815 ) gen_loss 867.28\n",
            "iteration 6921, epoch 14, batch 468/481,disc_loss 77.347, (real 79.715, fake 74.978 ) gen_loss 863.2\n",
            "iteration 6922, epoch 14, batch 469/481,disc_loss 78.819, (real 81.181, fake 76.457 ) gen_loss 942.4\n",
            "iteration 6923, epoch 14, batch 470/481,disc_loss 73.876, (real 76.91, fake 70.843 ) gen_loss 941.04\n",
            "iteration 6924, epoch 14, batch 471/481,disc_loss 72.571, (real 74.101, fake 71.042 ) gen_loss 938.5\n",
            "iteration 6925, epoch 14, batch 472/481,disc_loss 76.421, (real 79.067, fake 73.775 ) gen_loss 964.75\n",
            "iteration 6926, epoch 14, batch 473/481,disc_loss 76.844, (real 79.304, fake 74.384 ) gen_loss 902.98\n",
            "iteration 6927, epoch 14, batch 474/481,disc_loss 77.078, (real 80.065, fake 74.09 ) gen_loss 940.52\n",
            "iteration 6928, epoch 14, batch 475/481,disc_loss 79.167, (real 81.003, fake 77.331 ) gen_loss 862.64\n",
            "iteration 6929, epoch 14, batch 476/481,disc_loss 80.014, (real 82.834, fake 77.193 ) gen_loss 923.84\n",
            "iteration 6930, epoch 14, batch 477/481,disc_loss 73.23, (real 75.369, fake 71.092 ) gen_loss 961.01\n",
            "iteration 6931, epoch 14, batch 478/481,disc_loss 79.061, (real 81.37, fake 76.752 ) gen_loss 902.01\n",
            "iteration 6932, epoch 14, batch 479/481,disc_loss 76.685, (real 80.365, fake 73.005 ) gen_loss 921.54\n",
            "iteration 6933, epoch 14, batch 480/481,disc_loss 77.335, (real 79.537, fake 75.134 ) gen_loss 952.86\n",
            "iteration 6934, epoch 14, batch 481/481,disc_loss 77.284, (real 79.815, fake 74.752 ) gen_loss 964.15\n",
            "iteration 6935, epoch 15, batch 1/481,disc_loss 76.054, (real 78.386, fake 73.723 ) gen_loss 994.48\n",
            "iteration 6936, epoch 15, batch 2/481,disc_loss 76.308, (real 79.087, fake 73.529 ) gen_loss 971.33\n",
            "iteration 6937, epoch 15, batch 3/481,disc_loss 80.332, (real 82.711, fake 77.953 ) gen_loss 940.26\n",
            "iteration 6938, epoch 15, batch 4/481,disc_loss 75.584, (real 77.215, fake 73.952 ) gen_loss 863.72\n",
            "iteration 6939, epoch 15, batch 5/481,disc_loss 74.999, (real 76.946, fake 73.051 ) gen_loss 851.29\n",
            "iteration 6940, epoch 15, batch 6/481,disc_loss 74.103, (real 76.992, fake 71.214 ) gen_loss 846.47\n",
            "iteration 6941, epoch 15, batch 7/481,disc_loss 78.495, (real 80.208, fake 76.781 ) gen_loss 927.99\n",
            "iteration 6942, epoch 15, batch 8/481,disc_loss 72.086, (real 73.774, fake 70.399 ) gen_loss 951.34\n",
            "iteration 6943, epoch 15, batch 9/481,disc_loss 80.007, (real 81.869, fake 78.145 ) gen_loss 842.14\n",
            "iteration 6944, epoch 15, batch 10/481,disc_loss 77.59, (real 79.259, fake 75.921 ) gen_loss 902.57\n",
            "iteration 6945, epoch 15, batch 11/481,disc_loss 78.334, (real 80.789, fake 75.878 ) gen_loss 933.52\n",
            "iteration 6946, epoch 15, batch 12/481,disc_loss 74.86, (real 77.48, fake 72.24 ) gen_loss 884.48\n",
            "iteration 6947, epoch 15, batch 13/481,disc_loss 76.989, (real 79.349, fake 74.63 ) gen_loss 863.41\n",
            "iteration 6948, epoch 15, batch 14/481,disc_loss 71.684, (real 73.393, fake 69.976 ) gen_loss 859.59\n",
            "iteration 6949, epoch 15, batch 15/481,disc_loss 78.215, (real 80.11, fake 76.32 ) gen_loss 830.48\n",
            "iteration 6950, epoch 15, batch 16/481,disc_loss 75.351, (real 77.86, fake 72.842 ) gen_loss 859.18\n",
            "iteration 6951, epoch 15, batch 17/481,disc_loss 79.474, (real 82.202, fake 76.746 ) gen_loss 862.84\n",
            "iteration 6952, epoch 15, batch 18/481,disc_loss 78.475, (real 81.278, fake 75.672 ) gen_loss 880.2\n",
            "iteration 6953, epoch 15, batch 19/481,disc_loss 78.392, (real 80.861, fake 75.924 ) gen_loss 853.11\n",
            "iteration 6954, epoch 15, batch 20/481,disc_loss 79.57, (real 82.519, fake 76.62 ) gen_loss 876.8\n",
            "iteration 6955, epoch 15, batch 21/481,disc_loss 76.659, (real 78.955, fake 74.363 ) gen_loss 938.6\n",
            "iteration 6956, epoch 15, batch 22/481,disc_loss 73.883, (real 75.85, fake 71.916 ) gen_loss 845.63\n",
            "iteration 6957, epoch 15, batch 23/481,disc_loss 74.986, (real 77.18, fake 72.792 ) gen_loss 842.81\n",
            "iteration 6958, epoch 15, batch 24/481,disc_loss 79.174, (real 81.186, fake 77.161 ) gen_loss 965.79\n",
            "iteration 6959, epoch 15, batch 25/481,disc_loss 76.756, (real 78.562, fake 74.95 ) gen_loss 983.56\n",
            "iteration 6960, epoch 15, batch 26/481,disc_loss 77.941, (real 80.032, fake 75.85 ) gen_loss 1064.4\n",
            "iteration 6961, epoch 15, batch 27/481,disc_loss 76.003, (real 77.814, fake 74.192 ) gen_loss 916.5\n",
            "iteration 6962, epoch 15, batch 28/481,disc_loss 78.068, (real 79.402, fake 76.735 ) gen_loss 816.62\n",
            "iteration 6963, epoch 15, batch 29/481,disc_loss 77.699, (real 79.944, fake 75.455 ) gen_loss 789.68\n",
            "iteration 6964, epoch 15, batch 30/481,disc_loss 73.137, (real 75.406, fake 70.868 ) gen_loss 903.68\n",
            "iteration 6965, epoch 15, batch 31/481,disc_loss 75.383, (real 77.858, fake 72.907 ) gen_loss 826.89\n",
            "iteration 6966, epoch 15, batch 32/481,disc_loss 76.41, (real 78.234, fake 74.586 ) gen_loss 947.41\n",
            "iteration 6967, epoch 15, batch 33/481,disc_loss 80.082, (real 83.192, fake 76.972 ) gen_loss 890.44\n",
            "iteration 6968, epoch 15, batch 34/481,disc_loss 73.477, (real 75.655, fake 71.298 ) gen_loss 912.46\n",
            "iteration 6969, epoch 15, batch 35/481,disc_loss 72.851, (real 74.884, fake 70.819 ) gen_loss 996.59\n",
            "iteration 6970, epoch 15, batch 36/481,disc_loss 80.183, (real 82.478, fake 77.888 ) gen_loss 888.26\n",
            "iteration 6971, epoch 15, batch 37/481,disc_loss 77.866, (real 80.703, fake 75.029 ) gen_loss 833.26\n",
            "iteration 6972, epoch 15, batch 38/481,disc_loss 74.832, (real 76.939, fake 72.726 ) gen_loss 870.55\n",
            "iteration 6973, epoch 15, batch 39/481,disc_loss 79.351, (real 81.816, fake 76.887 ) gen_loss 830.32\n",
            "iteration 6974, epoch 15, batch 40/481,disc_loss 75.831, (real 77.941, fake 73.722 ) gen_loss 810.08\n",
            "iteration 6975, epoch 15, batch 41/481,disc_loss 79.745, (real 81.617, fake 77.873 ) gen_loss 823.38\n",
            "iteration 6976, epoch 15, batch 42/481,disc_loss 76.606, (real 78.095, fake 75.116 ) gen_loss 865.79\n",
            "iteration 6977, epoch 15, batch 43/481,disc_loss 78.116, (real 80.294, fake 75.937 ) gen_loss 937.36\n",
            "iteration 6978, epoch 15, batch 44/481,disc_loss 75.617, (real 77.805, fake 73.43 ) gen_loss 902.64\n",
            "iteration 6979, epoch 15, batch 45/481,disc_loss 76.313, (real 78.532, fake 74.094 ) gen_loss 902.26\n",
            "iteration 6980, epoch 15, batch 46/481,disc_loss 80.36, (real 83.061, fake 77.658 ) gen_loss 939.75\n",
            "iteration 6981, epoch 15, batch 47/481,disc_loss 78.379, (real 80.666, fake 76.092 ) gen_loss 883.74\n",
            "iteration 6982, epoch 15, batch 48/481,disc_loss 78.786, (real 80.91, fake 76.661 ) gen_loss 790.48\n",
            "iteration 6983, epoch 15, batch 49/481,disc_loss 73.826, (real 75.811, fake 71.842 ) gen_loss 887.23\n",
            "iteration 6984, epoch 15, batch 50/481,disc_loss 77.603, (real 79.508, fake 75.698 ) gen_loss 870.15\n",
            "iteration 6985, epoch 15, batch 51/481,disc_loss 83.074, (real 85.859, fake 80.29 ) gen_loss 820.81\n",
            "iteration 6986, epoch 15, batch 52/481,disc_loss 77.217, (real 79.824, fake 74.61 ) gen_loss 1005.2\n",
            "iteration 6987, epoch 15, batch 53/481,disc_loss 75.623, (real 78.999, fake 72.248 ) gen_loss 904.14\n",
            "iteration 6988, epoch 15, batch 54/481,disc_loss 76.568, (real 78.639, fake 74.496 ) gen_loss 885.36\n",
            "iteration 6989, epoch 15, batch 55/481,disc_loss 80.243, (real 81.615, fake 78.872 ) gen_loss 954.1\n",
            "iteration 6990, epoch 15, batch 56/481,disc_loss 81.782, (real 83.121, fake 80.444 ) gen_loss 1208.6\n",
            "iteration 6991, epoch 15, batch 57/481,disc_loss 87.947, (real 102.08, fake 73.816 ) gen_loss 1102.2\n",
            "iteration 6992, epoch 15, batch 58/481,disc_loss 83.163, (real 85.306, fake 81.019 ) gen_loss 1123.6\n",
            "iteration 6993, epoch 15, batch 59/481,disc_loss 96.38, (real 98.87, fake 93.89 ) gen_loss 2121.5\n",
            "iteration 6994, epoch 15, batch 60/481,disc_loss 168.74, (real 155.64, fake 181.83 ) gen_loss 2527.2\n",
            "iteration 6995, epoch 15, batch 61/481,disc_loss 436.13, (real 403.45, fake 468.81 ) gen_loss 6176.2\n",
            "iteration 6996, epoch 15, batch 62/481,disc_loss 339.77, (real 316.52, fake 363.01 ) gen_loss 1964.7\n",
            "iteration 6997, epoch 15, batch 63/481,disc_loss 710.98, (real 641.21, fake 780.76 ) gen_loss 2942.2\n",
            "iteration 6998, epoch 15, batch 64/481,disc_loss 345.16, (real 321.9, fake 368.43 ) gen_loss 1953.3\n",
            "iteration 6999, epoch 15, batch 65/481,disc_loss 388.03, (real 418.48, fake 357.59 ) gen_loss 818.72\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 7000, epoch 15, batch 66/481,disc_loss 187.64, (real 186.37, fake 188.9 ) gen_loss 462.31\n",
            "iteration 7001, epoch 15, batch 67/481,disc_loss 142.88, (real 110.45, fake 175.3 ) gen_loss 377.06\n",
            "iteration 7002, epoch 15, batch 68/481,disc_loss 159.03, (real 109.68, fake 208.39 ) gen_loss 362.08\n",
            "iteration 7003, epoch 15, batch 69/481,disc_loss 163.48, (real 112.97, fake 213.98 ) gen_loss 326.88\n",
            "iteration 7004, epoch 15, batch 70/481,disc_loss 166.88, (real 118.15, fake 215.6 ) gen_loss 314.23\n",
            "iteration 7005, epoch 15, batch 71/481,disc_loss 148.3, (real 97.195, fake 199.4 ) gen_loss 289.1\n",
            "iteration 7006, epoch 15, batch 72/481,disc_loss 142.97, (real 95.761, fake 190.17 ) gen_loss 281.94\n",
            "iteration 7007, epoch 15, batch 73/481,disc_loss 142.38, (real 94.753, fake 190.0 ) gen_loss 293.6\n",
            "iteration 7008, epoch 15, batch 74/481,disc_loss 140.59, (real 94.179, fake 186.99 ) gen_loss 287.78\n",
            "iteration 7009, epoch 15, batch 75/481,disc_loss 141.38, (real 97.254, fake 185.51 ) gen_loss 304.24\n",
            "iteration 7010, epoch 15, batch 76/481,disc_loss 141.04, (real 98.772, fake 183.31 ) gen_loss 320.97\n",
            "iteration 7011, epoch 15, batch 77/481,disc_loss 133.62, (real 95.635, fake 171.61 ) gen_loss 306.35\n",
            "iteration 7012, epoch 15, batch 78/481,disc_loss 123.82, (real 94.148, fake 153.49 ) gen_loss 318.77\n",
            "iteration 7013, epoch 15, batch 79/481,disc_loss 93.261, (real 91.976, fake 94.547 ) gen_loss 338.78\n",
            "iteration 7014, epoch 15, batch 80/481,disc_loss 93.094, (real 101.9, fake 84.285 ) gen_loss 335.81\n",
            "iteration 7015, epoch 15, batch 81/481,disc_loss 94.123, (real 102.42, fake 85.824 ) gen_loss 327.48\n",
            "iteration 7016, epoch 15, batch 82/481,disc_loss 92.929, (real 97.019, fake 88.838 ) gen_loss 320.42\n",
            "iteration 7017, epoch 15, batch 83/481,disc_loss 94.965, (real 99.46, fake 90.47 ) gen_loss 348.25\n",
            "iteration 7018, epoch 15, batch 84/481,disc_loss 92.246, (real 99.483, fake 85.009 ) gen_loss 345.45\n",
            "iteration 7019, epoch 15, batch 85/481,disc_loss 89.039, (real 93.168, fake 84.91 ) gen_loss 334.03\n",
            "iteration 7020, epoch 15, batch 86/481,disc_loss 84.105, (real 90.092, fake 78.118 ) gen_loss 375.99\n",
            "iteration 7021, epoch 15, batch 87/481,disc_loss 83.552, (real 89.272, fake 77.832 ) gen_loss 385.08\n",
            "iteration 7022, epoch 15, batch 88/481,disc_loss 87.263, (real 91.382, fake 83.144 ) gen_loss 371.47\n",
            "iteration 7023, epoch 15, batch 89/481,disc_loss 85.059, (real 90.34, fake 79.778 ) gen_loss 355.8\n",
            "iteration 7024, epoch 15, batch 90/481,disc_loss 85.241, (real 89.425, fake 81.057 ) gen_loss 354.58\n",
            "iteration 7025, epoch 15, batch 91/481,disc_loss 86.187, (real 91.926, fake 80.448 ) gen_loss 367.68\n",
            "iteration 7026, epoch 15, batch 92/481,disc_loss 82.761, (real 87.417, fake 78.105 ) gen_loss 375.99\n",
            "iteration 7027, epoch 15, batch 93/481,disc_loss 87.336, (real 92.852, fake 81.82 ) gen_loss 378.99\n",
            "iteration 7028, epoch 15, batch 94/481,disc_loss 88.279, (real 92.983, fake 83.574 ) gen_loss 393.96\n",
            "iteration 7029, epoch 15, batch 95/481,disc_loss 88.522, (real 94.901, fake 82.142 ) gen_loss 498.33\n",
            "iteration 7030, epoch 15, batch 96/481,disc_loss 86.323, (real 92.266, fake 80.38 ) gen_loss 409.87\n",
            "iteration 7031, epoch 15, batch 97/481,disc_loss 84.376, (real 91.247, fake 77.505 ) gen_loss 360.76\n",
            "iteration 7032, epoch 15, batch 98/481,disc_loss 84.451, (real 89.47, fake 79.432 ) gen_loss 395.95\n",
            "iteration 7033, epoch 15, batch 99/481,disc_loss 83.902, (real 90.0, fake 77.804 ) gen_loss 405.51\n",
            "iteration 7034, epoch 15, batch 100/481,disc_loss 84.884, (real 89.98, fake 79.787 ) gen_loss 419.04\n",
            "iteration 7035, epoch 15, batch 101/481,disc_loss 87.665, (real 93.602, fake 81.729 ) gen_loss 470.39\n",
            "iteration 7036, epoch 15, batch 102/481,disc_loss 84.386, (real 90.83, fake 77.942 ) gen_loss 448.78\n",
            "iteration 7037, epoch 15, batch 103/481,disc_loss 86.216, (real 91.402, fake 81.031 ) gen_loss 408.97\n",
            "iteration 7038, epoch 15, batch 104/481,disc_loss 83.94, (real 89.211, fake 78.668 ) gen_loss 409.84\n",
            "iteration 7039, epoch 15, batch 105/481,disc_loss 79.559, (real 85.431, fake 73.686 ) gen_loss 466.66\n",
            "iteration 7040, epoch 15, batch 106/481,disc_loss 79.62, (real 85.581, fake 73.658 ) gen_loss 466.7\n",
            "iteration 7041, epoch 15, batch 107/481,disc_loss 77.417, (real 81.192, fake 73.641 ) gen_loss 478.08\n",
            "iteration 7042, epoch 15, batch 108/481,disc_loss 81.273, (real 87.676, fake 74.87 ) gen_loss 449.17\n",
            "iteration 7043, epoch 15, batch 109/481,disc_loss 81.486, (real 86.42, fake 76.552 ) gen_loss 432.35\n",
            "iteration 7044, epoch 15, batch 110/481,disc_loss 80.514, (real 85.428, fake 75.6 ) gen_loss 489.32\n",
            "iteration 7045, epoch 15, batch 111/481,disc_loss 75.815, (real 80.027, fake 71.604 ) gen_loss 450.83\n",
            "iteration 7046, epoch 15, batch 112/481,disc_loss 80.741, (real 86.786, fake 74.695 ) gen_loss 481.73\n",
            "iteration 7047, epoch 15, batch 113/481,disc_loss 82.964, (real 87.718, fake 78.211 ) gen_loss 529.01\n",
            "iteration 7048, epoch 15, batch 114/481,disc_loss 83.98, (real 89.815, fake 78.145 ) gen_loss 503.44\n",
            "iteration 7049, epoch 15, batch 115/481,disc_loss 90.566, (real 95.871, fake 85.261 ) gen_loss 466.38\n",
            "iteration 7050, epoch 15, batch 116/481,disc_loss 90.824, (real 96.034, fake 85.614 ) gen_loss 515.35\n",
            "iteration 7051, epoch 15, batch 117/481,disc_loss 83.884, (real 90.1, fake 77.667 ) gen_loss 492.15\n",
            "iteration 7052, epoch 15, batch 118/481,disc_loss 80.734, (real 86.142, fake 75.326 ) gen_loss 465.26\n",
            "iteration 7053, epoch 15, batch 119/481,disc_loss 85.247, (real 90.558, fake 79.935 ) gen_loss 412.94\n",
            "iteration 7054, epoch 15, batch 120/481,disc_loss 82.9, (real 86.799, fake 79.0 ) gen_loss 412.27\n",
            "iteration 7055, epoch 15, batch 121/481,disc_loss 85.727, (real 90.576, fake 80.879 ) gen_loss 465.8\n",
            "iteration 7056, epoch 15, batch 122/481,disc_loss 79.611, (real 84.767, fake 74.454 ) gen_loss 508.52\n",
            "iteration 7057, epoch 15, batch 123/481,disc_loss 83.787, (real 89.701, fake 77.874 ) gen_loss 490.83\n",
            "iteration 7058, epoch 15, batch 124/481,disc_loss 82.044, (real 88.019, fake 76.07 ) gen_loss 533.9\n",
            "iteration 7059, epoch 15, batch 125/481,disc_loss 81.212, (real 86.927, fake 75.496 ) gen_loss 502.51\n",
            "iteration 7060, epoch 15, batch 126/481,disc_loss 81.275, (real 86.411, fake 76.138 ) gen_loss 557.54\n",
            "iteration 7061, epoch 15, batch 127/481,disc_loss 82.759, (real 88.922, fake 76.597 ) gen_loss 530.38\n",
            "iteration 7062, epoch 15, batch 128/481,disc_loss 81.486, (real 86.882, fake 76.09 ) gen_loss 516.44\n",
            "iteration 7063, epoch 15, batch 129/481,disc_loss 81.129, (real 85.955, fake 76.303 ) gen_loss 497.0\n",
            "iteration 7064, epoch 15, batch 130/481,disc_loss 77.629, (real 83.303, fake 71.956 ) gen_loss 575.06\n",
            "iteration 7065, epoch 15, batch 131/481,disc_loss 90.22, (real 95.89, fake 84.551 ) gen_loss 505.62\n",
            "iteration 7066, epoch 15, batch 132/481,disc_loss 79.026, (real 83.169, fake 74.884 ) gen_loss 478.48\n",
            "iteration 7067, epoch 15, batch 133/481,disc_loss 82.557, (real 90.315, fake 74.8 ) gen_loss 473.4\n",
            "iteration 7068, epoch 15, batch 134/481,disc_loss 88.766, (real 93.172, fake 84.361 ) gen_loss 480.3\n",
            "iteration 7069, epoch 15, batch 135/481,disc_loss 85.334, (real 89.485, fake 81.183 ) gen_loss 457.12\n",
            "iteration 7070, epoch 15, batch 136/481,disc_loss 81.792, (real 86.34, fake 77.244 ) gen_loss 417.14\n",
            "iteration 7071, epoch 15, batch 137/481,disc_loss 81.235, (real 86.063, fake 76.406 ) gen_loss 488.49\n",
            "iteration 7072, epoch 15, batch 138/481,disc_loss 81.049, (real 85.95, fake 76.148 ) gen_loss 502.71\n",
            "iteration 7073, epoch 15, batch 139/481,disc_loss 82.88, (real 87.357, fake 78.403 ) gen_loss 492.43\n",
            "iteration 7074, epoch 15, batch 140/481,disc_loss 82.296, (real 88.061, fake 76.53 ) gen_loss 460.34\n",
            "iteration 7075, epoch 15, batch 141/481,disc_loss 77.727, (real 82.921, fake 72.533 ) gen_loss 520.06\n",
            "iteration 7076, epoch 15, batch 142/481,disc_loss 80.001, (real 86.305, fake 73.696 ) gen_loss 525.88\n",
            "iteration 7077, epoch 15, batch 143/481,disc_loss 75.936, (real 79.963, fake 71.91 ) gen_loss 523.43\n",
            "iteration 7078, epoch 15, batch 144/481,disc_loss 80.234, (real 86.644, fake 73.824 ) gen_loss 484.13\n",
            "iteration 7079, epoch 15, batch 145/481,disc_loss 75.289, (real 80.181, fake 70.397 ) gen_loss 543.34\n",
            "iteration 7080, epoch 15, batch 146/481,disc_loss 77.116, (real 82.546, fake 71.685 ) gen_loss 477.89\n",
            "iteration 7081, epoch 15, batch 147/481,disc_loss 80.024, (real 85.295, fake 74.754 ) gen_loss 491.38\n",
            "iteration 7082, epoch 15, batch 148/481,disc_loss 77.88, (real 81.934, fake 73.825 ) gen_loss 511.42\n",
            "iteration 7083, epoch 15, batch 149/481,disc_loss 84.55, (real 89.58, fake 79.519 ) gen_loss 585.72\n",
            "iteration 7084, epoch 15, batch 150/481,disc_loss 80.857, (real 85.688, fake 76.026 ) gen_loss 522.08\n",
            "iteration 7085, epoch 15, batch 151/481,disc_loss 82.434, (real 86.714, fake 78.155 ) gen_loss 485.23\n",
            "iteration 7086, epoch 15, batch 152/481,disc_loss 78.134, (real 82.949, fake 73.319 ) gen_loss 514.83\n",
            "iteration 7087, epoch 15, batch 153/481,disc_loss 81.091, (real 86.27, fake 75.911 ) gen_loss 506.16\n",
            "iteration 7088, epoch 15, batch 154/481,disc_loss 84.269, (real 88.462, fake 80.077 ) gen_loss 523.86\n",
            "iteration 7089, epoch 15, batch 155/481,disc_loss 79.286, (real 83.872, fake 74.7 ) gen_loss 468.67\n",
            "iteration 7090, epoch 15, batch 156/481,disc_loss 78.049, (real 82.68, fake 73.418 ) gen_loss 476.58\n",
            "iteration 7091, epoch 15, batch 157/481,disc_loss 85.989, (real 92.141, fake 79.836 ) gen_loss 484.78\n",
            "iteration 7092, epoch 15, batch 158/481,disc_loss 82.266, (real 86.916, fake 77.616 ) gen_loss 521.79\n",
            "iteration 7093, epoch 15, batch 159/481,disc_loss 79.961, (real 84.924, fake 74.997 ) gen_loss 509.66\n",
            "iteration 7094, epoch 15, batch 160/481,disc_loss 80.093, (real 84.931, fake 75.254 ) gen_loss 472.04\n",
            "iteration 7095, epoch 15, batch 161/481,disc_loss 78.465, (real 85.357, fake 71.573 ) gen_loss 552.97\n",
            "iteration 7096, epoch 15, batch 162/481,disc_loss 83.694, (real 89.845, fake 77.544 ) gen_loss 559.75\n",
            "iteration 7097, epoch 15, batch 163/481,disc_loss 86.919, (real 92.001, fake 81.838 ) gen_loss 529.72\n",
            "iteration 7098, epoch 15, batch 164/481,disc_loss 79.975, (real 85.15, fake 74.801 ) gen_loss 522.78\n",
            "iteration 7099, epoch 15, batch 165/481,disc_loss 74.85, (real 79.571, fake 70.129 ) gen_loss 558.34\n",
            "iteration 7100, epoch 15, batch 166/481,disc_loss 79.41, (real 83.961, fake 74.859 ) gen_loss 575.08\n",
            "iteration 7101, epoch 15, batch 167/481,disc_loss 80.247, (real 86.099, fake 74.396 ) gen_loss 600.36\n",
            "iteration 7102, epoch 15, batch 168/481,disc_loss 83.852, (real 89.787, fake 77.917 ) gen_loss 522.61\n",
            "iteration 7103, epoch 15, batch 169/481,disc_loss 78.704, (real 83.748, fake 73.659 ) gen_loss 562.24\n",
            "iteration 7104, epoch 15, batch 170/481,disc_loss 84.804, (real 89.342, fake 80.266 ) gen_loss 550.67\n",
            "iteration 7105, epoch 15, batch 171/481,disc_loss 83.703, (real 88.89, fake 78.516 ) gen_loss 533.86\n",
            "iteration 7106, epoch 15, batch 172/481,disc_loss 79.303, (real 83.911, fake 74.694 ) gen_loss 549.93\n",
            "iteration 7107, epoch 15, batch 173/481,disc_loss 81.971, (real 87.037, fake 76.905 ) gen_loss 522.11\n",
            "iteration 7108, epoch 15, batch 174/481,disc_loss 86.829, (real 90.93, fake 82.729 ) gen_loss 469.29\n",
            "iteration 7109, epoch 15, batch 175/481,disc_loss 80.483, (real 85.496, fake 75.47 ) gen_loss 466.31\n",
            "iteration 7110, epoch 15, batch 176/481,disc_loss 78.407, (real 81.988, fake 74.827 ) gen_loss 521.34\n",
            "iteration 7111, epoch 15, batch 177/481,disc_loss 77.829, (real 83.001, fake 72.657 ) gen_loss 521.0\n",
            "iteration 7112, epoch 15, batch 178/481,disc_loss 80.638, (real 84.678, fake 76.599 ) gen_loss 515.03\n",
            "iteration 7113, epoch 15, batch 179/481,disc_loss 81.025, (real 85.779, fake 76.271 ) gen_loss 537.21\n",
            "iteration 7114, epoch 15, batch 180/481,disc_loss 77.958, (real 83.071, fake 72.846 ) gen_loss 593.09\n",
            "iteration 7115, epoch 15, batch 181/481,disc_loss 78.177, (real 83.851, fake 72.503 ) gen_loss 522.72\n",
            "iteration 7116, epoch 15, batch 182/481,disc_loss 84.209, (real 89.423, fake 78.995 ) gen_loss 447.11\n",
            "iteration 7117, epoch 15, batch 183/481,disc_loss 80.555, (real 85.361, fake 75.748 ) gen_loss 543.87\n",
            "iteration 7118, epoch 15, batch 184/481,disc_loss 80.915, (real 85.665, fake 76.164 ) gen_loss 486.17\n",
            "iteration 7119, epoch 15, batch 185/481,disc_loss 80.968, (real 84.918, fake 77.019 ) gen_loss 510.85\n",
            "iteration 7120, epoch 15, batch 186/481,disc_loss 82.648, (real 87.258, fake 78.039 ) gen_loss 551.57\n",
            "iteration 7121, epoch 15, batch 187/481,disc_loss 83.836, (real 88.544, fake 79.128 ) gen_loss 597.14\n",
            "iteration 7122, epoch 15, batch 188/481,disc_loss 81.777, (real 87.899, fake 75.655 ) gen_loss 495.38\n",
            "iteration 7123, epoch 15, batch 189/481,disc_loss 82.556, (real 86.531, fake 78.582 ) gen_loss 524.39\n",
            "iteration 7124, epoch 15, batch 190/481,disc_loss 79.735, (real 84.01, fake 75.46 ) gen_loss 528.19\n",
            "iteration 7125, epoch 15, batch 191/481,disc_loss 82.155, (real 87.187, fake 77.124 ) gen_loss 551.36\n",
            "iteration 7126, epoch 15, batch 192/481,disc_loss 79.968, (real 85.133, fake 74.803 ) gen_loss 687.44\n",
            "iteration 7127, epoch 15, batch 193/481,disc_loss 78.332, (real 83.387, fake 73.277 ) gen_loss 525.05\n",
            "iteration 7128, epoch 15, batch 194/481,disc_loss 86.66, (real 93.162, fake 80.158 ) gen_loss 627.93\n",
            "iteration 7129, epoch 15, batch 195/481,disc_loss 82.035, (real 88.26, fake 75.809 ) gen_loss 553.47\n",
            "iteration 7130, epoch 15, batch 196/481,disc_loss 79.202, (real 82.496, fake 75.907 ) gen_loss 542.18\n",
            "iteration 7131, epoch 15, batch 197/481,disc_loss 82.408, (real 87.347, fake 77.469 ) gen_loss 554.21\n",
            "iteration 7132, epoch 15, batch 198/481,disc_loss 78.542, (real 84.945, fake 72.139 ) gen_loss 510.1\n",
            "iteration 7133, epoch 15, batch 199/481,disc_loss 85.442, (real 90.421, fake 80.463 ) gen_loss 538.96\n",
            "iteration 7134, epoch 15, batch 200/481,disc_loss 76.853, (real 80.657, fake 73.05 ) gen_loss 544.45\n",
            "iteration 7135, epoch 15, batch 201/481,disc_loss 83.285, (real 87.697, fake 78.872 ) gen_loss 538.68\n",
            "iteration 7136, epoch 15, batch 202/481,disc_loss 84.91, (real 89.345, fake 80.476 ) gen_loss 604.11\n",
            "iteration 7137, epoch 15, batch 203/481,disc_loss 83.72, (real 88.19, fake 79.251 ) gen_loss 551.26\n",
            "iteration 7138, epoch 15, batch 204/481,disc_loss 82.936, (real 87.168, fake 78.704 ) gen_loss 534.7\n",
            "iteration 7139, epoch 15, batch 205/481,disc_loss 81.096, (real 85.336, fake 76.855 ) gen_loss 539.64\n",
            "iteration 7140, epoch 15, batch 206/481,disc_loss 79.129, (real 83.681, fake 74.576 ) gen_loss 551.47\n",
            "iteration 7141, epoch 15, batch 207/481,disc_loss 80.607, (real 85.044, fake 76.171 ) gen_loss 542.73\n",
            "iteration 7142, epoch 15, batch 208/481,disc_loss 85.405, (real 89.099, fake 81.711 ) gen_loss 517.77\n",
            "iteration 7143, epoch 15, batch 209/481,disc_loss 80.512, (real 86.497, fake 74.527 ) gen_loss 544.37\n",
            "iteration 7144, epoch 15, batch 210/481,disc_loss 85.069, (real 89.153, fake 80.985 ) gen_loss 536.24\n",
            "iteration 7145, epoch 15, batch 211/481,disc_loss 82.681, (real 87.599, fake 77.764 ) gen_loss 580.01\n",
            "iteration 7146, epoch 15, batch 212/481,disc_loss 81.157, (real 86.533, fake 75.781 ) gen_loss 541.95\n",
            "iteration 7147, epoch 15, batch 213/481,disc_loss 80.422, (real 85.026, fake 75.819 ) gen_loss 494.0\n",
            "iteration 7148, epoch 15, batch 214/481,disc_loss 80.44, (real 84.095, fake 76.785 ) gen_loss 532.51\n",
            "iteration 7149, epoch 15, batch 215/481,disc_loss 81.606, (real 86.084, fake 77.129 ) gen_loss 563.99\n",
            "iteration 7150, epoch 15, batch 216/481,disc_loss 79.237, (real 83.241, fake 75.234 ) gen_loss 549.18\n",
            "iteration 7151, epoch 15, batch 217/481,disc_loss 81.701, (real 86.29, fake 77.113 ) gen_loss 517.27\n",
            "iteration 7152, epoch 15, batch 218/481,disc_loss 85.082, (real 89.539, fake 80.624 ) gen_loss 509.27\n",
            "iteration 7153, epoch 15, batch 219/481,disc_loss 80.141, (real 85.943, fake 74.338 ) gen_loss 510.24\n",
            "iteration 7154, epoch 15, batch 220/481,disc_loss 80.044, (real 84.634, fake 75.453 ) gen_loss 564.11\n",
            "iteration 7155, epoch 15, batch 221/481,disc_loss 79.43, (real 83.608, fake 75.252 ) gen_loss 676.34\n",
            "iteration 7156, epoch 15, batch 222/481,disc_loss 80.688, (real 84.219, fake 77.157 ) gen_loss 575.15\n",
            "iteration 7157, epoch 15, batch 223/481,disc_loss 81.747, (real 86.025, fake 77.469 ) gen_loss 543.59\n",
            "iteration 7158, epoch 15, batch 224/481,disc_loss 80.686, (real 85.479, fake 75.893 ) gen_loss 533.63\n",
            "iteration 7159, epoch 15, batch 225/481,disc_loss 80.203, (real 84.333, fake 76.074 ) gen_loss 539.77\n",
            "iteration 7160, epoch 15, batch 226/481,disc_loss 82.468, (real 87.589, fake 77.346 ) gen_loss 522.96\n",
            "iteration 7161, epoch 15, batch 227/481,disc_loss 79.225, (real 83.372, fake 75.077 ) gen_loss 546.23\n",
            "iteration 7162, epoch 15, batch 228/481,disc_loss 82.814, (real 87.913, fake 77.715 ) gen_loss 635.06\n",
            "iteration 7163, epoch 15, batch 229/481,disc_loss 81.704, (real 85.8, fake 77.609 ) gen_loss 559.17\n",
            "iteration 7164, epoch 15, batch 230/481,disc_loss 79.235, (real 83.779, fake 74.692 ) gen_loss 563.3\n",
            "iteration 7165, epoch 15, batch 231/481,disc_loss 74.293, (real 80.174, fake 68.413 ) gen_loss 554.32\n",
            "iteration 7166, epoch 15, batch 232/481,disc_loss 82.488, (real 87.186, fake 77.79 ) gen_loss 586.46\n",
            "iteration 7167, epoch 15, batch 233/481,disc_loss 81.005, (real 85.799, fake 76.212 ) gen_loss 521.64\n",
            "iteration 7168, epoch 15, batch 234/481,disc_loss 75.832, (real 79.825, fake 71.84 ) gen_loss 568.46\n",
            "iteration 7169, epoch 15, batch 235/481,disc_loss 78.617, (real 83.975, fake 73.258 ) gen_loss 591.74\n",
            "iteration 7170, epoch 15, batch 236/481,disc_loss 79.952, (real 86.047, fake 73.857 ) gen_loss 578.44\n",
            "iteration 7171, epoch 15, batch 237/481,disc_loss 78.416, (real 83.59, fake 73.242 ) gen_loss 623.38\n",
            "iteration 7172, epoch 15, batch 238/481,disc_loss 77.83, (real 82.323, fake 73.337 ) gen_loss 585.52\n",
            "iteration 7173, epoch 15, batch 239/481,disc_loss 77.584, (real 82.232, fake 72.936 ) gen_loss 609.06\n",
            "iteration 7174, epoch 15, batch 240/481,disc_loss 81.494, (real 86.724, fake 76.264 ) gen_loss 597.96\n",
            "iteration 7175, epoch 15, batch 241/481,disc_loss 77.308, (real 82.492, fake 72.125 ) gen_loss 604.22\n",
            "iteration 7176, epoch 15, batch 242/481,disc_loss 80.431, (real 83.643, fake 77.219 ) gen_loss 613.61\n",
            "iteration 7177, epoch 15, batch 243/481,disc_loss 80.228, (real 84.917, fake 75.538 ) gen_loss 589.78\n",
            "iteration 7178, epoch 15, batch 244/481,disc_loss 82.272, (real 87.147, fake 77.396 ) gen_loss 593.87\n",
            "iteration 7179, epoch 15, batch 245/481,disc_loss 77.609, (real 82.254, fake 72.964 ) gen_loss 631.16\n",
            "iteration 7180, epoch 15, batch 246/481,disc_loss 77.655, (real 81.705, fake 73.605 ) gen_loss 628.17\n",
            "iteration 7181, epoch 15, batch 247/481,disc_loss 80.522, (real 84.664, fake 76.379 ) gen_loss 647.8\n",
            "iteration 7182, epoch 15, batch 248/481,disc_loss 77.654, (real 81.669, fake 73.64 ) gen_loss 573.23\n",
            "iteration 7183, epoch 15, batch 249/481,disc_loss 78.705, (real 82.801, fake 74.61 ) gen_loss 640.13\n",
            "iteration 7184, epoch 15, batch 250/481,disc_loss 80.159, (real 84.327, fake 75.99 ) gen_loss 621.43\n",
            "iteration 7185, epoch 15, batch 251/481,disc_loss 77.432, (real 82.332, fake 72.531 ) gen_loss 571.11\n",
            "iteration 7186, epoch 15, batch 252/481,disc_loss 82.216, (real 85.674, fake 78.757 ) gen_loss 564.16\n",
            "iteration 7187, epoch 15, batch 253/481,disc_loss 79.568, (real 83.674, fake 75.462 ) gen_loss 613.36\n",
            "iteration 7188, epoch 15, batch 254/481,disc_loss 81.486, (real 87.313, fake 75.659 ) gen_loss 596.53\n",
            "iteration 7189, epoch 15, batch 255/481,disc_loss 80.473, (real 84.996, fake 75.95 ) gen_loss 570.25\n",
            "iteration 7190, epoch 15, batch 256/481,disc_loss 79.847, (real 84.351, fake 75.342 ) gen_loss 601.65\n",
            "iteration 7191, epoch 15, batch 257/481,disc_loss 77.459, (real 81.446, fake 73.473 ) gen_loss 669.78\n",
            "iteration 7192, epoch 15, batch 258/481,disc_loss 78.535, (real 81.997, fake 75.074 ) gen_loss 587.08\n",
            "iteration 7193, epoch 15, batch 259/481,disc_loss 80.357, (real 85.142, fake 75.573 ) gen_loss 553.93\n",
            "iteration 7194, epoch 15, batch 260/481,disc_loss 77.064, (real 81.353, fake 72.776 ) gen_loss 591.71\n",
            "iteration 7195, epoch 15, batch 261/481,disc_loss 80.254, (real 83.769, fake 76.739 ) gen_loss 604.19\n",
            "iteration 7196, epoch 15, batch 262/481,disc_loss 80.48, (real 85.189, fake 75.771 ) gen_loss 683.51\n",
            "iteration 7197, epoch 15, batch 263/481,disc_loss 79.355, (real 84.123, fake 74.588 ) gen_loss 692.43\n",
            "iteration 7198, epoch 15, batch 264/481,disc_loss 76.673, (real 79.92, fake 73.425 ) gen_loss 636.66\n",
            "iteration 7199, epoch 15, batch 265/481,disc_loss 77.587, (real 83.246, fake 71.927 ) gen_loss 629.17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 7200, epoch 15, batch 266/481,disc_loss 80.982, (real 86.128, fake 75.836 ) gen_loss 598.07\n",
            "iteration 7201, epoch 15, batch 267/481,disc_loss 79.431, (real 84.088, fake 74.774 ) gen_loss 684.69\n",
            "iteration 7202, epoch 15, batch 268/481,disc_loss 80.2, (real 84.176, fake 76.225 ) gen_loss 631.21\n",
            "iteration 7203, epoch 15, batch 269/481,disc_loss 75.913, (real 79.659, fake 72.167 ) gen_loss 583.69\n",
            "iteration 7204, epoch 15, batch 270/481,disc_loss 85.035, (real 90.173, fake 79.898 ) gen_loss 668.88\n",
            "iteration 7205, epoch 15, batch 271/481,disc_loss 79.599, (real 83.541, fake 75.656 ) gen_loss 612.36\n",
            "iteration 7206, epoch 15, batch 272/481,disc_loss 83.565, (real 87.924, fake 79.206 ) gen_loss 566.75\n",
            "iteration 7207, epoch 15, batch 273/481,disc_loss 79.362, (real 84.28, fake 74.445 ) gen_loss 623.05\n",
            "iteration 7208, epoch 15, batch 274/481,disc_loss 77.243, (real 81.004, fake 73.482 ) gen_loss 667.69\n",
            "iteration 7209, epoch 15, batch 275/481,disc_loss 83.373, (real 89.022, fake 77.725 ) gen_loss 558.56\n",
            "iteration 7210, epoch 15, batch 276/481,disc_loss 81.177, (real 84.468, fake 77.887 ) gen_loss 568.47\n",
            "iteration 7211, epoch 15, batch 277/481,disc_loss 83.688, (real 88.623, fake 78.752 ) gen_loss 603.44\n",
            "iteration 7212, epoch 15, batch 278/481,disc_loss 78.929, (real 82.822, fake 75.036 ) gen_loss 643.45\n",
            "iteration 7213, epoch 15, batch 279/481,disc_loss 79.739, (real 84.363, fake 75.115 ) gen_loss 591.12\n",
            "iteration 7214, epoch 15, batch 280/481,disc_loss 76.012, (real 80.439, fake 71.586 ) gen_loss 665.59\n",
            "iteration 7215, epoch 15, batch 281/481,disc_loss 73.533, (real 77.474, fake 69.592 ) gen_loss 669.25\n",
            "iteration 7216, epoch 15, batch 282/481,disc_loss 76.07, (real 80.56, fake 71.581 ) gen_loss 647.62\n",
            "iteration 7217, epoch 15, batch 283/481,disc_loss 82.752, (real 87.284, fake 78.219 ) gen_loss 650.21\n",
            "iteration 7218, epoch 15, batch 284/481,disc_loss 77.325, (real 81.8, fake 72.85 ) gen_loss 711.18\n",
            "iteration 7219, epoch 15, batch 285/481,disc_loss 75.025, (real 78.847, fake 71.202 ) gen_loss 696.9\n",
            "iteration 7220, epoch 15, batch 286/481,disc_loss 79.88, (real 83.553, fake 76.208 ) gen_loss 609.51\n",
            "iteration 7221, epoch 15, batch 287/481,disc_loss 79.501, (real 84.845, fake 74.157 ) gen_loss 694.49\n",
            "iteration 7222, epoch 15, batch 288/481,disc_loss 79.258, (real 83.374, fake 75.142 ) gen_loss 670.96\n",
            "iteration 7223, epoch 15, batch 289/481,disc_loss 82.896, (real 87.22, fake 78.573 ) gen_loss 591.09\n",
            "iteration 7224, epoch 15, batch 290/481,disc_loss 80.376, (real 83.627, fake 77.125 ) gen_loss 691.12\n",
            "iteration 7225, epoch 15, batch 291/481,disc_loss 77.843, (real 81.786, fake 73.9 ) gen_loss 676.95\n",
            "iteration 7226, epoch 15, batch 292/481,disc_loss 75.85, (real 79.834, fake 71.867 ) gen_loss 622.95\n",
            "iteration 7227, epoch 15, batch 293/481,disc_loss 77.85, (real 81.819, fake 73.881 ) gen_loss 612.04\n",
            "iteration 7228, epoch 15, batch 294/481,disc_loss 79.76, (real 83.143, fake 76.377 ) gen_loss 692.32\n",
            "iteration 7229, epoch 15, batch 295/481,disc_loss 80.286, (real 84.579, fake 75.993 ) gen_loss 632.19\n",
            "iteration 7230, epoch 15, batch 296/481,disc_loss 75.607, (real 79.865, fake 71.348 ) gen_loss 690.77\n",
            "iteration 7231, epoch 15, batch 297/481,disc_loss 76.862, (real 80.991, fake 72.732 ) gen_loss 669.88\n",
            "iteration 7232, epoch 15, batch 298/481,disc_loss 79.441, (real 83.14, fake 75.743 ) gen_loss 678.09\n",
            "iteration 7233, epoch 15, batch 299/481,disc_loss 78.919, (real 82.684, fake 75.154 ) gen_loss 679.66\n",
            "iteration 7234, epoch 15, batch 300/481,disc_loss 79.335, (real 82.752, fake 75.918 ) gen_loss 652.08\n",
            "iteration 7235, epoch 15, batch 301/481,disc_loss 81.792, (real 86.432, fake 77.151 ) gen_loss 665.83\n",
            "iteration 7236, epoch 15, batch 302/481,disc_loss 72.35, (real 75.515, fake 69.185 ) gen_loss 708.31\n",
            "iteration 7237, epoch 15, batch 303/481,disc_loss 75.497, (real 78.169, fake 72.824 ) gen_loss 633.43\n",
            "iteration 7238, epoch 15, batch 304/481,disc_loss 80.264, (real 84.573, fake 75.954 ) gen_loss 612.56\n",
            "iteration 7239, epoch 15, batch 305/481,disc_loss 80.163, (real 83.486, fake 76.84 ) gen_loss 662.97\n",
            "iteration 7240, epoch 15, batch 306/481,disc_loss 80.425, (real 84.228, fake 76.621 ) gen_loss 667.63\n",
            "iteration 7241, epoch 15, batch 307/481,disc_loss 76.851, (real 81.313, fake 72.389 ) gen_loss 693.54\n",
            "iteration 7242, epoch 15, batch 308/481,disc_loss 75.868, (real 80.694, fake 71.043 ) gen_loss 681.42\n",
            "iteration 7243, epoch 15, batch 309/481,disc_loss 82.113, (real 86.182, fake 78.044 ) gen_loss 693.84\n",
            "iteration 7244, epoch 15, batch 310/481,disc_loss 81.82, (real 86.932, fake 76.708 ) gen_loss 616.97\n",
            "iteration 7245, epoch 15, batch 311/481,disc_loss 80.361, (real 84.605, fake 76.116 ) gen_loss 616.84\n",
            "iteration 7246, epoch 15, batch 312/481,disc_loss 78.029, (real 83.001, fake 73.058 ) gen_loss 608.09\n",
            "iteration 7247, epoch 15, batch 313/481,disc_loss 80.569, (real 83.797, fake 77.34 ) gen_loss 674.44\n",
            "iteration 7248, epoch 15, batch 314/481,disc_loss 80.002, (real 84.591, fake 75.413 ) gen_loss 622.23\n",
            "iteration 7249, epoch 15, batch 315/481,disc_loss 79.276, (real 83.554, fake 74.999 ) gen_loss 616.7\n",
            "iteration 7250, epoch 15, batch 316/481,disc_loss 76.823, (real 79.745, fake 73.901 ) gen_loss 1000.6\n",
            "iteration 7251, epoch 15, batch 317/481,disc_loss 78.98, (real 82.044, fake 75.917 ) gen_loss 735.77\n",
            "iteration 7252, epoch 15, batch 318/481,disc_loss 80.107, (real 84.212, fake 76.003 ) gen_loss 606.54\n",
            "iteration 7253, epoch 15, batch 319/481,disc_loss 82.619, (real 87.266, fake 77.973 ) gen_loss 703.15\n",
            "iteration 7254, epoch 15, batch 320/481,disc_loss 79.104, (real 83.912, fake 74.296 ) gen_loss 661.73\n",
            "iteration 7255, epoch 15, batch 321/481,disc_loss 75.8, (real 79.796, fake 71.803 ) gen_loss 710.06\n",
            "iteration 7256, epoch 15, batch 322/481,disc_loss 77.195, (real 81.155, fake 73.236 ) gen_loss 670.99\n",
            "iteration 7257, epoch 15, batch 323/481,disc_loss 83.875, (real 87.121, fake 80.628 ) gen_loss 654.62\n",
            "iteration 7258, epoch 15, batch 324/481,disc_loss 80.166, (real 84.283, fake 76.05 ) gen_loss 674.91\n",
            "iteration 7259, epoch 15, batch 325/481,disc_loss 83.385, (real 87.359, fake 79.412 ) gen_loss 684.94\n",
            "iteration 7260, epoch 15, batch 326/481,disc_loss 77.424, (real 81.597, fake 73.251 ) gen_loss 638.18\n",
            "iteration 7261, epoch 15, batch 327/481,disc_loss 80.109, (real 84.113, fake 76.105 ) gen_loss 687.68\n",
            "iteration 7262, epoch 15, batch 328/481,disc_loss 77.52, (real 82.886, fake 72.155 ) gen_loss 693.28\n",
            "iteration 7263, epoch 15, batch 329/481,disc_loss 83.055, (real 87.292, fake 78.818 ) gen_loss 604.52\n",
            "iteration 7264, epoch 15, batch 330/481,disc_loss 75.928, (real 79.589, fake 72.268 ) gen_loss 661.3\n",
            "iteration 7265, epoch 15, batch 331/481,disc_loss 79.747, (real 82.812, fake 76.682 ) gen_loss 596.47\n",
            "iteration 7266, epoch 15, batch 332/481,disc_loss 78.894, (real 83.66, fake 74.128 ) gen_loss 603.93\n",
            "iteration 7267, epoch 15, batch 333/481,disc_loss 79.715, (real 83.927, fake 75.502 ) gen_loss 639.92\n",
            "iteration 7268, epoch 15, batch 334/481,disc_loss 79.165, (real 83.171, fake 75.158 ) gen_loss 675.4\n",
            "iteration 7269, epoch 15, batch 335/481,disc_loss 82.075, (real 85.438, fake 78.713 ) gen_loss 642.84\n",
            "iteration 7270, epoch 15, batch 336/481,disc_loss 81.179, (real 85.481, fake 76.877 ) gen_loss 652.16\n",
            "iteration 7271, epoch 15, batch 337/481,disc_loss 80.956, (real 85.039, fake 76.872 ) gen_loss 661.55\n",
            "iteration 7272, epoch 15, batch 338/481,disc_loss 78.664, (real 82.02, fake 75.307 ) gen_loss 682.69\n",
            "iteration 7273, epoch 15, batch 339/481,disc_loss 76.498, (real 81.096, fake 71.9 ) gen_loss 745.83\n",
            "iteration 7274, epoch 15, batch 340/481,disc_loss 81.955, (real 85.936, fake 77.973 ) gen_loss 587.25\n",
            "iteration 7275, epoch 15, batch 341/481,disc_loss 76.995, (real 80.022, fake 73.969 ) gen_loss 604.12\n",
            "iteration 7276, epoch 15, batch 342/481,disc_loss 79.587, (real 83.585, fake 75.59 ) gen_loss 631.33\n",
            "iteration 7277, epoch 15, batch 343/481,disc_loss 79.131, (real 82.096, fake 76.165 ) gen_loss 635.94\n",
            "iteration 7278, epoch 15, batch 344/481,disc_loss 77.438, (real 80.988, fake 73.887 ) gen_loss 686.3\n",
            "iteration 7279, epoch 15, batch 345/481,disc_loss 79.772, (real 82.705, fake 76.84 ) gen_loss 691.7\n",
            "iteration 7280, epoch 15, batch 346/481,disc_loss 76.701, (real 80.465, fake 72.937 ) gen_loss 758.04\n",
            "iteration 7281, epoch 15, batch 347/481,disc_loss 82.588, (real 85.789, fake 79.387 ) gen_loss 713.68\n",
            "iteration 7282, epoch 15, batch 348/481,disc_loss 75.838, (real 79.274, fake 72.401 ) gen_loss 672.32\n",
            "iteration 7283, epoch 15, batch 349/481,disc_loss 76.508, (real 81.663, fake 71.353 ) gen_loss 693.14\n",
            "iteration 7284, epoch 15, batch 350/481,disc_loss 78.151, (real 81.342, fake 74.959 ) gen_loss 706.38\n",
            "iteration 7285, epoch 15, batch 351/481,disc_loss 78.584, (real 82.703, fake 74.466 ) gen_loss 666.22\n",
            "iteration 7286, epoch 15, batch 352/481,disc_loss 78.723, (real 81.945, fake 75.501 ) gen_loss 684.36\n",
            "iteration 7287, epoch 15, batch 353/481,disc_loss 81.488, (real 85.739, fake 77.237 ) gen_loss 656.42\n",
            "iteration 7288, epoch 15, batch 354/481,disc_loss 82.135, (real 85.912, fake 78.357 ) gen_loss 678.81\n",
            "iteration 7289, epoch 15, batch 355/481,disc_loss 75.82, (real 78.971, fake 72.669 ) gen_loss 733.35\n",
            "iteration 7290, epoch 15, batch 356/481,disc_loss 81.335, (real 84.572, fake 78.098 ) gen_loss 644.61\n",
            "iteration 7291, epoch 15, batch 357/481,disc_loss 78.534, (real 82.174, fake 74.895 ) gen_loss 724.66\n",
            "iteration 7292, epoch 15, batch 358/481,disc_loss 78.394, (real 82.927, fake 73.861 ) gen_loss 694.73\n",
            "iteration 7293, epoch 15, batch 359/481,disc_loss 84.594, (real 88.201, fake 80.987 ) gen_loss 773.14\n",
            "iteration 7294, epoch 15, batch 360/481,disc_loss 78.422, (real 81.618, fake 75.226 ) gen_loss 623.34\n",
            "iteration 7295, epoch 15, batch 361/481,disc_loss 80.644, (real 84.616, fake 76.672 ) gen_loss 721.09\n",
            "iteration 7296, epoch 15, batch 362/481,disc_loss 80.251, (real 84.044, fake 76.459 ) gen_loss 677.0\n",
            "iteration 7297, epoch 15, batch 363/481,disc_loss 73.942, (real 77.469, fake 70.416 ) gen_loss 714.36\n",
            "iteration 7298, epoch 15, batch 364/481,disc_loss 82.143, (real 86.325, fake 77.962 ) gen_loss 673.07\n",
            "iteration 7299, epoch 15, batch 365/481,disc_loss 78.245, (real 81.488, fake 75.002 ) gen_loss 727.9\n",
            "iteration 7300, epoch 15, batch 366/481,disc_loss 79.297, (real 82.689, fake 75.905 ) gen_loss 735.11\n",
            "iteration 7301, epoch 15, batch 367/481,disc_loss 83.192, (real 87.494, fake 78.891 ) gen_loss 664.86\n",
            "iteration 7302, epoch 15, batch 368/481,disc_loss 82.579, (real 86.442, fake 78.716 ) gen_loss 721.78\n",
            "iteration 7303, epoch 15, batch 369/481,disc_loss 79.772, (real 84.57, fake 74.974 ) gen_loss 726.72\n",
            "iteration 7304, epoch 15, batch 370/481,disc_loss 77.876, (real 81.295, fake 74.456 ) gen_loss 726.82\n",
            "iteration 7305, epoch 15, batch 371/481,disc_loss 79.776, (real 82.743, fake 76.808 ) gen_loss 703.31\n",
            "iteration 7306, epoch 15, batch 372/481,disc_loss 77.918, (real 82.474, fake 73.361 ) gen_loss 712.63\n",
            "iteration 7307, epoch 15, batch 373/481,disc_loss 81.721, (real 85.136, fake 78.306 ) gen_loss 699.88\n",
            "iteration 7308, epoch 15, batch 374/481,disc_loss 79.629, (real 83.802, fake 75.457 ) gen_loss 682.69\n",
            "iteration 7309, epoch 15, batch 375/481,disc_loss 82.466, (real 87.755, fake 77.178 ) gen_loss 626.08\n",
            "iteration 7310, epoch 15, batch 376/481,disc_loss 78.806, (real 82.82, fake 74.791 ) gen_loss 756.17\n",
            "iteration 7311, epoch 15, batch 377/481,disc_loss 78.812, (real 83.58, fake 74.043 ) gen_loss 683.09\n",
            "iteration 7312, epoch 15, batch 378/481,disc_loss 75.528, (real 78.895, fake 72.16 ) gen_loss 763.63\n",
            "iteration 7313, epoch 15, batch 379/481,disc_loss 76.915, (real 80.11, fake 73.719 ) gen_loss 723.99\n",
            "iteration 7314, epoch 15, batch 380/481,disc_loss 77.832, (real 81.222, fake 74.442 ) gen_loss 681.07\n",
            "iteration 7315, epoch 15, batch 381/481,disc_loss 79.325, (real 82.19, fake 76.46 ) gen_loss 645.97\n",
            "iteration 7316, epoch 15, batch 382/481,disc_loss 77.412, (real 80.854, fake 73.97 ) gen_loss 640.89\n",
            "iteration 7317, epoch 15, batch 383/481,disc_loss 79.357, (real 82.931, fake 75.783 ) gen_loss 735.49\n",
            "iteration 7318, epoch 15, batch 384/481,disc_loss 76.415, (real 79.59, fake 73.24 ) gen_loss 668.31\n",
            "iteration 7319, epoch 15, batch 385/481,disc_loss 78.971, (real 82.241, fake 75.701 ) gen_loss 625.46\n",
            "iteration 7320, epoch 15, batch 386/481,disc_loss 80.336, (real 84.389, fake 76.283 ) gen_loss 639.78\n",
            "iteration 7321, epoch 15, batch 387/481,disc_loss 73.959, (real 77.497, fake 70.421 ) gen_loss 739.41\n",
            "iteration 7322, epoch 15, batch 388/481,disc_loss 79.947, (real 83.297, fake 76.598 ) gen_loss 712.2\n",
            "iteration 7323, epoch 15, batch 389/481,disc_loss 79.976, (real 83.795, fake 76.157 ) gen_loss 697.33\n",
            "iteration 7324, epoch 15, batch 390/481,disc_loss 83.427, (real 86.744, fake 80.11 ) gen_loss 666.58\n",
            "iteration 7325, epoch 15, batch 391/481,disc_loss 79.175, (real 82.18, fake 76.169 ) gen_loss 744.16\n",
            "iteration 7326, epoch 15, batch 392/481,disc_loss 79.684, (real 83.301, fake 76.066 ) gen_loss 682.12\n",
            "iteration 7327, epoch 15, batch 393/481,disc_loss 80.565, (real 84.037, fake 77.092 ) gen_loss 676.9\n",
            "iteration 7328, epoch 15, batch 394/481,disc_loss 75.939, (real 79.854, fake 72.024 ) gen_loss 718.42\n",
            "iteration 7329, epoch 15, batch 395/481,disc_loss 76.895, (real 79.772, fake 74.017 ) gen_loss 674.29\n",
            "iteration 7330, epoch 15, batch 396/481,disc_loss 77.563, (real 80.595, fake 74.531 ) gen_loss 609.95\n",
            "iteration 7331, epoch 15, batch 397/481,disc_loss 78.989, (real 84.232, fake 73.746 ) gen_loss 679.49\n",
            "iteration 7332, epoch 15, batch 398/481,disc_loss 80.13, (real 82.966, fake 77.294 ) gen_loss 629.15\n",
            "iteration 7333, epoch 15, batch 399/481,disc_loss 77.521, (real 80.841, fake 74.201 ) gen_loss 617.13\n",
            "iteration 7334, epoch 15, batch 400/481,disc_loss 80.421, (real 83.62, fake 77.221 ) gen_loss 723.02\n",
            "iteration 7335, epoch 15, batch 401/481,disc_loss 82.849, (real 86.858, fake 78.841 ) gen_loss 730.34\n",
            "iteration 7336, epoch 15, batch 402/481,disc_loss 75.828, (real 79.532, fake 72.124 ) gen_loss 685.2\n",
            "iteration 7337, epoch 15, batch 403/481,disc_loss 79.691, (real 83.108, fake 76.274 ) gen_loss 730.47\n",
            "iteration 7338, epoch 15, batch 404/481,disc_loss 83.308, (real 87.479, fake 79.136 ) gen_loss 641.8\n",
            "iteration 7339, epoch 15, batch 405/481,disc_loss 73.618, (real 77.537, fake 69.7 ) gen_loss 713.8\n",
            "iteration 7340, epoch 15, batch 406/481,disc_loss 79.904, (real 82.535, fake 77.272 ) gen_loss 750.16\n",
            "iteration 7341, epoch 15, batch 407/481,disc_loss 75.842, (real 79.464, fake 72.221 ) gen_loss 719.08\n",
            "iteration 7342, epoch 15, batch 408/481,disc_loss 79.117, (real 81.787, fake 76.448 ) gen_loss 713.87\n",
            "iteration 7343, epoch 15, batch 409/481,disc_loss 76.082, (real 79.244, fake 72.919 ) gen_loss 728.45\n",
            "iteration 7344, epoch 15, batch 410/481,disc_loss 79.302, (real 81.681, fake 76.923 ) gen_loss 723.2\n",
            "iteration 7345, epoch 15, batch 411/481,disc_loss 83.223, (real 87.219, fake 79.226 ) gen_loss 647.8\n",
            "iteration 7346, epoch 15, batch 412/481,disc_loss 79.913, (real 83.277, fake 76.548 ) gen_loss 655.9\n",
            "iteration 7347, epoch 15, batch 413/481,disc_loss 79.344, (real 82.519, fake 76.17 ) gen_loss 645.66\n",
            "iteration 7348, epoch 15, batch 414/481,disc_loss 82.5, (real 86.702, fake 78.297 ) gen_loss 693.47\n",
            "iteration 7349, epoch 15, batch 415/481,disc_loss 79.649, (real 83.298, fake 76.0 ) gen_loss 757.11\n",
            "iteration 7350, epoch 15, batch 416/481,disc_loss 77.087, (real 80.223, fake 73.951 ) gen_loss 618.08\n",
            "iteration 7351, epoch 15, batch 417/481,disc_loss 76.636, (real 79.162, fake 74.11 ) gen_loss 584.48\n",
            "iteration 7352, epoch 15, batch 418/481,disc_loss 78.001, (real 81.675, fake 74.327 ) gen_loss 692.96\n",
            "iteration 7353, epoch 15, batch 419/481,disc_loss 79.784, (real 83.958, fake 75.61 ) gen_loss 665.77\n",
            "iteration 7354, epoch 15, batch 420/481,disc_loss 79.148, (real 82.726, fake 75.57 ) gen_loss 668.36\n",
            "iteration 7355, epoch 15, batch 421/481,disc_loss 79.141, (real 82.271, fake 76.012 ) gen_loss 692.43\n",
            "iteration 7356, epoch 15, batch 422/481,disc_loss 77.066, (real 81.664, fake 72.467 ) gen_loss 731.74\n",
            "iteration 7357, epoch 15, batch 423/481,disc_loss 75.926, (real 79.069, fake 72.783 ) gen_loss 725.93\n",
            "iteration 7358, epoch 15, batch 424/481,disc_loss 78.354, (real 81.377, fake 75.332 ) gen_loss 684.34\n",
            "iteration 7359, epoch 15, batch 425/481,disc_loss 78.445, (real 80.632, fake 76.258 ) gen_loss 646.15\n",
            "iteration 7360, epoch 15, batch 426/481,disc_loss 80.581, (real 84.066, fake 77.097 ) gen_loss 714.82\n",
            "iteration 7361, epoch 15, batch 427/481,disc_loss 77.176, (real 79.924, fake 74.429 ) gen_loss 650.84\n",
            "iteration 7362, epoch 15, batch 428/481,disc_loss 79.143, (real 82.912, fake 75.373 ) gen_loss 727.5\n",
            "iteration 7363, epoch 15, batch 429/481,disc_loss 81.702, (real 84.996, fake 78.408 ) gen_loss 694.39\n",
            "iteration 7364, epoch 15, batch 430/481,disc_loss 79.349, (real 83.016, fake 75.682 ) gen_loss 719.83\n",
            "iteration 7365, epoch 15, batch 431/481,disc_loss 81.442, (real 85.085, fake 77.798 ) gen_loss 725.43\n",
            "iteration 7366, epoch 15, batch 432/481,disc_loss 78.058, (real 81.45, fake 74.667 ) gen_loss 676.23\n",
            "iteration 7367, epoch 15, batch 433/481,disc_loss 76.702, (real 81.121, fake 72.283 ) gen_loss 766.53\n",
            "iteration 7368, epoch 15, batch 434/481,disc_loss 77.657, (real 81.946, fake 73.367 ) gen_loss 772.39\n",
            "iteration 7369, epoch 15, batch 435/481,disc_loss 82.095, (real 84.958, fake 79.232 ) gen_loss 703.9\n",
            "iteration 7370, epoch 15, batch 436/481,disc_loss 81.787, (real 86.182, fake 77.392 ) gen_loss 704.74\n",
            "iteration 7371, epoch 15, batch 437/481,disc_loss 79.221, (real 82.275, fake 76.168 ) gen_loss 678.23\n",
            "iteration 7372, epoch 15, batch 438/481,disc_loss 79.714, (real 83.885, fake 75.542 ) gen_loss 884.71\n",
            "iteration 7373, epoch 15, batch 439/481,disc_loss 79.69, (real 82.399, fake 76.98 ) gen_loss 664.18\n",
            "iteration 7374, epoch 15, batch 440/481,disc_loss 78.403, (real 81.453, fake 75.354 ) gen_loss 650.48\n",
            "iteration 7375, epoch 15, batch 441/481,disc_loss 73.651, (real 77.19, fake 70.113 ) gen_loss 703.91\n",
            "iteration 7376, epoch 15, batch 442/481,disc_loss 78.392, (real 82.45, fake 74.333 ) gen_loss 657.49\n",
            "iteration 7377, epoch 15, batch 443/481,disc_loss 79.839, (real 82.713, fake 76.965 ) gen_loss 690.51\n",
            "iteration 7378, epoch 15, batch 444/481,disc_loss 77.52, (real 81.137, fake 73.902 ) gen_loss 750.43\n",
            "iteration 7379, epoch 15, batch 445/481,disc_loss 78.214, (real 81.689, fake 74.74 ) gen_loss 669.65\n",
            "iteration 7380, epoch 15, batch 446/481,disc_loss 77.037, (real 81.12, fake 72.953 ) gen_loss 751.4\n",
            "iteration 7381, epoch 15, batch 447/481,disc_loss 76.197, (real 79.5, fake 72.893 ) gen_loss 693.21\n",
            "iteration 7382, epoch 15, batch 448/481,disc_loss 79.879, (real 82.811, fake 76.947 ) gen_loss 648.99\n",
            "iteration 7383, epoch 15, batch 449/481,disc_loss 77.767, (real 80.68, fake 74.854 ) gen_loss 631.39\n",
            "iteration 7384, epoch 15, batch 450/481,disc_loss 81.986, (real 85.355, fake 78.616 ) gen_loss 711.84\n",
            "iteration 7385, epoch 15, batch 451/481,disc_loss 76.91, (real 80.688, fake 73.131 ) gen_loss 710.26\n",
            "iteration 7386, epoch 15, batch 452/481,disc_loss 79.862, (real 83.397, fake 76.327 ) gen_loss 686.12\n",
            "iteration 7387, epoch 15, batch 453/481,disc_loss 77.514, (real 81.159, fake 73.869 ) gen_loss 693.58\n",
            "iteration 7388, epoch 15, batch 454/481,disc_loss 74.821, (real 78.692, fake 70.95 ) gen_loss 679.03\n",
            "iteration 7389, epoch 15, batch 455/481,disc_loss 76.614, (real 80.193, fake 73.035 ) gen_loss 589.21\n",
            "iteration 7390, epoch 15, batch 456/481,disc_loss 78.47, (real 82.199, fake 74.742 ) gen_loss 672.47\n",
            "iteration 7391, epoch 15, batch 457/481,disc_loss 80.786, (real 83.738, fake 77.833 ) gen_loss 663.02\n",
            "iteration 7392, epoch 15, batch 458/481,disc_loss 79.337, (real 82.488, fake 76.187 ) gen_loss 616.59\n",
            "iteration 7393, epoch 15, batch 459/481,disc_loss 81.13, (real 84.84, fake 77.42 ) gen_loss 686.1\n",
            "iteration 7394, epoch 15, batch 460/481,disc_loss 78.323, (real 81.592, fake 75.055 ) gen_loss 674.22\n",
            "iteration 7395, epoch 15, batch 461/481,disc_loss 80.631, (real 83.867, fake 77.395 ) gen_loss 714.4\n",
            "iteration 7396, epoch 15, batch 462/481,disc_loss 71.543, (real 75.198, fake 67.888 ) gen_loss 710.29\n",
            "iteration 7397, epoch 15, batch 463/481,disc_loss 76.681, (real 80.336, fake 73.026 ) gen_loss 712.31\n",
            "iteration 7398, epoch 15, batch 464/481,disc_loss 85.209, (real 90.143, fake 80.275 ) gen_loss 677.6\n",
            "iteration 7399, epoch 15, batch 465/481,disc_loss 79.835, (real 85.5, fake 74.171 ) gen_loss 711.48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 7400, epoch 15, batch 466/481,disc_loss 78.803, (real 82.585, fake 75.021 ) gen_loss 667.9\n",
            "iteration 7401, epoch 15, batch 467/481,disc_loss 78.158, (real 81.239, fake 75.076 ) gen_loss 689.22\n",
            "iteration 7402, epoch 15, batch 468/481,disc_loss 77.518, (real 79.742, fake 75.293 ) gen_loss 664.5\n",
            "iteration 7403, epoch 15, batch 469/481,disc_loss 77.17, (real 80.975, fake 73.365 ) gen_loss 695.96\n",
            "iteration 7404, epoch 15, batch 470/481,disc_loss 82.196, (real 85.632, fake 78.761 ) gen_loss 774.73\n",
            "iteration 7405, epoch 15, batch 471/481,disc_loss 80.715, (real 83.902, fake 77.529 ) gen_loss 697.6\n",
            "iteration 7406, epoch 15, batch 472/481,disc_loss 73.374, (real 77.02, fake 69.728 ) gen_loss 654.04\n",
            "iteration 7407, epoch 15, batch 473/481,disc_loss 80.285, (real 83.184, fake 77.387 ) gen_loss 592.97\n",
            "iteration 7408, epoch 15, batch 474/481,disc_loss 80.81, (real 83.135, fake 78.485 ) gen_loss 644.52\n",
            "iteration 7409, epoch 15, batch 475/481,disc_loss 75.308, (real 78.772, fake 71.844 ) gen_loss 611.55\n",
            "iteration 7410, epoch 15, batch 476/481,disc_loss 77.997, (real 81.471, fake 74.522 ) gen_loss 632.57\n",
            "iteration 7411, epoch 15, batch 477/481,disc_loss 74.763, (real 78.277, fake 71.249 ) gen_loss 709.59\n",
            "iteration 7412, epoch 15, batch 478/481,disc_loss 79.504, (real 82.886, fake 76.122 ) gen_loss 669.87\n",
            "iteration 7413, epoch 15, batch 479/481,disc_loss 77.884, (real 80.972, fake 74.796 ) gen_loss 615.57\n",
            "iteration 7414, epoch 15, batch 480/481,disc_loss 78.844, (real 82.733, fake 74.954 ) gen_loss 659.67\n",
            "iteration 7415, epoch 15, batch 481/481,disc_loss 78.587, (real 82.408, fake 74.766 ) gen_loss 688.26\n",
            "iteration 7416, epoch 16, batch 1/481,disc_loss 82.781, (real 86.358, fake 79.205 ) gen_loss 704.3\n",
            "iteration 7417, epoch 16, batch 2/481,disc_loss 76.323, (real 78.812, fake 73.833 ) gen_loss 632.3\n",
            "iteration 7418, epoch 16, batch 3/481,disc_loss 74.082, (real 76.46, fake 71.705 ) gen_loss 692.15\n",
            "iteration 7419, epoch 16, batch 4/481,disc_loss 75.498, (real 78.273, fake 72.724 ) gen_loss 657.43\n",
            "iteration 7420, epoch 16, batch 5/481,disc_loss 78.59, (real 80.859, fake 76.321 ) gen_loss 695.13\n",
            "iteration 7421, epoch 16, batch 6/481,disc_loss 73.836, (real 77.121, fake 70.551 ) gen_loss 707.24\n",
            "iteration 7422, epoch 16, batch 7/481,disc_loss 80.68, (real 84.179, fake 77.182 ) gen_loss 634.6\n",
            "iteration 7423, epoch 16, batch 8/481,disc_loss 79.015, (real 81.74, fake 76.291 ) gen_loss 674.26\n",
            "iteration 7424, epoch 16, batch 9/481,disc_loss 81.585, (real 84.628, fake 78.542 ) gen_loss 709.7\n",
            "iteration 7425, epoch 16, batch 10/481,disc_loss 75.869, (real 78.965, fake 72.773 ) gen_loss 708.07\n",
            "iteration 7426, epoch 16, batch 11/481,disc_loss 80.91, (real 84.865, fake 76.955 ) gen_loss 633.85\n",
            "iteration 7427, epoch 16, batch 12/481,disc_loss 75.328, (real 78.852, fake 71.804 ) gen_loss 662.92\n",
            "iteration 7428, epoch 16, batch 13/481,disc_loss 79.602, (real 81.93, fake 77.274 ) gen_loss 692.54\n",
            "iteration 7429, epoch 16, batch 14/481,disc_loss 77.856, (real 80.871, fake 74.841 ) gen_loss 644.75\n",
            "iteration 7430, epoch 16, batch 15/481,disc_loss 75.75, (real 79.082, fake 72.418 ) gen_loss 721.94\n",
            "iteration 7431, epoch 16, batch 16/481,disc_loss 74.605, (real 77.064, fake 72.145 ) gen_loss 696.46\n",
            "iteration 7432, epoch 16, batch 17/481,disc_loss 76.871, (real 80.512, fake 73.23 ) gen_loss 660.55\n",
            "iteration 7433, epoch 16, batch 18/481,disc_loss 81.763, (real 84.814, fake 78.712 ) gen_loss 680.68\n",
            "iteration 7434, epoch 16, batch 19/481,disc_loss 81.701, (real 85.264, fake 78.138 ) gen_loss 657.97\n",
            "iteration 7435, epoch 16, batch 20/481,disc_loss 77.791, (real 83.038, fake 72.543 ) gen_loss 742.18\n",
            "iteration 7436, epoch 16, batch 21/481,disc_loss 79.413, (real 82.002, fake 76.825 ) gen_loss 722.99\n",
            "iteration 7437, epoch 16, batch 22/481,disc_loss 81.073, (real 84.979, fake 77.168 ) gen_loss 680.87\n",
            "iteration 7438, epoch 16, batch 23/481,disc_loss 77.386, (real 80.984, fake 73.787 ) gen_loss 705.84\n",
            "iteration 7439, epoch 16, batch 24/481,disc_loss 76.602, (real 79.891, fake 73.313 ) gen_loss 648.51\n",
            "iteration 7440, epoch 16, batch 25/481,disc_loss 77.832, (real 80.84, fake 74.824 ) gen_loss 691.01\n",
            "iteration 7441, epoch 16, batch 26/481,disc_loss 80.815, (real 83.427, fake 78.204 ) gen_loss 720.82\n",
            "iteration 7442, epoch 16, batch 27/481,disc_loss 76.064, (real 78.268, fake 73.859 ) gen_loss 722.9\n",
            "iteration 7443, epoch 16, batch 28/481,disc_loss 74.491, (real 77.623, fake 71.359 ) gen_loss 727.49\n",
            "iteration 7444, epoch 16, batch 29/481,disc_loss 83.67, (real 87.84, fake 79.5 ) gen_loss 689.82\n",
            "iteration 7445, epoch 16, batch 30/481,disc_loss 77.612, (real 80.417, fake 74.808 ) gen_loss 671.45\n",
            "iteration 7446, epoch 16, batch 31/481,disc_loss 77.18, (real 80.664, fake 73.696 ) gen_loss 736.84\n",
            "iteration 7447, epoch 16, batch 32/481,disc_loss 80.276, (real 82.983, fake 77.569 ) gen_loss 694.8\n",
            "iteration 7448, epoch 16, batch 33/481,disc_loss 77.474, (real 80.634, fake 74.314 ) gen_loss 715.04\n",
            "iteration 7449, epoch 16, batch 34/481,disc_loss 82.757, (real 86.229, fake 79.285 ) gen_loss 722.91\n",
            "iteration 7450, epoch 16, batch 35/481,disc_loss 78.575, (real 82.325, fake 74.826 ) gen_loss 712.99\n",
            "iteration 7451, epoch 16, batch 36/481,disc_loss 78.265, (real 80.621, fake 75.908 ) gen_loss 719.76\n",
            "iteration 7452, epoch 16, batch 37/481,disc_loss 74.541, (real 77.511, fake 71.571 ) gen_loss 710.69\n",
            "iteration 7453, epoch 16, batch 38/481,disc_loss 81.705, (real 86.0, fake 77.411 ) gen_loss 686.9\n",
            "iteration 7454, epoch 16, batch 39/481,disc_loss 78.814, (real 81.59, fake 76.038 ) gen_loss 650.48\n",
            "iteration 7455, epoch 16, batch 40/481,disc_loss 80.297, (real 84.53, fake 76.064 ) gen_loss 679.62\n",
            "iteration 7456, epoch 16, batch 41/481,disc_loss 81.917, (real 84.42, fake 79.414 ) gen_loss 655.79\n",
            "iteration 7457, epoch 16, batch 42/481,disc_loss 77.241, (real 80.28, fake 74.202 ) gen_loss 742.32\n",
            "iteration 7458, epoch 16, batch 43/481,disc_loss 77.151, (real 80.65, fake 73.651 ) gen_loss 739.75\n",
            "iteration 7459, epoch 16, batch 44/481,disc_loss 76.603, (real 79.093, fake 74.113 ) gen_loss 723.63\n",
            "iteration 7460, epoch 16, batch 45/481,disc_loss 77.523, (real 80.854, fake 74.192 ) gen_loss 739.46\n",
            "iteration 7461, epoch 16, batch 46/481,disc_loss 75.547, (real 78.948, fake 72.146 ) gen_loss 836.39\n",
            "iteration 7462, epoch 16, batch 47/481,disc_loss 77.101, (real 80.724, fake 73.477 ) gen_loss 749.8\n",
            "iteration 7463, epoch 16, batch 48/481,disc_loss 82.073, (real 86.426, fake 77.72 ) gen_loss 750.43\n",
            "iteration 7464, epoch 16, batch 49/481,disc_loss 76.527, (real 80.418, fake 72.637 ) gen_loss 843.17\n",
            "iteration 7465, epoch 16, batch 50/481,disc_loss 84.203, (real 87.073, fake 81.333 ) gen_loss 776.88\n",
            "iteration 7466, epoch 16, batch 51/481,disc_loss 77.49, (real 80.043, fake 74.937 ) gen_loss 830.41\n",
            "iteration 7467, epoch 16, batch 52/481,disc_loss 74.82, (real 77.115, fake 72.526 ) gen_loss 700.89\n",
            "iteration 7468, epoch 16, batch 53/481,disc_loss 78.88, (real 82.067, fake 75.694 ) gen_loss 649.03\n",
            "iteration 7469, epoch 16, batch 54/481,disc_loss 77.458, (real 80.505, fake 74.41 ) gen_loss 729.25\n",
            "iteration 7470, epoch 16, batch 55/481,disc_loss 78.559, (real 81.553, fake 75.564 ) gen_loss 764.78\n",
            "iteration 7471, epoch 16, batch 56/481,disc_loss 82.747, (real 86.064, fake 79.429 ) gen_loss 733.1\n",
            "iteration 7472, epoch 16, batch 57/481,disc_loss 81.252, (real 84.683, fake 77.82 ) gen_loss 721.96\n",
            "iteration 7473, epoch 16, batch 58/481,disc_loss 79.568, (real 83.249, fake 75.887 ) gen_loss 703.01\n",
            "iteration 7474, epoch 16, batch 59/481,disc_loss 75.138, (real 77.788, fake 72.489 ) gen_loss 749.29\n",
            "iteration 7475, epoch 16, batch 60/481,disc_loss 77.356, (real 80.353, fake 74.36 ) gen_loss 790.83\n",
            "iteration 7476, epoch 16, batch 61/481,disc_loss 79.095, (real 82.34, fake 75.85 ) gen_loss 726.07\n",
            "iteration 7477, epoch 16, batch 62/481,disc_loss 77.702, (real 80.687, fake 74.716 ) gen_loss 743.26\n",
            "iteration 7478, epoch 16, batch 63/481,disc_loss 75.342, (real 77.841, fake 72.844 ) gen_loss 788.06\n",
            "iteration 7479, epoch 16, batch 64/481,disc_loss 78.986, (real 81.985, fake 75.986 ) gen_loss 764.95\n",
            "iteration 7480, epoch 16, batch 65/481,disc_loss 81.099, (real 85.67, fake 76.527 ) gen_loss 720.72\n",
            "iteration 7481, epoch 16, batch 66/481,disc_loss 77.665, (real 80.178, fake 75.152 ) gen_loss 724.56\n",
            "iteration 7482, epoch 16, batch 67/481,disc_loss 77.013, (real 80.446, fake 73.579 ) gen_loss 713.29\n",
            "iteration 7483, epoch 16, batch 68/481,disc_loss 76.52, (real 80.413, fake 72.628 ) gen_loss 793.87\n",
            "iteration 7484, epoch 16, batch 69/481,disc_loss 77.513, (real 79.953, fake 75.073 ) gen_loss 679.35\n",
            "iteration 7485, epoch 16, batch 70/481,disc_loss 78.967, (real 82.825, fake 75.109 ) gen_loss 753.82\n",
            "iteration 7486, epoch 16, batch 71/481,disc_loss 79.133, (real 81.938, fake 76.328 ) gen_loss 707.42\n",
            "iteration 7487, epoch 16, batch 72/481,disc_loss 78.323, (real 81.701, fake 74.945 ) gen_loss 807.04\n",
            "iteration 7488, epoch 16, batch 73/481,disc_loss 79.488, (real 82.854, fake 76.122 ) gen_loss 678.66\n",
            "iteration 7489, epoch 16, batch 74/481,disc_loss 76.469, (real 79.332, fake 73.607 ) gen_loss 723.35\n",
            "iteration 7490, epoch 16, batch 75/481,disc_loss 80.902, (real 84.295, fake 77.51 ) gen_loss 719.09\n",
            "iteration 7491, epoch 16, batch 76/481,disc_loss 76.223, (real 78.599, fake 73.847 ) gen_loss 701.2\n",
            "iteration 7492, epoch 16, batch 77/481,disc_loss 75.5, (real 78.785, fake 72.216 ) gen_loss 684.07\n",
            "iteration 7493, epoch 16, batch 78/481,disc_loss 75.241, (real 77.71, fake 72.772 ) gen_loss 760.74\n",
            "iteration 7494, epoch 16, batch 79/481,disc_loss 79.208, (real 82.374, fake 76.041 ) gen_loss 727.61\n",
            "iteration 7495, epoch 16, batch 80/481,disc_loss 79.061, (real 81.546, fake 76.576 ) gen_loss 703.51\n",
            "iteration 7496, epoch 16, batch 81/481,disc_loss 77.193, (real 79.564, fake 74.823 ) gen_loss 731.14\n",
            "iteration 7497, epoch 16, batch 82/481,disc_loss 79.763, (real 82.418, fake 77.108 ) gen_loss 723.14\n",
            "iteration 7498, epoch 16, batch 83/481,disc_loss 79.702, (real 82.556, fake 76.848 ) gen_loss 718.97\n",
            "iteration 7499, epoch 16, batch 84/481,disc_loss 82.058, (real 84.917, fake 79.199 ) gen_loss 699.27\n",
            "iteration 7500, epoch 16, batch 85/481,disc_loss 75.804, (real 77.841, fake 73.768 ) gen_loss 687.06\n",
            "iteration 7501, epoch 16, batch 86/481,disc_loss 75.102, (real 78.48, fake 71.723 ) gen_loss 738.53\n",
            "iteration 7502, epoch 16, batch 87/481,disc_loss 77.593, (real 80.74, fake 74.447 ) gen_loss 757.93\n",
            "iteration 7503, epoch 16, batch 88/481,disc_loss 77.692, (real 80.974, fake 74.41 ) gen_loss 742.29\n",
            "iteration 7504, epoch 16, batch 89/481,disc_loss 79.371, (real 82.053, fake 76.689 ) gen_loss 694.97\n",
            "iteration 7505, epoch 16, batch 90/481,disc_loss 79.435, (real 82.35, fake 76.519 ) gen_loss 696.9\n",
            "iteration 7506, epoch 16, batch 91/481,disc_loss 78.061, (real 81.356, fake 74.765 ) gen_loss 715.08\n",
            "iteration 7507, epoch 16, batch 92/481,disc_loss 78.405, (real 82.227, fake 74.582 ) gen_loss 748.43\n",
            "iteration 7508, epoch 16, batch 93/481,disc_loss 84.045, (real 86.721, fake 81.368 ) gen_loss 697.07\n",
            "iteration 7509, epoch 16, batch 94/481,disc_loss 78.583, (real 82.649, fake 74.516 ) gen_loss 698.58\n",
            "iteration 7510, epoch 16, batch 95/481,disc_loss 80.791, (real 83.783, fake 77.799 ) gen_loss 648.52\n",
            "iteration 7511, epoch 16, batch 96/481,disc_loss 76.034, (real 79.216, fake 72.852 ) gen_loss 746.16\n",
            "iteration 7512, epoch 16, batch 97/481,disc_loss 78.65, (real 81.067, fake 76.232 ) gen_loss 759.85\n",
            "iteration 7513, epoch 16, batch 98/481,disc_loss 82.852, (real 85.782, fake 79.921 ) gen_loss 742.76\n",
            "iteration 7514, epoch 16, batch 99/481,disc_loss 78.424, (real 80.717, fake 76.132 ) gen_loss 674.24\n",
            "iteration 7515, epoch 16, batch 100/481,disc_loss 77.079, (real 79.743, fake 74.414 ) gen_loss 689.3\n",
            "iteration 7516, epoch 16, batch 101/481,disc_loss 78.465, (real 81.917, fake 75.013 ) gen_loss 776.17\n",
            "iteration 7517, epoch 16, batch 102/481,disc_loss 81.168, (real 83.865, fake 78.47 ) gen_loss 676.88\n",
            "iteration 7518, epoch 16, batch 103/481,disc_loss 80.168, (real 83.373, fake 76.964 ) gen_loss 684.38\n",
            "iteration 7519, epoch 16, batch 104/481,disc_loss 78.563, (real 81.656, fake 75.471 ) gen_loss 667.95\n",
            "iteration 7520, epoch 16, batch 105/481,disc_loss 75.469, (real 78.567, fake 72.371 ) gen_loss 740.84\n",
            "iteration 7521, epoch 16, batch 106/481,disc_loss 79.574, (real 81.672, fake 77.476 ) gen_loss 693.83\n",
            "iteration 7522, epoch 16, batch 107/481,disc_loss 79.785, (real 82.893, fake 76.676 ) gen_loss 696.76\n",
            "iteration 7523, epoch 16, batch 108/481,disc_loss 76.924, (real 79.648, fake 74.201 ) gen_loss 725.7\n",
            "iteration 7524, epoch 16, batch 109/481,disc_loss 79.547, (real 82.465, fake 76.629 ) gen_loss 726.84\n",
            "iteration 7525, epoch 16, batch 110/481,disc_loss 80.728, (real 83.703, fake 77.753 ) gen_loss 659.16\n",
            "iteration 7526, epoch 16, batch 111/481,disc_loss 76.446, (real 79.297, fake 73.594 ) gen_loss 651.21\n",
            "iteration 7527, epoch 16, batch 112/481,disc_loss 76.188, (real 79.561, fake 72.816 ) gen_loss 735.29\n",
            "iteration 7528, epoch 16, batch 113/481,disc_loss 76.884, (real 79.933, fake 73.835 ) gen_loss 682.42\n",
            "iteration 7529, epoch 16, batch 114/481,disc_loss 74.958, (real 77.065, fake 72.852 ) gen_loss 707.64\n",
            "iteration 7530, epoch 16, batch 115/481,disc_loss 78.916, (real 82.237, fake 75.595 ) gen_loss 672.02\n",
            "iteration 7531, epoch 16, batch 116/481,disc_loss 81.26, (real 84.868, fake 77.653 ) gen_loss 757.62\n",
            "iteration 7532, epoch 16, batch 117/481,disc_loss 81.795, (real 84.979, fake 78.611 ) gen_loss 726.12\n",
            "iteration 7533, epoch 16, batch 118/481,disc_loss 77.467, (real 80.167, fake 74.767 ) gen_loss 767.34\n",
            "iteration 7534, epoch 16, batch 119/481,disc_loss 76.174, (real 79.167, fake 73.181 ) gen_loss 733.98\n",
            "iteration 7535, epoch 16, batch 120/481,disc_loss 74.113, (real 77.573, fake 70.654 ) gen_loss 736.64\n",
            "iteration 7536, epoch 16, batch 121/481,disc_loss 77.056, (real 80.419, fake 73.692 ) gen_loss 810.08\n",
            "iteration 7537, epoch 16, batch 122/481,disc_loss 73.006, (real 75.611, fake 70.402 ) gen_loss 653.25\n",
            "iteration 7538, epoch 16, batch 123/481,disc_loss 74.274, (real 76.32, fake 72.228 ) gen_loss 665.17\n",
            "iteration 7539, epoch 16, batch 124/481,disc_loss 78.362, (real 81.354, fake 75.369 ) gen_loss 752.77\n",
            "iteration 7540, epoch 16, batch 125/481,disc_loss 79.941, (real 82.339, fake 77.543 ) gen_loss 661.75\n",
            "iteration 7541, epoch 16, batch 126/481,disc_loss 79.981, (real 83.254, fake 76.708 ) gen_loss 707.63\n",
            "iteration 7542, epoch 16, batch 127/481,disc_loss 77.556, (real 80.875, fake 74.237 ) gen_loss 748.09\n",
            "iteration 7543, epoch 16, batch 128/481,disc_loss 73.933, (real 76.831, fake 71.034 ) gen_loss 808.29\n",
            "iteration 7544, epoch 16, batch 129/481,disc_loss 80.238, (real 83.556, fake 76.919 ) gen_loss 670.27\n",
            "iteration 7545, epoch 16, batch 130/481,disc_loss 78.13, (real 80.673, fake 75.587 ) gen_loss 764.87\n",
            "iteration 7546, epoch 16, batch 131/481,disc_loss 76.859, (real 79.623, fake 74.095 ) gen_loss 662.38\n",
            "iteration 7547, epoch 16, batch 132/481,disc_loss 80.808, (real 85.286, fake 76.329 ) gen_loss 746.79\n",
            "iteration 7548, epoch 16, batch 133/481,disc_loss 79.698, (real 81.949, fake 77.447 ) gen_loss 718.57\n",
            "iteration 7549, epoch 16, batch 134/481,disc_loss 78.444, (real 81.655, fake 75.233 ) gen_loss 724.79\n",
            "iteration 7550, epoch 16, batch 135/481,disc_loss 77.325, (real 80.265, fake 74.386 ) gen_loss 692.4\n",
            "iteration 7551, epoch 16, batch 136/481,disc_loss 77.492, (real 80.544, fake 74.44 ) gen_loss 755.72\n",
            "iteration 7552, epoch 16, batch 137/481,disc_loss 73.543, (real 76.99, fake 70.096 ) gen_loss 769.27\n",
            "iteration 7553, epoch 16, batch 138/481,disc_loss 79.496, (real 82.806, fake 76.186 ) gen_loss 750.06\n",
            "iteration 7554, epoch 16, batch 139/481,disc_loss 79.396, (real 83.207, fake 75.585 ) gen_loss 732.35\n",
            "iteration 7555, epoch 16, batch 140/481,disc_loss 73.166, (real 76.335, fake 69.998 ) gen_loss 713.56\n",
            "iteration 7556, epoch 16, batch 141/481,disc_loss 76.885, (real 79.834, fake 73.935 ) gen_loss 692.9\n",
            "iteration 7557, epoch 16, batch 142/481,disc_loss 76.61, (real 79.652, fake 73.569 ) gen_loss 778.4\n",
            "iteration 7558, epoch 16, batch 143/481,disc_loss 79.528, (real 82.361, fake 76.694 ) gen_loss 836.06\n",
            "iteration 7559, epoch 16, batch 144/481,disc_loss 78.898, (real 81.222, fake 76.573 ) gen_loss 733.56\n",
            "iteration 7560, epoch 16, batch 145/481,disc_loss 79.534, (real 82.686, fake 76.382 ) gen_loss 735.29\n",
            "iteration 7561, epoch 16, batch 146/481,disc_loss 78.548, (real 81.503, fake 75.593 ) gen_loss 841.75\n",
            "iteration 7562, epoch 16, batch 147/481,disc_loss 78.61, (real 81.356, fake 75.864 ) gen_loss 698.75\n",
            "iteration 7563, epoch 16, batch 148/481,disc_loss 77.59, (real 79.863, fake 75.316 ) gen_loss 787.96\n",
            "iteration 7564, epoch 16, batch 149/481,disc_loss 78.976, (real 82.403, fake 75.549 ) gen_loss 696.45\n",
            "iteration 7565, epoch 16, batch 150/481,disc_loss 72.89, (real 76.335, fake 69.445 ) gen_loss 720.98\n",
            "iteration 7566, epoch 16, batch 151/481,disc_loss 79.854, (real 82.842, fake 76.866 ) gen_loss 727.64\n",
            "iteration 7567, epoch 16, batch 152/481,disc_loss 76.428, (real 78.476, fake 74.38 ) gen_loss 766.63\n",
            "iteration 7568, epoch 16, batch 153/481,disc_loss 80.856, (real 83.931, fake 77.78 ) gen_loss 757.12\n",
            "iteration 7569, epoch 16, batch 154/481,disc_loss 73.815, (real 76.638, fake 70.991 ) gen_loss 754.12\n",
            "iteration 7570, epoch 16, batch 155/481,disc_loss 81.903, (real 85.755, fake 78.05 ) gen_loss 880.6\n",
            "iteration 7571, epoch 16, batch 156/481,disc_loss 79.755, (real 82.428, fake 77.083 ) gen_loss 791.09\n",
            "iteration 7572, epoch 16, batch 157/481,disc_loss 77.083, (real 80.009, fake 74.157 ) gen_loss 722.87\n",
            "iteration 7573, epoch 16, batch 158/481,disc_loss 78.37, (real 80.329, fake 76.412 ) gen_loss 715.34\n",
            "iteration 7574, epoch 16, batch 159/481,disc_loss 82.478, (real 85.905, fake 79.05 ) gen_loss 774.74\n",
            "iteration 7575, epoch 16, batch 160/481,disc_loss 76.399, (real 79.423, fake 73.375 ) gen_loss 733.63\n",
            "iteration 7576, epoch 16, batch 161/481,disc_loss 76.709, (real 79.58, fake 73.837 ) gen_loss 754.18\n",
            "iteration 7577, epoch 16, batch 162/481,disc_loss 75.644, (real 78.86, fake 72.429 ) gen_loss 685.98\n",
            "iteration 7578, epoch 16, batch 163/481,disc_loss 79.157, (real 82.079, fake 76.236 ) gen_loss 738.02\n",
            "iteration 7579, epoch 16, batch 164/481,disc_loss 77.496, (real 81.564, fake 73.428 ) gen_loss 778.73\n",
            "iteration 7580, epoch 16, batch 165/481,disc_loss 81.899, (real 85.018, fake 78.779 ) gen_loss 742.5\n",
            "iteration 7581, epoch 16, batch 166/481,disc_loss 78.418, (real 81.299, fake 75.538 ) gen_loss 691.48\n",
            "iteration 7582, epoch 16, batch 167/481,disc_loss 77.791, (real 80.803, fake 74.779 ) gen_loss 796.16\n",
            "iteration 7583, epoch 16, batch 168/481,disc_loss 75.069, (real 77.703, fake 72.436 ) gen_loss 762.5\n",
            "iteration 7584, epoch 16, batch 169/481,disc_loss 80.19, (real 83.655, fake 76.725 ) gen_loss 671.57\n",
            "iteration 7585, epoch 16, batch 170/481,disc_loss 76.296, (real 80.041, fake 72.552 ) gen_loss 744.76\n",
            "iteration 7586, epoch 16, batch 171/481,disc_loss 75.712, (real 79.018, fake 72.406 ) gen_loss 721.53\n",
            "iteration 7587, epoch 16, batch 172/481,disc_loss 76.501, (real 80.002, fake 73.0 ) gen_loss 795.09\n",
            "iteration 7588, epoch 16, batch 173/481,disc_loss 75.797, (real 79.085, fake 72.508 ) gen_loss 715.9\n",
            "iteration 7589, epoch 16, batch 174/481,disc_loss 80.541, (real 84.396, fake 76.686 ) gen_loss 720.0\n",
            "iteration 7590, epoch 16, batch 175/481,disc_loss 79.62, (real 82.784, fake 76.457 ) gen_loss 735.09\n",
            "iteration 7591, epoch 16, batch 176/481,disc_loss 83.082, (real 85.742, fake 80.423 ) gen_loss 784.57\n",
            "iteration 7592, epoch 16, batch 177/481,disc_loss 77.793, (real 80.571, fake 75.014 ) gen_loss 766.54\n",
            "iteration 7593, epoch 16, batch 178/481,disc_loss 79.905, (real 82.695, fake 77.115 ) gen_loss 741.8\n",
            "iteration 7594, epoch 16, batch 179/481,disc_loss 78.014, (real 80.923, fake 75.105 ) gen_loss 733.24\n",
            "iteration 7595, epoch 16, batch 180/481,disc_loss 77.631, (real 79.917, fake 75.345 ) gen_loss 703.38\n",
            "iteration 7596, epoch 16, batch 181/481,disc_loss 79.486, (real 82.189, fake 76.783 ) gen_loss 864.36\n",
            "iteration 7597, epoch 16, batch 182/481,disc_loss 80.914, (real 83.079, fake 78.748 ) gen_loss 693.38\n",
            "iteration 7598, epoch 16, batch 183/481,disc_loss 74.617, (real 77.897, fake 71.338 ) gen_loss 741.96\n",
            "iteration 7599, epoch 16, batch 184/481,disc_loss 75.641, (real 77.75, fake 73.533 ) gen_loss 692.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 7600, epoch 16, batch 185/481,disc_loss 77.401, (real 81.586, fake 73.216 ) gen_loss 767.41\n",
            "iteration 7601, epoch 16, batch 186/481,disc_loss 78.301, (real 81.073, fake 75.53 ) gen_loss 683.84\n",
            "iteration 7602, epoch 16, batch 187/481,disc_loss 74.73, (real 77.865, fake 71.596 ) gen_loss 676.21\n",
            "iteration 7603, epoch 16, batch 188/481,disc_loss 78.578, (real 81.697, fake 75.458 ) gen_loss 688.4\n",
            "iteration 7604, epoch 16, batch 189/481,disc_loss 81.415, (real 84.253, fake 78.577 ) gen_loss 722.69\n",
            "iteration 7605, epoch 16, batch 190/481,disc_loss 78.84, (real 81.989, fake 75.69 ) gen_loss 730.55\n",
            "iteration 7606, epoch 16, batch 191/481,disc_loss 76.543, (real 79.264, fake 73.823 ) gen_loss 725.44\n",
            "iteration 7607, epoch 16, batch 192/481,disc_loss 74.172, (real 76.72, fake 71.624 ) gen_loss 779.06\n",
            "iteration 7608, epoch 16, batch 193/481,disc_loss 77.332, (real 80.277, fake 74.386 ) gen_loss 780.02\n",
            "iteration 7609, epoch 16, batch 194/481,disc_loss 77.871, (real 79.851, fake 75.89 ) gen_loss 778.93\n",
            "iteration 7610, epoch 16, batch 195/481,disc_loss 82.993, (real 86.395, fake 79.592 ) gen_loss 825.47\n",
            "iteration 7611, epoch 16, batch 196/481,disc_loss 77.92, (real 81.414, fake 74.425 ) gen_loss 660.1\n",
            "iteration 7612, epoch 16, batch 197/481,disc_loss 78.461, (real 81.501, fake 75.42 ) gen_loss 725.91\n",
            "iteration 7613, epoch 16, batch 198/481,disc_loss 74.713, (real 76.825, fake 72.601 ) gen_loss 730.85\n",
            "iteration 7614, epoch 16, batch 199/481,disc_loss 80.216, (real 82.673, fake 77.758 ) gen_loss 692.65\n",
            "iteration 7615, epoch 16, batch 200/481,disc_loss 79.895, (real 82.993, fake 76.798 ) gen_loss 776.05\n",
            "iteration 7616, epoch 16, batch 201/481,disc_loss 75.716, (real 79.882, fake 71.551 ) gen_loss 805.28\n",
            "iteration 7617, epoch 16, batch 202/481,disc_loss 78.815, (real 82.743, fake 74.887 ) gen_loss 731.94\n",
            "iteration 7618, epoch 16, batch 203/481,disc_loss 77.242, (real 80.544, fake 73.94 ) gen_loss 777.41\n",
            "iteration 7619, epoch 16, batch 204/481,disc_loss 77.738, (real 80.763, fake 74.713 ) gen_loss 763.29\n",
            "iteration 7620, epoch 16, batch 205/481,disc_loss 79.045, (real 82.427, fake 75.663 ) gen_loss 831.06\n",
            "iteration 7621, epoch 16, batch 206/481,disc_loss 79.686, (real 82.279, fake 77.093 ) gen_loss 725.55\n",
            "iteration 7622, epoch 16, batch 207/481,disc_loss 75.449, (real 77.837, fake 73.062 ) gen_loss 718.49\n",
            "iteration 7623, epoch 16, batch 208/481,disc_loss 75.809, (real 78.679, fake 72.938 ) gen_loss 647.72\n",
            "iteration 7624, epoch 16, batch 209/481,disc_loss 75.446, (real 79.307, fake 71.586 ) gen_loss 742.43\n",
            "iteration 7625, epoch 16, batch 210/481,disc_loss 80.46, (real 83.594, fake 77.326 ) gen_loss 762.18\n",
            "iteration 7626, epoch 16, batch 211/481,disc_loss 78.156, (real 82.23, fake 74.081 ) gen_loss 767.19\n",
            "iteration 7627, epoch 16, batch 212/481,disc_loss 76.52, (real 80.38, fake 72.659 ) gen_loss 812.99\n",
            "iteration 7628, epoch 16, batch 213/481,disc_loss 77.743, (real 80.222, fake 75.263 ) gen_loss 685.94\n",
            "iteration 7629, epoch 16, batch 214/481,disc_loss 79.496, (real 82.491, fake 76.501 ) gen_loss 714.53\n",
            "iteration 7630, epoch 16, batch 215/481,disc_loss 76.246, (real 79.622, fake 72.871 ) gen_loss 711.28\n",
            "iteration 7631, epoch 16, batch 216/481,disc_loss 75.217, (real 77.562, fake 72.872 ) gen_loss 711.58\n",
            "iteration 7632, epoch 16, batch 217/481,disc_loss 75.986, (real 79.433, fake 72.54 ) gen_loss 720.3\n",
            "iteration 7633, epoch 16, batch 218/481,disc_loss 78.736, (real 81.82, fake 75.652 ) gen_loss 779.04\n",
            "iteration 7634, epoch 16, batch 219/481,disc_loss 78.685, (real 81.246, fake 76.124 ) gen_loss 711.03\n",
            "iteration 7635, epoch 16, batch 220/481,disc_loss 82.487, (real 84.744, fake 80.23 ) gen_loss 840.21\n",
            "iteration 7636, epoch 16, batch 221/481,disc_loss 75.885, (real 79.999, fake 71.77 ) gen_loss 778.57\n",
            "iteration 7637, epoch 16, batch 222/481,disc_loss 76.249, (real 78.403, fake 74.096 ) gen_loss 752.09\n",
            "iteration 7638, epoch 16, batch 223/481,disc_loss 78.993, (real 81.57, fake 76.416 ) gen_loss 862.45\n",
            "iteration 7639, epoch 16, batch 224/481,disc_loss 77.345, (real 78.833, fake 75.857 ) gen_loss 694.45\n",
            "iteration 7640, epoch 16, batch 225/481,disc_loss 74.625, (real 77.444, fake 71.807 ) gen_loss 758.04\n",
            "iteration 7641, epoch 16, batch 226/481,disc_loss 76.791, (real 78.895, fake 74.687 ) gen_loss 707.15\n",
            "iteration 7642, epoch 16, batch 227/481,disc_loss 79.392, (real 82.535, fake 76.249 ) gen_loss 768.04\n",
            "iteration 7643, epoch 16, batch 228/481,disc_loss 75.063, (real 78.202, fake 71.923 ) gen_loss 765.24\n",
            "iteration 7644, epoch 16, batch 229/481,disc_loss 88.739, (real 92.023, fake 85.455 ) gen_loss 703.4\n",
            "iteration 7645, epoch 16, batch 230/481,disc_loss 81.359, (real 84.866, fake 77.851 ) gen_loss 726.84\n",
            "iteration 7646, epoch 16, batch 231/481,disc_loss 81.045, (real 83.833, fake 78.256 ) gen_loss 676.89\n",
            "iteration 7647, epoch 16, batch 232/481,disc_loss 77.808, (real 80.183, fake 75.433 ) gen_loss 712.87\n",
            "iteration 7648, epoch 16, batch 233/481,disc_loss 77.26, (real 79.815, fake 74.705 ) gen_loss 750.57\n",
            "iteration 7649, epoch 16, batch 234/481,disc_loss 78.815, (real 81.948, fake 75.682 ) gen_loss 680.69\n",
            "iteration 7650, epoch 16, batch 235/481,disc_loss 76.963, (real 79.185, fake 74.741 ) gen_loss 732.55\n",
            "iteration 7651, epoch 16, batch 236/481,disc_loss 79.807, (real 82.8, fake 76.813 ) gen_loss 731.92\n",
            "iteration 7652, epoch 16, batch 237/481,disc_loss 77.107, (real 79.186, fake 75.029 ) gen_loss 678.98\n",
            "iteration 7653, epoch 16, batch 238/481,disc_loss 82.433, (real 85.514, fake 79.352 ) gen_loss 718.69\n",
            "iteration 7654, epoch 16, batch 239/481,disc_loss 74.753, (real 77.638, fake 71.869 ) gen_loss 723.11\n",
            "iteration 7655, epoch 16, batch 240/481,disc_loss 78.494, (real 82.133, fake 74.856 ) gen_loss 665.23\n",
            "iteration 7656, epoch 16, batch 241/481,disc_loss 76.386, (real 79.361, fake 73.41 ) gen_loss 725.99\n",
            "iteration 7657, epoch 16, batch 242/481,disc_loss 78.804, (real 81.832, fake 75.776 ) gen_loss 705.93\n",
            "iteration 7658, epoch 16, batch 243/481,disc_loss 75.767, (real 78.956, fake 72.577 ) gen_loss 696.61\n",
            "iteration 7659, epoch 16, batch 244/481,disc_loss 80.644, (real 83.487, fake 77.802 ) gen_loss 687.31\n",
            "iteration 7660, epoch 16, batch 245/481,disc_loss 79.386, (real 82.299, fake 76.474 ) gen_loss 740.25\n",
            "iteration 7661, epoch 16, batch 246/481,disc_loss 79.148, (real 81.462, fake 76.835 ) gen_loss 661.76\n",
            "iteration 7662, epoch 16, batch 247/481,disc_loss 77.948, (real 81.121, fake 74.776 ) gen_loss 699.83\n",
            "iteration 7663, epoch 16, batch 248/481,disc_loss 75.72, (real 78.271, fake 73.17 ) gen_loss 758.94\n",
            "iteration 7664, epoch 16, batch 249/481,disc_loss 77.797, (real 80.497, fake 75.098 ) gen_loss 730.85\n",
            "iteration 7665, epoch 16, batch 250/481,disc_loss 81.164, (real 84.619, fake 77.71 ) gen_loss 785.31\n",
            "iteration 7666, epoch 16, batch 251/481,disc_loss 74.456, (real 77.039, fake 71.873 ) gen_loss 734.08\n",
            "iteration 7667, epoch 16, batch 252/481,disc_loss 78.926, (real 82.236, fake 75.615 ) gen_loss 794.07\n",
            "iteration 7668, epoch 16, batch 253/481,disc_loss 79.685, (real 81.199, fake 78.171 ) gen_loss 741.3\n",
            "iteration 7669, epoch 16, batch 254/481,disc_loss 77.042, (real 80.369, fake 73.716 ) gen_loss 700.43\n",
            "iteration 7670, epoch 16, batch 255/481,disc_loss 77.811, (real 80.544, fake 75.079 ) gen_loss 735.18\n",
            "iteration 7671, epoch 16, batch 256/481,disc_loss 83.578, (real 87.213, fake 79.942 ) gen_loss 683.3\n",
            "iteration 7672, epoch 16, batch 257/481,disc_loss 81.909, (real 84.522, fake 79.296 ) gen_loss 674.49\n",
            "iteration 7673, epoch 16, batch 258/481,disc_loss 76.419, (real 78.944, fake 73.894 ) gen_loss 656.19\n",
            "iteration 7674, epoch 16, batch 259/481,disc_loss 78.949, (real 81.417, fake 76.482 ) gen_loss 767.12\n",
            "iteration 7675, epoch 16, batch 260/481,disc_loss 75.588, (real 78.458, fake 72.718 ) gen_loss 759.87\n",
            "iteration 7676, epoch 16, batch 261/481,disc_loss 76.721, (real 78.753, fake 74.69 ) gen_loss 719.45\n",
            "iteration 7677, epoch 16, batch 262/481,disc_loss 77.703, (real 80.49, fake 74.915 ) gen_loss 720.43\n",
            "iteration 7678, epoch 16, batch 263/481,disc_loss 76.267, (real 80.122, fake 72.411 ) gen_loss 670.0\n",
            "iteration 7679, epoch 16, batch 264/481,disc_loss 78.637, (real 82.058, fake 75.216 ) gen_loss 730.68\n",
            "iteration 7680, epoch 16, batch 265/481,disc_loss 77.895, (real 80.659, fake 75.131 ) gen_loss 702.41\n",
            "iteration 7681, epoch 16, batch 266/481,disc_loss 80.365, (real 83.659, fake 77.071 ) gen_loss 752.45\n",
            "iteration 7682, epoch 16, batch 267/481,disc_loss 77.682, (real 80.713, fake 74.651 ) gen_loss 701.38\n",
            "iteration 7683, epoch 16, batch 268/481,disc_loss 78.291, (real 81.4, fake 75.183 ) gen_loss 733.42\n",
            "iteration 7684, epoch 16, batch 269/481,disc_loss 77.077, (real 79.946, fake 74.208 ) gen_loss 663.39\n",
            "iteration 7685, epoch 16, batch 270/481,disc_loss 71.917, (real 75.101, fake 68.733 ) gen_loss 770.26\n",
            "iteration 7686, epoch 16, batch 271/481,disc_loss 77.805, (real 79.896, fake 75.714 ) gen_loss 727.2\n",
            "iteration 7687, epoch 16, batch 272/481,disc_loss 74.191, (real 77.18, fake 71.201 ) gen_loss 770.78\n",
            "iteration 7688, epoch 16, batch 273/481,disc_loss 78.204, (real 79.584, fake 76.823 ) gen_loss 713.45\n",
            "iteration 7689, epoch 16, batch 274/481,disc_loss 81.282, (real 83.646, fake 78.917 ) gen_loss 712.39\n",
            "iteration 7690, epoch 16, batch 275/481,disc_loss 76.634, (real 79.56, fake 73.708 ) gen_loss 703.4\n",
            "iteration 7691, epoch 16, batch 276/481,disc_loss 75.456, (real 78.504, fake 72.408 ) gen_loss 762.11\n",
            "iteration 7692, epoch 16, batch 277/481,disc_loss 81.718, (real 84.996, fake 78.441 ) gen_loss 708.75\n",
            "iteration 7693, epoch 16, batch 278/481,disc_loss 81.105, (real 83.299, fake 78.912 ) gen_loss 763.79\n",
            "iteration 7694, epoch 16, batch 279/481,disc_loss 81.989, (real 84.417, fake 79.56 ) gen_loss 746.67\n",
            "iteration 7695, epoch 16, batch 280/481,disc_loss 76.309, (real 79.613, fake 73.005 ) gen_loss 786.8\n",
            "iteration 7696, epoch 16, batch 281/481,disc_loss 80.322, (real 82.301, fake 78.342 ) gen_loss 678.65\n",
            "iteration 7697, epoch 16, batch 282/481,disc_loss 76.985, (real 79.79, fake 74.18 ) gen_loss 694.65\n",
            "iteration 7698, epoch 16, batch 283/481,disc_loss 75.92, (real 78.421, fake 73.42 ) gen_loss 693.91\n",
            "iteration 7699, epoch 16, batch 284/481,disc_loss 73.504, (real 76.773, fake 70.236 ) gen_loss 823.87\n",
            "iteration 7700, epoch 16, batch 285/481,disc_loss 80.211, (real 83.999, fake 76.424 ) gen_loss 874.1\n",
            "iteration 7701, epoch 16, batch 286/481,disc_loss 75.383, (real 79.365, fake 71.4 ) gen_loss 777.65\n",
            "iteration 7702, epoch 16, batch 287/481,disc_loss 78.077, (real 80.687, fake 75.467 ) gen_loss 715.2\n",
            "iteration 7703, epoch 16, batch 288/481,disc_loss 74.401, (real 76.553, fake 72.249 ) gen_loss 764.83\n",
            "iteration 7704, epoch 16, batch 289/481,disc_loss 76.624, (real 79.256, fake 73.992 ) gen_loss 707.1\n",
            "iteration 7705, epoch 16, batch 290/481,disc_loss 82.026, (real 86.803, fake 77.249 ) gen_loss 667.42\n",
            "iteration 7706, epoch 16, batch 291/481,disc_loss 81.856, (real 83.478, fake 80.233 ) gen_loss 717.36\n",
            "iteration 7707, epoch 16, batch 292/481,disc_loss 78.227, (real 81.043, fake 75.412 ) gen_loss 720.96\n",
            "iteration 7708, epoch 16, batch 293/481,disc_loss 79.042, (real 81.096, fake 76.988 ) gen_loss 699.12\n",
            "iteration 7709, epoch 16, batch 294/481,disc_loss 78.593, (real 81.316, fake 75.87 ) gen_loss 701.17\n",
            "iteration 7710, epoch 16, batch 295/481,disc_loss 77.888, (real 81.095, fake 74.681 ) gen_loss 645.7\n",
            "iteration 7711, epoch 16, batch 296/481,disc_loss 80.754, (real 84.725, fake 76.784 ) gen_loss 697.34\n",
            "iteration 7712, epoch 16, batch 297/481,disc_loss 76.653, (real 79.461, fake 73.845 ) gen_loss 765.21\n",
            "iteration 7713, epoch 16, batch 298/481,disc_loss 79.746, (real 82.147, fake 77.346 ) gen_loss 707.41\n",
            "iteration 7714, epoch 16, batch 299/481,disc_loss 79.298, (real 82.433, fake 76.163 ) gen_loss 825.53\n",
            "iteration 7715, epoch 16, batch 300/481,disc_loss 76.386, (real 79.808, fake 72.964 ) gen_loss 708.03\n",
            "iteration 7716, epoch 16, batch 301/481,disc_loss 80.565, (real 83.93, fake 77.199 ) gen_loss 744.41\n",
            "iteration 7717, epoch 16, batch 302/481,disc_loss 81.963, (real 84.628, fake 79.299 ) gen_loss 657.73\n",
            "iteration 7718, epoch 16, batch 303/481,disc_loss 75.189, (real 77.435, fake 72.943 ) gen_loss 684.68\n",
            "iteration 7719, epoch 16, batch 304/481,disc_loss 80.992, (real 84.078, fake 77.907 ) gen_loss 699.4\n",
            "iteration 7720, epoch 16, batch 305/481,disc_loss 76.116, (real 79.035, fake 73.197 ) gen_loss 738.52\n",
            "iteration 7721, epoch 16, batch 306/481,disc_loss 79.108, (real 81.718, fake 76.498 ) gen_loss 763.39\n",
            "iteration 7722, epoch 16, batch 307/481,disc_loss 71.736, (real 74.726, fake 68.745 ) gen_loss 771.37\n",
            "iteration 7723, epoch 16, batch 308/481,disc_loss 75.369, (real 78.471, fake 72.267 ) gen_loss 733.16\n",
            "iteration 7724, epoch 16, batch 309/481,disc_loss 75.524, (real 78.527, fake 72.521 ) gen_loss 783.41\n",
            "iteration 7725, epoch 16, batch 310/481,disc_loss 74.868, (real 77.358, fake 72.378 ) gen_loss 744.12\n",
            "iteration 7726, epoch 16, batch 311/481,disc_loss 76.793, (real 80.344, fake 73.241 ) gen_loss 793.94\n",
            "iteration 7727, epoch 16, batch 312/481,disc_loss 77.337, (real 79.976, fake 74.698 ) gen_loss 689.4\n",
            "iteration 7728, epoch 16, batch 313/481,disc_loss 78.338, (real 80.819, fake 75.857 ) gen_loss 744.24\n",
            "iteration 7729, epoch 16, batch 314/481,disc_loss 79.69, (real 81.735, fake 77.645 ) gen_loss 759.73\n",
            "iteration 7730, epoch 16, batch 315/481,disc_loss 78.883, (real 82.153, fake 75.612 ) gen_loss 774.71\n",
            "iteration 7731, epoch 16, batch 316/481,disc_loss 84.157, (real 87.381, fake 80.933 ) gen_loss 694.29\n",
            "iteration 7732, epoch 16, batch 317/481,disc_loss 75.196, (real 78.462, fake 71.931 ) gen_loss 781.46\n",
            "iteration 7733, epoch 16, batch 318/481,disc_loss 79.767, (real 82.756, fake 76.779 ) gen_loss 712.77\n",
            "iteration 7734, epoch 16, batch 319/481,disc_loss 76.352, (real 79.366, fake 73.338 ) gen_loss 676.2\n",
            "iteration 7735, epoch 16, batch 320/481,disc_loss 80.253, (real 82.97, fake 77.536 ) gen_loss 676.48\n",
            "iteration 7736, epoch 16, batch 321/481,disc_loss 74.774, (real 77.663, fake 71.885 ) gen_loss 729.19\n",
            "iteration 7737, epoch 16, batch 322/481,disc_loss 77.556, (real 80.408, fake 74.704 ) gen_loss 710.89\n",
            "iteration 7738, epoch 16, batch 323/481,disc_loss 77.124, (real 80.112, fake 74.137 ) gen_loss 806.23\n",
            "iteration 7739, epoch 16, batch 324/481,disc_loss 79.099, (real 82.405, fake 75.793 ) gen_loss 700.6\n",
            "iteration 7740, epoch 16, batch 325/481,disc_loss 82.139, (real 84.894, fake 79.383 ) gen_loss 769.86\n",
            "iteration 7741, epoch 16, batch 326/481,disc_loss 75.336, (real 77.744, fake 72.929 ) gen_loss 726.42\n",
            "iteration 7742, epoch 16, batch 327/481,disc_loss 78.558, (real 83.049, fake 74.068 ) gen_loss 782.48\n",
            "iteration 7743, epoch 16, batch 328/481,disc_loss 75.683, (real 78.217, fake 73.149 ) gen_loss 758.89\n",
            "iteration 7744, epoch 16, batch 329/481,disc_loss 80.347, (real 83.719, fake 76.975 ) gen_loss 736.79\n",
            "iteration 7745, epoch 16, batch 330/481,disc_loss 76.719, (real 79.145, fake 74.294 ) gen_loss 733.35\n",
            "iteration 7746, epoch 16, batch 331/481,disc_loss 78.842, (real 82.669, fake 75.016 ) gen_loss 686.81\n",
            "iteration 7747, epoch 16, batch 332/481,disc_loss 78.342, (real 80.85, fake 75.834 ) gen_loss 735.82\n",
            "iteration 7748, epoch 16, batch 333/481,disc_loss 81.032, (real 84.292, fake 77.772 ) gen_loss 649.44\n",
            "iteration 7749, epoch 16, batch 334/481,disc_loss 73.125, (real 75.332, fake 70.917 ) gen_loss 699.23\n",
            "iteration 7750, epoch 16, batch 335/481,disc_loss 76.984, (real 79.575, fake 74.394 ) gen_loss 711.33\n",
            "iteration 7751, epoch 16, batch 336/481,disc_loss 75.035, (real 78.062, fake 72.008 ) gen_loss 698.06\n",
            "iteration 7752, epoch 16, batch 337/481,disc_loss 79.009, (real 82.412, fake 75.606 ) gen_loss 798.14\n",
            "iteration 7753, epoch 16, batch 338/481,disc_loss 79.269, (real 82.076, fake 76.461 ) gen_loss 815.2\n",
            "iteration 7754, epoch 16, batch 339/481,disc_loss 76.299, (real 79.23, fake 73.367 ) gen_loss 810.11\n",
            "iteration 7755, epoch 16, batch 340/481,disc_loss 75.188, (real 78.645, fake 71.732 ) gen_loss 798.41\n",
            "iteration 7756, epoch 16, batch 341/481,disc_loss 75.296, (real 78.034, fake 72.558 ) gen_loss 816.96\n",
            "iteration 7757, epoch 16, batch 342/481,disc_loss 78.467, (real 81.219, fake 75.715 ) gen_loss 683.63\n",
            "iteration 7758, epoch 16, batch 343/481,disc_loss 77.78, (real 80.569, fake 74.991 ) gen_loss 778.05\n",
            "iteration 7759, epoch 16, batch 344/481,disc_loss 79.165, (real 81.638, fake 76.692 ) gen_loss 702.62\n",
            "iteration 7760, epoch 16, batch 345/481,disc_loss 81.865, (real 84.809, fake 78.921 ) gen_loss 887.71\n",
            "iteration 7761, epoch 16, batch 346/481,disc_loss 77.608, (real 80.439, fake 74.778 ) gen_loss 719.09\n",
            "iteration 7762, epoch 16, batch 347/481,disc_loss 81.132, (real 84.082, fake 78.182 ) gen_loss 733.29\n",
            "iteration 7763, epoch 16, batch 348/481,disc_loss 79.114, (real 81.841, fake 76.387 ) gen_loss 742.76\n",
            "iteration 7764, epoch 16, batch 349/481,disc_loss 77.083, (real 78.931, fake 75.234 ) gen_loss 804.39\n",
            "iteration 7765, epoch 16, batch 350/481,disc_loss 81.403, (real 85.169, fake 77.637 ) gen_loss 679.09\n",
            "iteration 7766, epoch 16, batch 351/481,disc_loss 75.077, (real 78.646, fake 71.508 ) gen_loss 632.84\n",
            "iteration 7767, epoch 16, batch 352/481,disc_loss 77.2, (real 80.206, fake 74.195 ) gen_loss 708.47\n",
            "iteration 7768, epoch 16, batch 353/481,disc_loss 75.024, (real 77.738, fake 72.31 ) gen_loss 699.43\n",
            "iteration 7769, epoch 16, batch 354/481,disc_loss 77.89, (real 80.391, fake 75.389 ) gen_loss 801.76\n",
            "iteration 7770, epoch 16, batch 355/481,disc_loss 81.197, (real 83.731, fake 78.663 ) gen_loss 792.73\n",
            "iteration 7771, epoch 16, batch 356/481,disc_loss 80.446, (real 83.122, fake 77.771 ) gen_loss 721.3\n",
            "iteration 7772, epoch 16, batch 357/481,disc_loss 77.103, (real 80.281, fake 73.925 ) gen_loss 795.03\n",
            "iteration 7773, epoch 16, batch 358/481,disc_loss 78.637, (real 81.904, fake 75.37 ) gen_loss 765.23\n",
            "iteration 7774, epoch 16, batch 359/481,disc_loss 80.38, (real 82.98, fake 77.78 ) gen_loss 756.74\n",
            "iteration 7775, epoch 16, batch 360/481,disc_loss 79.877, (real 82.593, fake 77.161 ) gen_loss 721.29\n",
            "iteration 7776, epoch 16, batch 361/481,disc_loss 75.135, (real 77.982, fake 72.288 ) gen_loss 818.88\n",
            "iteration 7777, epoch 16, batch 362/481,disc_loss 75.712, (real 77.941, fake 73.483 ) gen_loss 728.36\n",
            "iteration 7778, epoch 16, batch 363/481,disc_loss 77.609, (real 80.384, fake 74.834 ) gen_loss 718.55\n",
            "iteration 7779, epoch 16, batch 364/481,disc_loss 78.377, (real 80.812, fake 75.941 ) gen_loss 777.38\n",
            "iteration 7780, epoch 16, batch 365/481,disc_loss 77.247, (real 79.749, fake 74.746 ) gen_loss 735.97\n",
            "iteration 7781, epoch 16, batch 366/481,disc_loss 76.792, (real 79.518, fake 74.066 ) gen_loss 747.02\n",
            "iteration 7782, epoch 16, batch 367/481,disc_loss 83.414, (real 85.995, fake 80.834 ) gen_loss 788.09\n",
            "iteration 7783, epoch 16, batch 368/481,disc_loss 77.422, (real 80.544, fake 74.3 ) gen_loss 802.4\n",
            "iteration 7784, epoch 16, batch 369/481,disc_loss 75.64, (real 79.032, fake 72.247 ) gen_loss 735.25\n",
            "iteration 7785, epoch 16, batch 370/481,disc_loss 77.031, (real 79.66, fake 74.403 ) gen_loss 790.58\n",
            "iteration 7786, epoch 16, batch 371/481,disc_loss 75.136, (real 77.963, fake 72.31 ) gen_loss 767.07\n",
            "iteration 7787, epoch 16, batch 372/481,disc_loss 76.292, (real 78.614, fake 73.97 ) gen_loss 781.94\n",
            "iteration 7788, epoch 16, batch 373/481,disc_loss 74.631, (real 77.602, fake 71.66 ) gen_loss 762.22\n",
            "iteration 7789, epoch 16, batch 374/481,disc_loss 80.948, (real 84.492, fake 77.405 ) gen_loss 770.85\n",
            "iteration 7790, epoch 16, batch 375/481,disc_loss 80.203, (real 83.208, fake 77.199 ) gen_loss 856.5\n",
            "iteration 7791, epoch 16, batch 376/481,disc_loss 78.769, (real 81.545, fake 75.993 ) gen_loss 760.69\n",
            "iteration 7792, epoch 16, batch 377/481,disc_loss 77.165, (real 80.502, fake 73.828 ) gen_loss 717.98\n",
            "iteration 7793, epoch 16, batch 378/481,disc_loss 78.27, (real 81.151, fake 75.39 ) gen_loss 736.26\n",
            "iteration 7794, epoch 16, batch 379/481,disc_loss 81.921, (real 84.637, fake 79.205 ) gen_loss 745.23\n",
            "iteration 7795, epoch 16, batch 380/481,disc_loss 79.422, (real 81.945, fake 76.899 ) gen_loss 726.49\n",
            "iteration 7796, epoch 16, batch 381/481,disc_loss 73.904, (real 76.903, fake 70.904 ) gen_loss 794.47\n",
            "iteration 7797, epoch 16, batch 382/481,disc_loss 75.779, (real 78.671, fake 72.886 ) gen_loss 707.41\n",
            "iteration 7798, epoch 16, batch 383/481,disc_loss 77.446, (real 79.7, fake 75.191 ) gen_loss 781.76\n",
            "iteration 7799, epoch 16, batch 384/481,disc_loss 75.851, (real 78.561, fake 73.14 ) gen_loss 732.27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 7800, epoch 16, batch 385/481,disc_loss 79.745, (real 82.444, fake 77.046 ) gen_loss 741.53\n",
            "iteration 7801, epoch 16, batch 386/481,disc_loss 78.758, (real 81.509, fake 76.007 ) gen_loss 822.13\n",
            "iteration 7802, epoch 16, batch 387/481,disc_loss 75.689, (real 77.96, fake 73.418 ) gen_loss 720.69\n",
            "iteration 7803, epoch 16, batch 388/481,disc_loss 75.036, (real 77.785, fake 72.288 ) gen_loss 732.68\n",
            "iteration 7804, epoch 16, batch 389/481,disc_loss 76.027, (real 79.185, fake 72.87 ) gen_loss 831.14\n",
            "iteration 7805, epoch 16, batch 390/481,disc_loss 76.444, (real 79.815, fake 73.073 ) gen_loss 804.13\n",
            "iteration 7806, epoch 16, batch 391/481,disc_loss 74.289, (real 77.438, fake 71.139 ) gen_loss 850.86\n",
            "iteration 7807, epoch 16, batch 392/481,disc_loss 79.078, (real 81.771, fake 76.386 ) gen_loss 820.41\n",
            "iteration 7808, epoch 16, batch 393/481,disc_loss 74.147, (real 77.267, fake 71.028 ) gen_loss 777.42\n",
            "iteration 7809, epoch 16, batch 394/481,disc_loss 76.474, (real 79.149, fake 73.798 ) gen_loss 801.83\n",
            "iteration 7810, epoch 16, batch 395/481,disc_loss 77.861, (real 81.341, fake 74.381 ) gen_loss 816.01\n",
            "iteration 7811, epoch 16, batch 396/481,disc_loss 80.538, (real 83.051, fake 78.024 ) gen_loss 742.29\n",
            "iteration 7812, epoch 16, batch 397/481,disc_loss 77.8, (real 81.278, fake 74.322 ) gen_loss 729.36\n",
            "iteration 7813, epoch 16, batch 398/481,disc_loss 74.571, (real 77.652, fake 71.49 ) gen_loss 811.22\n",
            "iteration 7814, epoch 16, batch 399/481,disc_loss 76.604, (real 79.669, fake 73.54 ) gen_loss 810.29\n",
            "iteration 7815, epoch 16, batch 400/481,disc_loss 80.009, (real 82.945, fake 77.073 ) gen_loss 807.8\n",
            "iteration 7816, epoch 16, batch 401/481,disc_loss 82.475, (real 84.396, fake 80.555 ) gen_loss 818.09\n",
            "iteration 7817, epoch 16, batch 402/481,disc_loss 77.919, (real 80.764, fake 75.075 ) gen_loss 755.23\n",
            "iteration 7818, epoch 16, batch 403/481,disc_loss 81.172, (real 83.665, fake 78.678 ) gen_loss 712.97\n",
            "iteration 7819, epoch 16, batch 404/481,disc_loss 76.64, (real 79.299, fake 73.981 ) gen_loss 745.22\n",
            "iteration 7820, epoch 16, batch 405/481,disc_loss 75.914, (real 78.451, fake 73.378 ) gen_loss 697.53\n",
            "iteration 7821, epoch 16, batch 406/481,disc_loss 75.18, (real 78.582, fake 71.777 ) gen_loss 743.55\n",
            "iteration 7822, epoch 16, batch 407/481,disc_loss 78.966, (real 81.987, fake 75.946 ) gen_loss 678.24\n",
            "iteration 7823, epoch 16, batch 408/481,disc_loss 79.855, (real 81.217, fake 78.494 ) gen_loss 739.07\n",
            "iteration 7824, epoch 16, batch 409/481,disc_loss 70.872, (real 73.79, fake 67.954 ) gen_loss 824.67\n",
            "iteration 7825, epoch 16, batch 410/481,disc_loss 78.341, (real 81.088, fake 75.593 ) gen_loss 716.25\n",
            "iteration 7826, epoch 16, batch 411/481,disc_loss 78.387, (real 81.013, fake 75.761 ) gen_loss 1151.0\n",
            "iteration 7827, epoch 16, batch 412/481,disc_loss 78.309, (real 81.388, fake 75.23 ) gen_loss 690.22\n",
            "iteration 7828, epoch 16, batch 413/481,disc_loss 80.61, (real 84.162, fake 77.058 ) gen_loss 774.39\n",
            "iteration 7829, epoch 16, batch 414/481,disc_loss 78.206, (real 80.998, fake 75.414 ) gen_loss 778.47\n",
            "iteration 7830, epoch 16, batch 415/481,disc_loss 74.947, (real 77.508, fake 72.386 ) gen_loss 727.99\n",
            "iteration 7831, epoch 16, batch 416/481,disc_loss 75.008, (real 77.738, fake 72.278 ) gen_loss 759.59\n",
            "iteration 7832, epoch 16, batch 417/481,disc_loss 78.278, (real 81.271, fake 75.285 ) gen_loss 762.87\n",
            "iteration 7833, epoch 16, batch 418/481,disc_loss 77.409, (real 80.425, fake 74.394 ) gen_loss 797.39\n",
            "iteration 7834, epoch 16, batch 419/481,disc_loss 77.932, (real 81.487, fake 74.376 ) gen_loss 763.02\n",
            "iteration 7835, epoch 16, batch 420/481,disc_loss 72.41, (real 75.332, fake 69.488 ) gen_loss 707.67\n",
            "iteration 7836, epoch 16, batch 421/481,disc_loss 79.485, (real 83.587, fake 75.383 ) gen_loss 825.17\n",
            "iteration 7837, epoch 16, batch 422/481,disc_loss 78.897, (real 81.709, fake 76.085 ) gen_loss 803.5\n",
            "iteration 7838, epoch 16, batch 423/481,disc_loss 78.732, (real 81.604, fake 75.861 ) gen_loss 808.51\n",
            "iteration 7839, epoch 16, batch 424/481,disc_loss 74.26, (real 76.998, fake 71.523 ) gen_loss 778.79\n",
            "iteration 7840, epoch 16, batch 425/481,disc_loss 74.447, (real 77.312, fake 71.582 ) gen_loss 694.65\n",
            "iteration 7841, epoch 16, batch 426/481,disc_loss 76.815, (real 79.553, fake 74.077 ) gen_loss 736.07\n",
            "iteration 7842, epoch 16, batch 427/481,disc_loss 74.723, (real 77.406, fake 72.04 ) gen_loss 714.68\n",
            "iteration 7843, epoch 16, batch 428/481,disc_loss 78.326, (real 81.648, fake 75.004 ) gen_loss 724.19\n",
            "iteration 7844, epoch 16, batch 429/481,disc_loss 75.002, (real 78.756, fake 71.248 ) gen_loss 743.51\n",
            "iteration 7845, epoch 16, batch 430/481,disc_loss 77.11, (real 80.461, fake 73.76 ) gen_loss 805.36\n",
            "iteration 7846, epoch 16, batch 431/481,disc_loss 80.895, (real 84.268, fake 77.523 ) gen_loss 776.72\n",
            "iteration 7847, epoch 16, batch 432/481,disc_loss 77.981, (real 82.441, fake 73.521 ) gen_loss 785.67\n",
            "iteration 7848, epoch 16, batch 433/481,disc_loss 75.737, (real 78.087, fake 73.387 ) gen_loss 724.76\n",
            "iteration 7849, epoch 16, batch 434/481,disc_loss 75.408, (real 78.895, fake 71.921 ) gen_loss 688.35\n",
            "iteration 7850, epoch 16, batch 435/481,disc_loss 79.713, (real 82.033, fake 77.393 ) gen_loss 750.89\n",
            "iteration 7851, epoch 16, batch 436/481,disc_loss 77.698, (real 80.553, fake 74.844 ) gen_loss 758.46\n",
            "iteration 7852, epoch 16, batch 437/481,disc_loss 80.273, (real 82.939, fake 77.608 ) gen_loss 780.58\n",
            "iteration 7853, epoch 16, batch 438/481,disc_loss 74.42, (real 76.89, fake 71.949 ) gen_loss 714.63\n",
            "iteration 7854, epoch 16, batch 439/481,disc_loss 78.072, (real 80.959, fake 75.184 ) gen_loss 796.33\n",
            "iteration 7855, epoch 16, batch 440/481,disc_loss 76.238, (real 79.531, fake 72.944 ) gen_loss 736.54\n",
            "iteration 7856, epoch 16, batch 441/481,disc_loss 75.833, (real 78.023, fake 73.642 ) gen_loss 795.67\n",
            "iteration 7857, epoch 16, batch 442/481,disc_loss 77.852, (real 79.837, fake 75.867 ) gen_loss 822.97\n",
            "iteration 7858, epoch 16, batch 443/481,disc_loss 76.41, (real 79.67, fake 73.149 ) gen_loss 792.98\n",
            "iteration 7859, epoch 16, batch 444/481,disc_loss 73.004, (real 74.722, fake 71.286 ) gen_loss 804.24\n",
            "iteration 7860, epoch 16, batch 445/481,disc_loss 77.592, (real 80.432, fake 74.751 ) gen_loss 713.53\n",
            "iteration 7861, epoch 16, batch 446/481,disc_loss 77.076, (real 80.422, fake 73.731 ) gen_loss 812.53\n",
            "iteration 7862, epoch 16, batch 447/481,disc_loss 80.03, (real 84.094, fake 75.967 ) gen_loss 688.12\n",
            "iteration 7863, epoch 16, batch 448/481,disc_loss 79.772, (real 83.991, fake 75.553 ) gen_loss 723.94\n",
            "iteration 7864, epoch 16, batch 449/481,disc_loss 72.848, (real 75.801, fake 69.896 ) gen_loss 739.07\n",
            "iteration 7865, epoch 16, batch 450/481,disc_loss 78.722, (real 81.554, fake 75.889 ) gen_loss 780.89\n",
            "iteration 7866, epoch 16, batch 451/481,disc_loss 81.096, (real 83.793, fake 78.399 ) gen_loss 757.66\n",
            "iteration 7867, epoch 16, batch 452/481,disc_loss 74.407, (real 77.975, fake 70.838 ) gen_loss 748.56\n",
            "iteration 7868, epoch 16, batch 453/481,disc_loss 77.375, (real 79.888, fake 74.861 ) gen_loss 781.34\n",
            "iteration 7869, epoch 16, batch 454/481,disc_loss 79.005, (real 81.341, fake 76.67 ) gen_loss 722.27\n",
            "iteration 7870, epoch 16, batch 455/481,disc_loss 75.029, (real 77.781, fake 72.277 ) gen_loss 835.41\n",
            "iteration 7871, epoch 16, batch 456/481,disc_loss 79.198, (real 82.37, fake 76.025 ) gen_loss 724.94\n",
            "iteration 7872, epoch 16, batch 457/481,disc_loss 77.44, (real 80.625, fake 74.255 ) gen_loss 744.15\n",
            "iteration 7873, epoch 16, batch 458/481,disc_loss 74.715, (real 77.457, fake 71.974 ) gen_loss 758.27\n",
            "iteration 7874, epoch 16, batch 459/481,disc_loss 75.023, (real 77.327, fake 72.719 ) gen_loss 809.35\n",
            "iteration 7875, epoch 16, batch 460/481,disc_loss 78.86, (real 81.194, fake 76.527 ) gen_loss 770.3\n",
            "iteration 7876, epoch 16, batch 461/481,disc_loss 79.274, (real 82.853, fake 75.695 ) gen_loss 756.58\n",
            "iteration 7877, epoch 16, batch 462/481,disc_loss 77.278, (real 79.817, fake 74.739 ) gen_loss 734.09\n",
            "iteration 7878, epoch 16, batch 463/481,disc_loss 81.079, (real 84.633, fake 77.524 ) gen_loss 796.81\n",
            "iteration 7879, epoch 16, batch 464/481,disc_loss 77.735, (real 81.182, fake 74.288 ) gen_loss 741.65\n",
            "iteration 7880, epoch 16, batch 465/481,disc_loss 76.682, (real 79.503, fake 73.861 ) gen_loss 794.99\n",
            "iteration 7881, epoch 16, batch 466/481,disc_loss 74.907, (real 78.336, fake 71.479 ) gen_loss 801.54\n",
            "iteration 7882, epoch 16, batch 467/481,disc_loss 76.654, (real 79.773, fake 73.535 ) gen_loss 802.94\n",
            "iteration 7883, epoch 16, batch 468/481,disc_loss 78.126, (real 81.371, fake 74.881 ) gen_loss 781.03\n",
            "iteration 7884, epoch 16, batch 469/481,disc_loss 78.041, (real 81.543, fake 74.538 ) gen_loss 830.17\n",
            "iteration 7885, epoch 16, batch 470/481,disc_loss 77.9, (real 80.511, fake 75.288 ) gen_loss 770.01\n",
            "iteration 7886, epoch 16, batch 471/481,disc_loss 79.038, (real 81.325, fake 76.75 ) gen_loss 799.64\n",
            "iteration 7887, epoch 16, batch 472/481,disc_loss 73.66, (real 76.219, fake 71.101 ) gen_loss 746.15\n",
            "iteration 7888, epoch 16, batch 473/481,disc_loss 78.41, (real 80.749, fake 76.072 ) gen_loss 787.84\n",
            "iteration 7889, epoch 16, batch 474/481,disc_loss 79.228, (real 81.162, fake 77.294 ) gen_loss 787.31\n",
            "iteration 7890, epoch 16, batch 475/481,disc_loss 77.899, (real 81.218, fake 74.58 ) gen_loss 742.4\n",
            "iteration 7891, epoch 16, batch 476/481,disc_loss 75.218, (real 78.832, fake 71.604 ) gen_loss 676.74\n",
            "iteration 7892, epoch 16, batch 477/481,disc_loss 79.698, (real 82.395, fake 77.001 ) gen_loss 734.68\n",
            "iteration 7893, epoch 16, batch 478/481,disc_loss 75.655, (real 78.629, fake 72.682 ) gen_loss 761.16\n",
            "iteration 7894, epoch 16, batch 479/481,disc_loss 75.51, (real 77.856, fake 73.163 ) gen_loss 793.87\n",
            "iteration 7895, epoch 16, batch 480/481,disc_loss 76.897, (real 79.625, fake 74.168 ) gen_loss 757.82\n",
            "iteration 7896, epoch 16, batch 481/481,disc_loss 80.37, (real 83.529, fake 77.211 ) gen_loss 719.63\n",
            "iteration 7897, epoch 17, batch 1/481,disc_loss 77.715, (real 80.111, fake 75.318 ) gen_loss 747.46\n",
            "iteration 7898, epoch 17, batch 2/481,disc_loss 79.68, (real 81.185, fake 78.174 ) gen_loss 715.06\n",
            "iteration 7899, epoch 17, batch 3/481,disc_loss 83.522, (real 86.9, fake 80.143 ) gen_loss 718.43\n",
            "iteration 7900, epoch 17, batch 4/481,disc_loss 76.811, (real 79.021, fake 74.601 ) gen_loss 778.27\n",
            "iteration 7901, epoch 17, batch 5/481,disc_loss 77.126, (real 79.678, fake 74.573 ) gen_loss 773.57\n",
            "iteration 7902, epoch 17, batch 6/481,disc_loss 75.634, (real 77.76, fake 73.507 ) gen_loss 737.01\n",
            "iteration 7903, epoch 17, batch 7/481,disc_loss 79.345, (real 81.892, fake 76.798 ) gen_loss 790.15\n",
            "iteration 7904, epoch 17, batch 8/481,disc_loss 75.848, (real 78.114, fake 73.583 ) gen_loss 801.26\n",
            "iteration 7905, epoch 17, batch 9/481,disc_loss 77.646, (real 80.292, fake 75.001 ) gen_loss 845.3\n",
            "iteration 7906, epoch 17, batch 10/481,disc_loss 80.065, (real 83.244, fake 76.886 ) gen_loss 784.67\n",
            "iteration 7907, epoch 17, batch 11/481,disc_loss 77.822, (real 79.53, fake 76.114 ) gen_loss 830.25\n",
            "iteration 7908, epoch 17, batch 12/481,disc_loss 77.226, (real 80.112, fake 74.341 ) gen_loss 833.47\n",
            "iteration 7909, epoch 17, batch 13/481,disc_loss 79.43, (real 81.569, fake 77.291 ) gen_loss 733.12\n",
            "iteration 7910, epoch 17, batch 14/481,disc_loss 77.439, (real 79.934, fake 74.944 ) gen_loss 799.97\n",
            "iteration 7911, epoch 17, batch 15/481,disc_loss 72.535, (real 75.298, fake 69.772 ) gen_loss 873.36\n",
            "iteration 7912, epoch 17, batch 16/481,disc_loss 75.93, (real 78.812, fake 73.048 ) gen_loss 768.05\n",
            "iteration 7913, epoch 17, batch 17/481,disc_loss 75.782, (real 78.383, fake 73.182 ) gen_loss 800.46\n",
            "iteration 7914, epoch 17, batch 18/481,disc_loss 76.456, (real 79.953, fake 72.959 ) gen_loss 785.02\n",
            "iteration 7915, epoch 17, batch 19/481,disc_loss 78.277, (real 80.425, fake 76.13 ) gen_loss 697.48\n",
            "iteration 7916, epoch 17, batch 20/481,disc_loss 76.861, (real 80.161, fake 73.56 ) gen_loss 812.43\n",
            "iteration 7917, epoch 17, batch 21/481,disc_loss 78.373, (real 80.35, fake 76.395 ) gen_loss 742.87\n",
            "iteration 7918, epoch 17, batch 22/481,disc_loss 79.922, (real 82.212, fake 77.631 ) gen_loss 715.13\n",
            "iteration 7919, epoch 17, batch 23/481,disc_loss 77.955, (real 80.573, fake 75.336 ) gen_loss 745.29\n",
            "iteration 7920, epoch 17, batch 24/481,disc_loss 80.488, (real 83.294, fake 77.683 ) gen_loss 818.44\n",
            "iteration 7921, epoch 17, batch 25/481,disc_loss 73.211, (real 75.561, fake 70.862 ) gen_loss 718.44\n",
            "iteration 7922, epoch 17, batch 26/481,disc_loss 78.054, (real 80.382, fake 75.725 ) gen_loss 750.85\n",
            "iteration 7923, epoch 17, batch 27/481,disc_loss 77.956, (real 80.347, fake 75.564 ) gen_loss 828.47\n",
            "iteration 7924, epoch 17, batch 28/481,disc_loss 73.462, (real 75.855, fake 71.069 ) gen_loss 771.73\n",
            "iteration 7925, epoch 17, batch 29/481,disc_loss 78.546, (real 81.61, fake 75.482 ) gen_loss 773.19\n",
            "iteration 7926, epoch 17, batch 30/481,disc_loss 76.911, (real 78.928, fake 74.894 ) gen_loss 843.65\n",
            "iteration 7927, epoch 17, batch 31/481,disc_loss 77.817, (real 80.149, fake 75.486 ) gen_loss 781.28\n",
            "iteration 7928, epoch 17, batch 32/481,disc_loss 76.408, (real 78.474, fake 74.341 ) gen_loss 810.78\n",
            "iteration 7929, epoch 17, batch 33/481,disc_loss 76.777, (real 79.666, fake 73.888 ) gen_loss 801.22\n",
            "iteration 7930, epoch 17, batch 34/481,disc_loss 73.64, (real 76.239, fake 71.042 ) gen_loss 808.63\n",
            "iteration 7931, epoch 17, batch 35/481,disc_loss 78.215, (real 80.95, fake 75.479 ) gen_loss 775.95\n",
            "iteration 7932, epoch 17, batch 36/481,disc_loss 75.526, (real 78.192, fake 72.859 ) gen_loss 804.44\n",
            "iteration 7933, epoch 17, batch 37/481,disc_loss 78.281, (real 81.453, fake 75.109 ) gen_loss 770.16\n",
            "iteration 7934, epoch 17, batch 38/481,disc_loss 75.784, (real 77.765, fake 73.803 ) gen_loss 760.58\n",
            "iteration 7935, epoch 17, batch 39/481,disc_loss 76.585, (real 78.722, fake 74.447 ) gen_loss 764.0\n",
            "iteration 7936, epoch 17, batch 40/481,disc_loss 73.037, (real 75.261, fake 70.813 ) gen_loss 800.96\n",
            "iteration 7937, epoch 17, batch 41/481,disc_loss 73.836, (real 76.321, fake 71.351 ) gen_loss 718.24\n",
            "iteration 7938, epoch 17, batch 42/481,disc_loss 74.658, (real 77.249, fake 72.068 ) gen_loss 793.56\n",
            "iteration 7939, epoch 17, batch 43/481,disc_loss 77.335, (real 79.807, fake 74.863 ) gen_loss 793.7\n",
            "iteration 7940, epoch 17, batch 44/481,disc_loss 77.037, (real 79.275, fake 74.8 ) gen_loss 767.12\n",
            "iteration 7941, epoch 17, batch 45/481,disc_loss 73.767, (real 75.536, fake 71.999 ) gen_loss 737.92\n",
            "iteration 7942, epoch 17, batch 46/481,disc_loss 79.639, (real 82.716, fake 76.562 ) gen_loss 745.93\n",
            "iteration 7943, epoch 17, batch 47/481,disc_loss 78.679, (real 81.914, fake 75.444 ) gen_loss 808.02\n",
            "iteration 7944, epoch 17, batch 48/481,disc_loss 75.868, (real 78.828, fake 72.909 ) gen_loss 776.26\n",
            "iteration 7945, epoch 17, batch 49/481,disc_loss 78.751, (real 81.112, fake 76.391 ) gen_loss 812.86\n",
            "iteration 7946, epoch 17, batch 50/481,disc_loss 77.294, (real 79.881, fake 74.706 ) gen_loss 793.06\n",
            "iteration 7947, epoch 17, batch 51/481,disc_loss 78.088, (real 81.03, fake 75.147 ) gen_loss 770.61\n",
            "iteration 7948, epoch 17, batch 52/481,disc_loss 79.971, (real 82.473, fake 77.468 ) gen_loss 758.54\n",
            "iteration 7949, epoch 17, batch 53/481,disc_loss 77.611, (real 79.774, fake 75.447 ) gen_loss 764.18\n",
            "iteration 7950, epoch 17, batch 54/481,disc_loss 78.955, (real 81.556, fake 76.355 ) gen_loss 782.03\n",
            "iteration 7951, epoch 17, batch 55/481,disc_loss 77.556, (real 81.128, fake 73.985 ) gen_loss 732.2\n",
            "iteration 7952, epoch 17, batch 56/481,disc_loss 75.004, (real 77.797, fake 72.21 ) gen_loss 760.89\n",
            "iteration 7953, epoch 17, batch 57/481,disc_loss 80.581, (real 84.351, fake 76.811 ) gen_loss 755.53\n",
            "iteration 7954, epoch 17, batch 58/481,disc_loss 80.842, (real 83.415, fake 78.269 ) gen_loss 722.33\n",
            "iteration 7955, epoch 17, batch 59/481,disc_loss 80.486, (real 82.954, fake 78.017 ) gen_loss 777.86\n",
            "iteration 7956, epoch 17, batch 60/481,disc_loss 76.414, (real 79.378, fake 73.451 ) gen_loss 762.8\n",
            "iteration 7957, epoch 17, batch 61/481,disc_loss 77.699, (real 79.783, fake 75.616 ) gen_loss 780.81\n",
            "iteration 7958, epoch 17, batch 62/481,disc_loss 80.6, (real 84.152, fake 77.048 ) gen_loss 752.65\n",
            "iteration 7959, epoch 17, batch 63/481,disc_loss 75.42, (real 78.383, fake 72.458 ) gen_loss 783.4\n",
            "iteration 7960, epoch 17, batch 64/481,disc_loss 78.922, (real 81.979, fake 75.864 ) gen_loss 752.51\n",
            "iteration 7961, epoch 17, batch 65/481,disc_loss 76.594, (real 79.616, fake 73.572 ) gen_loss 838.1\n",
            "iteration 7962, epoch 17, batch 66/481,disc_loss 76.047, (real 77.915, fake 74.179 ) gen_loss 697.69\n",
            "iteration 7963, epoch 17, batch 67/481,disc_loss 72.343, (real 74.905, fake 69.78 ) gen_loss 773.99\n",
            "iteration 7964, epoch 17, batch 68/481,disc_loss 80.435, (real 83.578, fake 77.293 ) gen_loss 865.95\n",
            "iteration 7965, epoch 17, batch 69/481,disc_loss 75.391, (real 77.707, fake 73.075 ) gen_loss 772.57\n",
            "iteration 7966, epoch 17, batch 70/481,disc_loss 76.052, (real 79.491, fake 72.614 ) gen_loss 735.12\n",
            "iteration 7967, epoch 17, batch 71/481,disc_loss 77.956, (real 80.943, fake 74.97 ) gen_loss 852.08\n",
            "iteration 7968, epoch 17, batch 72/481,disc_loss 76.163, (real 79.079, fake 73.247 ) gen_loss 824.8\n",
            "iteration 7969, epoch 17, batch 73/481,disc_loss 78.984, (real 81.761, fake 76.207 ) gen_loss 816.38\n",
            "iteration 7970, epoch 17, batch 74/481,disc_loss 78.413, (real 80.817, fake 76.01 ) gen_loss 747.59\n",
            "iteration 7971, epoch 17, batch 75/481,disc_loss 78.137, (real 80.479, fake 75.794 ) gen_loss 777.62\n",
            "iteration 7972, epoch 17, batch 76/481,disc_loss 79.048, (real 81.573, fake 76.524 ) gen_loss 787.13\n",
            "iteration 7973, epoch 17, batch 77/481,disc_loss 75.879, (real 78.251, fake 73.506 ) gen_loss 817.24\n",
            "iteration 7974, epoch 17, batch 78/481,disc_loss 74.58, (real 77.574, fake 71.586 ) gen_loss 826.66\n",
            "iteration 7975, epoch 17, batch 79/481,disc_loss 79.128, (real 81.548, fake 76.708 ) gen_loss 801.54\n",
            "iteration 7976, epoch 17, batch 80/481,disc_loss 78.991, (real 81.414, fake 76.568 ) gen_loss 753.53\n",
            "iteration 7977, epoch 17, batch 81/481,disc_loss 73.557, (real 76.026, fake 71.087 ) gen_loss 690.1\n",
            "iteration 7978, epoch 17, batch 82/481,disc_loss 78.557, (real 81.302, fake 75.811 ) gen_loss 698.1\n",
            "iteration 7979, epoch 17, batch 83/481,disc_loss 77.715, (real 80.098, fake 75.333 ) gen_loss 733.89\n",
            "iteration 7980, epoch 17, batch 84/481,disc_loss 79.214, (real 82.467, fake 75.96 ) gen_loss 747.55\n",
            "iteration 7981, epoch 17, batch 85/481,disc_loss 76.009, (real 78.807, fake 73.212 ) gen_loss 789.87\n",
            "iteration 7982, epoch 17, batch 86/481,disc_loss 79.495, (real 82.319, fake 76.671 ) gen_loss 788.02\n",
            "iteration 7983, epoch 17, batch 87/481,disc_loss 77.968, (real 80.231, fake 75.705 ) gen_loss 758.04\n",
            "iteration 7984, epoch 17, batch 88/481,disc_loss 74.193, (real 76.648, fake 71.738 ) gen_loss 737.96\n",
            "iteration 7985, epoch 17, batch 89/481,disc_loss 76.023, (real 78.356, fake 73.691 ) gen_loss 743.89\n",
            "iteration 7986, epoch 17, batch 90/481,disc_loss 74.749, (real 77.522, fake 71.976 ) gen_loss 842.51\n",
            "iteration 7987, epoch 17, batch 91/481,disc_loss 78.366, (real 81.422, fake 75.311 ) gen_loss 804.14\n",
            "iteration 7988, epoch 17, batch 92/481,disc_loss 80.19, (real 82.738, fake 77.642 ) gen_loss 763.62\n",
            "iteration 7989, epoch 17, batch 93/481,disc_loss 76.367, (real 79.17, fake 73.564 ) gen_loss 770.89\n",
            "iteration 7990, epoch 17, batch 94/481,disc_loss 79.533, (real 82.411, fake 76.655 ) gen_loss 713.0\n",
            "iteration 7991, epoch 17, batch 95/481,disc_loss 77.427, (real 80.377, fake 74.478 ) gen_loss 789.13\n",
            "iteration 7992, epoch 17, batch 96/481,disc_loss 75.956, (real 78.115, fake 73.797 ) gen_loss 822.91\n",
            "iteration 7993, epoch 17, batch 97/481,disc_loss 77.621, (real 81.062, fake 74.179 ) gen_loss 795.35\n",
            "iteration 7994, epoch 17, batch 98/481,disc_loss 80.213, (real 82.705, fake 77.72 ) gen_loss 1185.2\n",
            "iteration 7995, epoch 17, batch 99/481,disc_loss 79.158, (real 82.206, fake 76.111 ) gen_loss 814.71\n",
            "iteration 7996, epoch 17, batch 100/481,disc_loss 79.999, (real 83.059, fake 76.939 ) gen_loss 713.41\n",
            "iteration 7997, epoch 17, batch 101/481,disc_loss 81.01, (real 83.491, fake 78.529 ) gen_loss 729.65\n",
            "iteration 7998, epoch 17, batch 102/481,disc_loss 75.379, (real 78.229, fake 72.53 ) gen_loss 853.86\n",
            "iteration 7999, epoch 17, batch 103/481,disc_loss 78.899, (real 81.572, fake 76.227 ) gen_loss 757.79\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 8000, epoch 17, batch 104/481,disc_loss 76.02, (real 79.276, fake 72.765 ) gen_loss 849.24\n",
            "iteration 8001, epoch 17, batch 105/481,disc_loss 76.105, (real 77.844, fake 74.366 ) gen_loss 754.07\n",
            "iteration 8002, epoch 17, batch 106/481,disc_loss 74.373, (real 76.693, fake 72.052 ) gen_loss 846.84\n",
            "iteration 8003, epoch 17, batch 107/481,disc_loss 78.061, (real 79.993, fake 76.129 ) gen_loss 743.8\n",
            "iteration 8004, epoch 17, batch 108/481,disc_loss 77.067, (real 79.928, fake 74.207 ) gen_loss 799.25\n",
            "iteration 8005, epoch 17, batch 109/481,disc_loss 73.816, (real 76.378, fake 71.253 ) gen_loss 861.25\n",
            "iteration 8006, epoch 17, batch 110/481,disc_loss 75.226, (real 77.797, fake 72.656 ) gen_loss 767.75\n",
            "iteration 8007, epoch 17, batch 111/481,disc_loss 76.974, (real 79.541, fake 74.407 ) gen_loss 804.26\n",
            "iteration 8008, epoch 17, batch 112/481,disc_loss 78.826, (real 81.294, fake 76.358 ) gen_loss 812.32\n",
            "iteration 8009, epoch 17, batch 113/481,disc_loss 78.368, (real 81.015, fake 75.72 ) gen_loss 807.83\n",
            "iteration 8010, epoch 17, batch 114/481,disc_loss 77.493, (real 80.014, fake 74.971 ) gen_loss 811.46\n",
            "iteration 8011, epoch 17, batch 115/481,disc_loss 81.835, (real 85.201, fake 78.469 ) gen_loss 750.75\n",
            "iteration 8012, epoch 17, batch 116/481,disc_loss 77.569, (real 80.385, fake 74.753 ) gen_loss 735.71\n",
            "iteration 8013, epoch 17, batch 117/481,disc_loss 75.739, (real 78.138, fake 73.34 ) gen_loss 813.69\n",
            "iteration 8014, epoch 17, batch 118/481,disc_loss 75.451, (real 77.532, fake 73.37 ) gen_loss 771.48\n",
            "iteration 8015, epoch 17, batch 119/481,disc_loss 75.147, (real 77.296, fake 72.998 ) gen_loss 778.46\n",
            "iteration 8016, epoch 17, batch 120/481,disc_loss 76.679, (real 78.917, fake 74.44 ) gen_loss 714.57\n",
            "iteration 8017, epoch 17, batch 121/481,disc_loss 75.24, (real 77.858, fake 72.623 ) gen_loss 787.42\n",
            "iteration 8018, epoch 17, batch 122/481,disc_loss 74.674, (real 77.157, fake 72.19 ) gen_loss 838.44\n",
            "iteration 8019, epoch 17, batch 123/481,disc_loss 70.187, (real 72.808, fake 67.566 ) gen_loss 780.32\n",
            "iteration 8020, epoch 17, batch 124/481,disc_loss 76.34, (real 79.287, fake 73.394 ) gen_loss 866.48\n",
            "iteration 8021, epoch 17, batch 125/481,disc_loss 78.953, (real 80.937, fake 76.97 ) gen_loss 745.79\n",
            "iteration 8022, epoch 17, batch 126/481,disc_loss 76.387, (real 79.84, fake 72.933 ) gen_loss 794.53\n",
            "iteration 8023, epoch 17, batch 127/481,disc_loss 79.434, (real 82.744, fake 76.123 ) gen_loss 764.83\n",
            "iteration 8024, epoch 17, batch 128/481,disc_loss 76.088, (real 78.252, fake 73.923 ) gen_loss 778.22\n",
            "iteration 8025, epoch 17, batch 129/481,disc_loss 77.817, (real 79.586, fake 76.048 ) gen_loss 770.46\n",
            "iteration 8026, epoch 17, batch 130/481,disc_loss 72.869, (real 75.059, fake 70.679 ) gen_loss 855.17\n",
            "iteration 8027, epoch 17, batch 131/481,disc_loss 76.568, (real 79.22, fake 73.916 ) gen_loss 817.22\n",
            "iteration 8028, epoch 17, batch 132/481,disc_loss 79.275, (real 81.691, fake 76.86 ) gen_loss 720.63\n",
            "iteration 8029, epoch 17, batch 133/481,disc_loss 80.771, (real 82.819, fake 78.723 ) gen_loss 850.47\n",
            "iteration 8030, epoch 17, batch 134/481,disc_loss 79.809, (real 82.454, fake 77.163 ) gen_loss 749.73\n",
            "iteration 8031, epoch 17, batch 135/481,disc_loss 79.932, (real 81.635, fake 78.23 ) gen_loss 808.73\n",
            "iteration 8032, epoch 17, batch 136/481,disc_loss 78.894, (real 81.66, fake 76.129 ) gen_loss 821.18\n",
            "iteration 8033, epoch 17, batch 137/481,disc_loss 77.838, (real 79.834, fake 75.842 ) gen_loss 764.76\n",
            "iteration 8034, epoch 17, batch 138/481,disc_loss 80.202, (real 83.007, fake 77.397 ) gen_loss 856.87\n",
            "iteration 8035, epoch 17, batch 139/481,disc_loss 76.467, (real 79.043, fake 73.892 ) gen_loss 825.09\n",
            "iteration 8036, epoch 17, batch 140/481,disc_loss 77.903, (real 80.493, fake 75.313 ) gen_loss 867.32\n",
            "iteration 8037, epoch 17, batch 141/481,disc_loss 82.448, (real 84.849, fake 80.046 ) gen_loss 729.74\n",
            "iteration 8038, epoch 17, batch 142/481,disc_loss 76.819, (real 79.649, fake 73.99 ) gen_loss 692.28\n",
            "iteration 8039, epoch 17, batch 143/481,disc_loss 78.465, (real 81.581, fake 75.348 ) gen_loss 852.49\n",
            "iteration 8040, epoch 17, batch 144/481,disc_loss 79.613, (real 82.509, fake 76.717 ) gen_loss 685.07\n",
            "iteration 8041, epoch 17, batch 145/481,disc_loss 75.693, (real 77.613, fake 73.773 ) gen_loss 723.32\n",
            "iteration 8042, epoch 17, batch 146/481,disc_loss 77.314, (real 80.61, fake 74.017 ) gen_loss 781.66\n",
            "iteration 8043, epoch 17, batch 147/481,disc_loss 80.082, (real 82.371, fake 77.793 ) gen_loss 783.22\n",
            "iteration 8044, epoch 17, batch 148/481,disc_loss 81.312, (real 84.637, fake 77.988 ) gen_loss 811.48\n",
            "iteration 8045, epoch 17, batch 149/481,disc_loss 77.113, (real 80.453, fake 73.774 ) gen_loss 767.45\n",
            "iteration 8046, epoch 17, batch 150/481,disc_loss 81.703, (real 84.345, fake 79.06 ) gen_loss 719.96\n",
            "iteration 8047, epoch 17, batch 151/481,disc_loss 78.629, (real 82.101, fake 75.157 ) gen_loss 752.65\n",
            "iteration 8048, epoch 17, batch 152/481,disc_loss 74.246, (real 76.857, fake 71.635 ) gen_loss 749.61\n",
            "iteration 8049, epoch 17, batch 153/481,disc_loss 73.582, (real 76.79, fake 70.375 ) gen_loss 715.49\n",
            "iteration 8050, epoch 17, batch 154/481,disc_loss 76.804, (real 79.013, fake 74.595 ) gen_loss 768.8\n",
            "iteration 8051, epoch 17, batch 155/481,disc_loss 79.533, (real 81.925, fake 77.142 ) gen_loss 849.24\n",
            "iteration 8052, epoch 17, batch 156/481,disc_loss 77.162, (real 79.789, fake 74.536 ) gen_loss 957.02\n",
            "iteration 8053, epoch 17, batch 157/481,disc_loss 80.12, (real 82.317, fake 77.923 ) gen_loss 829.61\n",
            "iteration 8054, epoch 17, batch 158/481,disc_loss 81.009, (real 84.071, fake 77.947 ) gen_loss 779.66\n",
            "iteration 8055, epoch 17, batch 159/481,disc_loss 79.125, (real 81.661, fake 76.588 ) gen_loss 790.42\n",
            "iteration 8056, epoch 17, batch 160/481,disc_loss 77.552, (real 79.869, fake 75.235 ) gen_loss 732.35\n",
            "iteration 8057, epoch 17, batch 161/481,disc_loss 81.255, (real 83.545, fake 78.964 ) gen_loss 846.46\n",
            "iteration 8058, epoch 17, batch 162/481,disc_loss 80.092, (real 82.947, fake 77.237 ) gen_loss 760.85\n",
            "iteration 8059, epoch 17, batch 163/481,disc_loss 76.837, (real 79.064, fake 74.611 ) gen_loss 834.26\n",
            "iteration 8060, epoch 17, batch 164/481,disc_loss 76.812, (real 79.289, fake 74.335 ) gen_loss 737.33\n",
            "iteration 8061, epoch 17, batch 165/481,disc_loss 75.123, (real 77.764, fake 72.482 ) gen_loss 797.3\n",
            "iteration 8062, epoch 17, batch 166/481,disc_loss 78.447, (real 81.315, fake 75.579 ) gen_loss 803.86\n",
            "iteration 8063, epoch 17, batch 167/481,disc_loss 77.245, (real 79.992, fake 74.498 ) gen_loss 826.16\n",
            "iteration 8064, epoch 17, batch 168/481,disc_loss 80.191, (real 83.577, fake 76.804 ) gen_loss 895.9\n",
            "iteration 8065, epoch 17, batch 169/481,disc_loss 76.63, (real 79.039, fake 74.221 ) gen_loss 748.32\n",
            "iteration 8066, epoch 17, batch 170/481,disc_loss 76.532, (real 79.066, fake 73.997 ) gen_loss 802.89\n",
            "iteration 8067, epoch 17, batch 171/481,disc_loss 76.214, (real 78.498, fake 73.93 ) gen_loss 774.02\n",
            "iteration 8068, epoch 17, batch 172/481,disc_loss 78.698, (real 81.345, fake 76.052 ) gen_loss 721.43\n",
            "iteration 8069, epoch 17, batch 173/481,disc_loss 72.36, (real 74.566, fake 70.153 ) gen_loss 812.06\n",
            "iteration 8070, epoch 17, batch 174/481,disc_loss 71.53, (real 74.059, fake 69.002 ) gen_loss 827.11\n",
            "iteration 8071, epoch 17, batch 175/481,disc_loss 77.023, (real 80.31, fake 73.736 ) gen_loss 761.48\n",
            "iteration 8072, epoch 17, batch 176/481,disc_loss 72.604, (real 74.352, fake 70.857 ) gen_loss 819.27\n",
            "iteration 8073, epoch 17, batch 177/481,disc_loss 75.457, (real 78.117, fake 72.797 ) gen_loss 853.05\n",
            "iteration 8074, epoch 17, batch 178/481,disc_loss 75.299, (real 77.954, fake 72.643 ) gen_loss 845.87\n",
            "iteration 8075, epoch 17, batch 179/481,disc_loss 79.338, (real 81.869, fake 76.807 ) gen_loss 770.12\n",
            "iteration 8076, epoch 17, batch 180/481,disc_loss 74.507, (real 76.591, fake 72.424 ) gen_loss 793.65\n",
            "iteration 8077, epoch 17, batch 181/481,disc_loss 81.867, (real 84.081, fake 79.653 ) gen_loss 735.28\n",
            "iteration 8078, epoch 17, batch 182/481,disc_loss 75.003, (real 77.531, fake 72.475 ) gen_loss 885.27\n",
            "iteration 8079, epoch 17, batch 183/481,disc_loss 76.436, (real 78.614, fake 74.258 ) gen_loss 766.14\n",
            "iteration 8080, epoch 17, batch 184/481,disc_loss 80.45, (real 83.494, fake 77.405 ) gen_loss 666.81\n",
            "iteration 8081, epoch 17, batch 185/481,disc_loss 77.156, (real 79.801, fake 74.512 ) gen_loss 691.68\n",
            "iteration 8082, epoch 17, batch 186/481,disc_loss 79.188, (real 82.486, fake 75.889 ) gen_loss 790.02\n",
            "iteration 8083, epoch 17, batch 187/481,disc_loss 74.653, (real 77.731, fake 71.575 ) gen_loss 767.84\n",
            "iteration 8084, epoch 17, batch 188/481,disc_loss 74.792, (real 77.63, fake 71.954 ) gen_loss 742.15\n",
            "iteration 8085, epoch 17, batch 189/481,disc_loss 75.478, (real 77.902, fake 73.053 ) gen_loss 815.09\n",
            "iteration 8086, epoch 17, batch 190/481,disc_loss 77.913, (real 80.773, fake 75.053 ) gen_loss 739.14\n",
            "iteration 8087, epoch 17, batch 191/481,disc_loss 78.267, (real 80.381, fake 76.153 ) gen_loss 888.19\n",
            "iteration 8088, epoch 17, batch 192/481,disc_loss 78.187, (real 80.849, fake 75.524 ) gen_loss 746.91\n",
            "iteration 8089, epoch 17, batch 193/481,disc_loss 79.935, (real 82.667, fake 77.204 ) gen_loss 768.84\n",
            "iteration 8090, epoch 17, batch 194/481,disc_loss 76.95, (real 79.401, fake 74.499 ) gen_loss 773.14\n",
            "iteration 8091, epoch 17, batch 195/481,disc_loss 77.432, (real 80.489, fake 74.375 ) gen_loss 802.51\n",
            "iteration 8092, epoch 17, batch 196/481,disc_loss 78.208, (real 81.181, fake 75.236 ) gen_loss 776.66\n",
            "iteration 8093, epoch 17, batch 197/481,disc_loss 83.381, (real 85.82, fake 80.942 ) gen_loss 710.28\n",
            "iteration 8094, epoch 17, batch 198/481,disc_loss 75.541, (real 78.162, fake 72.919 ) gen_loss 710.55\n",
            "iteration 8095, epoch 17, batch 199/481,disc_loss 79.179, (real 81.355, fake 77.003 ) gen_loss 740.13\n",
            "iteration 8096, epoch 17, batch 200/481,disc_loss 77.498, (real 80.596, fake 74.4 ) gen_loss 833.2\n",
            "iteration 8097, epoch 17, batch 201/481,disc_loss 77.544, (real 80.175, fake 74.914 ) gen_loss 808.25\n",
            "iteration 8098, epoch 17, batch 202/481,disc_loss 78.648, (real 81.816, fake 75.48 ) gen_loss 834.94\n",
            "iteration 8099, epoch 17, batch 203/481,disc_loss 77.646, (real 79.514, fake 75.778 ) gen_loss 812.99\n",
            "iteration 8100, epoch 17, batch 204/481,disc_loss 80.872, (real 84.058, fake 77.686 ) gen_loss 754.42\n",
            "iteration 8101, epoch 17, batch 205/481,disc_loss 81.161, (real 83.602, fake 78.72 ) gen_loss 752.72\n",
            "iteration 8102, epoch 17, batch 206/481,disc_loss 78.035, (real 80.822, fake 75.248 ) gen_loss 842.73\n",
            "iteration 8103, epoch 17, batch 207/481,disc_loss 76.709, (real 79.102, fake 74.316 ) gen_loss 868.39\n",
            "iteration 8104, epoch 17, batch 208/481,disc_loss 77.969, (real 79.995, fake 75.943 ) gen_loss 768.18\n",
            "iteration 8105, epoch 17, batch 209/481,disc_loss 79.634, (real 82.29, fake 76.978 ) gen_loss 725.31\n",
            "iteration 8106, epoch 17, batch 210/481,disc_loss 79.641, (real 82.726, fake 76.557 ) gen_loss 689.65\n",
            "iteration 8107, epoch 17, batch 211/481,disc_loss 76.536, (real 79.04, fake 74.032 ) gen_loss 685.64\n",
            "iteration 8108, epoch 17, batch 212/481,disc_loss 78.528, (real 81.985, fake 75.072 ) gen_loss 780.34\n",
            "iteration 8109, epoch 17, batch 213/481,disc_loss 77.427, (real 79.769, fake 75.084 ) gen_loss 771.57\n",
            "iteration 8110, epoch 17, batch 214/481,disc_loss 79.063, (real 81.288, fake 76.838 ) gen_loss 835.28\n",
            "iteration 8111, epoch 17, batch 215/481,disc_loss 73.24, (real 75.807, fake 70.674 ) gen_loss 744.5\n",
            "iteration 8112, epoch 17, batch 216/481,disc_loss 75.732, (real 77.993, fake 73.471 ) gen_loss 899.62\n",
            "iteration 8113, epoch 17, batch 217/481,disc_loss 79.497, (real 81.998, fake 76.995 ) gen_loss 858.25\n",
            "iteration 8114, epoch 17, batch 218/481,disc_loss 77.636, (real 80.178, fake 75.094 ) gen_loss 765.95\n",
            "iteration 8115, epoch 17, batch 219/481,disc_loss 77.501, (real 80.079, fake 74.923 ) gen_loss 815.12\n",
            "iteration 8116, epoch 17, batch 220/481,disc_loss 82.277, (real 84.854, fake 79.7 ) gen_loss 791.84\n",
            "iteration 8117, epoch 17, batch 221/481,disc_loss 77.858, (real 81.262, fake 74.454 ) gen_loss 878.98\n",
            "iteration 8118, epoch 17, batch 222/481,disc_loss 77.335, (real 80.594, fake 74.076 ) gen_loss 921.41\n",
            "iteration 8119, epoch 17, batch 223/481,disc_loss 79.752, (real 82.836, fake 76.668 ) gen_loss 885.89\n",
            "iteration 8120, epoch 17, batch 224/481,disc_loss 76.841, (real 78.673, fake 75.008 ) gen_loss 811.35\n",
            "iteration 8121, epoch 17, batch 225/481,disc_loss 75.756, (real 77.449, fake 74.063 ) gen_loss 814.37\n",
            "iteration 8122, epoch 17, batch 226/481,disc_loss 75.807, (real 78.652, fake 72.962 ) gen_loss 765.89\n",
            "iteration 8123, epoch 17, batch 227/481,disc_loss 78.633, (real 80.3, fake 76.966 ) gen_loss 766.33\n",
            "iteration 8124, epoch 17, batch 228/481,disc_loss 73.538, (real 76.195, fake 70.88 ) gen_loss 815.74\n",
            "iteration 8125, epoch 17, batch 229/481,disc_loss 72.566, (real 75.121, fake 70.011 ) gen_loss 773.49\n",
            "iteration 8126, epoch 17, batch 230/481,disc_loss 79.45, (real 81.816, fake 77.084 ) gen_loss 862.32\n",
            "iteration 8127, epoch 17, batch 231/481,disc_loss 73.76, (real 76.302, fake 71.217 ) gen_loss 848.04\n",
            "iteration 8128, epoch 17, batch 232/481,disc_loss 78.039, (real 80.964, fake 75.113 ) gen_loss 803.5\n",
            "iteration 8129, epoch 17, batch 233/481,disc_loss 77.377, (real 80.06, fake 74.695 ) gen_loss 825.57\n",
            "iteration 8130, epoch 17, batch 234/481,disc_loss 78.753, (real 80.956, fake 76.55 ) gen_loss 769.62\n",
            "iteration 8131, epoch 17, batch 235/481,disc_loss 73.1, (real 75.447, fake 70.753 ) gen_loss 853.64\n",
            "iteration 8132, epoch 17, batch 236/481,disc_loss 73.345, (real 75.715, fake 70.974 ) gen_loss 800.12\n",
            "iteration 8133, epoch 17, batch 237/481,disc_loss 75.906, (real 78.513, fake 73.3 ) gen_loss 741.79\n",
            "iteration 8134, epoch 17, batch 238/481,disc_loss 75.33, (real 77.387, fake 73.273 ) gen_loss 738.99\n",
            "iteration 8135, epoch 17, batch 239/481,disc_loss 74.726, (real 78.044, fake 71.407 ) gen_loss 851.32\n",
            "iteration 8136, epoch 17, batch 240/481,disc_loss 81.715, (real 84.577, fake 78.854 ) gen_loss 784.81\n",
            "iteration 8137, epoch 17, batch 241/481,disc_loss 79.173, (real 81.731, fake 76.616 ) gen_loss 780.52\n",
            "iteration 8138, epoch 17, batch 242/481,disc_loss 76.536, (real 78.862, fake 74.21 ) gen_loss 794.51\n",
            "iteration 8139, epoch 17, batch 243/481,disc_loss 78.831, (real 81.536, fake 76.125 ) gen_loss 838.93\n",
            "iteration 8140, epoch 17, batch 244/481,disc_loss 78.001, (real 80.084, fake 75.918 ) gen_loss 798.29\n",
            "iteration 8141, epoch 17, batch 245/481,disc_loss 77.989, (real 80.512, fake 75.467 ) gen_loss 887.08\n",
            "iteration 8142, epoch 17, batch 246/481,disc_loss 80.667, (real 83.263, fake 78.071 ) gen_loss 842.46\n",
            "iteration 8143, epoch 17, batch 247/481,disc_loss 75.451, (real 78.215, fake 72.687 ) gen_loss 834.92\n",
            "iteration 8144, epoch 17, batch 248/481,disc_loss 79.035, (real 81.516, fake 76.555 ) gen_loss 726.04\n",
            "iteration 8145, epoch 17, batch 249/481,disc_loss 75.337, (real 78.063, fake 72.611 ) gen_loss 835.4\n",
            "iteration 8146, epoch 17, batch 250/481,disc_loss 75.059, (real 77.775, fake 72.343 ) gen_loss 848.07\n",
            "iteration 8147, epoch 17, batch 251/481,disc_loss 81.052, (real 83.043, fake 79.062 ) gen_loss 778.48\n",
            "iteration 8148, epoch 17, batch 252/481,disc_loss 80.387, (real 83.136, fake 77.639 ) gen_loss 786.58\n",
            "iteration 8149, epoch 17, batch 253/481,disc_loss 80.237, (real 85.003, fake 75.47 ) gen_loss 748.6\n",
            "iteration 8150, epoch 17, batch 254/481,disc_loss 76.28, (real 78.116, fake 74.444 ) gen_loss 763.3\n",
            "iteration 8151, epoch 17, batch 255/481,disc_loss 78.391, (real 81.902, fake 74.881 ) gen_loss 734.97\n",
            "iteration 8152, epoch 17, batch 256/481,disc_loss 81.352, (real 83.113, fake 79.59 ) gen_loss 738.62\n",
            "iteration 8153, epoch 17, batch 257/481,disc_loss 75.372, (real 78.399, fake 72.346 ) gen_loss 723.96\n",
            "iteration 8154, epoch 17, batch 258/481,disc_loss 76.403, (real 79.506, fake 73.299 ) gen_loss 797.78\n",
            "iteration 8155, epoch 17, batch 259/481,disc_loss 71.911, (real 74.505, fake 69.317 ) gen_loss 798.25\n",
            "iteration 8156, epoch 17, batch 260/481,disc_loss 76.916, (real 79.073, fake 74.76 ) gen_loss 778.64\n",
            "iteration 8157, epoch 17, batch 261/481,disc_loss 79.349, (real 81.722, fake 76.975 ) gen_loss 759.52\n",
            "iteration 8158, epoch 17, batch 262/481,disc_loss 79.267, (real 82.176, fake 76.358 ) gen_loss 750.2\n",
            "iteration 8159, epoch 17, batch 263/481,disc_loss 76.459, (real 78.54, fake 74.378 ) gen_loss 699.6\n",
            "iteration 8160, epoch 17, batch 264/481,disc_loss 80.203, (real 82.971, fake 77.435 ) gen_loss 748.15\n",
            "iteration 8161, epoch 17, batch 265/481,disc_loss 76.748, (real 79.028, fake 74.469 ) gen_loss 777.16\n",
            "iteration 8162, epoch 17, batch 266/481,disc_loss 75.618, (real 78.96, fake 72.276 ) gen_loss 789.06\n",
            "iteration 8163, epoch 17, batch 267/481,disc_loss 73.39, (real 76.438, fake 70.343 ) gen_loss 896.32\n",
            "iteration 8164, epoch 17, batch 268/481,disc_loss 73.435, (real 75.67, fake 71.201 ) gen_loss 744.02\n",
            "iteration 8165, epoch 17, batch 269/481,disc_loss 75.913, (real 78.641, fake 73.184 ) gen_loss 788.64\n",
            "iteration 8166, epoch 17, batch 270/481,disc_loss 73.532, (real 75.791, fake 71.274 ) gen_loss 822.28\n",
            "iteration 8167, epoch 17, batch 271/481,disc_loss 77.174, (real 79.638, fake 74.71 ) gen_loss 779.73\n",
            "iteration 8168, epoch 17, batch 272/481,disc_loss 75.875, (real 78.188, fake 73.563 ) gen_loss 800.13\n",
            "iteration 8169, epoch 17, batch 273/481,disc_loss 75.02, (real 77.063, fake 72.977 ) gen_loss 893.08\n",
            "iteration 8170, epoch 17, batch 274/481,disc_loss 72.908, (real 74.976, fake 70.839 ) gen_loss 793.8\n",
            "iteration 8171, epoch 17, batch 275/481,disc_loss 76.473, (real 79.317, fake 73.629 ) gen_loss 852.69\n",
            "iteration 8172, epoch 17, batch 276/481,disc_loss 80.41, (real 83.006, fake 77.813 ) gen_loss 807.14\n",
            "iteration 8173, epoch 17, batch 277/481,disc_loss 77.599, (real 80.139, fake 75.059 ) gen_loss 772.2\n",
            "iteration 8174, epoch 17, batch 278/481,disc_loss 78.173, (real 81.269, fake 75.078 ) gen_loss 849.29\n",
            "iteration 8175, epoch 17, batch 279/481,disc_loss 79.888, (real 82.033, fake 77.742 ) gen_loss 898.37\n",
            "iteration 8176, epoch 17, batch 280/481,disc_loss 80.492, (real 82.902, fake 78.081 ) gen_loss 821.46\n",
            "iteration 8177, epoch 17, batch 281/481,disc_loss 78.934, (real 81.406, fake 76.462 ) gen_loss 868.47\n",
            "iteration 8178, epoch 17, batch 282/481,disc_loss 82.313, (real 84.325, fake 80.3 ) gen_loss 795.27\n",
            "iteration 8179, epoch 17, batch 283/481,disc_loss 80.439, (real 82.231, fake 78.646 ) gen_loss 820.62\n",
            "iteration 8180, epoch 17, batch 284/481,disc_loss 79.218, (real 81.853, fake 76.583 ) gen_loss 775.69\n",
            "iteration 8181, epoch 17, batch 285/481,disc_loss 80.606, (real 82.982, fake 78.229 ) gen_loss 881.92\n",
            "iteration 8182, epoch 17, batch 286/481,disc_loss 78.101, (real 80.828, fake 75.373 ) gen_loss 938.21\n",
            "iteration 8183, epoch 17, batch 287/481,disc_loss 74.261, (real 77.001, fake 71.521 ) gen_loss 818.4\n",
            "iteration 8184, epoch 17, batch 288/481,disc_loss 76.647, (real 79.502, fake 73.792 ) gen_loss 835.43\n",
            "iteration 8185, epoch 17, batch 289/481,disc_loss 81.173, (real 84.24, fake 78.107 ) gen_loss 888.57\n",
            "iteration 8186, epoch 17, batch 290/481,disc_loss 74.705, (real 77.147, fake 72.264 ) gen_loss 901.34\n",
            "iteration 8187, epoch 17, batch 291/481,disc_loss 72.291, (real 74.153, fake 70.429 ) gen_loss 867.73\n",
            "iteration 8188, epoch 17, batch 292/481,disc_loss 79.229, (real 81.454, fake 77.004 ) gen_loss 827.79\n",
            "iteration 8189, epoch 17, batch 293/481,disc_loss 71.541, (real 74.155, fake 68.927 ) gen_loss 880.12\n",
            "iteration 8190, epoch 17, batch 294/481,disc_loss 79.568, (real 82.366, fake 76.77 ) gen_loss 866.07\n",
            "iteration 8191, epoch 17, batch 295/481,disc_loss 77.691, (real 79.754, fake 75.627 ) gen_loss 797.99\n",
            "iteration 8192, epoch 17, batch 296/481,disc_loss 76.744, (real 79.278, fake 74.209 ) gen_loss 878.18\n",
            "iteration 8193, epoch 17, batch 297/481,disc_loss 78.752, (real 81.0, fake 76.503 ) gen_loss 770.35\n",
            "iteration 8194, epoch 17, batch 298/481,disc_loss 78.102, (real 80.601, fake 75.603 ) gen_loss 877.76\n",
            "iteration 8195, epoch 17, batch 299/481,disc_loss 75.166, (real 77.711, fake 72.621 ) gen_loss 856.4\n",
            "iteration 8196, epoch 17, batch 300/481,disc_loss 77.895, (real 80.746, fake 75.044 ) gen_loss 784.53\n",
            "iteration 8197, epoch 17, batch 301/481,disc_loss 80.204, (real 82.741, fake 77.667 ) gen_loss 818.29\n",
            "iteration 8198, epoch 17, batch 302/481,disc_loss 78.685, (real 81.382, fake 75.988 ) gen_loss 849.29\n",
            "iteration 8199, epoch 17, batch 303/481,disc_loss 75.638, (real 77.687, fake 73.59 ) gen_loss 757.41\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 8200, epoch 17, batch 304/481,disc_loss 78.071, (real 81.015, fake 75.128 ) gen_loss 794.36\n",
            "iteration 8201, epoch 17, batch 305/481,disc_loss 74.088, (real 77.508, fake 70.667 ) gen_loss 772.12\n",
            "iteration 8202, epoch 17, batch 306/481,disc_loss 82.376, (real 85.31, fake 79.442 ) gen_loss 812.18\n",
            "iteration 8203, epoch 17, batch 307/481,disc_loss 76.201, (real 79.279, fake 73.122 ) gen_loss 838.44\n",
            "iteration 8204, epoch 17, batch 308/481,disc_loss 81.512, (real 84.546, fake 78.478 ) gen_loss 725.82\n",
            "iteration 8205, epoch 17, batch 309/481,disc_loss 79.718, (real 82.449, fake 76.986 ) gen_loss 825.62\n",
            "iteration 8206, epoch 17, batch 310/481,disc_loss 85.624, (real 88.08, fake 83.169 ) gen_loss 798.06\n",
            "iteration 8207, epoch 17, batch 311/481,disc_loss 79.78, (real 82.698, fake 76.862 ) gen_loss 818.85\n",
            "iteration 8208, epoch 17, batch 312/481,disc_loss 78.734, (real 81.074, fake 76.395 ) gen_loss 736.68\n",
            "iteration 8209, epoch 17, batch 313/481,disc_loss 75.921, (real 79.775, fake 72.067 ) gen_loss 763.35\n",
            "iteration 8210, epoch 17, batch 314/481,disc_loss 77.508, (real 80.124, fake 74.893 ) gen_loss 815.92\n",
            "iteration 8211, epoch 17, batch 315/481,disc_loss 80.575, (real 84.563, fake 76.587 ) gen_loss 753.63\n",
            "iteration 8212, epoch 17, batch 316/481,disc_loss 79.251, (real 82.319, fake 76.184 ) gen_loss 848.53\n",
            "iteration 8213, epoch 17, batch 317/481,disc_loss 78.105, (real 80.576, fake 75.634 ) gen_loss 820.74\n",
            "iteration 8214, epoch 17, batch 318/481,disc_loss 76.813, (real 80.001, fake 73.624 ) gen_loss 900.41\n",
            "iteration 8215, epoch 17, batch 319/481,disc_loss 77.139, (real 79.404, fake 74.874 ) gen_loss 792.05\n",
            "iteration 8216, epoch 17, batch 320/481,disc_loss 77.345, (real 79.357, fake 75.333 ) gen_loss 788.95\n",
            "iteration 8217, epoch 17, batch 321/481,disc_loss 75.162, (real 77.429, fake 72.895 ) gen_loss 816.52\n",
            "iteration 8218, epoch 17, batch 322/481,disc_loss 77.616, (real 80.392, fake 74.841 ) gen_loss 814.1\n",
            "iteration 8219, epoch 17, batch 323/481,disc_loss 70.798, (real 73.121, fake 68.475 ) gen_loss 753.77\n",
            "iteration 8220, epoch 17, batch 324/481,disc_loss 76.645, (real 78.612, fake 74.678 ) gen_loss 808.22\n",
            "iteration 8221, epoch 17, batch 325/481,disc_loss 80.746, (real 82.983, fake 78.508 ) gen_loss 764.89\n",
            "iteration 8222, epoch 17, batch 326/481,disc_loss 79.289, (real 81.165, fake 77.413 ) gen_loss 776.46\n",
            "iteration 8223, epoch 17, batch 327/481,disc_loss 72.378, (real 74.587, fake 70.169 ) gen_loss 777.4\n",
            "iteration 8224, epoch 17, batch 328/481,disc_loss 76.999, (real 79.251, fake 74.746 ) gen_loss 789.82\n",
            "iteration 8225, epoch 17, batch 329/481,disc_loss 79.242, (real 82.073, fake 76.41 ) gen_loss 774.2\n",
            "iteration 8226, epoch 17, batch 330/481,disc_loss 76.293, (real 79.404, fake 73.181 ) gen_loss 849.37\n",
            "iteration 8227, epoch 17, batch 331/481,disc_loss 77.565, (real 80.474, fake 74.657 ) gen_loss 827.8\n",
            "iteration 8228, epoch 17, batch 332/481,disc_loss 75.533, (real 78.241, fake 72.825 ) gen_loss 814.03\n",
            "iteration 8229, epoch 17, batch 333/481,disc_loss 82.807, (real 85.47, fake 80.143 ) gen_loss 843.55\n",
            "iteration 8230, epoch 17, batch 334/481,disc_loss 76.868, (real 78.829, fake 74.906 ) gen_loss 787.95\n",
            "iteration 8231, epoch 17, batch 335/481,disc_loss 79.562, (real 82.223, fake 76.901 ) gen_loss 873.48\n",
            "iteration 8232, epoch 17, batch 336/481,disc_loss 80.84, (real 84.151, fake 77.529 ) gen_loss 833.97\n",
            "iteration 8233, epoch 17, batch 337/481,disc_loss 75.675, (real 77.745, fake 73.606 ) gen_loss 826.8\n",
            "iteration 8234, epoch 17, batch 338/481,disc_loss 77.193, (real 80.484, fake 73.902 ) gen_loss 827.16\n",
            "iteration 8235, epoch 17, batch 339/481,disc_loss 77.975, (real 81.607, fake 74.344 ) gen_loss 790.89\n",
            "iteration 8236, epoch 17, batch 340/481,disc_loss 75.936, (real 78.54, fake 73.331 ) gen_loss 744.16\n",
            "iteration 8237, epoch 17, batch 341/481,disc_loss 78.255, (real 81.228, fake 75.282 ) gen_loss 908.15\n",
            "iteration 8238, epoch 17, batch 342/481,disc_loss 77.864, (real 80.182, fake 75.547 ) gen_loss 855.96\n",
            "iteration 8239, epoch 17, batch 343/481,disc_loss 72.588, (real 74.877, fake 70.299 ) gen_loss 927.4\n",
            "iteration 8240, epoch 17, batch 344/481,disc_loss 78.046, (real 80.908, fake 75.184 ) gen_loss 843.42\n",
            "iteration 8241, epoch 17, batch 345/481,disc_loss 79.305, (real 82.656, fake 75.954 ) gen_loss 856.57\n",
            "iteration 8242, epoch 17, batch 346/481,disc_loss 75.668, (real 78.623, fake 72.714 ) gen_loss 816.19\n",
            "iteration 8243, epoch 17, batch 347/481,disc_loss 83.521, (real 86.675, fake 80.367 ) gen_loss 770.94\n",
            "iteration 8244, epoch 17, batch 348/481,disc_loss 78.988, (real 82.253, fake 75.723 ) gen_loss 892.3\n",
            "iteration 8245, epoch 17, batch 349/481,disc_loss 73.191, (real 75.933, fake 70.449 ) gen_loss 815.23\n",
            "iteration 8246, epoch 17, batch 350/481,disc_loss 79.352, (real 81.852, fake 76.852 ) gen_loss 949.01\n",
            "iteration 8247, epoch 17, batch 351/481,disc_loss 76.034, (real 78.444, fake 73.624 ) gen_loss 852.2\n",
            "iteration 8248, epoch 17, batch 352/481,disc_loss 80.408, (real 82.56, fake 78.255 ) gen_loss 821.54\n",
            "iteration 8249, epoch 17, batch 353/481,disc_loss 81.879, (real 84.162, fake 79.596 ) gen_loss 929.57\n",
            "iteration 8250, epoch 17, batch 354/481,disc_loss 80.652, (real 82.23, fake 79.075 ) gen_loss 786.49\n",
            "iteration 8251, epoch 17, batch 355/481,disc_loss 78.501, (real 81.499, fake 75.502 ) gen_loss 851.27\n",
            "iteration 8252, epoch 17, batch 356/481,disc_loss 77.776, (real 79.751, fake 75.801 ) gen_loss 818.98\n",
            "iteration 8253, epoch 17, batch 357/481,disc_loss 80.108, (real 82.742, fake 77.473 ) gen_loss 875.91\n",
            "iteration 8254, epoch 17, batch 358/481,disc_loss 85.23, (real 88.068, fake 82.392 ) gen_loss 817.65\n",
            "iteration 8255, epoch 17, batch 359/481,disc_loss 76.515, (real 79.259, fake 73.771 ) gen_loss 917.83\n",
            "iteration 8256, epoch 17, batch 360/481,disc_loss 73.997, (real 77.105, fake 70.889 ) gen_loss 853.67\n",
            "iteration 8257, epoch 17, batch 361/481,disc_loss 73.635, (real 76.921, fake 70.35 ) gen_loss 965.03\n",
            "iteration 8258, epoch 17, batch 362/481,disc_loss 75.956, (real 77.782, fake 74.129 ) gen_loss 795.9\n",
            "iteration 8259, epoch 17, batch 363/481,disc_loss 72.408, (real 74.53, fake 70.287 ) gen_loss 778.5\n",
            "iteration 8260, epoch 17, batch 364/481,disc_loss 74.893, (real 76.488, fake 73.298 ) gen_loss 825.14\n",
            "iteration 8261, epoch 17, batch 365/481,disc_loss 77.518, (real 80.744, fake 74.293 ) gen_loss 867.03\n",
            "iteration 8262, epoch 17, batch 366/481,disc_loss 77.628, (real 80.701, fake 74.555 ) gen_loss 893.35\n",
            "iteration 8263, epoch 17, batch 367/481,disc_loss 77.771, (real 80.91, fake 74.631 ) gen_loss 865.6\n",
            "iteration 8264, epoch 17, batch 368/481,disc_loss 79.966, (real 81.999, fake 77.934 ) gen_loss 814.6\n",
            "iteration 8265, epoch 17, batch 369/481,disc_loss 75.717, (real 77.926, fake 73.509 ) gen_loss 815.07\n",
            "iteration 8266, epoch 17, batch 370/481,disc_loss 77.185, (real 79.491, fake 74.88 ) gen_loss 785.69\n",
            "iteration 8267, epoch 17, batch 371/481,disc_loss 75.691, (real 77.823, fake 73.559 ) gen_loss 743.05\n",
            "iteration 8268, epoch 17, batch 372/481,disc_loss 79.232, (real 81.582, fake 76.881 ) gen_loss 795.92\n",
            "iteration 8269, epoch 17, batch 373/481,disc_loss 80.994, (real 84.453, fake 77.536 ) gen_loss 897.76\n",
            "iteration 8270, epoch 17, batch 374/481,disc_loss 80.126, (real 82.511, fake 77.742 ) gen_loss 882.39\n",
            "iteration 8271, epoch 17, batch 375/481,disc_loss 75.59, (real 79.232, fake 71.949 ) gen_loss 783.0\n",
            "iteration 8272, epoch 17, batch 376/481,disc_loss 76.863, (real 78.522, fake 75.203 ) gen_loss 929.43\n",
            "iteration 8273, epoch 17, batch 377/481,disc_loss 77.912, (real 80.388, fake 75.435 ) gen_loss 957.57\n",
            "iteration 8274, epoch 17, batch 378/481,disc_loss 73.902, (real 76.776, fake 71.028 ) gen_loss 863.53\n",
            "iteration 8275, epoch 17, batch 379/481,disc_loss 77.348, (real 79.988, fake 74.708 ) gen_loss 823.29\n",
            "iteration 8276, epoch 17, batch 380/481,disc_loss 78.807, (real 81.421, fake 76.193 ) gen_loss 741.48\n",
            "iteration 8277, epoch 17, batch 381/481,disc_loss 79.954, (real 82.137, fake 77.771 ) gen_loss 832.16\n",
            "iteration 8278, epoch 17, batch 382/481,disc_loss 75.669, (real 78.126, fake 73.213 ) gen_loss 832.86\n",
            "iteration 8279, epoch 17, batch 383/481,disc_loss 81.595, (real 84.81, fake 78.38 ) gen_loss 812.89\n",
            "iteration 8280, epoch 17, batch 384/481,disc_loss 79.104, (real 81.95, fake 76.258 ) gen_loss 829.76\n",
            "iteration 8281, epoch 17, batch 385/481,disc_loss 72.148, (real 74.745, fake 69.551 ) gen_loss 831.94\n",
            "iteration 8282, epoch 17, batch 386/481,disc_loss 76.803, (real 78.592, fake 75.014 ) gen_loss 727.75\n",
            "iteration 8283, epoch 17, batch 387/481,disc_loss 77.517, (real 79.865, fake 75.168 ) gen_loss 748.39\n",
            "iteration 8284, epoch 17, batch 388/481,disc_loss 70.774, (real 73.719, fake 67.829 ) gen_loss 814.53\n",
            "iteration 8285, epoch 17, batch 389/481,disc_loss 77.025, (real 79.471, fake 74.579 ) gen_loss 790.14\n",
            "iteration 8286, epoch 17, batch 390/481,disc_loss 75.815, (real 78.861, fake 72.768 ) gen_loss 776.96\n",
            "iteration 8287, epoch 17, batch 391/481,disc_loss 79.444, (real 81.812, fake 77.076 ) gen_loss 846.31\n",
            "iteration 8288, epoch 17, batch 392/481,disc_loss 74.414, (real 77.156, fake 71.672 ) gen_loss 857.4\n",
            "iteration 8289, epoch 17, batch 393/481,disc_loss 79.766, (real 81.819, fake 77.713 ) gen_loss 805.62\n",
            "iteration 8290, epoch 17, batch 394/481,disc_loss 80.81, (real 83.543, fake 78.076 ) gen_loss 788.35\n",
            "iteration 8291, epoch 17, batch 395/481,disc_loss 79.005, (real 81.572, fake 76.438 ) gen_loss 790.29\n",
            "iteration 8292, epoch 17, batch 396/481,disc_loss 80.587, (real 84.106, fake 77.067 ) gen_loss 930.53\n",
            "iteration 8293, epoch 17, batch 397/481,disc_loss 79.502, (real 82.271, fake 76.733 ) gen_loss 800.96\n",
            "iteration 8294, epoch 17, batch 398/481,disc_loss 81.419, (real 84.569, fake 78.27 ) gen_loss 791.78\n",
            "iteration 8295, epoch 17, batch 399/481,disc_loss 76.764, (real 79.105, fake 74.423 ) gen_loss 876.32\n",
            "iteration 8296, epoch 17, batch 400/481,disc_loss 77.392, (real 80.09, fake 74.694 ) gen_loss 787.47\n",
            "iteration 8297, epoch 17, batch 401/481,disc_loss 75.73, (real 78.443, fake 73.018 ) gen_loss 848.81\n",
            "iteration 8298, epoch 17, batch 402/481,disc_loss 77.133, (real 79.487, fake 74.779 ) gen_loss 829.94\n",
            "iteration 8299, epoch 17, batch 403/481,disc_loss 80.109, (real 81.787, fake 78.431 ) gen_loss 782.05\n",
            "iteration 8300, epoch 17, batch 404/481,disc_loss 75.257, (real 78.073, fake 72.442 ) gen_loss 810.8\n",
            "iteration 8301, epoch 17, batch 405/481,disc_loss 74.189, (real 77.141, fake 71.236 ) gen_loss 787.07\n",
            "iteration 8302, epoch 17, batch 406/481,disc_loss 71.753, (real 74.438, fake 69.068 ) gen_loss 816.11\n",
            "iteration 8303, epoch 17, batch 407/481,disc_loss 76.609, (real 79.133, fake 74.085 ) gen_loss 881.05\n",
            "iteration 8304, epoch 17, batch 408/481,disc_loss 76.444, (real 79.157, fake 73.73 ) gen_loss 914.34\n",
            "iteration 8305, epoch 17, batch 409/481,disc_loss 76.372, (real 78.976, fake 73.769 ) gen_loss 837.11\n",
            "iteration 8306, epoch 17, batch 410/481,disc_loss 75.467, (real 77.563, fake 73.37 ) gen_loss 846.32\n",
            "iteration 8307, epoch 17, batch 411/481,disc_loss 74.135, (real 76.719, fake 71.551 ) gen_loss 809.94\n",
            "iteration 8308, epoch 17, batch 412/481,disc_loss 78.318, (real 81.468, fake 75.168 ) gen_loss 783.04\n",
            "iteration 8309, epoch 17, batch 413/481,disc_loss 70.79, (real 73.476, fake 68.103 ) gen_loss 775.37\n",
            "iteration 8310, epoch 17, batch 414/481,disc_loss 78.641, (real 81.498, fake 75.783 ) gen_loss 849.4\n",
            "iteration 8311, epoch 17, batch 415/481,disc_loss 76.356, (real 78.776, fake 73.936 ) gen_loss 839.59\n",
            "iteration 8312, epoch 17, batch 416/481,disc_loss 72.747, (real 75.117, fake 70.378 ) gen_loss 855.84\n",
            "iteration 8313, epoch 17, batch 417/481,disc_loss 77.744, (real 80.945, fake 74.543 ) gen_loss 908.51\n",
            "iteration 8314, epoch 17, batch 418/481,disc_loss 75.236, (real 78.23, fake 72.242 ) gen_loss 922.18\n",
            "iteration 8315, epoch 17, batch 419/481,disc_loss 74.882, (real 76.997, fake 72.767 ) gen_loss 829.92\n",
            "iteration 8316, epoch 17, batch 420/481,disc_loss 74.929, (real 77.531, fake 72.326 ) gen_loss 757.6\n",
            "iteration 8317, epoch 17, batch 421/481,disc_loss 73.811, (real 75.593, fake 72.029 ) gen_loss 817.42\n",
            "iteration 8318, epoch 17, batch 422/481,disc_loss 80.892, (real 84.168, fake 77.616 ) gen_loss 793.19\n",
            "iteration 8319, epoch 17, batch 423/481,disc_loss 80.597, (real 83.029, fake 78.165 ) gen_loss 814.89\n",
            "iteration 8320, epoch 17, batch 424/481,disc_loss 74.638, (real 77.288, fake 71.988 ) gen_loss 844.14\n",
            "iteration 8321, epoch 17, batch 425/481,disc_loss 75.417, (real 78.614, fake 72.221 ) gen_loss 913.92\n",
            "iteration 8322, epoch 17, batch 426/481,disc_loss 78.662, (real 81.39, fake 75.934 ) gen_loss 798.35\n",
            "iteration 8323, epoch 17, batch 427/481,disc_loss 74.66, (real 77.636, fake 71.684 ) gen_loss 903.28\n",
            "iteration 8324, epoch 17, batch 428/481,disc_loss 76.167, (real 79.006, fake 73.327 ) gen_loss 865.0\n",
            "iteration 8325, epoch 17, batch 429/481,disc_loss 75.657, (real 79.102, fake 72.212 ) gen_loss 822.76\n",
            "iteration 8326, epoch 17, batch 430/481,disc_loss 77.01, (real 79.377, fake 74.644 ) gen_loss 748.91\n",
            "iteration 8327, epoch 17, batch 431/481,disc_loss 76.171, (real 79.81, fake 72.531 ) gen_loss 948.19\n",
            "iteration 8328, epoch 17, batch 432/481,disc_loss 74.196, (real 76.374, fake 72.018 ) gen_loss 770.1\n",
            "iteration 8329, epoch 17, batch 433/481,disc_loss 76.942, (real 79.924, fake 73.961 ) gen_loss 821.36\n",
            "iteration 8330, epoch 17, batch 434/481,disc_loss 76.717, (real 79.006, fake 74.428 ) gen_loss 740.39\n",
            "iteration 8331, epoch 17, batch 435/481,disc_loss 72.931, (real 75.894, fake 69.968 ) gen_loss 794.71\n",
            "iteration 8332, epoch 17, batch 436/481,disc_loss 74.72, (real 77.378, fake 72.062 ) gen_loss 854.91\n",
            "iteration 8333, epoch 17, batch 437/481,disc_loss 75.304, (real 77.932, fake 72.676 ) gen_loss 775.05\n",
            "iteration 8334, epoch 17, batch 438/481,disc_loss 80.456, (real 83.441, fake 77.471 ) gen_loss 878.34\n",
            "iteration 8335, epoch 17, batch 439/481,disc_loss 80.714, (real 83.811, fake 77.617 ) gen_loss 798.87\n",
            "iteration 8336, epoch 17, batch 440/481,disc_loss 75.584, (real 77.816, fake 73.351 ) gen_loss 950.0\n",
            "iteration 8337, epoch 17, batch 441/481,disc_loss 74.008, (real 76.84, fake 71.176 ) gen_loss 982.61\n",
            "iteration 8338, epoch 17, batch 442/481,disc_loss 74.502, (real 76.591, fake 72.414 ) gen_loss 897.31\n",
            "iteration 8339, epoch 17, batch 443/481,disc_loss 80.261, (real 83.317, fake 77.205 ) gen_loss 857.06\n",
            "iteration 8340, epoch 17, batch 444/481,disc_loss 78.516, (real 81.646, fake 75.386 ) gen_loss 778.75\n",
            "iteration 8341, epoch 17, batch 445/481,disc_loss 85.384, (real 88.854, fake 81.915 ) gen_loss 824.73\n",
            "iteration 8342, epoch 17, batch 446/481,disc_loss 77.046, (real 80.138, fake 73.955 ) gen_loss 777.05\n",
            "iteration 8343, epoch 17, batch 447/481,disc_loss 78.115, (real 80.626, fake 75.604 ) gen_loss 811.74\n",
            "iteration 8344, epoch 17, batch 448/481,disc_loss 75.272, (real 77.945, fake 72.599 ) gen_loss 782.9\n",
            "iteration 8345, epoch 17, batch 449/481,disc_loss 80.037, (real 82.315, fake 77.76 ) gen_loss 841.4\n",
            "iteration 8346, epoch 17, batch 450/481,disc_loss 81.19, (real 82.979, fake 79.401 ) gen_loss 775.68\n",
            "iteration 8347, epoch 17, batch 451/481,disc_loss 77.25, (real 80.085, fake 74.415 ) gen_loss 886.09\n",
            "iteration 8348, epoch 17, batch 452/481,disc_loss 74.368, (real 77.322, fake 71.413 ) gen_loss 767.31\n",
            "iteration 8349, epoch 17, batch 453/481,disc_loss 77.176, (real 79.817, fake 74.535 ) gen_loss 786.04\n",
            "iteration 8350, epoch 17, batch 454/481,disc_loss 76.702, (real 79.539, fake 73.865 ) gen_loss 727.67\n",
            "iteration 8351, epoch 17, batch 455/481,disc_loss 72.916, (real 76.016, fake 69.816 ) gen_loss 732.53\n",
            "iteration 8352, epoch 17, batch 456/481,disc_loss 76.507, (real 79.652, fake 73.362 ) gen_loss 808.08\n",
            "iteration 8353, epoch 17, batch 457/481,disc_loss 76.369, (real 78.298, fake 74.44 ) gen_loss 698.66\n",
            "iteration 8354, epoch 17, batch 458/481,disc_loss 75.922, (real 78.386, fake 73.459 ) gen_loss 804.38\n",
            "iteration 8355, epoch 17, batch 459/481,disc_loss 75.859, (real 79.245, fake 72.472 ) gen_loss 800.48\n",
            "iteration 8356, epoch 17, batch 460/481,disc_loss 76.837, (real 79.331, fake 74.343 ) gen_loss 849.82\n",
            "iteration 8357, epoch 17, batch 461/481,disc_loss 76.526, (real 79.606, fake 73.446 ) gen_loss 749.38\n",
            "iteration 8358, epoch 17, batch 462/481,disc_loss 78.369, (real 80.982, fake 75.756 ) gen_loss 788.73\n",
            "iteration 8359, epoch 17, batch 463/481,disc_loss 81.831, (real 84.072, fake 79.59 ) gen_loss 853.23\n",
            "iteration 8360, epoch 17, batch 464/481,disc_loss 78.875, (real 82.373, fake 75.377 ) gen_loss 819.01\n",
            "iteration 8361, epoch 17, batch 465/481,disc_loss 78.352, (real 80.686, fake 76.017 ) gen_loss 856.73\n",
            "iteration 8362, epoch 17, batch 466/481,disc_loss 75.041, (real 77.808, fake 72.273 ) gen_loss 918.13\n",
            "iteration 8363, epoch 17, batch 467/481,disc_loss 76.84, (real 79.287, fake 74.394 ) gen_loss 923.54\n",
            "iteration 8364, epoch 17, batch 468/481,disc_loss 74.08, (real 76.598, fake 71.562 ) gen_loss 922.18\n",
            "iteration 8365, epoch 17, batch 469/481,disc_loss 75.781, (real 79.03, fake 72.533 ) gen_loss 876.1\n",
            "iteration 8366, epoch 17, batch 470/481,disc_loss 75.729, (real 78.026, fake 73.431 ) gen_loss 836.98\n",
            "iteration 8367, epoch 17, batch 471/481,disc_loss 74.026, (real 75.972, fake 72.08 ) gen_loss 840.58\n",
            "iteration 8368, epoch 17, batch 472/481,disc_loss 80.111, (real 81.969, fake 78.254 ) gen_loss 813.62\n",
            "iteration 8369, epoch 17, batch 473/481,disc_loss 76.778, (real 79.505, fake 74.05 ) gen_loss 793.77\n",
            "iteration 8370, epoch 17, batch 474/481,disc_loss 80.662, (real 83.027, fake 78.298 ) gen_loss 804.6\n",
            "iteration 8371, epoch 17, batch 475/481,disc_loss 79.351, (real 81.649, fake 77.052 ) gen_loss 935.61\n",
            "iteration 8372, epoch 17, batch 476/481,disc_loss 78.517, (real 81.389, fake 75.645 ) gen_loss 876.17\n",
            "iteration 8373, epoch 17, batch 477/481,disc_loss 79.833, (real 82.428, fake 77.237 ) gen_loss 803.16\n",
            "iteration 8374, epoch 17, batch 478/481,disc_loss 77.442, (real 80.052, fake 74.832 ) gen_loss 880.31\n",
            "iteration 8375, epoch 17, batch 479/481,disc_loss 79.02, (real 81.571, fake 76.47 ) gen_loss 805.71\n",
            "iteration 8376, epoch 17, batch 480/481,disc_loss 77.496, (real 78.883, fake 76.11 ) gen_loss 830.68\n",
            "iteration 8377, epoch 17, batch 481/481,disc_loss 78.208, (real 80.968, fake 75.449 ) gen_loss 765.31\n",
            "iteration 8378, epoch 18, batch 1/481,disc_loss 83.019, (real 85.485, fake 80.554 ) gen_loss 782.26\n",
            "iteration 8379, epoch 18, batch 2/481,disc_loss 75.109, (real 77.298, fake 72.92 ) gen_loss 814.46\n",
            "iteration 8380, epoch 18, batch 3/481,disc_loss 79.502, (real 81.22, fake 77.784 ) gen_loss 840.1\n",
            "iteration 8381, epoch 18, batch 4/481,disc_loss 77.496, (real 80.237, fake 74.755 ) gen_loss 829.98\n",
            "iteration 8382, epoch 18, batch 5/481,disc_loss 76.775, (real 78.717, fake 74.833 ) gen_loss 819.68\n",
            "iteration 8383, epoch 18, batch 6/481,disc_loss 78.823, (real 81.619, fake 76.026 ) gen_loss 873.48\n",
            "iteration 8384, epoch 18, batch 7/481,disc_loss 73.763, (real 76.181, fake 71.345 ) gen_loss 803.11\n",
            "iteration 8385, epoch 18, batch 8/481,disc_loss 86.569, (real 89.07, fake 84.068 ) gen_loss 835.55\n",
            "iteration 8386, epoch 18, batch 9/481,disc_loss 76.265, (real 78.044, fake 74.486 ) gen_loss 858.51\n",
            "iteration 8387, epoch 18, batch 10/481,disc_loss 76.06, (real 77.717, fake 74.402 ) gen_loss 734.34\n",
            "iteration 8388, epoch 18, batch 11/481,disc_loss 73.503, (real 75.322, fake 71.684 ) gen_loss 695.87\n",
            "iteration 8389, epoch 18, batch 12/481,disc_loss 75.029, (real 77.297, fake 72.761 ) gen_loss 796.43\n",
            "iteration 8390, epoch 18, batch 13/481,disc_loss 75.736, (real 77.657, fake 73.815 ) gen_loss 758.68\n",
            "iteration 8391, epoch 18, batch 14/481,disc_loss 78.53, (real 80.88, fake 76.181 ) gen_loss 800.44\n",
            "iteration 8392, epoch 18, batch 15/481,disc_loss 75.712, (real 77.697, fake 73.728 ) gen_loss 842.98\n",
            "iteration 8393, epoch 18, batch 16/481,disc_loss 75.829, (real 78.13, fake 73.529 ) gen_loss 739.37\n",
            "iteration 8394, epoch 18, batch 17/481,disc_loss 77.4, (real 80.466, fake 74.333 ) gen_loss 795.92\n",
            "iteration 8395, epoch 18, batch 18/481,disc_loss 77.938, (real 80.275, fake 75.6 ) gen_loss 830.0\n",
            "iteration 8396, epoch 18, batch 19/481,disc_loss 75.256, (real 77.701, fake 72.812 ) gen_loss 917.79\n",
            "iteration 8397, epoch 18, batch 20/481,disc_loss 80.09, (real 82.606, fake 77.575 ) gen_loss 849.76\n",
            "iteration 8398, epoch 18, batch 21/481,disc_loss 72.002, (real 73.879, fake 70.126 ) gen_loss 792.57\n",
            "iteration 8399, epoch 18, batch 22/481,disc_loss 72.161, (real 73.593, fake 70.728 ) gen_loss 856.42\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 8400, epoch 18, batch 23/481,disc_loss 77.042, (real 79.444, fake 74.64 ) gen_loss 874.25\n",
            "iteration 8401, epoch 18, batch 24/481,disc_loss 72.336, (real 73.951, fake 70.721 ) gen_loss 803.0\n",
            "iteration 8402, epoch 18, batch 25/481,disc_loss 82.772, (real 85.324, fake 80.22 ) gen_loss 834.42\n",
            "iteration 8403, epoch 18, batch 26/481,disc_loss 80.549, (real 83.093, fake 78.005 ) gen_loss 817.8\n",
            "iteration 8404, epoch 18, batch 27/481,disc_loss 78.916, (real 81.534, fake 76.298 ) gen_loss 735.31\n",
            "iteration 8405, epoch 18, batch 28/481,disc_loss 77.229, (real 79.657, fake 74.802 ) gen_loss 866.57\n",
            "iteration 8406, epoch 18, batch 29/481,disc_loss 73.535, (real 75.784, fake 71.286 ) gen_loss 922.31\n",
            "iteration 8407, epoch 18, batch 30/481,disc_loss 77.037, (real 79.437, fake 74.637 ) gen_loss 801.71\n",
            "iteration 8408, epoch 18, batch 31/481,disc_loss 79.19, (real 81.65, fake 76.729 ) gen_loss 833.94\n",
            "iteration 8409, epoch 18, batch 32/481,disc_loss 75.52, (real 77.486, fake 73.555 ) gen_loss 933.46\n",
            "iteration 8410, epoch 18, batch 33/481,disc_loss 78.69, (real 81.424, fake 75.956 ) gen_loss 828.93\n",
            "iteration 8411, epoch 18, batch 34/481,disc_loss 79.146, (real 81.368, fake 76.924 ) gen_loss 807.8\n",
            "iteration 8412, epoch 18, batch 35/481,disc_loss 77.084, (real 79.519, fake 74.648 ) gen_loss 791.92\n",
            "iteration 8413, epoch 18, batch 36/481,disc_loss 78.684, (real 81.973, fake 75.396 ) gen_loss 835.86\n",
            "iteration 8414, epoch 18, batch 37/481,disc_loss 78.105, (real 80.55, fake 75.66 ) gen_loss 814.23\n",
            "iteration 8415, epoch 18, batch 38/481,disc_loss 76.109, (real 78.68, fake 73.537 ) gen_loss 870.79\n",
            "iteration 8416, epoch 18, batch 39/481,disc_loss 81.024, (real 82.945, fake 79.103 ) gen_loss 833.54\n",
            "iteration 8417, epoch 18, batch 40/481,disc_loss 79.487, (real 81.959, fake 77.014 ) gen_loss 821.63\n",
            "iteration 8418, epoch 18, batch 41/481,disc_loss 74.174, (real 76.654, fake 71.695 ) gen_loss 880.11\n",
            "iteration 8419, epoch 18, batch 42/481,disc_loss 80.966, (real 83.479, fake 78.453 ) gen_loss 770.73\n",
            "iteration 8420, epoch 18, batch 43/481,disc_loss 79.369, (real 82.337, fake 76.402 ) gen_loss 795.82\n",
            "iteration 8421, epoch 18, batch 44/481,disc_loss 75.376, (real 77.756, fake 72.996 ) gen_loss 763.71\n",
            "iteration 8422, epoch 18, batch 45/481,disc_loss 76.103, (real 79.124, fake 73.081 ) gen_loss 730.58\n",
            "iteration 8423, epoch 18, batch 46/481,disc_loss 73.688, (real 76.047, fake 71.329 ) gen_loss 902.7\n",
            "iteration 8424, epoch 18, batch 47/481,disc_loss 79.417, (real 81.577, fake 77.256 ) gen_loss 816.8\n",
            "iteration 8425, epoch 18, batch 48/481,disc_loss 76.459, (real 78.086, fake 74.832 ) gen_loss 823.02\n",
            "iteration 8426, epoch 18, batch 49/481,disc_loss 78.683, (real 80.643, fake 76.723 ) gen_loss 843.95\n",
            "iteration 8427, epoch 18, batch 50/481,disc_loss 74.399, (real 77.391, fake 71.407 ) gen_loss 853.39\n",
            "iteration 8428, epoch 18, batch 51/481,disc_loss 80.093, (real 81.838, fake 78.348 ) gen_loss 965.02\n",
            "iteration 8429, epoch 18, batch 52/481,disc_loss 77.571, (real 80.035, fake 75.106 ) gen_loss 833.68\n",
            "iteration 8430, epoch 18, batch 53/481,disc_loss 77.237, (real 79.481, fake 74.992 ) gen_loss 778.86\n",
            "iteration 8431, epoch 18, batch 54/481,disc_loss 75.448, (real 78.366, fake 72.531 ) gen_loss 780.43\n",
            "iteration 8432, epoch 18, batch 55/481,disc_loss 78.197, (real 81.007, fake 75.388 ) gen_loss 739.98\n",
            "iteration 8433, epoch 18, batch 56/481,disc_loss 78.113, (real 79.895, fake 76.331 ) gen_loss 835.25\n",
            "iteration 8434, epoch 18, batch 57/481,disc_loss 73.279, (real 74.963, fake 71.595 ) gen_loss 831.82\n",
            "iteration 8435, epoch 18, batch 58/481,disc_loss 77.583, (real 79.72, fake 75.446 ) gen_loss 876.09\n",
            "iteration 8436, epoch 18, batch 59/481,disc_loss 76.819, (real 79.821, fake 73.818 ) gen_loss 923.95\n",
            "iteration 8437, epoch 18, batch 60/481,disc_loss 76.399, (real 78.438, fake 74.361 ) gen_loss 861.01\n",
            "iteration 8438, epoch 18, batch 61/481,disc_loss 74.235, (real 76.639, fake 71.831 ) gen_loss 799.54\n",
            "iteration 8439, epoch 18, batch 62/481,disc_loss 76.849, (real 78.492, fake 75.206 ) gen_loss 779.05\n",
            "iteration 8440, epoch 18, batch 63/481,disc_loss 76.81, (real 79.426, fake 74.195 ) gen_loss 908.99\n",
            "iteration 8441, epoch 18, batch 64/481,disc_loss 75.729, (real 77.375, fake 74.082 ) gen_loss 782.15\n",
            "iteration 8442, epoch 18, batch 65/481,disc_loss 73.516, (real 76.153, fake 70.879 ) gen_loss 882.07\n",
            "iteration 8443, epoch 18, batch 66/481,disc_loss 79.803, (real 82.262, fake 77.343 ) gen_loss 811.31\n",
            "iteration 8444, epoch 18, batch 67/481,disc_loss 77.45, (real 80.005, fake 74.895 ) gen_loss 811.78\n",
            "iteration 8445, epoch 18, batch 68/481,disc_loss 79.608, (real 81.253, fake 77.962 ) gen_loss 822.46\n",
            "iteration 8446, epoch 18, batch 69/481,disc_loss 75.336, (real 78.206, fake 72.467 ) gen_loss 842.46\n",
            "iteration 8447, epoch 18, batch 70/481,disc_loss 74.496, (real 77.104, fake 71.889 ) gen_loss 798.03\n",
            "iteration 8448, epoch 18, batch 71/481,disc_loss 71.43, (real 73.364, fake 69.497 ) gen_loss 781.93\n",
            "iteration 8449, epoch 18, batch 72/481,disc_loss 72.671, (real 75.767, fake 69.575 ) gen_loss 887.62\n",
            "iteration 8450, epoch 18, batch 73/481,disc_loss 76.689, (real 78.906, fake 74.472 ) gen_loss 828.22\n",
            "iteration 8451, epoch 18, batch 74/481,disc_loss 76.872, (real 79.335, fake 74.409 ) gen_loss 846.61\n",
            "iteration 8452, epoch 18, batch 75/481,disc_loss 78.814, (real 80.857, fake 76.77 ) gen_loss 902.78\n",
            "iteration 8453, epoch 18, batch 76/481,disc_loss 76.894, (real 78.804, fake 74.984 ) gen_loss 908.3\n",
            "iteration 8454, epoch 18, batch 77/481,disc_loss 81.003, (real 83.243, fake 78.763 ) gen_loss 811.33\n",
            "iteration 8455, epoch 18, batch 78/481,disc_loss 77.731, (real 80.417, fake 75.046 ) gen_loss 841.44\n",
            "iteration 8456, epoch 18, batch 79/481,disc_loss 78.822, (real 80.939, fake 76.705 ) gen_loss 751.92\n",
            "iteration 8457, epoch 18, batch 80/481,disc_loss 76.549, (real 78.357, fake 74.74 ) gen_loss 796.92\n",
            "iteration 8458, epoch 18, batch 81/481,disc_loss 75.198, (real 77.641, fake 72.756 ) gen_loss 774.47\n",
            "iteration 8459, epoch 18, batch 82/481,disc_loss 73.54, (real 75.906, fake 71.174 ) gen_loss 763.02\n",
            "iteration 8460, epoch 18, batch 83/481,disc_loss 74.238, (real 76.822, fake 71.655 ) gen_loss 768.67\n",
            "iteration 8461, epoch 18, batch 84/481,disc_loss 73.487, (real 76.352, fake 70.622 ) gen_loss 843.97\n",
            "iteration 8462, epoch 18, batch 85/481,disc_loss 81.19, (real 83.686, fake 78.694 ) gen_loss 838.44\n",
            "iteration 8463, epoch 18, batch 86/481,disc_loss 81.698, (real 84.18, fake 79.215 ) gen_loss 833.24\n",
            "iteration 8464, epoch 18, batch 87/481,disc_loss 78.024, (real 80.231, fake 75.817 ) gen_loss 787.37\n",
            "iteration 8465, epoch 18, batch 88/481,disc_loss 79.303, (real 82.305, fake 76.3 ) gen_loss 948.33\n",
            "iteration 8466, epoch 18, batch 89/481,disc_loss 82.177, (real 85.311, fake 79.043 ) gen_loss 828.52\n",
            "iteration 8467, epoch 18, batch 90/481,disc_loss 74.317, (real 76.719, fake 71.915 ) gen_loss 837.18\n",
            "iteration 8468, epoch 18, batch 91/481,disc_loss 81.109, (real 84.535, fake 77.684 ) gen_loss 797.33\n",
            "iteration 8469, epoch 18, batch 92/481,disc_loss 76.869, (real 79.191, fake 74.547 ) gen_loss 835.55\n",
            "iteration 8470, epoch 18, batch 93/481,disc_loss 76.812, (real 79.348, fake 74.276 ) gen_loss 882.3\n",
            "iteration 8471, epoch 18, batch 94/481,disc_loss 75.829, (real 78.34, fake 73.317 ) gen_loss 875.07\n",
            "iteration 8472, epoch 18, batch 95/481,disc_loss 82.391, (real 85.115, fake 79.668 ) gen_loss 842.4\n",
            "iteration 8473, epoch 18, batch 96/481,disc_loss 75.814, (real 78.202, fake 73.425 ) gen_loss 864.21\n",
            "iteration 8474, epoch 18, batch 97/481,disc_loss 78.985, (real 80.915, fake 77.055 ) gen_loss 816.98\n",
            "iteration 8475, epoch 18, batch 98/481,disc_loss 74.263, (real 75.968, fake 72.557 ) gen_loss 836.85\n",
            "iteration 8476, epoch 18, batch 99/481,disc_loss 74.485, (real 76.422, fake 72.549 ) gen_loss 804.06\n",
            "iteration 8477, epoch 18, batch 100/481,disc_loss 78.811, (real 81.386, fake 76.236 ) gen_loss 818.59\n",
            "iteration 8478, epoch 18, batch 101/481,disc_loss 83.362, (real 85.966, fake 80.758 ) gen_loss 772.95\n",
            "iteration 8479, epoch 18, batch 102/481,disc_loss 79.078, (real 83.594, fake 74.563 ) gen_loss 940.54\n",
            "iteration 8480, epoch 18, batch 103/481,disc_loss 73.234, (real 75.715, fake 70.754 ) gen_loss 809.02\n",
            "iteration 8481, epoch 18, batch 104/481,disc_loss 80.167, (real 82.925, fake 77.408 ) gen_loss 794.61\n",
            "iteration 8482, epoch 18, batch 105/481,disc_loss 77.136, (real 80.511, fake 73.76 ) gen_loss 900.47\n",
            "iteration 8483, epoch 18, batch 106/481,disc_loss 79.415, (real 82.032, fake 76.799 ) gen_loss 880.86\n",
            "iteration 8484, epoch 18, batch 107/481,disc_loss 77.154, (real 79.723, fake 74.585 ) gen_loss 906.36\n",
            "iteration 8485, epoch 18, batch 108/481,disc_loss 75.749, (real 77.896, fake 73.602 ) gen_loss 810.8\n",
            "iteration 8486, epoch 18, batch 109/481,disc_loss 77.699, (real 79.76, fake 75.638 ) gen_loss 779.72\n",
            "iteration 8487, epoch 18, batch 110/481,disc_loss 78.269, (real 79.976, fake 76.561 ) gen_loss 804.04\n",
            "iteration 8488, epoch 18, batch 111/481,disc_loss 82.527, (real 84.733, fake 80.32 ) gen_loss 852.39\n",
            "iteration 8489, epoch 18, batch 112/481,disc_loss 76.609, (real 78.773, fake 74.446 ) gen_loss 833.22\n",
            "iteration 8490, epoch 18, batch 113/481,disc_loss 76.125, (real 79.135, fake 73.115 ) gen_loss 852.41\n",
            "iteration 8491, epoch 18, batch 114/481,disc_loss 75.725, (real 77.984, fake 73.466 ) gen_loss 792.14\n",
            "iteration 8492, epoch 18, batch 115/481,disc_loss 77.037, (real 79.724, fake 74.349 ) gen_loss 866.7\n",
            "iteration 8493, epoch 18, batch 116/481,disc_loss 82.987, (real 85.444, fake 80.53 ) gen_loss 791.54\n",
            "iteration 8494, epoch 18, batch 117/481,disc_loss 75.863, (real 78.233, fake 73.492 ) gen_loss 863.76\n",
            "iteration 8495, epoch 18, batch 118/481,disc_loss 79.364, (real 81.676, fake 77.052 ) gen_loss 843.1\n",
            "iteration 8496, epoch 18, batch 119/481,disc_loss 76.551, (real 78.417, fake 74.684 ) gen_loss 936.03\n",
            "iteration 8497, epoch 18, batch 120/481,disc_loss 80.498, (real 83.19, fake 77.806 ) gen_loss 827.47\n",
            "iteration 8498, epoch 18, batch 121/481,disc_loss 73.117, (real 76.429, fake 69.806 ) gen_loss 778.29\n",
            "iteration 8499, epoch 18, batch 122/481,disc_loss 76.105, (real 78.135, fake 74.075 ) gen_loss 791.01\n",
            "iteration 8500, epoch 18, batch 123/481,disc_loss 78.926, (real 81.36, fake 76.491 ) gen_loss 776.93\n",
            "iteration 8501, epoch 18, batch 124/481,disc_loss 77.843, (real 81.062, fake 74.625 ) gen_loss 793.24\n",
            "iteration 8502, epoch 18, batch 125/481,disc_loss 78.257, (real 80.989, fake 75.524 ) gen_loss 810.08\n",
            "iteration 8503, epoch 18, batch 126/481,disc_loss 72.91, (real 75.968, fake 69.853 ) gen_loss 834.4\n",
            "iteration 8504, epoch 18, batch 127/481,disc_loss 78.511, (real 80.884, fake 76.139 ) gen_loss 783.93\n",
            "iteration 8505, epoch 18, batch 128/481,disc_loss 77.586, (real 80.111, fake 75.06 ) gen_loss 812.1\n",
            "iteration 8506, epoch 18, batch 129/481,disc_loss 77.394, (real 79.423, fake 75.366 ) gen_loss 902.72\n",
            "iteration 8507, epoch 18, batch 130/481,disc_loss 75.397, (real 77.536, fake 73.258 ) gen_loss 797.45\n",
            "iteration 8508, epoch 18, batch 131/481,disc_loss 77.901, (real 80.019, fake 75.784 ) gen_loss 842.57\n",
            "iteration 8509, epoch 18, batch 132/481,disc_loss 76.943, (real 79.153, fake 74.733 ) gen_loss 942.31\n",
            "iteration 8510, epoch 18, batch 133/481,disc_loss 75.84, (real 78.214, fake 73.467 ) gen_loss 803.03\n",
            "iteration 8511, epoch 18, batch 134/481,disc_loss 81.799, (real 84.66, fake 78.937 ) gen_loss 809.27\n",
            "iteration 8512, epoch 18, batch 135/481,disc_loss 78.859, (real 81.183, fake 76.535 ) gen_loss 812.03\n",
            "iteration 8513, epoch 18, batch 136/481,disc_loss 78.65, (real 81.828, fake 75.473 ) gen_loss 750.33\n",
            "iteration 8514, epoch 18, batch 137/481,disc_loss 76.866, (real 79.422, fake 74.311 ) gen_loss 850.64\n",
            "iteration 8515, epoch 18, batch 138/481,disc_loss 78.768, (real 81.597, fake 75.94 ) gen_loss 814.92\n",
            "iteration 8516, epoch 18, batch 139/481,disc_loss 76.684, (real 79.086, fake 74.282 ) gen_loss 1005.3\n",
            "iteration 8517, epoch 18, batch 140/481,disc_loss 80.978, (real 84.025, fake 77.931 ) gen_loss 839.34\n",
            "iteration 8518, epoch 18, batch 141/481,disc_loss 77.829, (real 79.856, fake 75.802 ) gen_loss 894.83\n",
            "iteration 8519, epoch 18, batch 142/481,disc_loss 80.039, (real 83.136, fake 76.942 ) gen_loss 823.13\n",
            "iteration 8520, epoch 18, batch 143/481,disc_loss 80.535, (real 83.49, fake 77.579 ) gen_loss 893.58\n",
            "iteration 8521, epoch 18, batch 144/481,disc_loss 71.124, (real 73.7, fake 68.547 ) gen_loss 867.4\n",
            "iteration 8522, epoch 18, batch 145/481,disc_loss 76.717, (real 79.419, fake 74.014 ) gen_loss 734.33\n",
            "iteration 8523, epoch 18, batch 146/481,disc_loss 77.638, (real 79.84, fake 75.437 ) gen_loss 822.06\n",
            "iteration 8524, epoch 18, batch 147/481,disc_loss 79.548, (real 81.376, fake 77.72 ) gen_loss 765.45\n",
            "iteration 8525, epoch 18, batch 148/481,disc_loss 76.266, (real 78.238, fake 74.293 ) gen_loss 766.14\n",
            "iteration 8526, epoch 18, batch 149/481,disc_loss 75.316, (real 77.704, fake 72.928 ) gen_loss 767.4\n",
            "iteration 8527, epoch 18, batch 150/481,disc_loss 78.338, (real 80.618, fake 76.059 ) gen_loss 925.04\n",
            "iteration 8528, epoch 18, batch 151/481,disc_loss 75.783, (real 78.111, fake 73.456 ) gen_loss 966.59\n",
            "iteration 8529, epoch 18, batch 152/481,disc_loss 77.721, (real 80.053, fake 75.388 ) gen_loss 899.61\n",
            "iteration 8530, epoch 18, batch 153/481,disc_loss 79.013, (real 81.208, fake 76.818 ) gen_loss 848.79\n",
            "iteration 8531, epoch 18, batch 154/481,disc_loss 74.482, (real 76.085, fake 72.88 ) gen_loss 802.2\n",
            "iteration 8532, epoch 18, batch 155/481,disc_loss 74.303, (real 76.477, fake 72.13 ) gen_loss 874.3\n",
            "iteration 8533, epoch 18, batch 156/481,disc_loss 76.523, (real 79.425, fake 73.621 ) gen_loss 782.01\n",
            "iteration 8534, epoch 18, batch 157/481,disc_loss 76.142, (real 78.835, fake 73.449 ) gen_loss 782.39\n",
            "iteration 8535, epoch 18, batch 158/481,disc_loss 72.569, (real 74.825, fake 70.313 ) gen_loss 892.53\n",
            "iteration 8536, epoch 18, batch 159/481,disc_loss 77.013, (real 79.672, fake 74.354 ) gen_loss 781.45\n",
            "iteration 8537, epoch 18, batch 160/481,disc_loss 76.446, (real 78.888, fake 74.004 ) gen_loss 858.83\n",
            "iteration 8538, epoch 18, batch 161/481,disc_loss 79.484, (real 82.179, fake 76.789 ) gen_loss 805.26\n",
            "iteration 8539, epoch 18, batch 162/481,disc_loss 75.517, (real 77.682, fake 73.353 ) gen_loss 866.06\n",
            "iteration 8540, epoch 18, batch 163/481,disc_loss 80.448, (real 82.647, fake 78.249 ) gen_loss 788.51\n",
            "iteration 8541, epoch 18, batch 164/481,disc_loss 74.901, (real 77.151, fake 72.651 ) gen_loss 846.59\n",
            "iteration 8542, epoch 18, batch 165/481,disc_loss 74.598, (real 76.925, fake 72.271 ) gen_loss 899.02\n",
            "iteration 8543, epoch 18, batch 166/481,disc_loss 75.038, (real 77.98, fake 72.097 ) gen_loss 913.48\n",
            "iteration 8544, epoch 18, batch 167/481,disc_loss 76.585, (real 78.817, fake 74.353 ) gen_loss 823.54\n",
            "iteration 8545, epoch 18, batch 168/481,disc_loss 77.847, (real 80.469, fake 75.225 ) gen_loss 811.9\n",
            "iteration 8546, epoch 18, batch 169/481,disc_loss 78.038, (real 80.68, fake 75.397 ) gen_loss 940.78\n",
            "iteration 8547, epoch 18, batch 170/481,disc_loss 78.055, (real 81.43, fake 74.681 ) gen_loss 778.45\n",
            "iteration 8548, epoch 18, batch 171/481,disc_loss 77.848, (real 81.079, fake 74.618 ) gen_loss 930.2\n",
            "iteration 8549, epoch 18, batch 172/481,disc_loss 71.213, (real 72.748, fake 69.678 ) gen_loss 888.51\n",
            "iteration 8550, epoch 18, batch 173/481,disc_loss 75.959, (real 77.842, fake 74.076 ) gen_loss 928.41\n",
            "iteration 8551, epoch 18, batch 174/481,disc_loss 77.247, (real 78.868, fake 75.626 ) gen_loss 825.61\n",
            "iteration 8552, epoch 18, batch 175/481,disc_loss 78.064, (real 80.728, fake 75.4 ) gen_loss 825.42\n",
            "iteration 8553, epoch 18, batch 176/481,disc_loss 74.352, (real 76.946, fake 71.757 ) gen_loss 963.03\n",
            "iteration 8554, epoch 18, batch 177/481,disc_loss 78.61, (real 81.06, fake 76.16 ) gen_loss 861.0\n",
            "iteration 8555, epoch 18, batch 178/481,disc_loss 76.684, (real 79.264, fake 74.104 ) gen_loss 870.82\n",
            "iteration 8556, epoch 18, batch 179/481,disc_loss 75.99, (real 78.241, fake 73.739 ) gen_loss 946.95\n",
            "iteration 8557, epoch 18, batch 180/481,disc_loss 74.663, (real 77.192, fake 72.134 ) gen_loss 880.94\n",
            "iteration 8558, epoch 18, batch 181/481,disc_loss 73.414, (real 75.787, fake 71.041 ) gen_loss 833.82\n",
            "iteration 8559, epoch 18, batch 182/481,disc_loss 79.372, (real 81.837, fake 76.908 ) gen_loss 825.2\n",
            "iteration 8560, epoch 18, batch 183/481,disc_loss 79.389, (real 81.845, fake 76.932 ) gen_loss 770.44\n",
            "iteration 8561, epoch 18, batch 184/481,disc_loss 77.002, (real 79.193, fake 74.811 ) gen_loss 778.43\n",
            "iteration 8562, epoch 18, batch 185/481,disc_loss 78.193, (real 81.362, fake 75.025 ) gen_loss 888.89\n",
            "iteration 8563, epoch 18, batch 186/481,disc_loss 83.199, (real 85.728, fake 80.671 ) gen_loss 791.93\n",
            "iteration 8564, epoch 18, batch 187/481,disc_loss 78.815, (real 81.117, fake 76.513 ) gen_loss 822.5\n",
            "iteration 8565, epoch 18, batch 188/481,disc_loss 76.932, (real 79.627, fake 74.237 ) gen_loss 817.62\n",
            "iteration 8566, epoch 18, batch 189/481,disc_loss 76.216, (real 78.233, fake 74.198 ) gen_loss 827.86\n",
            "iteration 8567, epoch 18, batch 190/481,disc_loss 75.198, (real 77.564, fake 72.833 ) gen_loss 820.68\n",
            "iteration 8568, epoch 18, batch 191/481,disc_loss 78.449, (real 80.85, fake 76.048 ) gen_loss 819.81\n",
            "iteration 8569, epoch 18, batch 192/481,disc_loss 79.715, (real 81.815, fake 77.614 ) gen_loss 872.23\n",
            "iteration 8570, epoch 18, batch 193/481,disc_loss 77.379, (real 79.172, fake 75.586 ) gen_loss 857.62\n",
            "iteration 8571, epoch 18, batch 194/481,disc_loss 76.652, (real 79.917, fake 73.387 ) gen_loss 836.93\n",
            "iteration 8572, epoch 18, batch 195/481,disc_loss 77.738, (real 79.948, fake 75.528 ) gen_loss 799.24\n",
            "iteration 8573, epoch 18, batch 196/481,disc_loss 78.82, (real 81.14, fake 76.5 ) gen_loss 763.64\n",
            "iteration 8574, epoch 18, batch 197/481,disc_loss 72.26, (real 74.871, fake 69.649 ) gen_loss 814.34\n",
            "iteration 8575, epoch 18, batch 198/481,disc_loss 76.823, (real 78.625, fake 75.021 ) gen_loss 893.32\n",
            "iteration 8576, epoch 18, batch 199/481,disc_loss 73.788, (real 75.994, fake 71.582 ) gen_loss 905.99\n",
            "iteration 8577, epoch 18, batch 200/481,disc_loss 81.326, (real 83.861, fake 78.791 ) gen_loss 939.28\n",
            "iteration 8578, epoch 18, batch 201/481,disc_loss 77.482, (real 80.053, fake 74.911 ) gen_loss 852.8\n",
            "iteration 8579, epoch 18, batch 202/481,disc_loss 79.079, (real 82.14, fake 76.017 ) gen_loss 831.05\n",
            "iteration 8580, epoch 18, batch 203/481,disc_loss 76.289, (real 77.913, fake 74.665 ) gen_loss 899.87\n",
            "iteration 8581, epoch 18, batch 204/481,disc_loss 77.358, (real 79.762, fake 74.953 ) gen_loss 818.87\n",
            "iteration 8582, epoch 18, batch 205/481,disc_loss 75.211, (real 77.176, fake 73.245 ) gen_loss 870.25\n",
            "iteration 8583, epoch 18, batch 206/481,disc_loss 76.091, (real 79.313, fake 72.869 ) gen_loss 980.58\n",
            "iteration 8584, epoch 18, batch 207/481,disc_loss 80.546, (real 83.812, fake 77.281 ) gen_loss 858.59\n",
            "iteration 8585, epoch 18, batch 208/481,disc_loss 74.125, (real 76.487, fake 71.763 ) gen_loss 812.15\n",
            "iteration 8586, epoch 18, batch 209/481,disc_loss 73.803, (real 75.337, fake 72.269 ) gen_loss 827.13\n",
            "iteration 8587, epoch 18, batch 210/481,disc_loss 74.174, (real 77.31, fake 71.038 ) gen_loss 856.12\n",
            "iteration 8588, epoch 18, batch 211/481,disc_loss 79.416, (real 81.208, fake 77.624 ) gen_loss 716.2\n",
            "iteration 8589, epoch 18, batch 212/481,disc_loss 77.265, (real 79.57, fake 74.961 ) gen_loss 781.13\n",
            "iteration 8590, epoch 18, batch 213/481,disc_loss 76.294, (real 78.317, fake 74.271 ) gen_loss 863.55\n",
            "iteration 8591, epoch 18, batch 214/481,disc_loss 78.19, (real 80.372, fake 76.007 ) gen_loss 802.79\n",
            "iteration 8592, epoch 18, batch 215/481,disc_loss 77.652, (real 79.73, fake 75.574 ) gen_loss 864.13\n",
            "iteration 8593, epoch 18, batch 216/481,disc_loss 74.556, (real 76.719, fake 72.392 ) gen_loss 791.64\n",
            "iteration 8594, epoch 18, batch 217/481,disc_loss 79.011, (real 80.948, fake 77.074 ) gen_loss 826.56\n",
            "iteration 8595, epoch 18, batch 218/481,disc_loss 75.097, (real 77.609, fake 72.585 ) gen_loss 766.82\n",
            "iteration 8596, epoch 18, batch 219/481,disc_loss 76.621, (real 79.21, fake 74.032 ) gen_loss 758.59\n",
            "iteration 8597, epoch 18, batch 220/481,disc_loss 71.964, (real 74.511, fake 69.418 ) gen_loss 811.07\n",
            "iteration 8598, epoch 18, batch 221/481,disc_loss 77.022, (real 79.291, fake 74.752 ) gen_loss 864.32\n",
            "iteration 8599, epoch 18, batch 222/481,disc_loss 76.694, (real 79.079, fake 74.309 ) gen_loss 870.87\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 8600, epoch 18, batch 223/481,disc_loss 74.5, (real 77.79, fake 71.21 ) gen_loss 908.5\n",
            "iteration 8601, epoch 18, batch 224/481,disc_loss 75.297, (real 77.585, fake 73.008 ) gen_loss 845.04\n",
            "iteration 8602, epoch 18, batch 225/481,disc_loss 77.748, (real 79.634, fake 75.862 ) gen_loss 803.92\n",
            "iteration 8603, epoch 18, batch 226/481,disc_loss 79.052, (real 81.397, fake 76.706 ) gen_loss 765.43\n",
            "iteration 8604, epoch 18, batch 227/481,disc_loss 74.237, (real 76.431, fake 72.042 ) gen_loss 894.68\n",
            "iteration 8605, epoch 18, batch 228/481,disc_loss 77.106, (real 78.412, fake 75.8 ) gen_loss 840.33\n",
            "iteration 8606, epoch 18, batch 229/481,disc_loss 77.02, (real 79.661, fake 74.379 ) gen_loss 826.04\n",
            "iteration 8607, epoch 18, batch 230/481,disc_loss 76.969, (real 79.188, fake 74.75 ) gen_loss 808.77\n",
            "iteration 8608, epoch 18, batch 231/481,disc_loss 76.768, (real 78.688, fake 74.849 ) gen_loss 809.14\n",
            "iteration 8609, epoch 18, batch 232/481,disc_loss 74.749, (real 76.526, fake 72.973 ) gen_loss 790.5\n",
            "iteration 8610, epoch 18, batch 233/481,disc_loss 76.949, (real 79.363, fake 74.534 ) gen_loss 822.52\n",
            "iteration 8611, epoch 18, batch 234/481,disc_loss 78.722, (real 81.136, fake 76.308 ) gen_loss 795.4\n",
            "iteration 8612, epoch 18, batch 235/481,disc_loss 75.548, (real 78.054, fake 73.043 ) gen_loss 867.36\n",
            "iteration 8613, epoch 18, batch 236/481,disc_loss 79.214, (real 81.781, fake 76.648 ) gen_loss 835.09\n",
            "iteration 8614, epoch 18, batch 237/481,disc_loss 78.29, (real 80.783, fake 75.797 ) gen_loss 874.13\n",
            "iteration 8615, epoch 18, batch 238/481,disc_loss 76.313, (real 78.374, fake 74.252 ) gen_loss 958.27\n",
            "iteration 8616, epoch 18, batch 239/481,disc_loss 76.649, (real 79.086, fake 74.211 ) gen_loss 880.15\n",
            "iteration 8617, epoch 18, batch 240/481,disc_loss 75.201, (real 77.595, fake 72.808 ) gen_loss 838.38\n",
            "iteration 8618, epoch 18, batch 241/481,disc_loss 72.896, (real 74.912, fake 70.88 ) gen_loss 848.32\n",
            "iteration 8619, epoch 18, batch 242/481,disc_loss 78.179, (real 80.607, fake 75.751 ) gen_loss 844.04\n",
            "iteration 8620, epoch 18, batch 243/481,disc_loss 73.3, (real 75.312, fake 71.287 ) gen_loss 822.83\n",
            "iteration 8621, epoch 18, batch 244/481,disc_loss 77.309, (real 79.616, fake 75.001 ) gen_loss 895.57\n",
            "iteration 8622, epoch 18, batch 245/481,disc_loss 77.377, (real 79.093, fake 75.661 ) gen_loss 789.95\n",
            "iteration 8623, epoch 18, batch 246/481,disc_loss 77.37, (real 79.686, fake 75.055 ) gen_loss 851.17\n",
            "iteration 8624, epoch 18, batch 247/481,disc_loss 77.528, (real 80.28, fake 74.776 ) gen_loss 864.16\n",
            "iteration 8625, epoch 18, batch 248/481,disc_loss 77.011, (real 79.296, fake 74.725 ) gen_loss 783.36\n",
            "iteration 8626, epoch 18, batch 249/481,disc_loss 74.499, (real 76.739, fake 72.259 ) gen_loss 777.16\n",
            "iteration 8627, epoch 18, batch 250/481,disc_loss 72.092, (real 74.253, fake 69.93 ) gen_loss 787.17\n",
            "iteration 8628, epoch 18, batch 251/481,disc_loss 74.445, (real 75.95, fake 72.94 ) gen_loss 889.4\n",
            "iteration 8629, epoch 18, batch 252/481,disc_loss 75.086, (real 77.142, fake 73.03 ) gen_loss 900.02\n",
            "iteration 8630, epoch 18, batch 253/481,disc_loss 74.718, (real 77.372, fake 72.065 ) gen_loss 813.89\n",
            "iteration 8631, epoch 18, batch 254/481,disc_loss 76.423, (real 79.288, fake 73.557 ) gen_loss 837.48\n",
            "iteration 8632, epoch 18, batch 255/481,disc_loss 75.537, (real 78.298, fake 72.776 ) gen_loss 809.97\n",
            "iteration 8633, epoch 18, batch 256/481,disc_loss 72.634, (real 75.764, fake 69.505 ) gen_loss 754.29\n",
            "iteration 8634, epoch 18, batch 257/481,disc_loss 75.502, (real 78.701, fake 72.303 ) gen_loss 832.87\n",
            "iteration 8635, epoch 18, batch 258/481,disc_loss 79.07, (real 80.674, fake 77.467 ) gen_loss 824.4\n",
            "iteration 8636, epoch 18, batch 259/481,disc_loss 79.978, (real 81.363, fake 78.594 ) gen_loss 871.27\n",
            "iteration 8637, epoch 18, batch 260/481,disc_loss 83.277, (real 86.76, fake 79.794 ) gen_loss 812.94\n",
            "iteration 8638, epoch 18, batch 261/481,disc_loss 78.333, (real 80.604, fake 76.061 ) gen_loss 834.38\n",
            "iteration 8639, epoch 18, batch 262/481,disc_loss 76.206, (real 78.563, fake 73.848 ) gen_loss 853.97\n",
            "iteration 8640, epoch 18, batch 263/481,disc_loss 76.077, (real 78.266, fake 73.887 ) gen_loss 808.3\n",
            "iteration 8641, epoch 18, batch 264/481,disc_loss 73.066, (real 75.086, fake 71.047 ) gen_loss 831.45\n",
            "iteration 8642, epoch 18, batch 265/481,disc_loss 79.692, (real 82.706, fake 76.679 ) gen_loss 806.58\n",
            "iteration 8643, epoch 18, batch 266/481,disc_loss 72.994, (real 75.585, fake 70.402 ) gen_loss 903.21\n",
            "iteration 8644, epoch 18, batch 267/481,disc_loss 74.795, (real 77.211, fake 72.378 ) gen_loss 840.28\n",
            "iteration 8645, epoch 18, batch 268/481,disc_loss 74.298, (real 76.502, fake 72.093 ) gen_loss 896.47\n",
            "iteration 8646, epoch 18, batch 269/481,disc_loss 76.609, (real 78.995, fake 74.223 ) gen_loss 817.68\n",
            "iteration 8647, epoch 18, batch 270/481,disc_loss 75.391, (real 77.481, fake 73.302 ) gen_loss 900.51\n",
            "iteration 8648, epoch 18, batch 271/481,disc_loss 76.931, (real 79.215, fake 74.648 ) gen_loss 900.59\n",
            "iteration 8649, epoch 18, batch 272/481,disc_loss 77.904, (real 80.186, fake 75.621 ) gen_loss 803.6\n",
            "iteration 8650, epoch 18, batch 273/481,disc_loss 79.386, (real 81.879, fake 76.893 ) gen_loss 811.41\n",
            "iteration 8651, epoch 18, batch 274/481,disc_loss 79.465, (real 81.987, fake 76.943 ) gen_loss 833.24\n",
            "iteration 8652, epoch 18, batch 275/481,disc_loss 74.621, (real 77.273, fake 71.969 ) gen_loss 910.1\n",
            "iteration 8653, epoch 18, batch 276/481,disc_loss 80.754, (real 83.058, fake 78.451 ) gen_loss 869.16\n",
            "iteration 8654, epoch 18, batch 277/481,disc_loss 82.536, (real 84.853, fake 80.219 ) gen_loss 846.2\n",
            "iteration 8655, epoch 18, batch 278/481,disc_loss 80.933, (real 84.041, fake 77.824 ) gen_loss 911.46\n",
            "iteration 8656, epoch 18, batch 279/481,disc_loss 79.084, (real 82.134, fake 76.034 ) gen_loss 826.54\n",
            "iteration 8657, epoch 18, batch 280/481,disc_loss 73.307, (real 76.121, fake 70.492 ) gen_loss 906.2\n",
            "iteration 8658, epoch 18, batch 281/481,disc_loss 78.474, (real 80.818, fake 76.13 ) gen_loss 854.22\n",
            "iteration 8659, epoch 18, batch 282/481,disc_loss 79.593, (real 81.755, fake 77.431 ) gen_loss 896.11\n",
            "iteration 8660, epoch 18, batch 283/481,disc_loss 75.348, (real 77.772, fake 72.924 ) gen_loss 848.57\n",
            "iteration 8661, epoch 18, batch 284/481,disc_loss 76.737, (real 79.529, fake 73.945 ) gen_loss 864.97\n",
            "iteration 8662, epoch 18, batch 285/481,disc_loss 80.349, (real 82.742, fake 77.957 ) gen_loss 791.02\n",
            "iteration 8663, epoch 18, batch 286/481,disc_loss 76.791, (real 79.38, fake 74.202 ) gen_loss 915.22\n",
            "iteration 8664, epoch 18, batch 287/481,disc_loss 76.675, (real 79.275, fake 74.076 ) gen_loss 916.94\n",
            "iteration 8665, epoch 18, batch 288/481,disc_loss 75.742, (real 78.331, fake 73.152 ) gen_loss 829.71\n",
            "iteration 8666, epoch 18, batch 289/481,disc_loss 76.501, (real 79.123, fake 73.879 ) gen_loss 738.39\n",
            "iteration 8667, epoch 18, batch 290/481,disc_loss 81.374, (real 84.015, fake 78.732 ) gen_loss 794.56\n",
            "iteration 8668, epoch 18, batch 291/481,disc_loss 79.986, (real 82.898, fake 77.073 ) gen_loss 970.48\n",
            "iteration 8669, epoch 18, batch 292/481,disc_loss 74.33, (real 76.181, fake 72.479 ) gen_loss 883.91\n",
            "iteration 8670, epoch 18, batch 293/481,disc_loss 77.134, (real 79.811, fake 74.457 ) gen_loss 820.52\n",
            "iteration 8671, epoch 18, batch 294/481,disc_loss 77.76, (real 80.002, fake 75.518 ) gen_loss 852.81\n",
            "iteration 8672, epoch 18, batch 295/481,disc_loss 77.74, (real 79.971, fake 75.509 ) gen_loss 745.72\n",
            "iteration 8673, epoch 18, batch 296/481,disc_loss 79.487, (real 81.564, fake 77.411 ) gen_loss 816.01\n",
            "iteration 8674, epoch 18, batch 297/481,disc_loss 71.927, (real 74.296, fake 69.559 ) gen_loss 825.65\n",
            "iteration 8675, epoch 18, batch 298/481,disc_loss 73.953, (real 75.911, fake 71.994 ) gen_loss 832.81\n",
            "iteration 8676, epoch 18, batch 299/481,disc_loss 74.813, (real 77.026, fake 72.599 ) gen_loss 775.32\n",
            "iteration 8677, epoch 18, batch 300/481,disc_loss 77.08, (real 79.391, fake 74.769 ) gen_loss 795.12\n",
            "iteration 8678, epoch 18, batch 301/481,disc_loss 76.187, (real 78.763, fake 73.61 ) gen_loss 746.0\n",
            "iteration 8679, epoch 18, batch 302/481,disc_loss 78.759, (real 81.933, fake 75.585 ) gen_loss 812.43\n",
            "iteration 8680, epoch 18, batch 303/481,disc_loss 71.753, (real 74.298, fake 69.208 ) gen_loss 916.5\n",
            "iteration 8681, epoch 18, batch 304/481,disc_loss 77.819, (real 80.671, fake 74.967 ) gen_loss 811.07\n",
            "iteration 8682, epoch 18, batch 305/481,disc_loss 76.563, (real 78.746, fake 74.379 ) gen_loss 858.71\n",
            "iteration 8683, epoch 18, batch 306/481,disc_loss 77.696, (real 79.5, fake 75.893 ) gen_loss 800.26\n",
            "iteration 8684, epoch 18, batch 307/481,disc_loss 72.617, (real 75.553, fake 69.681 ) gen_loss 922.83\n",
            "iteration 8685, epoch 18, batch 308/481,disc_loss 77.07, (real 79.711, fake 74.429 ) gen_loss 879.19\n",
            "iteration 8686, epoch 18, batch 309/481,disc_loss 74.37, (real 76.395, fake 72.345 ) gen_loss 821.93\n",
            "iteration 8687, epoch 18, batch 310/481,disc_loss 80.93, (real 83.837, fake 78.023 ) gen_loss 801.44\n",
            "iteration 8688, epoch 18, batch 311/481,disc_loss 80.567, (real 83.211, fake 77.924 ) gen_loss 799.96\n",
            "iteration 8689, epoch 18, batch 312/481,disc_loss 77.8, (real 80.292, fake 75.308 ) gen_loss 871.83\n",
            "iteration 8690, epoch 18, batch 313/481,disc_loss 72.138, (real 74.397, fake 69.879 ) gen_loss 791.59\n",
            "iteration 8691, epoch 18, batch 314/481,disc_loss 79.192, (real 81.982, fake 76.403 ) gen_loss 792.31\n",
            "iteration 8692, epoch 18, batch 315/481,disc_loss 72.443, (real 74.699, fake 70.187 ) gen_loss 866.42\n",
            "iteration 8693, epoch 18, batch 316/481,disc_loss 75.425, (real 77.727, fake 73.123 ) gen_loss 751.55\n",
            "iteration 8694, epoch 18, batch 317/481,disc_loss 77.217, (real 79.585, fake 74.849 ) gen_loss 844.01\n",
            "iteration 8695, epoch 18, batch 318/481,disc_loss 78.61, (real 81.638, fake 75.582 ) gen_loss 835.51\n",
            "iteration 8696, epoch 18, batch 319/481,disc_loss 78.806, (real 80.9, fake 76.712 ) gen_loss 798.64\n",
            "iteration 8697, epoch 18, batch 320/481,disc_loss 78.188, (real 81.166, fake 75.21 ) gen_loss 813.54\n",
            "iteration 8698, epoch 18, batch 321/481,disc_loss 75.907, (real 79.134, fake 72.681 ) gen_loss 840.85\n",
            "iteration 8699, epoch 18, batch 322/481,disc_loss 78.114, (real 80.71, fake 75.518 ) gen_loss 899.49\n",
            "iteration 8700, epoch 18, batch 323/481,disc_loss 73.315, (real 75.83, fake 70.801 ) gen_loss 947.47\n",
            "iteration 8701, epoch 18, batch 324/481,disc_loss 79.245, (real 81.321, fake 77.169 ) gen_loss 852.13\n",
            "iteration 8702, epoch 18, batch 325/481,disc_loss 78.754, (real 81.091, fake 76.416 ) gen_loss 805.51\n",
            "iteration 8703, epoch 18, batch 326/481,disc_loss 80.153, (real 82.943, fake 77.362 ) gen_loss 845.42\n",
            "iteration 8704, epoch 18, batch 327/481,disc_loss 80.257, (real 82.781, fake 77.734 ) gen_loss 835.36\n",
            "iteration 8705, epoch 18, batch 328/481,disc_loss 73.2, (real 75.472, fake 70.928 ) gen_loss 875.61\n",
            "iteration 8706, epoch 18, batch 329/481,disc_loss 75.685, (real 78.779, fake 72.59 ) gen_loss 847.02\n",
            "iteration 8707, epoch 18, batch 330/481,disc_loss 79.597, (real 82.303, fake 76.891 ) gen_loss 837.15\n",
            "iteration 8708, epoch 18, batch 331/481,disc_loss 81.444, (real 83.564, fake 79.323 ) gen_loss 817.47\n",
            "iteration 8709, epoch 18, batch 332/481,disc_loss 75.151, (real 77.08, fake 73.223 ) gen_loss 806.74\n",
            "iteration 8710, epoch 18, batch 333/481,disc_loss 79.591, (real 81.9, fake 77.282 ) gen_loss 875.94\n",
            "iteration 8711, epoch 18, batch 334/481,disc_loss 78.413, (real 81.359, fake 75.467 ) gen_loss 796.46\n",
            "iteration 8712, epoch 18, batch 335/481,disc_loss 77.287, (real 79.648, fake 74.925 ) gen_loss 810.6\n",
            "iteration 8713, epoch 18, batch 336/481,disc_loss 73.624, (real 76.733, fake 70.514 ) gen_loss 813.59\n",
            "iteration 8714, epoch 18, batch 337/481,disc_loss 77.581, (real 79.997, fake 75.165 ) gen_loss 779.76\n",
            "iteration 8715, epoch 18, batch 338/481,disc_loss 77.453, (real 79.517, fake 75.388 ) gen_loss 816.71\n",
            "iteration 8716, epoch 18, batch 339/481,disc_loss 78.391, (real 80.75, fake 76.033 ) gen_loss 870.55\n",
            "iteration 8717, epoch 18, batch 340/481,disc_loss 76.45, (real 78.802, fake 74.098 ) gen_loss 852.88\n",
            "iteration 8718, epoch 18, batch 341/481,disc_loss 79.687, (real 82.448, fake 76.926 ) gen_loss 839.12\n",
            "iteration 8719, epoch 18, batch 342/481,disc_loss 79.262, (real 81.339, fake 77.185 ) gen_loss 829.91\n",
            "iteration 8720, epoch 18, batch 343/481,disc_loss 73.532, (real 75.778, fake 71.285 ) gen_loss 742.79\n",
            "iteration 8721, epoch 18, batch 344/481,disc_loss 82.094, (real 84.994, fake 79.193 ) gen_loss 814.02\n",
            "iteration 8722, epoch 18, batch 345/481,disc_loss 76.345, (real 78.887, fake 73.804 ) gen_loss 879.87\n",
            "iteration 8723, epoch 18, batch 346/481,disc_loss 81.182, (real 83.813, fake 78.55 ) gen_loss 819.2\n",
            "iteration 8724, epoch 18, batch 347/481,disc_loss 78.752, (real 81.059, fake 76.445 ) gen_loss 855.78\n",
            "iteration 8725, epoch 18, batch 348/481,disc_loss 79.138, (real 80.563, fake 77.714 ) gen_loss 901.86\n",
            "iteration 8726, epoch 18, batch 349/481,disc_loss 80.13, (real 82.559, fake 77.702 ) gen_loss 851.24\n",
            "iteration 8727, epoch 18, batch 350/481,disc_loss 78.624, (real 81.132, fake 76.116 ) gen_loss 826.12\n",
            "iteration 8728, epoch 18, batch 351/481,disc_loss 75.12, (real 77.456, fake 72.783 ) gen_loss 925.14\n",
            "iteration 8729, epoch 18, batch 352/481,disc_loss 73.394, (real 75.447, fake 71.342 ) gen_loss 903.46\n",
            "iteration 8730, epoch 18, batch 353/481,disc_loss 79.434, (real 81.653, fake 77.214 ) gen_loss 897.17\n",
            "iteration 8731, epoch 18, batch 354/481,disc_loss 74.38, (real 76.803, fake 71.958 ) gen_loss 927.98\n",
            "iteration 8732, epoch 18, batch 355/481,disc_loss 79.591, (real 82.062, fake 77.119 ) gen_loss 853.36\n",
            "iteration 8733, epoch 18, batch 356/481,disc_loss 79.232, (real 81.667, fake 76.798 ) gen_loss 792.57\n",
            "iteration 8734, epoch 18, batch 357/481,disc_loss 80.501, (real 83.537, fake 77.466 ) gen_loss 808.44\n",
            "iteration 8735, epoch 18, batch 358/481,disc_loss 74.783, (real 77.631, fake 71.936 ) gen_loss 910.77\n",
            "iteration 8736, epoch 18, batch 359/481,disc_loss 78.983, (real 82.276, fake 75.691 ) gen_loss 977.86\n",
            "iteration 8737, epoch 18, batch 360/481,disc_loss 77.956, (real 80.968, fake 74.944 ) gen_loss 853.95\n",
            "iteration 8738, epoch 18, batch 361/481,disc_loss 75.464, (real 78.081, fake 72.847 ) gen_loss 960.32\n",
            "iteration 8739, epoch 18, batch 362/481,disc_loss 78.022, (real 81.197, fake 74.846 ) gen_loss 895.77\n",
            "iteration 8740, epoch 18, batch 363/481,disc_loss 76.454, (real 78.958, fake 73.951 ) gen_loss 819.91\n",
            "iteration 8741, epoch 18, batch 364/481,disc_loss 77.874, (real 79.97, fake 75.778 ) gen_loss 779.35\n",
            "iteration 8742, epoch 18, batch 365/481,disc_loss 81.65, (real 84.347, fake 78.953 ) gen_loss 685.79\n",
            "iteration 8743, epoch 18, batch 366/481,disc_loss 75.425, (real 77.887, fake 72.964 ) gen_loss 780.51\n",
            "iteration 8744, epoch 18, batch 367/481,disc_loss 78.089, (real 80.789, fake 75.388 ) gen_loss 803.19\n",
            "iteration 8745, epoch 18, batch 368/481,disc_loss 76.417, (real 78.214, fake 74.62 ) gen_loss 823.33\n",
            "iteration 8746, epoch 18, batch 369/481,disc_loss 78.767, (real 81.268, fake 76.267 ) gen_loss 906.63\n",
            "iteration 8747, epoch 18, batch 370/481,disc_loss 76.627, (real 78.773, fake 74.481 ) gen_loss 853.41\n",
            "iteration 8748, epoch 18, batch 371/481,disc_loss 78.524, (real 81.01, fake 76.037 ) gen_loss 796.37\n",
            "iteration 8749, epoch 18, batch 372/481,disc_loss 74.741, (real 77.089, fake 72.392 ) gen_loss 925.67\n",
            "iteration 8750, epoch 18, batch 373/481,disc_loss 75.177, (real 77.919, fake 72.434 ) gen_loss 843.27\n",
            "iteration 8751, epoch 18, batch 374/481,disc_loss 75.273, (real 77.645, fake 72.902 ) gen_loss 951.21\n",
            "iteration 8752, epoch 18, batch 375/481,disc_loss 73.647, (real 76.394, fake 70.899 ) gen_loss 859.66\n",
            "iteration 8753, epoch 18, batch 376/481,disc_loss 77.975, (real 80.655, fake 75.295 ) gen_loss 870.44\n",
            "iteration 8754, epoch 18, batch 377/481,disc_loss 74.542, (real 76.49, fake 72.594 ) gen_loss 792.53\n",
            "iteration 8755, epoch 18, batch 378/481,disc_loss 78.732, (real 80.762, fake 76.702 ) gen_loss 803.99\n",
            "iteration 8756, epoch 18, batch 379/481,disc_loss 72.043, (real 74.294, fake 69.792 ) gen_loss 801.15\n",
            "iteration 8757, epoch 18, batch 380/481,disc_loss 74.633, (real 76.937, fake 72.328 ) gen_loss 848.45\n",
            "iteration 8758, epoch 18, batch 381/481,disc_loss 79.2, (real 81.909, fake 76.491 ) gen_loss 863.78\n",
            "iteration 8759, epoch 18, batch 382/481,disc_loss 74.902, (real 77.6, fake 72.203 ) gen_loss 825.03\n",
            "iteration 8760, epoch 18, batch 383/481,disc_loss 72.09, (real 75.666, fake 68.513 ) gen_loss 765.68\n",
            "iteration 8761, epoch 18, batch 384/481,disc_loss 73.939, (real 76.077, fake 71.8 ) gen_loss 1050.9\n",
            "iteration 8762, epoch 18, batch 385/481,disc_loss 72.997, (real 74.443, fake 71.55 ) gen_loss 976.46\n",
            "iteration 8763, epoch 18, batch 386/481,disc_loss 81.771, (real 83.416, fake 80.125 ) gen_loss 960.88\n",
            "iteration 8764, epoch 18, batch 387/481,disc_loss 76.052, (real 78.356, fake 73.749 ) gen_loss 925.37\n",
            "iteration 8765, epoch 18, batch 388/481,disc_loss 79.372, (real 82.266, fake 76.477 ) gen_loss 914.77\n",
            "iteration 8766, epoch 18, batch 389/481,disc_loss 76.86, (real 78.863, fake 74.857 ) gen_loss 802.24\n",
            "iteration 8767, epoch 18, batch 390/481,disc_loss 71.583, (real 73.657, fake 69.509 ) gen_loss 775.4\n",
            "iteration 8768, epoch 18, batch 391/481,disc_loss 76.36, (real 79.0, fake 73.721 ) gen_loss 925.4\n",
            "iteration 8769, epoch 18, batch 392/481,disc_loss 76.475, (real 79.271, fake 73.68 ) gen_loss 813.46\n",
            "iteration 8770, epoch 18, batch 393/481,disc_loss 75.643, (real 77.791, fake 73.495 ) gen_loss 866.35\n",
            "iteration 8771, epoch 18, batch 394/481,disc_loss 75.775, (real 77.293, fake 74.257 ) gen_loss 725.03\n",
            "iteration 8772, epoch 18, batch 395/481,disc_loss 78.784, (real 81.018, fake 76.55 ) gen_loss 839.34\n",
            "iteration 8773, epoch 18, batch 396/481,disc_loss 79.111, (real 81.041, fake 77.181 ) gen_loss 904.65\n",
            "iteration 8774, epoch 18, batch 397/481,disc_loss 72.935, (real 75.151, fake 70.719 ) gen_loss 837.17\n",
            "iteration 8775, epoch 18, batch 398/481,disc_loss 74.716, (real 77.398, fake 72.034 ) gen_loss 885.03\n",
            "iteration 8776, epoch 18, batch 399/481,disc_loss 75.376, (real 77.847, fake 72.905 ) gen_loss 820.41\n",
            "iteration 8777, epoch 18, batch 400/481,disc_loss 79.662, (real 82.286, fake 77.039 ) gen_loss 873.77\n",
            "iteration 8778, epoch 18, batch 401/481,disc_loss 78.215, (real 80.412, fake 76.019 ) gen_loss 903.57\n",
            "iteration 8779, epoch 18, batch 402/481,disc_loss 76.297, (real 79.469, fake 73.125 ) gen_loss 890.05\n",
            "iteration 8780, epoch 18, batch 403/481,disc_loss 76.772, (real 79.348, fake 74.196 ) gen_loss 888.82\n",
            "iteration 8781, epoch 18, batch 404/481,disc_loss 74.859, (real 77.012, fake 72.706 ) gen_loss 887.64\n",
            "iteration 8782, epoch 18, batch 405/481,disc_loss 80.314, (real 83.289, fake 77.338 ) gen_loss 865.71\n",
            "iteration 8783, epoch 18, batch 406/481,disc_loss 80.167, (real 82.312, fake 78.023 ) gen_loss 840.08\n",
            "iteration 8784, epoch 18, batch 407/481,disc_loss 75.051, (real 77.68, fake 72.422 ) gen_loss 803.15\n",
            "iteration 8785, epoch 18, batch 408/481,disc_loss 78.463, (real 80.914, fake 76.012 ) gen_loss 789.93\n",
            "iteration 8786, epoch 18, batch 409/481,disc_loss 81.004, (real 84.015, fake 77.993 ) gen_loss 892.3\n",
            "iteration 8787, epoch 18, batch 410/481,disc_loss 76.114, (real 79.202, fake 73.025 ) gen_loss 931.76\n",
            "iteration 8788, epoch 18, batch 411/481,disc_loss 70.111, (real 71.77, fake 68.453 ) gen_loss 900.68\n",
            "iteration 8789, epoch 18, batch 412/481,disc_loss 71.143, (real 74.042, fake 68.244 ) gen_loss 847.81\n",
            "iteration 8790, epoch 18, batch 413/481,disc_loss 76.515, (real 78.736, fake 74.294 ) gen_loss 830.08\n",
            "iteration 8791, epoch 18, batch 414/481,disc_loss 76.019, (real 78.331, fake 73.706 ) gen_loss 865.1\n",
            "iteration 8792, epoch 18, batch 415/481,disc_loss 77.878, (real 80.608, fake 75.149 ) gen_loss 898.42\n",
            "iteration 8793, epoch 18, batch 416/481,disc_loss 77.325, (real 79.321, fake 75.329 ) gen_loss 819.27\n",
            "iteration 8794, epoch 18, batch 417/481,disc_loss 79.312, (real 82.233, fake 76.39 ) gen_loss 895.76\n",
            "iteration 8795, epoch 18, batch 418/481,disc_loss 79.939, (real 81.994, fake 77.884 ) gen_loss 824.78\n",
            "iteration 8796, epoch 18, batch 419/481,disc_loss 72.421, (real 74.21, fake 70.631 ) gen_loss 940.11\n",
            "iteration 8797, epoch 18, batch 420/481,disc_loss 79.039, (real 81.37, fake 76.707 ) gen_loss 896.73\n",
            "iteration 8798, epoch 18, batch 421/481,disc_loss 74.705, (real 76.602, fake 72.807 ) gen_loss 847.78\n",
            "iteration 8799, epoch 18, batch 422/481,disc_loss 76.508, (real 79.128, fake 73.888 ) gen_loss 843.96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 8800, epoch 18, batch 423/481,disc_loss 76.603, (real 79.437, fake 73.77 ) gen_loss 876.74\n",
            "iteration 8801, epoch 18, batch 424/481,disc_loss 75.603, (real 78.313, fake 72.892 ) gen_loss 812.78\n",
            "iteration 8802, epoch 18, batch 425/481,disc_loss 76.465, (real 79.688, fake 73.241 ) gen_loss 889.19\n",
            "iteration 8803, epoch 18, batch 426/481,disc_loss 74.338, (real 76.208, fake 72.469 ) gen_loss 829.97\n",
            "iteration 8804, epoch 18, batch 427/481,disc_loss 77.215, (real 79.667, fake 74.764 ) gen_loss 799.44\n",
            "iteration 8805, epoch 18, batch 428/481,disc_loss 78.551, (real 81.006, fake 76.096 ) gen_loss 815.0\n",
            "iteration 8806, epoch 18, batch 429/481,disc_loss 78.892, (real 81.367, fake 76.418 ) gen_loss 857.22\n",
            "iteration 8807, epoch 18, batch 430/481,disc_loss 78.213, (real 80.451, fake 75.975 ) gen_loss 934.76\n",
            "iteration 8808, epoch 18, batch 431/481,disc_loss 79.625, (real 81.894, fake 77.356 ) gen_loss 912.7\n",
            "iteration 8809, epoch 18, batch 432/481,disc_loss 81.759, (real 84.401, fake 79.117 ) gen_loss 937.61\n",
            "iteration 8810, epoch 18, batch 433/481,disc_loss 76.751, (real 79.577, fake 73.925 ) gen_loss 945.77\n",
            "iteration 8811, epoch 18, batch 434/481,disc_loss 79.785, (real 82.953, fake 76.617 ) gen_loss 814.59\n",
            "iteration 8812, epoch 18, batch 435/481,disc_loss 75.07, (real 76.989, fake 73.152 ) gen_loss 844.67\n",
            "iteration 8813, epoch 18, batch 436/481,disc_loss 73.9, (real 76.108, fake 71.693 ) gen_loss 831.8\n",
            "iteration 8814, epoch 18, batch 437/481,disc_loss 73.024, (real 75.594, fake 70.454 ) gen_loss 792.5\n",
            "iteration 8815, epoch 18, batch 438/481,disc_loss 77.497, (real 80.356, fake 74.638 ) gen_loss 881.7\n",
            "iteration 8816, epoch 18, batch 439/481,disc_loss 77.597, (real 80.061, fake 75.133 ) gen_loss 904.74\n",
            "iteration 8817, epoch 18, batch 440/481,disc_loss 78.506, (real 80.882, fake 76.13 ) gen_loss 866.84\n",
            "iteration 8818, epoch 18, batch 441/481,disc_loss 79.557, (real 82.246, fake 76.869 ) gen_loss 879.13\n",
            "iteration 8819, epoch 18, batch 442/481,disc_loss 76.871, (real 80.084, fake 73.658 ) gen_loss 867.51\n",
            "iteration 8820, epoch 18, batch 443/481,disc_loss 76.407, (real 78.42, fake 74.393 ) gen_loss 777.1\n",
            "iteration 8821, epoch 18, batch 444/481,disc_loss 74.05, (real 76.626, fake 71.475 ) gen_loss 781.39\n",
            "iteration 8822, epoch 18, batch 445/481,disc_loss 72.057, (real 75.031, fake 69.082 ) gen_loss 796.76\n",
            "iteration 8823, epoch 18, batch 446/481,disc_loss 76.218, (real 78.223, fake 74.213 ) gen_loss 844.31\n",
            "iteration 8824, epoch 18, batch 447/481,disc_loss 73.657, (real 75.729, fake 71.584 ) gen_loss 822.98\n",
            "iteration 8825, epoch 18, batch 448/481,disc_loss 76.147, (real 78.092, fake 74.202 ) gen_loss 933.25\n",
            "iteration 8826, epoch 18, batch 449/481,disc_loss 78.359, (real 81.427, fake 75.291 ) gen_loss 882.58\n",
            "iteration 8827, epoch 18, batch 450/481,disc_loss 77.273, (real 80.35, fake 74.196 ) gen_loss 858.41\n",
            "iteration 8828, epoch 18, batch 451/481,disc_loss 77.907, (real 80.202, fake 75.612 ) gen_loss 989.53\n",
            "iteration 8829, epoch 18, batch 452/481,disc_loss 74.996, (real 77.6, fake 72.391 ) gen_loss 892.56\n",
            "iteration 8830, epoch 18, batch 453/481,disc_loss 78.605, (real 80.592, fake 76.618 ) gen_loss 792.0\n",
            "iteration 8831, epoch 18, batch 454/481,disc_loss 81.1, (real 83.69, fake 78.51 ) gen_loss 865.1\n",
            "iteration 8832, epoch 18, batch 455/481,disc_loss 74.868, (real 77.798, fake 71.939 ) gen_loss 854.35\n",
            "iteration 8833, epoch 18, batch 456/481,disc_loss 74.529, (real 76.408, fake 72.649 ) gen_loss 762.15\n",
            "iteration 8834, epoch 18, batch 457/481,disc_loss 75.829, (real 78.962, fake 72.696 ) gen_loss 878.44\n",
            "iteration 8835, epoch 18, batch 458/481,disc_loss 74.233, (real 76.616, fake 71.85 ) gen_loss 868.65\n",
            "iteration 8836, epoch 18, batch 459/481,disc_loss 76.095, (real 78.833, fake 73.358 ) gen_loss 867.01\n",
            "iteration 8837, epoch 18, batch 460/481,disc_loss 76.153, (real 78.552, fake 73.754 ) gen_loss 882.45\n",
            "iteration 8838, epoch 18, batch 461/481,disc_loss 71.877, (real 74.433, fake 69.321 ) gen_loss 1004.4\n",
            "iteration 8839, epoch 18, batch 462/481,disc_loss 80.531, (real 83.876, fake 77.186 ) gen_loss 945.4\n",
            "iteration 8840, epoch 18, batch 463/481,disc_loss 78.949, (real 81.659, fake 76.239 ) gen_loss 864.68\n",
            "iteration 8841, epoch 18, batch 464/481,disc_loss 82.063, (real 84.227, fake 79.9 ) gen_loss 878.84\n",
            "iteration 8842, epoch 18, batch 465/481,disc_loss 80.029, (real 81.23, fake 78.827 ) gen_loss 891.57\n",
            "iteration 8843, epoch 18, batch 466/481,disc_loss 76.763, (real 79.163, fake 74.362 ) gen_loss 844.58\n",
            "iteration 8844, epoch 18, batch 467/481,disc_loss 79.932, (real 82.763, fake 77.1 ) gen_loss 776.42\n",
            "iteration 8845, epoch 18, batch 468/481,disc_loss 80.634, (real 83.042, fake 78.226 ) gen_loss 823.02\n",
            "iteration 8846, epoch 18, batch 469/481,disc_loss 80.349, (real 82.724, fake 77.975 ) gen_loss 808.42\n",
            "iteration 8847, epoch 18, batch 470/481,disc_loss 73.74, (real 76.45, fake 71.03 ) gen_loss 973.89\n",
            "iteration 8848, epoch 18, batch 471/481,disc_loss 75.168, (real 77.587, fake 72.75 ) gen_loss 885.87\n",
            "iteration 8849, epoch 18, batch 472/481,disc_loss 74.222, (real 76.372, fake 72.073 ) gen_loss 972.37\n",
            "iteration 8850, epoch 18, batch 473/481,disc_loss 78.128, (real 80.984, fake 75.271 ) gen_loss 957.6\n",
            "iteration 8851, epoch 18, batch 474/481,disc_loss 79.74, (real 81.941, fake 77.54 ) gen_loss 791.87\n",
            "iteration 8852, epoch 18, batch 475/481,disc_loss 77.726, (real 79.932, fake 75.52 ) gen_loss 771.11\n",
            "iteration 8853, epoch 18, batch 476/481,disc_loss 77.954, (real 81.181, fake 74.728 ) gen_loss 1010.4\n",
            "iteration 8854, epoch 18, batch 477/481,disc_loss 78.367, (real 80.301, fake 76.433 ) gen_loss 912.09\n",
            "iteration 8855, epoch 18, batch 478/481,disc_loss 78.723, (real 81.332, fake 76.113 ) gen_loss 836.88\n",
            "iteration 8856, epoch 18, batch 479/481,disc_loss 75.738, (real 78.121, fake 73.355 ) gen_loss 870.93\n",
            "iteration 8857, epoch 18, batch 480/481,disc_loss 72.705, (real 76.027, fake 69.383 ) gen_loss 868.03\n",
            "iteration 8858, epoch 18, batch 481/481,disc_loss 77.81, (real 80.249, fake 75.371 ) gen_loss 842.79\n",
            "iteration 8859, epoch 19, batch 1/481,disc_loss 75.797, (real 79.088, fake 72.506 ) gen_loss 937.14\n",
            "iteration 8860, epoch 19, batch 2/481,disc_loss 71.643, (real 73.496, fake 69.79 ) gen_loss 914.38\n",
            "iteration 8861, epoch 19, batch 3/481,disc_loss 74.842, (real 76.413, fake 73.27 ) gen_loss 920.0\n",
            "iteration 8862, epoch 19, batch 4/481,disc_loss 75.375, (real 77.884, fake 72.865 ) gen_loss 833.21\n",
            "iteration 8863, epoch 19, batch 5/481,disc_loss 78.248, (real 80.576, fake 75.92 ) gen_loss 822.43\n",
            "iteration 8864, epoch 19, batch 6/481,disc_loss 79.269, (real 81.812, fake 76.726 ) gen_loss 914.47\n",
            "iteration 8865, epoch 19, batch 7/481,disc_loss 73.627, (real 75.513, fake 71.74 ) gen_loss 901.41\n",
            "iteration 8866, epoch 19, batch 8/481,disc_loss 74.901, (real 76.86, fake 72.941 ) gen_loss 847.53\n",
            "iteration 8867, epoch 19, batch 9/481,disc_loss 78.37, (real 80.269, fake 76.471 ) gen_loss 874.67\n",
            "iteration 8868, epoch 19, batch 10/481,disc_loss 71.969, (real 73.933, fake 70.006 ) gen_loss 857.05\n",
            "iteration 8869, epoch 19, batch 11/481,disc_loss 80.174, (real 82.097, fake 78.251 ) gen_loss 860.11\n",
            "iteration 8870, epoch 19, batch 12/481,disc_loss 78.866, (real 80.98, fake 76.752 ) gen_loss 872.86\n",
            "iteration 8871, epoch 19, batch 13/481,disc_loss 82.47, (real 84.503, fake 80.438 ) gen_loss 935.58\n",
            "iteration 8872, epoch 19, batch 14/481,disc_loss 78.05, (real 80.314, fake 75.786 ) gen_loss 866.93\n",
            "iteration 8873, epoch 19, batch 15/481,disc_loss 77.941, (real 80.154, fake 75.729 ) gen_loss 854.99\n",
            "iteration 8874, epoch 19, batch 16/481,disc_loss 76.988, (real 78.021, fake 75.954 ) gen_loss 887.44\n",
            "iteration 8875, epoch 19, batch 17/481,disc_loss 81.776, (real 83.694, fake 79.859 ) gen_loss 837.08\n",
            "iteration 8876, epoch 19, batch 18/481,disc_loss 76.908, (real 78.807, fake 75.009 ) gen_loss 813.41\n",
            "iteration 8877, epoch 19, batch 19/481,disc_loss 74.66, (real 76.819, fake 72.5 ) gen_loss 838.27\n",
            "iteration 8878, epoch 19, batch 20/481,disc_loss 77.832, (real 80.419, fake 75.244 ) gen_loss 912.43\n",
            "iteration 8879, epoch 19, batch 21/481,disc_loss 79.02, (real 81.126, fake 76.914 ) gen_loss 847.19\n",
            "iteration 8880, epoch 19, batch 22/481,disc_loss 74.102, (real 76.903, fake 71.301 ) gen_loss 883.11\n",
            "iteration 8881, epoch 19, batch 23/481,disc_loss 76.6, (real 78.827, fake 74.373 ) gen_loss 796.25\n",
            "iteration 8882, epoch 19, batch 24/481,disc_loss 78.455, (real 80.722, fake 76.189 ) gen_loss 961.25\n",
            "iteration 8883, epoch 19, batch 25/481,disc_loss 75.198, (real 76.715, fake 73.681 ) gen_loss 923.32\n",
            "iteration 8884, epoch 19, batch 26/481,disc_loss 74.918, (real 77.887, fake 71.95 ) gen_loss 900.99\n",
            "iteration 8885, epoch 19, batch 27/481,disc_loss 76.315, (real 78.588, fake 74.041 ) gen_loss 898.77\n",
            "iteration 8886, epoch 19, batch 28/481,disc_loss 79.612, (real 81.73, fake 77.494 ) gen_loss 761.29\n",
            "iteration 8887, epoch 19, batch 29/481,disc_loss 83.387, (real 85.85, fake 80.924 ) gen_loss 926.31\n",
            "iteration 8888, epoch 19, batch 30/481,disc_loss 83.135, (real 85.584, fake 80.685 ) gen_loss 970.5\n",
            "iteration 8889, epoch 19, batch 31/481,disc_loss 78.239, (real 80.742, fake 75.736 ) gen_loss 929.74\n",
            "iteration 8890, epoch 19, batch 32/481,disc_loss 77.953, (real 80.113, fake 75.792 ) gen_loss 874.78\n",
            "iteration 8891, epoch 19, batch 33/481,disc_loss 81.887, (real 84.033, fake 79.741 ) gen_loss 890.88\n",
            "iteration 8892, epoch 19, batch 34/481,disc_loss 77.504, (real 80.024, fake 74.984 ) gen_loss 838.39\n",
            "iteration 8893, epoch 19, batch 35/481,disc_loss 77.143, (real 80.193, fake 74.094 ) gen_loss 798.97\n",
            "iteration 8894, epoch 19, batch 36/481,disc_loss 77.348, (real 79.084, fake 75.612 ) gen_loss 880.65\n",
            "iteration 8895, epoch 19, batch 37/481,disc_loss 73.444, (real 75.185, fake 71.704 ) gen_loss 877.36\n",
            "iteration 8896, epoch 19, batch 38/481,disc_loss 76.969, (real 79.347, fake 74.59 ) gen_loss 919.36\n",
            "iteration 8897, epoch 19, batch 39/481,disc_loss 76.957, (real 79.607, fake 74.307 ) gen_loss 953.86\n",
            "iteration 8898, epoch 19, batch 40/481,disc_loss 75.944, (real 78.378, fake 73.51 ) gen_loss 843.0\n",
            "iteration 8899, epoch 19, batch 41/481,disc_loss 75.905, (real 78.292, fake 73.518 ) gen_loss 1031.8\n",
            "iteration 8900, epoch 19, batch 42/481,disc_loss 74.266, (real 76.326, fake 72.207 ) gen_loss 950.35\n",
            "iteration 8901, epoch 19, batch 43/481,disc_loss 78.339, (real 80.102, fake 76.576 ) gen_loss 849.34\n",
            "iteration 8902, epoch 19, batch 44/481,disc_loss 76.277, (real 78.707, fake 73.847 ) gen_loss 911.33\n",
            "iteration 8903, epoch 19, batch 45/481,disc_loss 77.504, (real 80.236, fake 74.771 ) gen_loss 911.62\n",
            "iteration 8904, epoch 19, batch 46/481,disc_loss 75.911, (real 77.812, fake 74.011 ) gen_loss 930.25\n",
            "iteration 8905, epoch 19, batch 47/481,disc_loss 79.473, (real 81.819, fake 77.127 ) gen_loss 879.19\n",
            "iteration 8906, epoch 19, batch 48/481,disc_loss 83.591, (real 85.681, fake 81.5 ) gen_loss 889.06\n",
            "iteration 8907, epoch 19, batch 49/481,disc_loss 73.038, (real 75.339, fake 70.736 ) gen_loss 940.0\n",
            "iteration 8908, epoch 19, batch 50/481,disc_loss 76.904, (real 77.564, fake 76.243 ) gen_loss 919.5\n",
            "iteration 8909, epoch 19, batch 51/481,disc_loss 75.51, (real 77.717, fake 73.303 ) gen_loss 837.49\n",
            "iteration 8910, epoch 19, batch 52/481,disc_loss 78.087, (real 80.166, fake 76.008 ) gen_loss 880.47\n",
            "iteration 8911, epoch 19, batch 53/481,disc_loss 75.181, (real 77.992, fake 72.37 ) gen_loss 926.48\n",
            "iteration 8912, epoch 19, batch 54/481,disc_loss 80.599, (real 82.318, fake 78.881 ) gen_loss 904.35\n",
            "iteration 8913, epoch 19, batch 55/481,disc_loss 74.843, (real 77.011, fake 72.675 ) gen_loss 839.1\n",
            "iteration 8914, epoch 19, batch 56/481,disc_loss 76.661, (real 78.377, fake 74.945 ) gen_loss 837.49\n",
            "iteration 8915, epoch 19, batch 57/481,disc_loss 72.88, (real 74.501, fake 71.259 ) gen_loss 881.0\n",
            "iteration 8916, epoch 19, batch 58/481,disc_loss 73.556, (real 76.228, fake 70.885 ) gen_loss 826.69\n",
            "iteration 8917, epoch 19, batch 59/481,disc_loss 77.187, (real 79.503, fake 74.871 ) gen_loss 894.74\n",
            "iteration 8918, epoch 19, batch 60/481,disc_loss 79.136, (real 81.169, fake 77.103 ) gen_loss 863.03\n",
            "iteration 8919, epoch 19, batch 61/481,disc_loss 75.615, (real 77.214, fake 74.016 ) gen_loss 805.64\n",
            "iteration 8920, epoch 19, batch 62/481,disc_loss 75.928, (real 78.213, fake 73.643 ) gen_loss 907.07\n",
            "iteration 8921, epoch 19, batch 63/481,disc_loss 76.681, (real 78.636, fake 74.726 ) gen_loss 855.27\n",
            "iteration 8922, epoch 19, batch 64/481,disc_loss 79.993, (real 81.301, fake 78.685 ) gen_loss 896.52\n",
            "iteration 8923, epoch 19, batch 65/481,disc_loss 76.596, (real 79.718, fake 73.475 ) gen_loss 906.42\n",
            "iteration 8924, epoch 19, batch 66/481,disc_loss 75.613, (real 76.379, fake 74.847 ) gen_loss 904.34\n",
            "iteration 8925, epoch 19, batch 67/481,disc_loss 77.02, (real 79.391, fake 74.649 ) gen_loss 820.22\n",
            "iteration 8926, epoch 19, batch 68/481,disc_loss 75.03, (real 76.821, fake 73.239 ) gen_loss 792.03\n",
            "iteration 8927, epoch 19, batch 69/481,disc_loss 75.993, (real 78.18, fake 73.805 ) gen_loss 806.77\n",
            "iteration 8928, epoch 19, batch 70/481,disc_loss 79.474, (real 81.258, fake 77.69 ) gen_loss 706.06\n",
            "iteration 8929, epoch 19, batch 71/481,disc_loss 78.117, (real 79.638, fake 76.597 ) gen_loss 936.58\n",
            "iteration 8930, epoch 19, batch 72/481,disc_loss 74.564, (real 76.39, fake 72.738 ) gen_loss 856.03\n",
            "iteration 8931, epoch 19, batch 73/481,disc_loss 78.197, (real 80.875, fake 75.52 ) gen_loss 862.81\n",
            "iteration 8932, epoch 19, batch 74/481,disc_loss 79.548, (real 81.958, fake 77.137 ) gen_loss 949.79\n",
            "iteration 8933, epoch 19, batch 75/481,disc_loss 75.006, (real 76.774, fake 73.238 ) gen_loss 858.87\n",
            "iteration 8934, epoch 19, batch 76/481,disc_loss 77.509, (real 79.914, fake 75.104 ) gen_loss 862.65\n",
            "iteration 8935, epoch 19, batch 77/481,disc_loss 82.687, (real 85.09, fake 80.285 ) gen_loss 829.36\n",
            "iteration 8936, epoch 19, batch 78/481,disc_loss 76.677, (real 78.977, fake 74.378 ) gen_loss 798.65\n",
            "iteration 8937, epoch 19, batch 79/481,disc_loss 75.903, (real 78.259, fake 73.547 ) gen_loss 796.53\n",
            "iteration 8938, epoch 19, batch 80/481,disc_loss 78.627, (real 81.447, fake 75.807 ) gen_loss 818.74\n",
            "iteration 8939, epoch 19, batch 81/481,disc_loss 75.808, (real 77.197, fake 74.419 ) gen_loss 835.04\n",
            "iteration 8940, epoch 19, batch 82/481,disc_loss 75.935, (real 77.327, fake 74.543 ) gen_loss 879.37\n",
            "iteration 8941, epoch 19, batch 83/481,disc_loss 76.728, (real 79.401, fake 74.054 ) gen_loss 890.62\n",
            "iteration 8942, epoch 19, batch 84/481,disc_loss 80.789, (real 83.161, fake 78.418 ) gen_loss 912.35\n",
            "iteration 8943, epoch 19, batch 85/481,disc_loss 77.668, (real 79.833, fake 75.502 ) gen_loss 828.16\n",
            "iteration 8944, epoch 19, batch 86/481,disc_loss 75.883, (real 78.793, fake 72.972 ) gen_loss 880.97\n",
            "iteration 8945, epoch 19, batch 87/481,disc_loss 77.06, (real 79.211, fake 74.91 ) gen_loss 803.64\n",
            "iteration 8946, epoch 19, batch 88/481,disc_loss 81.884, (real 84.224, fake 79.544 ) gen_loss 802.69\n",
            "iteration 8947, epoch 19, batch 89/481,disc_loss 77.925, (real 79.129, fake 76.722 ) gen_loss 857.64\n",
            "iteration 8948, epoch 19, batch 90/481,disc_loss 80.247, (real 83.056, fake 77.437 ) gen_loss 866.91\n",
            "iteration 8949, epoch 19, batch 91/481,disc_loss 75.286, (real 77.573, fake 73.0 ) gen_loss 826.0\n",
            "iteration 8950, epoch 19, batch 92/481,disc_loss 73.268, (real 75.463, fake 71.074 ) gen_loss 889.59\n",
            "iteration 8951, epoch 19, batch 93/481,disc_loss 78.738, (real 80.882, fake 76.594 ) gen_loss 842.75\n",
            "iteration 8952, epoch 19, batch 94/481,disc_loss 77.581, (real 78.84, fake 76.321 ) gen_loss 915.12\n",
            "iteration 8953, epoch 19, batch 95/481,disc_loss 78.507, (real 80.904, fake 76.111 ) gen_loss 843.05\n",
            "iteration 8954, epoch 19, batch 96/481,disc_loss 72.639, (real 74.328, fake 70.95 ) gen_loss 869.84\n",
            "iteration 8955, epoch 19, batch 97/481,disc_loss 81.497, (real 83.92, fake 79.075 ) gen_loss 870.67\n",
            "iteration 8956, epoch 19, batch 98/481,disc_loss 74.883, (real 77.724, fake 72.043 ) gen_loss 914.0\n",
            "iteration 8957, epoch 19, batch 99/481,disc_loss 77.241, (real 78.843, fake 75.638 ) gen_loss 795.3\n",
            "iteration 8958, epoch 19, batch 100/481,disc_loss 78.095, (real 80.151, fake 76.04 ) gen_loss 842.94\n",
            "iteration 8959, epoch 19, batch 101/481,disc_loss 77.762, (real 79.773, fake 75.752 ) gen_loss 933.35\n",
            "iteration 8960, epoch 19, batch 102/481,disc_loss 72.355, (real 74.357, fake 70.353 ) gen_loss 997.49\n",
            "iteration 8961, epoch 19, batch 103/481,disc_loss 74.737, (real 76.76, fake 72.714 ) gen_loss 840.49\n",
            "iteration 8962, epoch 19, batch 104/481,disc_loss 74.707, (real 76.266, fake 73.148 ) gen_loss 988.33\n",
            "iteration 8963, epoch 19, batch 105/481,disc_loss 75.839, (real 78.353, fake 73.325 ) gen_loss 823.67\n",
            "iteration 8964, epoch 19, batch 106/481,disc_loss 78.591, (real 81.006, fake 76.177 ) gen_loss 888.45\n",
            "iteration 8965, epoch 19, batch 107/481,disc_loss 78.689, (real 80.424, fake 76.955 ) gen_loss 844.23\n",
            "iteration 8966, epoch 19, batch 108/481,disc_loss 74.568, (real 76.743, fake 72.393 ) gen_loss 830.56\n",
            "iteration 8967, epoch 19, batch 109/481,disc_loss 73.199, (real 75.238, fake 71.16 ) gen_loss 842.46\n",
            "iteration 8968, epoch 19, batch 110/481,disc_loss 75.595, (real 77.575, fake 73.615 ) gen_loss 869.33\n",
            "iteration 8969, epoch 19, batch 111/481,disc_loss 78.228, (real 80.407, fake 76.048 ) gen_loss 864.68\n",
            "iteration 8970, epoch 19, batch 112/481,disc_loss 78.668, (real 81.443, fake 75.894 ) gen_loss 787.43\n",
            "iteration 8971, epoch 19, batch 113/481,disc_loss 73.153, (real 75.1, fake 71.207 ) gen_loss 959.83\n",
            "iteration 8972, epoch 19, batch 114/481,disc_loss 77.166, (real 79.186, fake 75.146 ) gen_loss 958.79\n",
            "iteration 8973, epoch 19, batch 115/481,disc_loss 75.143, (real 78.214, fake 72.072 ) gen_loss 870.45\n",
            "iteration 8974, epoch 19, batch 116/481,disc_loss 72.957, (real 75.456, fake 70.458 ) gen_loss 884.82\n",
            "iteration 8975, epoch 19, batch 117/481,disc_loss 75.862, (real 77.974, fake 73.75 ) gen_loss 880.44\n",
            "iteration 8976, epoch 19, batch 118/481,disc_loss 81.134, (real 83.415, fake 78.854 ) gen_loss 778.78\n",
            "iteration 8977, epoch 19, batch 119/481,disc_loss 72.838, (real 75.438, fake 70.237 ) gen_loss 823.84\n",
            "iteration 8978, epoch 19, batch 120/481,disc_loss 76.693, (real 79.0, fake 74.386 ) gen_loss 1015.0\n",
            "iteration 8979, epoch 19, batch 121/481,disc_loss 78.513, (real 79.723, fake 77.304 ) gen_loss 1106.2\n",
            "iteration 8980, epoch 19, batch 122/481,disc_loss 78.643, (real 80.622, fake 76.664 ) gen_loss 975.6\n",
            "iteration 8981, epoch 19, batch 123/481,disc_loss 77.634, (real 80.14, fake 75.128 ) gen_loss 822.46\n",
            "iteration 8982, epoch 19, batch 124/481,disc_loss 78.628, (real 81.091, fake 76.165 ) gen_loss 842.4\n",
            "iteration 8983, epoch 19, batch 125/481,disc_loss 71.588, (real 73.565, fake 69.611 ) gen_loss 831.83\n",
            "iteration 8984, epoch 19, batch 126/481,disc_loss 72.194, (real 74.022, fake 70.365 ) gen_loss 847.02\n",
            "iteration 8985, epoch 19, batch 127/481,disc_loss 79.71, (real 82.023, fake 77.397 ) gen_loss 836.38\n",
            "iteration 8986, epoch 19, batch 128/481,disc_loss 74.651, (real 76.706, fake 72.596 ) gen_loss 869.15\n",
            "iteration 8987, epoch 19, batch 129/481,disc_loss 73.643, (real 76.193, fake 71.093 ) gen_loss 904.35\n",
            "iteration 8988, epoch 19, batch 130/481,disc_loss 76.504, (real 78.624, fake 74.383 ) gen_loss 913.58\n",
            "iteration 8989, epoch 19, batch 131/481,disc_loss 75.469, (real 77.888, fake 73.05 ) gen_loss 910.85\n",
            "iteration 8990, epoch 19, batch 132/481,disc_loss 76.167, (real 78.016, fake 74.317 ) gen_loss 932.45\n",
            "iteration 8991, epoch 19, batch 133/481,disc_loss 76.167, (real 77.891, fake 74.443 ) gen_loss 912.69\n",
            "iteration 8992, epoch 19, batch 134/481,disc_loss 75.747, (real 78.226, fake 73.268 ) gen_loss 961.33\n",
            "iteration 8993, epoch 19, batch 135/481,disc_loss 77.767, (real 80.118, fake 75.415 ) gen_loss 858.42\n",
            "iteration 8994, epoch 19, batch 136/481,disc_loss 73.192, (real 75.587, fake 70.797 ) gen_loss 974.69\n",
            "iteration 8995, epoch 19, batch 137/481,disc_loss 74.978, (real 77.163, fake 72.793 ) gen_loss 896.45\n",
            "iteration 8996, epoch 19, batch 138/481,disc_loss 69.689, (real 71.876, fake 67.502 ) gen_loss 989.64\n",
            "iteration 8997, epoch 19, batch 139/481,disc_loss 74.247, (real 75.533, fake 72.961 ) gen_loss 884.73\n",
            "iteration 8998, epoch 19, batch 140/481,disc_loss 81.634, (real 83.837, fake 79.431 ) gen_loss 904.33\n",
            "iteration 8999, epoch 19, batch 141/481,disc_loss 76.684, (real 79.424, fake 73.943 ) gen_loss 719.91\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 9000, epoch 19, batch 142/481,disc_loss 74.525, (real 76.46, fake 72.589 ) gen_loss 865.89\n",
            "iteration 9001, epoch 19, batch 143/481,disc_loss 79.084, (real 81.62, fake 76.548 ) gen_loss 823.42\n",
            "iteration 9002, epoch 19, batch 144/481,disc_loss 78.211, (real 80.44, fake 75.982 ) gen_loss 906.33\n",
            "iteration 9003, epoch 19, batch 145/481,disc_loss 76.117, (real 78.529, fake 73.705 ) gen_loss 879.05\n",
            "iteration 9004, epoch 19, batch 146/481,disc_loss 72.255, (real 74.004, fake 70.506 ) gen_loss 972.39\n",
            "iteration 9005, epoch 19, batch 147/481,disc_loss 77.378, (real 79.663, fake 75.092 ) gen_loss 868.87\n",
            "iteration 9006, epoch 19, batch 148/481,disc_loss 77.963, (real 80.909, fake 75.017 ) gen_loss 962.41\n",
            "iteration 9007, epoch 19, batch 149/481,disc_loss 73.631, (real 75.716, fake 71.546 ) gen_loss 1042.6\n",
            "iteration 9008, epoch 19, batch 150/481,disc_loss 74.75, (real 77.088, fake 72.413 ) gen_loss 857.46\n",
            "iteration 9009, epoch 19, batch 151/481,disc_loss 76.592, (real 78.007, fake 75.177 ) gen_loss 938.34\n",
            "iteration 9010, epoch 19, batch 152/481,disc_loss 76.982, (real 79.283, fake 74.68 ) gen_loss 825.63\n",
            "iteration 9011, epoch 19, batch 153/481,disc_loss 79.018, (real 80.926, fake 77.11 ) gen_loss 805.82\n",
            "iteration 9012, epoch 19, batch 154/481,disc_loss 75.895, (real 78.738, fake 73.052 ) gen_loss 932.29\n",
            "iteration 9013, epoch 19, batch 155/481,disc_loss 73.788, (real 75.638, fake 71.938 ) gen_loss 850.03\n",
            "iteration 9014, epoch 19, batch 156/481,disc_loss 73.542, (real 75.344, fake 71.74 ) gen_loss 824.81\n",
            "iteration 9015, epoch 19, batch 157/481,disc_loss 78.274, (real 79.97, fake 76.578 ) gen_loss 882.75\n",
            "iteration 9016, epoch 19, batch 158/481,disc_loss 79.188, (real 81.345, fake 77.031 ) gen_loss 767.65\n",
            "iteration 9017, epoch 19, batch 159/481,disc_loss 75.377, (real 77.442, fake 73.312 ) gen_loss 860.7\n",
            "iteration 9018, epoch 19, batch 160/481,disc_loss 78.785, (real 80.598, fake 76.972 ) gen_loss 890.0\n",
            "iteration 9019, epoch 19, batch 161/481,disc_loss 74.732, (real 77.107, fake 72.357 ) gen_loss 909.58\n",
            "iteration 9020, epoch 19, batch 162/481,disc_loss 77.588, (real 80.197, fake 74.978 ) gen_loss 884.94\n",
            "iteration 9021, epoch 19, batch 163/481,disc_loss 76.572, (real 78.473, fake 74.67 ) gen_loss 907.6\n",
            "iteration 9022, epoch 19, batch 164/481,disc_loss 79.321, (real 81.53, fake 77.113 ) gen_loss 831.8\n",
            "iteration 9023, epoch 19, batch 165/481,disc_loss 81.07, (real 83.261, fake 78.879 ) gen_loss 730.31\n",
            "iteration 9024, epoch 19, batch 166/481,disc_loss 78.79, (real 81.079, fake 76.502 ) gen_loss 832.25\n",
            "iteration 9025, epoch 19, batch 167/481,disc_loss 77.916, (real 79.813, fake 76.019 ) gen_loss 850.55\n",
            "iteration 9026, epoch 19, batch 168/481,disc_loss 78.424, (real 80.142, fake 76.707 ) gen_loss 815.83\n",
            "iteration 9027, epoch 19, batch 169/481,disc_loss 74.669, (real 76.788, fake 72.551 ) gen_loss 830.85\n",
            "iteration 9028, epoch 19, batch 170/481,disc_loss 79.783, (real 82.024, fake 77.541 ) gen_loss 903.8\n",
            "iteration 9029, epoch 19, batch 171/481,disc_loss 78.987, (real 81.575, fake 76.4 ) gen_loss 907.16\n",
            "iteration 9030, epoch 19, batch 172/481,disc_loss 79.293, (real 81.876, fake 76.709 ) gen_loss 854.16\n",
            "iteration 9031, epoch 19, batch 173/481,disc_loss 78.979, (real 81.511, fake 76.448 ) gen_loss 835.58\n",
            "iteration 9032, epoch 19, batch 174/481,disc_loss 79.078, (real 81.108, fake 77.049 ) gen_loss 877.57\n",
            "iteration 9033, epoch 19, batch 175/481,disc_loss 75.264, (real 77.284, fake 73.245 ) gen_loss 776.6\n",
            "iteration 9034, epoch 19, batch 176/481,disc_loss 72.122, (real 74.546, fake 69.697 ) gen_loss 914.79\n",
            "iteration 9035, epoch 19, batch 177/481,disc_loss 71.595, (real 74.223, fake 68.967 ) gen_loss 862.22\n",
            "iteration 9036, epoch 19, batch 178/481,disc_loss 81.956, (real 85.842, fake 78.07 ) gen_loss 850.44\n",
            "iteration 9037, epoch 19, batch 179/481,disc_loss 75.924, (real 78.476, fake 73.373 ) gen_loss 914.57\n",
            "iteration 9038, epoch 19, batch 180/481,disc_loss 75.356, (real 77.677, fake 73.036 ) gen_loss 869.22\n",
            "iteration 9039, epoch 19, batch 181/481,disc_loss 79.92, (real 82.168, fake 77.672 ) gen_loss 884.34\n",
            "iteration 9040, epoch 19, batch 182/481,disc_loss 76.612, (real 78.808, fake 74.415 ) gen_loss 938.21\n",
            "iteration 9041, epoch 19, batch 183/481,disc_loss 74.975, (real 76.757, fake 73.193 ) gen_loss 948.85\n",
            "iteration 9042, epoch 19, batch 184/481,disc_loss 76.814, (real 79.584, fake 74.044 ) gen_loss 834.94\n",
            "iteration 9043, epoch 19, batch 185/481,disc_loss 77.191, (real 79.668, fake 74.714 ) gen_loss 858.87\n",
            "iteration 9044, epoch 19, batch 186/481,disc_loss 75.295, (real 77.848, fake 72.742 ) gen_loss 931.63\n",
            "iteration 9045, epoch 19, batch 187/481,disc_loss 80.219, (real 82.426, fake 78.012 ) gen_loss 849.47\n",
            "iteration 9046, epoch 19, batch 188/481,disc_loss 78.421, (real 80.798, fake 76.044 ) gen_loss 931.98\n",
            "iteration 9047, epoch 19, batch 189/481,disc_loss 77.896, (real 80.288, fake 75.503 ) gen_loss 916.19\n",
            "iteration 9048, epoch 19, batch 190/481,disc_loss 79.277, (real 81.178, fake 77.375 ) gen_loss 932.6\n",
            "iteration 9049, epoch 19, batch 191/481,disc_loss 77.336, (real 79.362, fake 75.31 ) gen_loss 839.12\n",
            "iteration 9050, epoch 19, batch 192/481,disc_loss 76.425, (real 79.5, fake 73.35 ) gen_loss 777.59\n",
            "iteration 9051, epoch 19, batch 193/481,disc_loss 79.81, (real 82.499, fake 77.121 ) gen_loss 931.53\n",
            "iteration 9052, epoch 19, batch 194/481,disc_loss 78.497, (real 80.816, fake 76.178 ) gen_loss 984.49\n",
            "iteration 9053, epoch 19, batch 195/481,disc_loss 75.324, (real 77.44, fake 73.209 ) gen_loss 900.49\n",
            "iteration 9054, epoch 19, batch 196/481,disc_loss 75.817, (real 78.132, fake 73.503 ) gen_loss 853.32\n",
            "iteration 9055, epoch 19, batch 197/481,disc_loss 73.728, (real 76.026, fake 71.429 ) gen_loss 892.36\n",
            "iteration 9056, epoch 19, batch 198/481,disc_loss 81.014, (real 83.548, fake 78.479 ) gen_loss 976.6\n",
            "iteration 9057, epoch 19, batch 199/481,disc_loss 74.905, (real 77.314, fake 72.496 ) gen_loss 895.2\n",
            "iteration 9058, epoch 19, batch 200/481,disc_loss 81.575, (real 84.249, fake 78.902 ) gen_loss 925.53\n",
            "iteration 9059, epoch 19, batch 201/481,disc_loss 74.773, (real 77.273, fake 72.273 ) gen_loss 893.78\n",
            "iteration 9060, epoch 19, batch 202/481,disc_loss 80.555, (real 82.303, fake 78.808 ) gen_loss 885.15\n",
            "iteration 9061, epoch 19, batch 203/481,disc_loss 77.135, (real 79.298, fake 74.971 ) gen_loss 841.16\n",
            "iteration 9062, epoch 19, batch 204/481,disc_loss 79.631, (real 82.035, fake 77.228 ) gen_loss 857.65\n",
            "iteration 9063, epoch 19, batch 205/481,disc_loss 77.191, (real 79.556, fake 74.827 ) gen_loss 904.81\n",
            "iteration 9064, epoch 19, batch 206/481,disc_loss 75.284, (real 77.664, fake 72.904 ) gen_loss 926.86\n",
            "iteration 9065, epoch 19, batch 207/481,disc_loss 77.194, (real 79.404, fake 74.984 ) gen_loss 961.51\n",
            "iteration 9066, epoch 19, batch 208/481,disc_loss 77.816, (real 79.966, fake 75.667 ) gen_loss 903.88\n",
            "iteration 9067, epoch 19, batch 209/481,disc_loss 78.699, (real 80.493, fake 76.904 ) gen_loss 809.88\n",
            "iteration 9068, epoch 19, batch 210/481,disc_loss 77.233, (real 78.978, fake 75.489 ) gen_loss 888.95\n",
            "iteration 9069, epoch 19, batch 211/481,disc_loss 79.402, (real 81.378, fake 77.425 ) gen_loss 872.53\n",
            "iteration 9070, epoch 19, batch 212/481,disc_loss 76.469, (real 77.851, fake 75.088 ) gen_loss 900.59\n",
            "iteration 9071, epoch 19, batch 213/481,disc_loss 75.554, (real 77.366, fake 73.743 ) gen_loss 841.28\n",
            "iteration 9072, epoch 19, batch 214/481,disc_loss 75.675, (real 78.752, fake 72.597 ) gen_loss 921.98\n",
            "iteration 9073, epoch 19, batch 215/481,disc_loss 78.766, (real 80.948, fake 76.584 ) gen_loss 839.7\n",
            "iteration 9074, epoch 19, batch 216/481,disc_loss 80.75, (real 83.303, fake 78.197 ) gen_loss 746.87\n",
            "iteration 9075, epoch 19, batch 217/481,disc_loss 80.08, (real 83.403, fake 76.758 ) gen_loss 808.81\n",
            "iteration 9076, epoch 19, batch 218/481,disc_loss 79.566, (real 81.484, fake 77.649 ) gen_loss 884.74\n",
            "iteration 9077, epoch 19, batch 219/481,disc_loss 76.807, (real 79.108, fake 74.507 ) gen_loss 814.71\n",
            "iteration 9078, epoch 19, batch 220/481,disc_loss 78.669, (real 80.548, fake 76.79 ) gen_loss 876.02\n",
            "iteration 9079, epoch 19, batch 221/481,disc_loss 77.685, (real 79.299, fake 76.07 ) gen_loss 842.66\n",
            "iteration 9080, epoch 19, batch 222/481,disc_loss 77.174, (real 78.911, fake 75.437 ) gen_loss 878.64\n",
            "iteration 9081, epoch 19, batch 223/481,disc_loss 82.163, (real 84.111, fake 80.214 ) gen_loss 837.75\n",
            "iteration 9082, epoch 19, batch 224/481,disc_loss 73.63, (real 76.377, fake 70.882 ) gen_loss 1023.2\n",
            "iteration 9083, epoch 19, batch 225/481,disc_loss 78.794, (real 79.879, fake 77.708 ) gen_loss 865.27\n",
            "iteration 9084, epoch 19, batch 226/481,disc_loss 74.873, (real 76.175, fake 73.57 ) gen_loss 819.07\n",
            "iteration 9085, epoch 19, batch 227/481,disc_loss 81.533, (real 83.422, fake 79.644 ) gen_loss 839.06\n",
            "iteration 9086, epoch 19, batch 228/481,disc_loss 77.673, (real 79.342, fake 76.004 ) gen_loss 836.39\n",
            "iteration 9087, epoch 19, batch 229/481,disc_loss 77.52, (real 79.781, fake 75.26 ) gen_loss 815.49\n",
            "iteration 9088, epoch 19, batch 230/481,disc_loss 75.42, (real 77.865, fake 72.974 ) gen_loss 873.31\n",
            "iteration 9089, epoch 19, batch 231/481,disc_loss 83.046, (real 85.482, fake 80.609 ) gen_loss 886.53\n",
            "iteration 9090, epoch 19, batch 232/481,disc_loss 76.72, (real 79.626, fake 73.813 ) gen_loss 902.48\n",
            "iteration 9091, epoch 19, batch 233/481,disc_loss 80.264, (real 82.154, fake 78.373 ) gen_loss 790.56\n",
            "iteration 9092, epoch 19, batch 234/481,disc_loss 77.042, (real 79.381, fake 74.703 ) gen_loss 891.11\n",
            "iteration 9093, epoch 19, batch 235/481,disc_loss 73.856, (real 75.707, fake 72.005 ) gen_loss 822.99\n",
            "iteration 9094, epoch 19, batch 236/481,disc_loss 76.827, (real 79.201, fake 74.453 ) gen_loss 862.81\n",
            "iteration 9095, epoch 19, batch 237/481,disc_loss 78.609, (real 80.525, fake 76.693 ) gen_loss 894.27\n",
            "iteration 9096, epoch 19, batch 238/481,disc_loss 74.572, (real 76.869, fake 72.274 ) gen_loss 912.08\n",
            "iteration 9097, epoch 19, batch 239/481,disc_loss 79.775, (real 82.304, fake 77.246 ) gen_loss 845.93\n",
            "iteration 9098, epoch 19, batch 240/481,disc_loss 77.861, (real 80.024, fake 75.698 ) gen_loss 851.68\n",
            "iteration 9099, epoch 19, batch 241/481,disc_loss 76.853, (real 79.049, fake 74.657 ) gen_loss 918.09\n",
            "iteration 9100, epoch 19, batch 242/481,disc_loss 74.411, (real 76.589, fake 72.233 ) gen_loss 931.77\n",
            "iteration 9101, epoch 19, batch 243/481,disc_loss 73.496, (real 76.163, fake 70.828 ) gen_loss 904.08\n",
            "iteration 9102, epoch 19, batch 244/481,disc_loss 75.744, (real 77.959, fake 73.529 ) gen_loss 946.33\n",
            "iteration 9103, epoch 19, batch 245/481,disc_loss 78.182, (real 80.257, fake 76.106 ) gen_loss 878.65\n",
            "iteration 9104, epoch 19, batch 246/481,disc_loss 74.116, (real 76.307, fake 71.924 ) gen_loss 909.07\n",
            "iteration 9105, epoch 19, batch 247/481,disc_loss 81.326, (real 83.866, fake 78.786 ) gen_loss 863.19\n",
            "iteration 9106, epoch 19, batch 248/481,disc_loss 75.661, (real 77.833, fake 73.49 ) gen_loss 934.14\n",
            "iteration 9107, epoch 19, batch 249/481,disc_loss 73.6, (real 75.555, fake 71.646 ) gen_loss 823.85\n",
            "iteration 9108, epoch 19, batch 250/481,disc_loss 70.878, (real 72.409, fake 69.347 ) gen_loss 809.39\n",
            "iteration 9109, epoch 19, batch 251/481,disc_loss 73.596, (real 75.417, fake 71.775 ) gen_loss 840.14\n",
            "iteration 9110, epoch 19, batch 252/481,disc_loss 77.983, (real 80.677, fake 75.289 ) gen_loss 931.17\n",
            "iteration 9111, epoch 19, batch 253/481,disc_loss 73.128, (real 76.022, fake 70.233 ) gen_loss 932.01\n",
            "iteration 9112, epoch 19, batch 254/481,disc_loss 74.941, (real 77.009, fake 72.873 ) gen_loss 857.6\n",
            "iteration 9113, epoch 19, batch 255/481,disc_loss 73.422, (real 75.38, fake 71.465 ) gen_loss 801.71\n",
            "iteration 9114, epoch 19, batch 256/481,disc_loss 78.803, (real 80.443, fake 77.163 ) gen_loss 828.91\n",
            "iteration 9115, epoch 19, batch 257/481,disc_loss 75.373, (real 77.787, fake 72.959 ) gen_loss 948.38\n",
            "iteration 9116, epoch 19, batch 258/481,disc_loss 77.073, (real 79.549, fake 74.598 ) gen_loss 955.47\n",
            "iteration 9117, epoch 19, batch 259/481,disc_loss 81.051, (real 83.306, fake 78.797 ) gen_loss 843.33\n",
            "iteration 9118, epoch 19, batch 260/481,disc_loss 74.607, (real 76.888, fake 72.327 ) gen_loss 840.9\n",
            "iteration 9119, epoch 19, batch 261/481,disc_loss 78.713, (real 81.093, fake 76.333 ) gen_loss 828.3\n",
            "iteration 9120, epoch 19, batch 262/481,disc_loss 75.55, (real 77.926, fake 73.173 ) gen_loss 822.13\n",
            "iteration 9121, epoch 19, batch 263/481,disc_loss 77.55, (real 79.509, fake 75.592 ) gen_loss 784.51\n",
            "iteration 9122, epoch 19, batch 264/481,disc_loss 76.683, (real 78.732, fake 74.635 ) gen_loss 840.57\n",
            "iteration 9123, epoch 19, batch 265/481,disc_loss 77.618, (real 79.609, fake 75.628 ) gen_loss 885.25\n",
            "iteration 9124, epoch 19, batch 266/481,disc_loss 72.908, (real 75.557, fake 70.259 ) gen_loss 827.84\n",
            "iteration 9125, epoch 19, batch 267/481,disc_loss 78.021, (real 80.776, fake 75.266 ) gen_loss 904.97\n",
            "iteration 9126, epoch 19, batch 268/481,disc_loss 72.645, (real 75.684, fake 69.607 ) gen_loss 819.47\n",
            "iteration 9127, epoch 19, batch 269/481,disc_loss 79.69, (real 82.718, fake 76.663 ) gen_loss 843.53\n",
            "iteration 9128, epoch 19, batch 270/481,disc_loss 77.134, (real 78.997, fake 75.271 ) gen_loss 865.61\n",
            "iteration 9129, epoch 19, batch 271/481,disc_loss 74.253, (real 75.629, fake 72.877 ) gen_loss 927.79\n",
            "iteration 9130, epoch 19, batch 272/481,disc_loss 76.386, (real 79.466, fake 73.307 ) gen_loss 809.12\n",
            "iteration 9131, epoch 19, batch 273/481,disc_loss 76.502, (real 78.622, fake 74.381 ) gen_loss 888.75\n",
            "iteration 9132, epoch 19, batch 274/481,disc_loss 77.353, (real 79.136, fake 75.569 ) gen_loss 903.45\n",
            "iteration 9133, epoch 19, batch 275/481,disc_loss 75.613, (real 77.816, fake 73.41 ) gen_loss 912.49\n",
            "iteration 9134, epoch 19, batch 276/481,disc_loss 75.355, (real 78.051, fake 72.659 ) gen_loss 889.67\n",
            "iteration 9135, epoch 19, batch 277/481,disc_loss 74.981, (real 77.294, fake 72.667 ) gen_loss 934.84\n",
            "iteration 9136, epoch 19, batch 278/481,disc_loss 77.839, (real 81.014, fake 74.664 ) gen_loss 861.31\n",
            "iteration 9137, epoch 19, batch 279/481,disc_loss 79.716, (real 81.986, fake 77.445 ) gen_loss 889.76\n",
            "iteration 9138, epoch 19, batch 280/481,disc_loss 75.232, (real 78.012, fake 72.452 ) gen_loss 948.46\n",
            "iteration 9139, epoch 19, batch 281/481,disc_loss 79.692, (real 82.669, fake 76.716 ) gen_loss 872.27\n",
            "iteration 9140, epoch 19, batch 282/481,disc_loss 78.091, (real 79.981, fake 76.201 ) gen_loss 897.71\n",
            "iteration 9141, epoch 19, batch 283/481,disc_loss 80.545, (real 82.496, fake 78.594 ) gen_loss 869.41\n",
            "iteration 9142, epoch 19, batch 284/481,disc_loss 76.954, (real 79.221, fake 74.686 ) gen_loss 949.0\n",
            "iteration 9143, epoch 19, batch 285/481,disc_loss 78.114, (real 80.569, fake 75.66 ) gen_loss 960.69\n",
            "iteration 9144, epoch 19, batch 286/481,disc_loss 82.845, (real 85.672, fake 80.018 ) gen_loss 860.14\n",
            "iteration 9145, epoch 19, batch 287/481,disc_loss 76.407, (real 79.665, fake 73.15 ) gen_loss 990.19\n",
            "iteration 9146, epoch 19, batch 288/481,disc_loss 78.925, (real 81.378, fake 76.472 ) gen_loss 971.25\n",
            "iteration 9147, epoch 19, batch 289/481,disc_loss 76.847, (real 79.498, fake 74.197 ) gen_loss 870.11\n",
            "iteration 9148, epoch 19, batch 290/481,disc_loss 74.608, (real 77.272, fake 71.943 ) gen_loss 881.21\n",
            "iteration 9149, epoch 19, batch 291/481,disc_loss 72.462, (real 74.327, fake 70.597 ) gen_loss 826.0\n",
            "iteration 9150, epoch 19, batch 292/481,disc_loss 78.843, (real 81.04, fake 76.646 ) gen_loss 837.79\n",
            "iteration 9151, epoch 19, batch 293/481,disc_loss 77.565, (real 79.732, fake 75.398 ) gen_loss 894.94\n",
            "iteration 9152, epoch 19, batch 294/481,disc_loss 75.103, (real 77.514, fake 72.692 ) gen_loss 842.77\n",
            "iteration 9153, epoch 19, batch 295/481,disc_loss 77.514, (real 79.497, fake 75.532 ) gen_loss 913.3\n",
            "iteration 9154, epoch 19, batch 296/481,disc_loss 74.802, (real 77.046, fake 72.559 ) gen_loss 808.0\n",
            "iteration 9155, epoch 19, batch 297/481,disc_loss 77.439, (real 80.384, fake 74.495 ) gen_loss 840.61\n",
            "iteration 9156, epoch 19, batch 298/481,disc_loss 75.243, (real 77.834, fake 72.651 ) gen_loss 877.05\n",
            "iteration 9157, epoch 19, batch 299/481,disc_loss 75.942, (real 78.057, fake 73.826 ) gen_loss 981.06\n",
            "iteration 9158, epoch 19, batch 300/481,disc_loss 71.224, (real 73.699, fake 68.75 ) gen_loss 847.14\n",
            "iteration 9159, epoch 19, batch 301/481,disc_loss 74.703, (real 76.907, fake 72.498 ) gen_loss 824.81\n",
            "iteration 9160, epoch 19, batch 302/481,disc_loss 74.982, (real 76.936, fake 73.027 ) gen_loss 858.08\n",
            "iteration 9161, epoch 19, batch 303/481,disc_loss 80.221, (real 82.419, fake 78.023 ) gen_loss 911.84\n",
            "iteration 9162, epoch 19, batch 304/481,disc_loss 75.945, (real 78.22, fake 73.67 ) gen_loss 812.41\n",
            "iteration 9163, epoch 19, batch 305/481,disc_loss 77.6, (real 80.137, fake 75.063 ) gen_loss 846.48\n",
            "iteration 9164, epoch 19, batch 306/481,disc_loss 72.272, (real 74.254, fake 70.29 ) gen_loss 936.81\n",
            "iteration 9165, epoch 19, batch 307/481,disc_loss 80.448, (real 82.996, fake 77.9 ) gen_loss 876.84\n",
            "iteration 9166, epoch 19, batch 308/481,disc_loss 79.307, (real 81.811, fake 76.803 ) gen_loss 917.72\n",
            "iteration 9167, epoch 19, batch 309/481,disc_loss 76.353, (real 79.004, fake 73.702 ) gen_loss 830.55\n",
            "iteration 9168, epoch 19, batch 310/481,disc_loss 75.647, (real 78.073, fake 73.22 ) gen_loss 826.42\n",
            "iteration 9169, epoch 19, batch 311/481,disc_loss 77.171, (real 79.927, fake 74.415 ) gen_loss 838.15\n",
            "iteration 9170, epoch 19, batch 312/481,disc_loss 75.12, (real 77.355, fake 72.884 ) gen_loss 926.74\n",
            "iteration 9171, epoch 19, batch 313/481,disc_loss 75.811, (real 78.286, fake 73.335 ) gen_loss 856.01\n",
            "iteration 9172, epoch 19, batch 314/481,disc_loss 79.48, (real 81.641, fake 77.319 ) gen_loss 814.77\n",
            "iteration 9173, epoch 19, batch 315/481,disc_loss 78.024, (real 80.402, fake 75.646 ) gen_loss 779.33\n",
            "iteration 9174, epoch 19, batch 316/481,disc_loss 77.674, (real 79.672, fake 75.676 ) gen_loss 817.57\n",
            "iteration 9175, epoch 19, batch 317/481,disc_loss 79.671, (real 81.894, fake 77.448 ) gen_loss 770.17\n",
            "iteration 9176, epoch 19, batch 318/481,disc_loss 72.975, (real 75.136, fake 70.814 ) gen_loss 815.95\n",
            "iteration 9177, epoch 19, batch 319/481,disc_loss 73.968, (real 76.255, fake 71.681 ) gen_loss 905.86\n",
            "iteration 9178, epoch 19, batch 320/481,disc_loss 73.948, (real 76.054, fake 71.842 ) gen_loss 898.06\n",
            "iteration 9179, epoch 19, batch 321/481,disc_loss 77.711, (real 80.607, fake 74.815 ) gen_loss 781.56\n",
            "iteration 9180, epoch 19, batch 322/481,disc_loss 78.806, (real 81.198, fake 76.414 ) gen_loss 827.99\n",
            "iteration 9181, epoch 19, batch 323/481,disc_loss 77.196, (real 79.697, fake 74.694 ) gen_loss 831.05\n",
            "iteration 9182, epoch 19, batch 324/481,disc_loss 74.881, (real 77.128, fake 72.635 ) gen_loss 848.35\n",
            "iteration 9183, epoch 19, batch 325/481,disc_loss 73.329, (real 75.93, fake 70.728 ) gen_loss 852.35\n",
            "iteration 9184, epoch 19, batch 326/481,disc_loss 79.588, (real 82.427, fake 76.75 ) gen_loss 924.52\n",
            "iteration 9185, epoch 19, batch 327/481,disc_loss 80.536, (real 82.245, fake 78.828 ) gen_loss 871.68\n",
            "iteration 9186, epoch 19, batch 328/481,disc_loss 76.277, (real 78.923, fake 73.63 ) gen_loss 885.17\n",
            "iteration 9187, epoch 19, batch 329/481,disc_loss 77.473, (real 79.587, fake 75.359 ) gen_loss 943.13\n",
            "iteration 9188, epoch 19, batch 330/481,disc_loss 78.05, (real 80.856, fake 75.244 ) gen_loss 857.11\n",
            "iteration 9189, epoch 19, batch 331/481,disc_loss 79.017, (real 81.892, fake 76.143 ) gen_loss 851.51\n",
            "iteration 9190, epoch 19, batch 332/481,disc_loss 74.555, (real 76.576, fake 72.534 ) gen_loss 757.59\n",
            "iteration 9191, epoch 19, batch 333/481,disc_loss 76.374, (real 78.125, fake 74.623 ) gen_loss 884.91\n",
            "iteration 9192, epoch 19, batch 334/481,disc_loss 76.564, (real 78.388, fake 74.739 ) gen_loss 874.51\n",
            "iteration 9193, epoch 19, batch 335/481,disc_loss 73.891, (real 76.003, fake 71.78 ) gen_loss 899.06\n",
            "iteration 9194, epoch 19, batch 336/481,disc_loss 74.061, (real 75.77, fake 72.353 ) gen_loss 935.26\n",
            "iteration 9195, epoch 19, batch 337/481,disc_loss 74.945, (real 77.364, fake 72.526 ) gen_loss 860.09\n",
            "iteration 9196, epoch 19, batch 338/481,disc_loss 77.172, (real 79.614, fake 74.731 ) gen_loss 851.56\n",
            "iteration 9197, epoch 19, batch 339/481,disc_loss 76.39, (real 78.514, fake 74.265 ) gen_loss 875.23\n",
            "iteration 9198, epoch 19, batch 340/481,disc_loss 79.692, (real 81.751, fake 77.633 ) gen_loss 852.97\n",
            "iteration 9199, epoch 19, batch 341/481,disc_loss 78.297, (real 80.53, fake 76.064 ) gen_loss 811.17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 9200, epoch 19, batch 342/481,disc_loss 75.873, (real 77.626, fake 74.12 ) gen_loss 889.87\n",
            "iteration 9201, epoch 19, batch 343/481,disc_loss 76.649, (real 78.284, fake 75.014 ) gen_loss 863.25\n",
            "iteration 9202, epoch 19, batch 344/481,disc_loss 79.083, (real 81.525, fake 76.641 ) gen_loss 831.2\n",
            "iteration 9203, epoch 19, batch 345/481,disc_loss 79.564, (real 81.617, fake 77.511 ) gen_loss 894.9\n",
            "iteration 9204, epoch 19, batch 346/481,disc_loss 76.307, (real 79.249, fake 73.364 ) gen_loss 895.33\n",
            "iteration 9205, epoch 19, batch 347/481,disc_loss 80.945, (real 83.399, fake 78.49 ) gen_loss 936.4\n",
            "iteration 9206, epoch 19, batch 348/481,disc_loss 77.435, (real 79.612, fake 75.259 ) gen_loss 889.13\n",
            "iteration 9207, epoch 19, batch 349/481,disc_loss 78.867, (real 81.508, fake 76.227 ) gen_loss 863.76\n",
            "iteration 9208, epoch 19, batch 350/481,disc_loss 74.769, (real 77.543, fake 71.994 ) gen_loss 865.02\n",
            "iteration 9209, epoch 19, batch 351/481,disc_loss 75.819, (real 78.077, fake 73.561 ) gen_loss 891.62\n",
            "iteration 9210, epoch 19, batch 352/481,disc_loss 77.79, (real 79.681, fake 75.9 ) gen_loss 897.81\n",
            "iteration 9211, epoch 19, batch 353/481,disc_loss 74.126, (real 76.556, fake 71.696 ) gen_loss 909.43\n",
            "iteration 9212, epoch 19, batch 354/481,disc_loss 78.957, (real 81.764, fake 76.15 ) gen_loss 912.84\n",
            "iteration 9213, epoch 19, batch 355/481,disc_loss 77.873, (real 80.235, fake 75.512 ) gen_loss 865.92\n",
            "iteration 9214, epoch 19, batch 356/481,disc_loss 76.434, (real 79.175, fake 73.694 ) gen_loss 1071.0\n",
            "iteration 9215, epoch 19, batch 357/481,disc_loss 77.072, (real 79.084, fake 75.059 ) gen_loss 973.45\n",
            "iteration 9216, epoch 19, batch 358/481,disc_loss 76.978, (real 79.774, fake 74.181 ) gen_loss 925.1\n",
            "iteration 9217, epoch 19, batch 359/481,disc_loss 80.13, (real 82.572, fake 77.689 ) gen_loss 906.09\n",
            "iteration 9218, epoch 19, batch 360/481,disc_loss 72.311, (real 73.981, fake 70.64 ) gen_loss 818.09\n",
            "iteration 9219, epoch 19, batch 361/481,disc_loss 75.672, (real 77.363, fake 73.98 ) gen_loss 994.21\n",
            "iteration 9220, epoch 19, batch 362/481,disc_loss 77.411, (real 79.959, fake 74.864 ) gen_loss 957.48\n",
            "iteration 9221, epoch 19, batch 363/481,disc_loss 79.426, (real 81.719, fake 77.132 ) gen_loss 891.43\n",
            "iteration 9222, epoch 19, batch 364/481,disc_loss 76.377, (real 78.621, fake 74.132 ) gen_loss 906.93\n",
            "iteration 9223, epoch 19, batch 365/481,disc_loss 76.96, (real 79.026, fake 74.894 ) gen_loss 889.35\n",
            "iteration 9224, epoch 19, batch 366/481,disc_loss 76.453, (real 79.554, fake 73.353 ) gen_loss 900.23\n",
            "iteration 9225, epoch 19, batch 367/481,disc_loss 80.296, (real 81.786, fake 78.806 ) gen_loss 913.44\n",
            "iteration 9226, epoch 19, batch 368/481,disc_loss 73.684, (real 75.588, fake 71.779 ) gen_loss 986.72\n",
            "iteration 9227, epoch 19, batch 369/481,disc_loss 72.119, (real 74.488, fake 69.75 ) gen_loss 994.12\n",
            "iteration 9228, epoch 19, batch 370/481,disc_loss 80.013, (real 83.334, fake 76.692 ) gen_loss 964.55\n",
            "iteration 9229, epoch 19, batch 371/481,disc_loss 76.703, (real 78.97, fake 74.435 ) gen_loss 927.86\n",
            "iteration 9230, epoch 19, batch 372/481,disc_loss 79.004, (real 81.334, fake 76.675 ) gen_loss 872.32\n",
            "iteration 9231, epoch 19, batch 373/481,disc_loss 76.759, (real 78.286, fake 75.232 ) gen_loss 933.1\n",
            "iteration 9232, epoch 19, batch 374/481,disc_loss 74.733, (real 77.26, fake 72.207 ) gen_loss 899.49\n",
            "iteration 9233, epoch 19, batch 375/481,disc_loss 74.274, (real 76.936, fake 71.612 ) gen_loss 932.13\n",
            "iteration 9234, epoch 19, batch 376/481,disc_loss 81.563, (real 84.34, fake 78.786 ) gen_loss 909.04\n",
            "iteration 9235, epoch 19, batch 377/481,disc_loss 70.877, (real 73.456, fake 68.298 ) gen_loss 896.39\n",
            "iteration 9236, epoch 19, batch 378/481,disc_loss 74.797, (real 76.242, fake 73.351 ) gen_loss 879.78\n",
            "iteration 9237, epoch 19, batch 379/481,disc_loss 72.662, (real 74.705, fake 70.618 ) gen_loss 976.16\n",
            "iteration 9238, epoch 19, batch 380/481,disc_loss 75.899, (real 78.489, fake 73.309 ) gen_loss 916.58\n",
            "iteration 9239, epoch 19, batch 381/481,disc_loss 74.602, (real 76.561, fake 72.643 ) gen_loss 1005.7\n",
            "iteration 9240, epoch 19, batch 382/481,disc_loss 78.796, (real 81.598, fake 75.994 ) gen_loss 890.06\n",
            "iteration 9241, epoch 19, batch 383/481,disc_loss 73.842, (real 75.713, fake 71.971 ) gen_loss 902.0\n",
            "iteration 9242, epoch 19, batch 384/481,disc_loss 74.354, (real 76.46, fake 72.249 ) gen_loss 876.47\n",
            "iteration 9243, epoch 19, batch 385/481,disc_loss 74.809, (real 77.004, fake 72.615 ) gen_loss 879.18\n",
            "iteration 9244, epoch 19, batch 386/481,disc_loss 76.332, (real 78.941, fake 73.723 ) gen_loss 928.21\n",
            "iteration 9245, epoch 19, batch 387/481,disc_loss 79.179, (real 81.31, fake 77.048 ) gen_loss 827.25\n",
            "iteration 9246, epoch 19, batch 388/481,disc_loss 78.847, (real 80.802, fake 76.892 ) gen_loss 910.15\n",
            "iteration 9247, epoch 19, batch 389/481,disc_loss 77.424, (real 80.042, fake 74.806 ) gen_loss 1089.0\n",
            "iteration 9248, epoch 19, batch 390/481,disc_loss 72.687, (real 74.362, fake 71.011 ) gen_loss 1106.6\n",
            "iteration 9249, epoch 19, batch 391/481,disc_loss 76.89, (real 79.307, fake 74.473 ) gen_loss 931.15\n",
            "iteration 9250, epoch 19, batch 392/481,disc_loss 75.023, (real 76.947, fake 73.098 ) gen_loss 937.03\n",
            "iteration 9251, epoch 19, batch 393/481,disc_loss 76.926, (real 79.526, fake 74.327 ) gen_loss 932.03\n",
            "iteration 9252, epoch 19, batch 394/481,disc_loss 74.234, (real 77.408, fake 71.06 ) gen_loss 880.69\n",
            "iteration 9253, epoch 19, batch 395/481,disc_loss 77.995, (real 80.036, fake 75.955 ) gen_loss 852.6\n",
            "iteration 9254, epoch 19, batch 396/481,disc_loss 73.815, (real 76.046, fake 71.584 ) gen_loss 878.69\n",
            "iteration 9255, epoch 19, batch 397/481,disc_loss 75.949, (real 78.354, fake 73.545 ) gen_loss 930.28\n",
            "iteration 9256, epoch 19, batch 398/481,disc_loss 78.582, (real 81.125, fake 76.038 ) gen_loss 931.39\n",
            "iteration 9257, epoch 19, batch 399/481,disc_loss 80.621, (real 82.642, fake 78.6 ) gen_loss 1002.0\n",
            "iteration 9258, epoch 19, batch 400/481,disc_loss 82.452, (real 84.419, fake 80.485 ) gen_loss 852.7\n",
            "iteration 9259, epoch 19, batch 401/481,disc_loss 72.633, (real 75.384, fake 69.882 ) gen_loss 1059.2\n",
            "iteration 9260, epoch 19, batch 402/481,disc_loss 78.042, (real 80.166, fake 75.918 ) gen_loss 891.67\n",
            "iteration 9261, epoch 19, batch 403/481,disc_loss 78.997, (real 81.311, fake 76.683 ) gen_loss 940.26\n",
            "iteration 9262, epoch 19, batch 404/481,disc_loss 76.808, (real 79.099, fake 74.516 ) gen_loss 927.33\n",
            "iteration 9263, epoch 19, batch 405/481,disc_loss 77.088, (real 79.492, fake 74.685 ) gen_loss 833.72\n",
            "iteration 9264, epoch 19, batch 406/481,disc_loss 75.341, (real 77.085, fake 73.597 ) gen_loss 950.27\n",
            "iteration 9265, epoch 19, batch 407/481,disc_loss 76.77, (real 79.326, fake 74.214 ) gen_loss 831.19\n",
            "iteration 9266, epoch 19, batch 408/481,disc_loss 74.431, (real 77.31, fake 71.551 ) gen_loss 925.32\n",
            "iteration 9267, epoch 19, batch 409/481,disc_loss 74.687, (real 76.811, fake 72.563 ) gen_loss 883.71\n",
            "iteration 9268, epoch 19, batch 410/481,disc_loss 75.569, (real 77.322, fake 73.815 ) gen_loss 979.88\n",
            "iteration 9269, epoch 19, batch 411/481,disc_loss 79.003, (real 80.282, fake 77.725 ) gen_loss 871.97\n",
            "iteration 9270, epoch 19, batch 412/481,disc_loss 77.245, (real 79.62, fake 74.87 ) gen_loss 874.22\n",
            "iteration 9271, epoch 19, batch 413/481,disc_loss 75.282, (real 76.643, fake 73.92 ) gen_loss 916.16\n",
            "iteration 9272, epoch 19, batch 414/481,disc_loss 79.084, (real 81.014, fake 77.154 ) gen_loss 1010.0\n",
            "iteration 9273, epoch 19, batch 415/481,disc_loss 80.148, (real 82.246, fake 78.05 ) gen_loss 932.16\n",
            "iteration 9274, epoch 19, batch 416/481,disc_loss 76.629, (real 79.025, fake 74.233 ) gen_loss 926.86\n",
            "iteration 9275, epoch 19, batch 417/481,disc_loss 77.621, (real 79.699, fake 75.543 ) gen_loss 906.47\n",
            "iteration 9276, epoch 19, batch 418/481,disc_loss 77.831, (real 79.75, fake 75.913 ) gen_loss 987.3\n",
            "iteration 9277, epoch 19, batch 419/481,disc_loss 77.423, (real 79.188, fake 75.658 ) gen_loss 789.77\n",
            "iteration 9278, epoch 19, batch 420/481,disc_loss 77.668, (real 80.792, fake 74.543 ) gen_loss 855.85\n",
            "iteration 9279, epoch 19, batch 421/481,disc_loss 77.442, (real 79.957, fake 74.926 ) gen_loss 897.3\n",
            "iteration 9280, epoch 19, batch 422/481,disc_loss 81.539, (real 84.353, fake 78.725 ) gen_loss 825.51\n",
            "iteration 9281, epoch 19, batch 423/481,disc_loss 78.123, (real 80.576, fake 75.671 ) gen_loss 927.91\n",
            "iteration 9282, epoch 19, batch 424/481,disc_loss 74.759, (real 76.666, fake 72.852 ) gen_loss 880.22\n",
            "iteration 9283, epoch 19, batch 425/481,disc_loss 84.801, (real 88.031, fake 81.571 ) gen_loss 963.54\n",
            "iteration 9284, epoch 19, batch 426/481,disc_loss 77.208, (real 79.81, fake 74.606 ) gen_loss 867.65\n",
            "iteration 9285, epoch 19, batch 427/481,disc_loss 75.631, (real 77.741, fake 73.522 ) gen_loss 858.89\n",
            "iteration 9286, epoch 19, batch 428/481,disc_loss 73.863, (real 76.345, fake 71.38 ) gen_loss 925.0\n",
            "iteration 9287, epoch 19, batch 429/481,disc_loss 77.005, (real 79.385, fake 74.625 ) gen_loss 882.62\n",
            "iteration 9288, epoch 19, batch 430/481,disc_loss 78.147, (real 80.329, fake 75.965 ) gen_loss 951.68\n",
            "iteration 9289, epoch 19, batch 431/481,disc_loss 69.151, (real 72.164, fake 66.138 ) gen_loss 967.19\n",
            "iteration 9290, epoch 19, batch 432/481,disc_loss 78.067, (real 80.235, fake 75.899 ) gen_loss 932.96\n",
            "iteration 9291, epoch 19, batch 433/481,disc_loss 78.643, (real 80.885, fake 76.401 ) gen_loss 849.05\n",
            "iteration 9292, epoch 19, batch 434/481,disc_loss 77.825, (real 80.342, fake 75.308 ) gen_loss 905.56\n",
            "iteration 9293, epoch 19, batch 435/481,disc_loss 75.678, (real 77.969, fake 73.388 ) gen_loss 918.7\n",
            "iteration 9294, epoch 19, batch 436/481,disc_loss 78.667, (real 80.66, fake 76.674 ) gen_loss 806.84\n",
            "iteration 9295, epoch 19, batch 437/481,disc_loss 73.471, (real 75.607, fake 71.336 ) gen_loss 899.6\n",
            "iteration 9296, epoch 19, batch 438/481,disc_loss 75.622, (real 77.712, fake 73.533 ) gen_loss 985.9\n",
            "iteration 9297, epoch 19, batch 439/481,disc_loss 75.731, (real 77.715, fake 73.747 ) gen_loss 1048.9\n",
            "iteration 9298, epoch 19, batch 440/481,disc_loss 74.424, (real 76.831, fake 72.017 ) gen_loss 932.17\n",
            "iteration 9299, epoch 19, batch 441/481,disc_loss 74.371, (real 76.702, fake 72.04 ) gen_loss 905.96\n",
            "iteration 9300, epoch 19, batch 442/481,disc_loss 76.391, (real 79.262, fake 73.52 ) gen_loss 884.85\n",
            "iteration 9301, epoch 19, batch 443/481,disc_loss 79.388, (real 81.59, fake 77.186 ) gen_loss 890.93\n",
            "iteration 9302, epoch 19, batch 444/481,disc_loss 76.319, (real 79.24, fake 73.397 ) gen_loss 852.08\n",
            "iteration 9303, epoch 19, batch 445/481,disc_loss 78.077, (real 80.049, fake 76.106 ) gen_loss 886.58\n",
            "iteration 9304, epoch 19, batch 446/481,disc_loss 77.031, (real 79.853, fake 74.21 ) gen_loss 863.86\n",
            "iteration 9305, epoch 19, batch 447/481,disc_loss 74.329, (real 76.486, fake 72.172 ) gen_loss 885.19\n",
            "iteration 9306, epoch 19, batch 448/481,disc_loss 78.388, (real 80.393, fake 76.384 ) gen_loss 874.63\n",
            "iteration 9307, epoch 19, batch 449/481,disc_loss 82.555, (real 85.318, fake 79.792 ) gen_loss 870.63\n",
            "iteration 9308, epoch 19, batch 450/481,disc_loss 77.494, (real 80.48, fake 74.507 ) gen_loss 850.43\n",
            "iteration 9309, epoch 19, batch 451/481,disc_loss 77.53, (real 79.552, fake 75.508 ) gen_loss 1045.7\n",
            "iteration 9310, epoch 19, batch 452/481,disc_loss 72.99, (real 75.041, fake 70.938 ) gen_loss 969.26\n",
            "iteration 9311, epoch 19, batch 453/481,disc_loss 71.816, (real 73.588, fake 70.044 ) gen_loss 918.11\n",
            "iteration 9312, epoch 19, batch 454/481,disc_loss 77.231, (real 79.762, fake 74.7 ) gen_loss 897.22\n",
            "iteration 9313, epoch 19, batch 455/481,disc_loss 75.696, (real 77.908, fake 73.485 ) gen_loss 872.11\n",
            "iteration 9314, epoch 19, batch 456/481,disc_loss 76.193, (real 78.496, fake 73.889 ) gen_loss 822.47\n",
            "iteration 9315, epoch 19, batch 457/481,disc_loss 80.143, (real 83.487, fake 76.8 ) gen_loss 933.62\n",
            "iteration 9316, epoch 19, batch 458/481,disc_loss 78.771, (real 81.866, fake 75.676 ) gen_loss 910.86\n",
            "iteration 9317, epoch 19, batch 459/481,disc_loss 73.048, (real 74.724, fake 71.372 ) gen_loss 1054.2\n",
            "iteration 9318, epoch 19, batch 460/481,disc_loss 72.219, (real 74.306, fake 70.132 ) gen_loss 930.23\n",
            "iteration 9319, epoch 19, batch 461/481,disc_loss 74.448, (real 76.475, fake 72.421 ) gen_loss 874.9\n",
            "iteration 9320, epoch 19, batch 462/481,disc_loss 75.066, (real 77.588, fake 72.543 ) gen_loss 909.33\n",
            "iteration 9321, epoch 19, batch 463/481,disc_loss 79.661, (real 81.649, fake 77.673 ) gen_loss 830.04\n",
            "iteration 9322, epoch 19, batch 464/481,disc_loss 75.148, (real 76.292, fake 74.004 ) gen_loss 863.3\n",
            "iteration 9323, epoch 19, batch 465/481,disc_loss 76.722, (real 78.585, fake 74.858 ) gen_loss 948.69\n",
            "iteration 9324, epoch 19, batch 466/481,disc_loss 75.831, (real 77.659, fake 74.004 ) gen_loss 853.08\n",
            "iteration 9325, epoch 19, batch 467/481,disc_loss 74.129, (real 76.539, fake 71.719 ) gen_loss 817.33\n",
            "iteration 9326, epoch 19, batch 468/481,disc_loss 77.691, (real 80.235, fake 75.147 ) gen_loss 879.86\n",
            "iteration 9327, epoch 19, batch 469/481,disc_loss 75.79, (real 77.832, fake 73.748 ) gen_loss 856.12\n",
            "iteration 9328, epoch 19, batch 470/481,disc_loss 75.311, (real 78.184, fake 72.439 ) gen_loss 887.29\n",
            "iteration 9329, epoch 19, batch 471/481,disc_loss 76.579, (real 78.896, fake 74.263 ) gen_loss 938.37\n",
            "iteration 9330, epoch 19, batch 472/481,disc_loss 75.664, (real 77.314, fake 74.013 ) gen_loss 932.44\n",
            "iteration 9331, epoch 19, batch 473/481,disc_loss 77.235, (real 78.89, fake 75.581 ) gen_loss 867.39\n",
            "iteration 9332, epoch 19, batch 474/481,disc_loss 78.575, (real 81.405, fake 75.746 ) gen_loss 821.03\n",
            "iteration 9333, epoch 19, batch 475/481,disc_loss 79.582, (real 82.331, fake 76.833 ) gen_loss 878.87\n",
            "iteration 9334, epoch 19, batch 476/481,disc_loss 75.921, (real 78.798, fake 73.044 ) gen_loss 1118.4\n",
            "iteration 9335, epoch 19, batch 477/481,disc_loss 77.694, (real 80.577, fake 74.811 ) gen_loss 956.35\n",
            "iteration 9336, epoch 19, batch 478/481,disc_loss 77.372, (real 79.468, fake 75.276 ) gen_loss 876.42\n",
            "iteration 9337, epoch 19, batch 479/481,disc_loss 71.409, (real 73.814, fake 69.003 ) gen_loss 972.86\n",
            "iteration 9338, epoch 19, batch 480/481,disc_loss 77.85, (real 80.413, fake 75.286 ) gen_loss 908.26\n",
            "iteration 9339, epoch 19, batch 481/481,disc_loss 74.518, (real 76.581, fake 72.454 ) gen_loss 857.75\n",
            "iteration 9340, epoch 20, batch 1/481,disc_loss 75.846, (real 77.855, fake 73.838 ) gen_loss 969.26\n",
            "iteration 9341, epoch 20, batch 2/481,disc_loss 77.965, (real 80.22, fake 75.709 ) gen_loss 958.16\n",
            "iteration 9342, epoch 20, batch 3/481,disc_loss 74.938, (real 77.214, fake 72.663 ) gen_loss 957.67\n",
            "iteration 9343, epoch 20, batch 4/481,disc_loss 74.01, (real 75.818, fake 72.201 ) gen_loss 981.97\n",
            "iteration 9344, epoch 20, batch 5/481,disc_loss 73.089, (real 75.165, fake 71.014 ) gen_loss 1029.4\n",
            "iteration 9345, epoch 20, batch 6/481,disc_loss 76.536, (real 78.588, fake 74.483 ) gen_loss 909.97\n",
            "iteration 9346, epoch 20, batch 7/481,disc_loss 75.397, (real 76.861, fake 73.933 ) gen_loss 924.05\n",
            "iteration 9347, epoch 20, batch 8/481,disc_loss 70.384, (real 71.978, fake 68.79 ) gen_loss 955.81\n",
            "iteration 9348, epoch 20, batch 9/481,disc_loss 75.653, (real 77.863, fake 73.443 ) gen_loss 873.07\n",
            "iteration 9349, epoch 20, batch 10/481,disc_loss 77.277, (real 78.973, fake 75.582 ) gen_loss 937.46\n",
            "iteration 9350, epoch 20, batch 11/481,disc_loss 77.399, (real 80.395, fake 74.404 ) gen_loss 945.38\n",
            "iteration 9351, epoch 20, batch 12/481,disc_loss 78.237, (real 80.547, fake 75.928 ) gen_loss 877.3\n",
            "iteration 9352, epoch 20, batch 13/481,disc_loss 73.322, (real 75.451, fake 71.193 ) gen_loss 975.02\n",
            "iteration 9353, epoch 20, batch 14/481,disc_loss 76.582, (real 78.547, fake 74.616 ) gen_loss 850.06\n",
            "iteration 9354, epoch 20, batch 15/481,disc_loss 76.451, (real 78.957, fake 73.945 ) gen_loss 946.8\n",
            "iteration 9355, epoch 20, batch 16/481,disc_loss 74.612, (real 76.149, fake 73.076 ) gen_loss 1035.8\n",
            "iteration 9356, epoch 20, batch 17/481,disc_loss 71.72, (real 73.854, fake 69.586 ) gen_loss 921.1\n",
            "iteration 9357, epoch 20, batch 18/481,disc_loss 73.379, (real 74.908, fake 71.849 ) gen_loss 1011.3\n",
            "iteration 9358, epoch 20, batch 19/481,disc_loss 78.103, (real 80.344, fake 75.861 ) gen_loss 981.37\n",
            "iteration 9359, epoch 20, batch 20/481,disc_loss 73.339, (real 75.703, fake 70.976 ) gen_loss 904.81\n",
            "iteration 9360, epoch 20, batch 21/481,disc_loss 74.893, (real 76.42, fake 73.366 ) gen_loss 901.02\n",
            "iteration 9361, epoch 20, batch 22/481,disc_loss 80.695, (real 82.577, fake 78.814 ) gen_loss 895.37\n",
            "iteration 9362, epoch 20, batch 23/481,disc_loss 77.354, (real 79.243, fake 75.465 ) gen_loss 972.25\n",
            "iteration 9363, epoch 20, batch 24/481,disc_loss 74.539, (real 76.41, fake 72.669 ) gen_loss 921.79\n",
            "iteration 9364, epoch 20, batch 25/481,disc_loss 73.08, (real 75.121, fake 71.039 ) gen_loss 868.63\n",
            "iteration 9365, epoch 20, batch 26/481,disc_loss 76.975, (real 77.812, fake 76.137 ) gen_loss 850.08\n",
            "iteration 9366, epoch 20, batch 27/481,disc_loss 76.361, (real 78.49, fake 74.231 ) gen_loss 931.78\n",
            "iteration 9367, epoch 20, batch 28/481,disc_loss 80.286, (real 81.694, fake 78.878 ) gen_loss 878.66\n",
            "iteration 9368, epoch 20, batch 29/481,disc_loss 78.585, (real 80.795, fake 76.374 ) gen_loss 939.84\n",
            "iteration 9369, epoch 20, batch 30/481,disc_loss 76.569, (real 78.481, fake 74.656 ) gen_loss 892.3\n",
            "iteration 9370, epoch 20, batch 31/481,disc_loss 75.433, (real 77.204, fake 73.662 ) gen_loss 810.72\n",
            "iteration 9371, epoch 20, batch 32/481,disc_loss 79.579, (real 82.1, fake 77.058 ) gen_loss 895.93\n",
            "iteration 9372, epoch 20, batch 33/481,disc_loss 78.556, (real 81.023, fake 76.088 ) gen_loss 835.54\n",
            "iteration 9373, epoch 20, batch 34/481,disc_loss 75.878, (real 78.231, fake 73.526 ) gen_loss 910.04\n",
            "iteration 9374, epoch 20, batch 35/481,disc_loss 70.693, (real 72.566, fake 68.82 ) gen_loss 968.99\n",
            "iteration 9375, epoch 20, batch 36/481,disc_loss 78.517, (real 80.812, fake 76.222 ) gen_loss 906.7\n",
            "iteration 9376, epoch 20, batch 37/481,disc_loss 81.024, (real 83.136, fake 78.912 ) gen_loss 929.54\n",
            "iteration 9377, epoch 20, batch 38/481,disc_loss 76.872, (real 78.968, fake 74.776 ) gen_loss 948.34\n",
            "iteration 9378, epoch 20, batch 39/481,disc_loss 79.731, (real 82.115, fake 77.348 ) gen_loss 968.0\n",
            "iteration 9379, epoch 20, batch 40/481,disc_loss 75.506, (real 77.071, fake 73.942 ) gen_loss 915.91\n",
            "iteration 9380, epoch 20, batch 41/481,disc_loss 76.792, (real 78.374, fake 75.21 ) gen_loss 889.84\n",
            "iteration 9381, epoch 20, batch 42/481,disc_loss 75.337, (real 77.594, fake 73.08 ) gen_loss 890.63\n",
            "iteration 9382, epoch 20, batch 43/481,disc_loss 79.786, (real 81.871, fake 77.7 ) gen_loss 833.05\n",
            "iteration 9383, epoch 20, batch 44/481,disc_loss 77.213, (real 79.149, fake 75.276 ) gen_loss 941.81\n",
            "iteration 9384, epoch 20, batch 45/481,disc_loss 75.821, (real 77.123, fake 74.519 ) gen_loss 918.99\n",
            "iteration 9385, epoch 20, batch 46/481,disc_loss 77.621, (real 79.908, fake 75.334 ) gen_loss 998.93\n",
            "iteration 9386, epoch 20, batch 47/481,disc_loss 75.071, (real 76.607, fake 73.534 ) gen_loss 875.95\n",
            "iteration 9387, epoch 20, batch 48/481,disc_loss 77.738, (real 79.55, fake 75.926 ) gen_loss 844.52\n",
            "iteration 9388, epoch 20, batch 49/481,disc_loss 75.42, (real 77.205, fake 73.635 ) gen_loss 806.52\n",
            "iteration 9389, epoch 20, batch 50/481,disc_loss 80.993, (real 82.98, fake 79.007 ) gen_loss 772.85\n",
            "iteration 9390, epoch 20, batch 51/481,disc_loss 76.684, (real 78.769, fake 74.599 ) gen_loss 783.49\n",
            "iteration 9391, epoch 20, batch 52/481,disc_loss 70.936, (real 73.054, fake 68.818 ) gen_loss 933.52\n",
            "iteration 9392, epoch 20, batch 53/481,disc_loss 75.816, (real 78.761, fake 72.871 ) gen_loss 924.82\n",
            "iteration 9393, epoch 20, batch 54/481,disc_loss 75.843, (real 77.483, fake 74.203 ) gen_loss 922.89\n",
            "iteration 9394, epoch 20, batch 55/481,disc_loss 76.169, (real 78.163, fake 74.176 ) gen_loss 869.43\n",
            "iteration 9395, epoch 20, batch 56/481,disc_loss 72.308, (real 74.127, fake 70.488 ) gen_loss 981.16\n",
            "iteration 9396, epoch 20, batch 57/481,disc_loss 81.591, (real 83.595, fake 79.587 ) gen_loss 884.31\n",
            "iteration 9397, epoch 20, batch 58/481,disc_loss 77.696, (real 80.096, fake 75.297 ) gen_loss 911.8\n",
            "iteration 9398, epoch 20, batch 59/481,disc_loss 77.083, (real 79.198, fake 74.968 ) gen_loss 918.4\n",
            "iteration 9399, epoch 20, batch 60/481,disc_loss 72.269, (real 74.097, fake 70.44 ) gen_loss 857.45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 9400, epoch 20, batch 61/481,disc_loss 73.855, (real 76.283, fake 71.427 ) gen_loss 890.28\n",
            "iteration 9401, epoch 20, batch 62/481,disc_loss 74.517, (real 76.946, fake 72.088 ) gen_loss 867.81\n",
            "iteration 9402, epoch 20, batch 63/481,disc_loss 75.636, (real 78.207, fake 73.064 ) gen_loss 833.18\n",
            "iteration 9403, epoch 20, batch 64/481,disc_loss 74.546, (real 76.141, fake 72.95 ) gen_loss 883.88\n",
            "iteration 9404, epoch 20, batch 65/481,disc_loss 74.491, (real 76.834, fake 72.147 ) gen_loss 867.05\n",
            "iteration 9405, epoch 20, batch 66/481,disc_loss 75.198, (real 76.983, fake 73.414 ) gen_loss 879.56\n",
            "iteration 9406, epoch 20, batch 67/481,disc_loss 69.784, (real 71.34, fake 68.229 ) gen_loss 954.62\n",
            "iteration 9407, epoch 20, batch 68/481,disc_loss 70.806, (real 72.647, fake 68.965 ) gen_loss 933.1\n",
            "iteration 9408, epoch 20, batch 69/481,disc_loss 75.397, (real 77.385, fake 73.409 ) gen_loss 939.11\n",
            "iteration 9409, epoch 20, batch 70/481,disc_loss 76.832, (real 79.193, fake 74.471 ) gen_loss 837.05\n",
            "iteration 9410, epoch 20, batch 71/481,disc_loss 78.649, (real 81.063, fake 76.234 ) gen_loss 882.18\n",
            "iteration 9411, epoch 20, batch 72/481,disc_loss 73.116, (real 75.253, fake 70.98 ) gen_loss 872.57\n",
            "iteration 9412, epoch 20, batch 73/481,disc_loss 75.544, (real 77.42, fake 73.668 ) gen_loss 800.29\n",
            "iteration 9413, epoch 20, batch 74/481,disc_loss 79.473, (real 81.335, fake 77.61 ) gen_loss 956.37\n",
            "iteration 9414, epoch 20, batch 75/481,disc_loss 74.619, (real 76.714, fake 72.524 ) gen_loss 960.78\n",
            "iteration 9415, epoch 20, batch 76/481,disc_loss 75.761, (real 77.852, fake 73.669 ) gen_loss 950.8\n",
            "iteration 9416, epoch 20, batch 77/481,disc_loss 76.675, (real 78.781, fake 74.569 ) gen_loss 891.71\n",
            "iteration 9417, epoch 20, batch 78/481,disc_loss 80.51, (real 83.599, fake 77.422 ) gen_loss 876.22\n",
            "iteration 9418, epoch 20, batch 79/481,disc_loss 74.792, (real 76.286, fake 73.298 ) gen_loss 942.87\n",
            "iteration 9419, epoch 20, batch 80/481,disc_loss 77.568, (real 80.478, fake 74.658 ) gen_loss 837.27\n",
            "iteration 9420, epoch 20, batch 81/481,disc_loss 73.692, (real 75.68, fake 71.705 ) gen_loss 916.45\n",
            "iteration 9421, epoch 20, batch 82/481,disc_loss 73.789, (real 75.886, fake 71.692 ) gen_loss 787.25\n",
            "iteration 9422, epoch 20, batch 83/481,disc_loss 77.127, (real 79.291, fake 74.964 ) gen_loss 938.76\n",
            "iteration 9423, epoch 20, batch 84/481,disc_loss 73.543, (real 75.588, fake 71.499 ) gen_loss 876.54\n",
            "iteration 9424, epoch 20, batch 85/481,disc_loss 71.912, (real 73.138, fake 70.687 ) gen_loss 916.79\n",
            "iteration 9425, epoch 20, batch 86/481,disc_loss 77.043, (real 78.994, fake 75.092 ) gen_loss 1019.4\n",
            "iteration 9426, epoch 20, batch 87/481,disc_loss 76.949, (real 78.356, fake 75.542 ) gen_loss 921.74\n",
            "iteration 9427, epoch 20, batch 88/481,disc_loss 77.693, (real 79.863, fake 75.524 ) gen_loss 890.23\n",
            "iteration 9428, epoch 20, batch 89/481,disc_loss 77.324, (real 79.538, fake 75.109 ) gen_loss 869.39\n",
            "iteration 9429, epoch 20, batch 90/481,disc_loss 77.738, (real 79.365, fake 76.111 ) gen_loss 935.71\n",
            "iteration 9430, epoch 20, batch 91/481,disc_loss 77.737, (real 79.277, fake 76.198 ) gen_loss 838.02\n",
            "iteration 9431, epoch 20, batch 92/481,disc_loss 79.748, (real 81.812, fake 77.683 ) gen_loss 890.12\n",
            "iteration 9432, epoch 20, batch 93/481,disc_loss 80.358, (real 82.216, fake 78.499 ) gen_loss 879.09\n",
            "iteration 9433, epoch 20, batch 94/481,disc_loss 79.868, (real 81.677, fake 78.06 ) gen_loss 881.15\n",
            "iteration 9434, epoch 20, batch 95/481,disc_loss 77.064, (real 78.661, fake 75.468 ) gen_loss 873.45\n",
            "iteration 9435, epoch 20, batch 96/481,disc_loss 78.088, (real 80.259, fake 75.917 ) gen_loss 1009.9\n",
            "iteration 9436, epoch 20, batch 97/481,disc_loss 77.872, (real 80.947, fake 74.797 ) gen_loss 868.63\n",
            "iteration 9437, epoch 20, batch 98/481,disc_loss 73.767, (real 75.88, fake 71.654 ) gen_loss 853.74\n",
            "iteration 9438, epoch 20, batch 99/481,disc_loss 78.531, (real 80.357, fake 76.705 ) gen_loss 887.19\n",
            "iteration 9439, epoch 20, batch 100/481,disc_loss 76.375, (real 78.289, fake 74.461 ) gen_loss 908.47\n",
            "iteration 9440, epoch 20, batch 101/481,disc_loss 78.588, (real 80.914, fake 76.261 ) gen_loss 747.46\n",
            "iteration 9441, epoch 20, batch 102/481,disc_loss 76.739, (real 79.046, fake 74.432 ) gen_loss 922.97\n",
            "iteration 9442, epoch 20, batch 103/481,disc_loss 75.884, (real 78.043, fake 73.725 ) gen_loss 872.9\n",
            "iteration 9443, epoch 20, batch 104/481,disc_loss 76.213, (real 78.785, fake 73.64 ) gen_loss 906.22\n",
            "iteration 9444, epoch 20, batch 105/481,disc_loss 77.297, (real 79.079, fake 75.516 ) gen_loss 911.3\n",
            "iteration 9445, epoch 20, batch 106/481,disc_loss 76.934, (real 78.932, fake 74.937 ) gen_loss 855.72\n",
            "iteration 9446, epoch 20, batch 107/481,disc_loss 77.988, (real 79.916, fake 76.06 ) gen_loss 852.33\n",
            "iteration 9447, epoch 20, batch 108/481,disc_loss 72.165, (real 74.029, fake 70.301 ) gen_loss 924.22\n",
            "iteration 9448, epoch 20, batch 109/481,disc_loss 71.583, (real 73.512, fake 69.654 ) gen_loss 891.77\n",
            "iteration 9449, epoch 20, batch 110/481,disc_loss 78.801, (real 80.726, fake 76.875 ) gen_loss 976.38\n",
            "iteration 9450, epoch 20, batch 111/481,disc_loss 78.256, (real 81.104, fake 75.408 ) gen_loss 1080.7\n",
            "iteration 9451, epoch 20, batch 112/481,disc_loss 74.058, (real 75.448, fake 72.668 ) gen_loss 865.04\n",
            "iteration 9452, epoch 20, batch 113/481,disc_loss 74.56, (real 77.148, fake 71.973 ) gen_loss 918.2\n",
            "iteration 9453, epoch 20, batch 114/481,disc_loss 78.337, (real 81.575, fake 75.1 ) gen_loss 846.38\n",
            "iteration 9454, epoch 20, batch 115/481,disc_loss 69.714, (real 72.02, fake 67.407 ) gen_loss 893.95\n",
            "iteration 9455, epoch 20, batch 116/481,disc_loss 77.502, (real 79.567, fake 75.437 ) gen_loss 888.78\n",
            "iteration 9456, epoch 20, batch 117/481,disc_loss 75.779, (real 77.314, fake 74.244 ) gen_loss 974.72\n",
            "iteration 9457, epoch 20, batch 118/481,disc_loss 78.435, (real 80.352, fake 76.518 ) gen_loss 973.66\n",
            "iteration 9458, epoch 20, batch 119/481,disc_loss 78.558, (real 79.899, fake 77.218 ) gen_loss 1119.1\n",
            "iteration 9459, epoch 20, batch 120/481,disc_loss 78.766, (real 80.241, fake 77.291 ) gen_loss 984.39\n",
            "iteration 9460, epoch 20, batch 121/481,disc_loss 78.606, (real 80.569, fake 76.643 ) gen_loss 1028.5\n",
            "iteration 9461, epoch 20, batch 122/481,disc_loss 77.22, (real 78.728, fake 75.713 ) gen_loss 938.66\n",
            "iteration 9462, epoch 20, batch 123/481,disc_loss 73.805, (real 75.538, fake 72.071 ) gen_loss 1013.3\n",
            "iteration 9463, epoch 20, batch 124/481,disc_loss 75.864, (real 78.24, fake 73.488 ) gen_loss 1023.7\n",
            "iteration 9464, epoch 20, batch 125/481,disc_loss 77.366, (real 79.608, fake 75.124 ) gen_loss 851.66\n",
            "iteration 9465, epoch 20, batch 126/481,disc_loss 76.906, (real 78.795, fake 75.017 ) gen_loss 910.45\n",
            "iteration 9466, epoch 20, batch 127/481,disc_loss 71.835, (real 73.365, fake 70.305 ) gen_loss 941.51\n",
            "iteration 9467, epoch 20, batch 128/481,disc_loss 80.548, (real 82.544, fake 78.551 ) gen_loss 885.1\n",
            "iteration 9468, epoch 20, batch 129/481,disc_loss 75.66, (real 77.566, fake 73.754 ) gen_loss 884.72\n",
            "iteration 9469, epoch 20, batch 130/481,disc_loss 79.531, (real 82.077, fake 76.985 ) gen_loss 896.98\n",
            "iteration 9470, epoch 20, batch 131/481,disc_loss 79.321, (real 81.63, fake 77.011 ) gen_loss 967.17\n",
            "iteration 9471, epoch 20, batch 132/481,disc_loss 74.494, (real 76.161, fake 72.827 ) gen_loss 892.04\n",
            "iteration 9472, epoch 20, batch 133/481,disc_loss 76.522, (real 77.924, fake 75.12 ) gen_loss 860.96\n",
            "iteration 9473, epoch 20, batch 134/481,disc_loss 76.423, (real 78.193, fake 74.653 ) gen_loss 887.28\n",
            "iteration 9474, epoch 20, batch 135/481,disc_loss 77.491, (real 79.83, fake 75.152 ) gen_loss 971.62\n",
            "iteration 9475, epoch 20, batch 136/481,disc_loss 78.136, (real 80.108, fake 76.164 ) gen_loss 963.02\n",
            "iteration 9476, epoch 20, batch 137/481,disc_loss 76.239, (real 78.581, fake 73.897 ) gen_loss 911.91\n",
            "iteration 9477, epoch 20, batch 138/481,disc_loss 76.772, (real 78.963, fake 74.58 ) gen_loss 899.52\n",
            "iteration 9478, epoch 20, batch 139/481,disc_loss 73.592, (real 75.793, fake 71.391 ) gen_loss 877.26\n",
            "iteration 9479, epoch 20, batch 140/481,disc_loss 78.558, (real 80.924, fake 76.192 ) gen_loss 984.0\n",
            "iteration 9480, epoch 20, batch 141/481,disc_loss 77.728, (real 80.262, fake 75.195 ) gen_loss 1007.1\n",
            "iteration 9481, epoch 20, batch 142/481,disc_loss 73.653, (real 75.59, fake 71.716 ) gen_loss 909.91\n",
            "iteration 9482, epoch 20, batch 143/481,disc_loss 79.573, (real 81.894, fake 77.252 ) gen_loss 935.17\n",
            "iteration 9483, epoch 20, batch 144/481,disc_loss 79.521, (real 81.914, fake 77.127 ) gen_loss 840.05\n",
            "iteration 9484, epoch 20, batch 145/481,disc_loss 74.096, (real 76.025, fake 72.167 ) gen_loss 889.33\n",
            "iteration 9485, epoch 20, batch 146/481,disc_loss 78.706, (real 80.469, fake 76.944 ) gen_loss 1032.3\n",
            "iteration 9486, epoch 20, batch 147/481,disc_loss 78.611, (real 80.587, fake 76.635 ) gen_loss 978.65\n",
            "iteration 9487, epoch 20, batch 148/481,disc_loss 74.376, (real 76.527, fake 72.224 ) gen_loss 945.12\n",
            "iteration 9488, epoch 20, batch 149/481,disc_loss 71.098, (real 73.065, fake 69.131 ) gen_loss 978.03\n",
            "iteration 9489, epoch 20, batch 150/481,disc_loss 79.476, (real 81.999, fake 76.953 ) gen_loss 845.58\n",
            "iteration 9490, epoch 20, batch 151/481,disc_loss 75.485, (real 77.092, fake 73.878 ) gen_loss 1095.2\n",
            "iteration 9491, epoch 20, batch 152/481,disc_loss 75.045, (real 77.85, fake 72.24 ) gen_loss 956.87\n",
            "iteration 9492, epoch 20, batch 153/481,disc_loss 79.391, (real 81.023, fake 77.758 ) gen_loss 905.18\n",
            "iteration 9493, epoch 20, batch 154/481,disc_loss 79.151, (real 81.641, fake 76.661 ) gen_loss 895.5\n",
            "iteration 9494, epoch 20, batch 155/481,disc_loss 79.174, (real 81.568, fake 76.78 ) gen_loss 805.31\n",
            "iteration 9495, epoch 20, batch 156/481,disc_loss 74.168, (real 75.672, fake 72.665 ) gen_loss 816.24\n",
            "iteration 9496, epoch 20, batch 157/481,disc_loss 75.028, (real 76.471, fake 73.585 ) gen_loss 881.33\n",
            "iteration 9497, epoch 20, batch 158/481,disc_loss 72.901, (real 75.257, fake 70.545 ) gen_loss 1024.7\n",
            "iteration 9498, epoch 20, batch 159/481,disc_loss 77.094, (real 79.336, fake 74.852 ) gen_loss 860.14\n",
            "iteration 9499, epoch 20, batch 160/481,disc_loss 76.886, (real 78.805, fake 74.967 ) gen_loss 898.08\n",
            "iteration 9500, epoch 20, batch 161/481,disc_loss 72.257, (real 74.525, fake 69.989 ) gen_loss 863.62\n",
            "iteration 9501, epoch 20, batch 162/481,disc_loss 75.46, (real 77.241, fake 73.678 ) gen_loss 839.07\n",
            "iteration 9502, epoch 20, batch 163/481,disc_loss 77.927, (real 79.634, fake 76.219 ) gen_loss 923.72\n",
            "iteration 9503, epoch 20, batch 164/481,disc_loss 79.059, (real 80.95, fake 77.167 ) gen_loss 917.2\n",
            "iteration 9504, epoch 20, batch 165/481,disc_loss 75.274, (real 76.63, fake 73.918 ) gen_loss 852.74\n",
            "iteration 9505, epoch 20, batch 166/481,disc_loss 74.375, (real 76.251, fake 72.5 ) gen_loss 838.32\n",
            "iteration 9506, epoch 20, batch 167/481,disc_loss 75.919, (real 77.323, fake 74.514 ) gen_loss 854.47\n",
            "iteration 9507, epoch 20, batch 168/481,disc_loss 82.218, (real 85.062, fake 79.375 ) gen_loss 924.82\n",
            "iteration 9508, epoch 20, batch 169/481,disc_loss 80.57, (real 82.391, fake 78.75 ) gen_loss 947.49\n",
            "iteration 9509, epoch 20, batch 170/481,disc_loss 74.478, (real 77.17, fake 71.786 ) gen_loss 843.1\n",
            "iteration 9510, epoch 20, batch 171/481,disc_loss 77.595, (real 79.055, fake 76.136 ) gen_loss 875.59\n",
            "iteration 9511, epoch 20, batch 172/481,disc_loss 78.501, (real 80.542, fake 76.459 ) gen_loss 760.58\n",
            "iteration 9512, epoch 20, batch 173/481,disc_loss 76.816, (real 78.928, fake 74.704 ) gen_loss 904.15\n",
            "iteration 9513, epoch 20, batch 174/481,disc_loss 72.294, (real 74.012, fake 70.576 ) gen_loss 912.13\n",
            "iteration 9514, epoch 20, batch 175/481,disc_loss 74.673, (real 76.99, fake 72.356 ) gen_loss 850.38\n",
            "iteration 9515, epoch 20, batch 176/481,disc_loss 76.466, (real 78.403, fake 74.53 ) gen_loss 947.21\n",
            "iteration 9516, epoch 20, batch 177/481,disc_loss 76.457, (real 78.311, fake 74.603 ) gen_loss 995.49\n",
            "iteration 9517, epoch 20, batch 178/481,disc_loss 70.829, (real 73.314, fake 68.343 ) gen_loss 1091.9\n",
            "iteration 9518, epoch 20, batch 179/481,disc_loss 78.461, (real 80.691, fake 76.232 ) gen_loss 882.06\n",
            "iteration 9519, epoch 20, batch 180/481,disc_loss 78.546, (real 80.853, fake 76.239 ) gen_loss 934.85\n",
            "iteration 9520, epoch 20, batch 181/481,disc_loss 79.497, (real 82.653, fake 76.34 ) gen_loss 1128.0\n",
            "iteration 9521, epoch 20, batch 182/481,disc_loss 78.581, (real 81.046, fake 76.117 ) gen_loss 904.69\n",
            "iteration 9522, epoch 20, batch 183/481,disc_loss 74.186, (real 76.407, fake 71.966 ) gen_loss 894.71\n",
            "iteration 9523, epoch 20, batch 184/481,disc_loss 75.679, (real 77.246, fake 74.112 ) gen_loss 841.32\n",
            "iteration 9524, epoch 20, batch 185/481,disc_loss 75.689, (real 78.023, fake 73.356 ) gen_loss 837.89\n",
            "iteration 9525, epoch 20, batch 186/481,disc_loss 76.774, (real 78.811, fake 74.736 ) gen_loss 872.26\n",
            "iteration 9526, epoch 20, batch 187/481,disc_loss 78.938, (real 81.319, fake 76.556 ) gen_loss 866.56\n",
            "iteration 9527, epoch 20, batch 188/481,disc_loss 71.154, (real 73.398, fake 68.911 ) gen_loss 871.75\n",
            "iteration 9528, epoch 20, batch 189/481,disc_loss 77.711, (real 80.286, fake 75.135 ) gen_loss 864.08\n",
            "iteration 9529, epoch 20, batch 190/481,disc_loss 78.313, (real 79.967, fake 76.659 ) gen_loss 849.45\n",
            "iteration 9530, epoch 20, batch 191/481,disc_loss 76.01, (real 78.118, fake 73.901 ) gen_loss 835.83\n",
            "iteration 9531, epoch 20, batch 192/481,disc_loss 73.054, (real 74.585, fake 71.524 ) gen_loss 932.5\n",
            "iteration 9532, epoch 20, batch 193/481,disc_loss 75.16, (real 77.62, fake 72.699 ) gen_loss 908.02\n",
            "iteration 9533, epoch 20, batch 194/481,disc_loss 78.854, (real 81.294, fake 76.414 ) gen_loss 918.31\n",
            "iteration 9534, epoch 20, batch 195/481,disc_loss 75.191, (real 77.054, fake 73.328 ) gen_loss 903.05\n",
            "iteration 9535, epoch 20, batch 196/481,disc_loss 76.802, (real 79.023, fake 74.581 ) gen_loss 876.13\n",
            "iteration 9536, epoch 20, batch 197/481,disc_loss 73.895, (real 76.475, fake 71.315 ) gen_loss 845.98\n",
            "iteration 9537, epoch 20, batch 198/481,disc_loss 76.714, (real 79.054, fake 74.374 ) gen_loss 898.76\n",
            "iteration 9538, epoch 20, batch 199/481,disc_loss 76.96, (real 79.38, fake 74.541 ) gen_loss 881.2\n",
            "iteration 9539, epoch 20, batch 200/481,disc_loss 70.442, (real 72.785, fake 68.098 ) gen_loss 809.79\n",
            "iteration 9540, epoch 20, batch 201/481,disc_loss 76.446, (real 78.37, fake 74.523 ) gen_loss 842.87\n",
            "iteration 9541, epoch 20, batch 202/481,disc_loss 80.529, (real 82.766, fake 78.292 ) gen_loss 865.83\n",
            "iteration 9542, epoch 20, batch 203/481,disc_loss 75.768, (real 78.281, fake 73.256 ) gen_loss 890.96\n",
            "iteration 9543, epoch 20, batch 204/481,disc_loss 78.986, (real 81.431, fake 76.542 ) gen_loss 892.47\n",
            "iteration 9544, epoch 20, batch 205/481,disc_loss 77.116, (real 79.251, fake 74.981 ) gen_loss 986.05\n",
            "iteration 9545, epoch 20, batch 206/481,disc_loss 75.876, (real 77.797, fake 73.956 ) gen_loss 864.72\n",
            "iteration 9546, epoch 20, batch 207/481,disc_loss 78.974, (real 81.03, fake 76.918 ) gen_loss 862.78\n",
            "iteration 9547, epoch 20, batch 208/481,disc_loss 74.503, (real 76.398, fake 72.609 ) gen_loss 971.62\n",
            "iteration 9548, epoch 20, batch 209/481,disc_loss 72.828, (real 74.317, fake 71.339 ) gen_loss 933.21\n",
            "iteration 9549, epoch 20, batch 210/481,disc_loss 76.893, (real 78.9, fake 74.887 ) gen_loss 881.96\n",
            "iteration 9550, epoch 20, batch 211/481,disc_loss 79.838, (real 83.601, fake 76.075 ) gen_loss 927.36\n",
            "iteration 9551, epoch 20, batch 212/481,disc_loss 79.757, (real 82.441, fake 77.072 ) gen_loss 947.06\n",
            "iteration 9552, epoch 20, batch 213/481,disc_loss 73.415, (real 75.327, fake 71.504 ) gen_loss 913.55\n",
            "iteration 9553, epoch 20, batch 214/481,disc_loss 79.952, (real 82.523, fake 77.381 ) gen_loss 870.31\n",
            "iteration 9554, epoch 20, batch 215/481,disc_loss 79.22, (real 81.32, fake 77.121 ) gen_loss 854.28\n",
            "iteration 9555, epoch 20, batch 216/481,disc_loss 78.503, (real 80.969, fake 76.038 ) gen_loss 854.31\n",
            "iteration 9556, epoch 20, batch 217/481,disc_loss 77.324, (real 79.609, fake 75.039 ) gen_loss 850.93\n",
            "iteration 9557, epoch 20, batch 218/481,disc_loss 79.935, (real 82.164, fake 77.707 ) gen_loss 893.16\n",
            "iteration 9558, epoch 20, batch 219/481,disc_loss 76.488, (real 79.397, fake 73.579 ) gen_loss 869.93\n",
            "iteration 9559, epoch 20, batch 220/481,disc_loss 78.819, (real 80.482, fake 77.155 ) gen_loss 855.45\n",
            "iteration 9560, epoch 20, batch 221/481,disc_loss 75.627, (real 77.392, fake 73.863 ) gen_loss 905.18\n",
            "iteration 9561, epoch 20, batch 222/481,disc_loss 78.554, (real 80.52, fake 76.587 ) gen_loss 918.91\n",
            "iteration 9562, epoch 20, batch 223/481,disc_loss 76.429, (real 78.197, fake 74.661 ) gen_loss 905.52\n",
            "iteration 9563, epoch 20, batch 224/481,disc_loss 72.428, (real 74.068, fake 70.789 ) gen_loss 818.92\n",
            "iteration 9564, epoch 20, batch 225/481,disc_loss 73.946, (real 75.824, fake 72.067 ) gen_loss 958.47\n",
            "iteration 9565, epoch 20, batch 226/481,disc_loss 72.588, (real 74.923, fake 70.252 ) gen_loss 952.55\n",
            "iteration 9566, epoch 20, batch 227/481,disc_loss 75.388, (real 77.573, fake 73.203 ) gen_loss 920.28\n",
            "iteration 9567, epoch 20, batch 228/481,disc_loss 79.885, (real 81.563, fake 78.206 ) gen_loss 928.46\n",
            "iteration 9568, epoch 20, batch 229/481,disc_loss 79.503, (real 81.032, fake 77.974 ) gen_loss 873.67\n",
            "iteration 9569, epoch 20, batch 230/481,disc_loss 79.997, (real 82.221, fake 77.774 ) gen_loss 869.59\n",
            "iteration 9570, epoch 20, batch 231/481,disc_loss 74.682, (real 76.854, fake 72.51 ) gen_loss 911.47\n",
            "iteration 9571, epoch 20, batch 232/481,disc_loss 76.261, (real 78.402, fake 74.121 ) gen_loss 940.95\n",
            "iteration 9572, epoch 20, batch 233/481,disc_loss 75.477, (real 77.092, fake 73.863 ) gen_loss 821.83\n",
            "iteration 9573, epoch 20, batch 234/481,disc_loss 74.356, (real 76.724, fake 71.988 ) gen_loss 1015.2\n",
            "iteration 9574, epoch 20, batch 235/481,disc_loss 72.991, (real 74.285, fake 71.696 ) gen_loss 1002.0\n",
            "iteration 9575, epoch 20, batch 236/481,disc_loss 75.934, (real 77.945, fake 73.923 ) gen_loss 939.8\n",
            "iteration 9576, epoch 20, batch 237/481,disc_loss 75.072, (real 76.713, fake 73.431 ) gen_loss 876.33\n",
            "iteration 9577, epoch 20, batch 238/481,disc_loss 79.637, (real 82.072, fake 77.203 ) gen_loss 868.82\n",
            "iteration 9578, epoch 20, batch 239/481,disc_loss 77.587, (real 79.245, fake 75.929 ) gen_loss 920.63\n",
            "iteration 9579, epoch 20, batch 240/481,disc_loss 75.916, (real 78.259, fake 73.573 ) gen_loss 890.3\n",
            "iteration 9580, epoch 20, batch 241/481,disc_loss 73.129, (real 75.856, fake 70.402 ) gen_loss 948.69\n",
            "iteration 9581, epoch 20, batch 242/481,disc_loss 73.354, (real 75.174, fake 71.534 ) gen_loss 926.79\n",
            "iteration 9582, epoch 20, batch 243/481,disc_loss 83.366, (real 85.545, fake 81.187 ) gen_loss 880.78\n",
            "iteration 9583, epoch 20, batch 244/481,disc_loss 76.663, (real 78.781, fake 74.545 ) gen_loss 883.19\n",
            "iteration 9584, epoch 20, batch 245/481,disc_loss 77.738, (real 80.13, fake 75.347 ) gen_loss 861.34\n",
            "iteration 9585, epoch 20, batch 246/481,disc_loss 78.23, (real 80.329, fake 76.131 ) gen_loss 856.08\n",
            "iteration 9586, epoch 20, batch 247/481,disc_loss 74.415, (real 76.606, fake 72.223 ) gen_loss 949.09\n",
            "iteration 9587, epoch 20, batch 248/481,disc_loss 80.108, (real 82.033, fake 78.184 ) gen_loss 809.76\n",
            "iteration 9588, epoch 20, batch 249/481,disc_loss 72.249, (real 73.711, fake 70.787 ) gen_loss 980.72\n",
            "iteration 9589, epoch 20, batch 250/481,disc_loss 76.938, (real 78.677, fake 75.2 ) gen_loss 939.09\n",
            "iteration 9590, epoch 20, batch 251/481,disc_loss 70.324, (real 72.037, fake 68.611 ) gen_loss 943.36\n",
            "iteration 9591, epoch 20, batch 252/481,disc_loss 77.059, (real 79.371, fake 74.747 ) gen_loss 881.89\n",
            "iteration 9592, epoch 20, batch 253/481,disc_loss 74.817, (real 77.319, fake 72.315 ) gen_loss 895.3\n",
            "iteration 9593, epoch 20, batch 254/481,disc_loss 75.693, (real 78.097, fake 73.289 ) gen_loss 971.03\n",
            "iteration 9594, epoch 20, batch 255/481,disc_loss 76.579, (real 78.269, fake 74.888 ) gen_loss 900.06\n",
            "iteration 9595, epoch 20, batch 256/481,disc_loss 74.53, (real 76.645, fake 72.416 ) gen_loss 941.04\n",
            "iteration 9596, epoch 20, batch 257/481,disc_loss 79.01, (real 81.268, fake 76.753 ) gen_loss 848.83\n",
            "iteration 9597, epoch 20, batch 258/481,disc_loss 75.92, (real 78.125, fake 73.716 ) gen_loss 941.13\n",
            "iteration 9598, epoch 20, batch 259/481,disc_loss 72.851, (real 74.311, fake 71.392 ) gen_loss 880.76\n",
            "iteration 9599, epoch 20, batch 260/481,disc_loss 76.017, (real 78.4, fake 73.633 ) gen_loss 949.93\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 9600, epoch 20, batch 261/481,disc_loss 71.932, (real 74.641, fake 69.223 ) gen_loss 936.03\n",
            "iteration 9601, epoch 20, batch 262/481,disc_loss 77.773, (real 79.795, fake 75.751 ) gen_loss 929.1\n",
            "iteration 9602, epoch 20, batch 263/481,disc_loss 74.508, (real 76.602, fake 72.414 ) gen_loss 1035.3\n",
            "iteration 9603, epoch 20, batch 264/481,disc_loss 78.772, (real 80.806, fake 76.738 ) gen_loss 838.03\n",
            "iteration 9604, epoch 20, batch 265/481,disc_loss 75.162, (real 77.097, fake 73.226 ) gen_loss 917.74\n",
            "iteration 9605, epoch 20, batch 266/481,disc_loss 80.836, (real 82.995, fake 78.677 ) gen_loss 874.63\n",
            "iteration 9606, epoch 20, batch 267/481,disc_loss 78.99, (real 81.17, fake 76.81 ) gen_loss 932.99\n",
            "iteration 9607, epoch 20, batch 268/481,disc_loss 77.885, (real 80.951, fake 74.82 ) gen_loss 1030.7\n",
            "iteration 9608, epoch 20, batch 269/481,disc_loss 78.957, (real 81.821, fake 76.094 ) gen_loss 925.53\n",
            "iteration 9609, epoch 20, batch 270/481,disc_loss 77.397, (real 79.859, fake 74.934 ) gen_loss 874.5\n",
            "iteration 9610, epoch 20, batch 271/481,disc_loss 78.935, (real 82.465, fake 75.405 ) gen_loss 884.99\n",
            "iteration 9611, epoch 20, batch 272/481,disc_loss 76.217, (real 78.542, fake 73.892 ) gen_loss 849.62\n",
            "iteration 9612, epoch 20, batch 273/481,disc_loss 71.599, (real 73.406, fake 69.792 ) gen_loss 901.72\n",
            "iteration 9613, epoch 20, batch 274/481,disc_loss 78.87, (real 80.749, fake 76.991 ) gen_loss 865.17\n",
            "iteration 9614, epoch 20, batch 275/481,disc_loss 78.963, (real 80.244, fake 77.681 ) gen_loss 828.55\n",
            "iteration 9615, epoch 20, batch 276/481,disc_loss 79.95, (real 82.315, fake 77.585 ) gen_loss 993.98\n",
            "iteration 9616, epoch 20, batch 277/481,disc_loss 74.998, (real 76.714, fake 73.283 ) gen_loss 984.22\n",
            "iteration 9617, epoch 20, batch 278/481,disc_loss 73.093, (real 75.252, fake 70.934 ) gen_loss 974.83\n",
            "iteration 9618, epoch 20, batch 279/481,disc_loss 79.748, (real 81.701, fake 77.794 ) gen_loss 1005.3\n",
            "iteration 9619, epoch 20, batch 280/481,disc_loss 79.086, (real 81.096, fake 77.077 ) gen_loss 910.49\n",
            "iteration 9620, epoch 20, batch 281/481,disc_loss 75.383, (real 77.524, fake 73.243 ) gen_loss 911.41\n",
            "iteration 9621, epoch 20, batch 282/481,disc_loss 76.005, (real 77.836, fake 74.174 ) gen_loss 815.37\n",
            "iteration 9622, epoch 20, batch 283/481,disc_loss 76.568, (real 78.437, fake 74.699 ) gen_loss 847.55\n",
            "iteration 9623, epoch 20, batch 284/481,disc_loss 81.033, (real 83.506, fake 78.56 ) gen_loss 942.84\n",
            "iteration 9624, epoch 20, batch 285/481,disc_loss 77.88, (real 80.176, fake 75.585 ) gen_loss 948.67\n",
            "iteration 9625, epoch 20, batch 286/481,disc_loss 78.327, (real 79.789, fake 76.864 ) gen_loss 849.47\n",
            "iteration 9626, epoch 20, batch 287/481,disc_loss 81.049, (real 83.077, fake 79.02 ) gen_loss 845.61\n",
            "iteration 9627, epoch 20, batch 288/481,disc_loss 77.678, (real 80.036, fake 75.32 ) gen_loss 894.6\n",
            "iteration 9628, epoch 20, batch 289/481,disc_loss 78.866, (real 81.547, fake 76.185 ) gen_loss 887.96\n",
            "iteration 9629, epoch 20, batch 290/481,disc_loss 79.333, (real 81.463, fake 77.204 ) gen_loss 889.16\n",
            "iteration 9630, epoch 20, batch 291/481,disc_loss 78.09, (real 80.183, fake 75.998 ) gen_loss 918.95\n",
            "iteration 9631, epoch 20, batch 292/481,disc_loss 77.633, (real 80.226, fake 75.041 ) gen_loss 818.02\n",
            "iteration 9632, epoch 20, batch 293/481,disc_loss 77.551, (real 80.37, fake 74.732 ) gen_loss 971.92\n",
            "iteration 9633, epoch 20, batch 294/481,disc_loss 76.175, (real 77.859, fake 74.491 ) gen_loss 817.79\n",
            "iteration 9634, epoch 20, batch 295/481,disc_loss 74.881, (real 76.707, fake 73.054 ) gen_loss 874.91\n",
            "iteration 9635, epoch 20, batch 296/481,disc_loss 78.615, (real 81.246, fake 75.985 ) gen_loss 897.53\n",
            "iteration 9636, epoch 20, batch 297/481,disc_loss 75.464, (real 77.543, fake 73.386 ) gen_loss 981.82\n",
            "iteration 9637, epoch 20, batch 298/481,disc_loss 75.547, (real 77.535, fake 73.559 ) gen_loss 907.69\n",
            "iteration 9638, epoch 20, batch 299/481,disc_loss 75.917, (real 77.875, fake 73.96 ) gen_loss 889.66\n",
            "iteration 9639, epoch 20, batch 300/481,disc_loss 75.653, (real 77.894, fake 73.412 ) gen_loss 873.12\n",
            "iteration 9640, epoch 20, batch 301/481,disc_loss 74.5, (real 76.675, fake 72.324 ) gen_loss 890.29\n",
            "iteration 9641, epoch 20, batch 302/481,disc_loss 80.811, (real 82.961, fake 78.66 ) gen_loss 1033.0\n",
            "iteration 9642, epoch 20, batch 303/481,disc_loss 75.156, (real 77.035, fake 73.278 ) gen_loss 1057.5\n",
            "iteration 9643, epoch 20, batch 304/481,disc_loss 75.234, (real 77.969, fake 72.499 ) gen_loss 943.23\n",
            "iteration 9644, epoch 20, batch 305/481,disc_loss 73.745, (real 75.618, fake 71.872 ) gen_loss 853.62\n",
            "iteration 9645, epoch 20, batch 306/481,disc_loss 76.972, (real 79.458, fake 74.487 ) gen_loss 863.86\n",
            "iteration 9646, epoch 20, batch 307/481,disc_loss 82.082, (real 84.365, fake 79.8 ) gen_loss 812.28\n",
            "iteration 9647, epoch 20, batch 308/481,disc_loss 76.758, (real 79.473, fake 74.043 ) gen_loss 821.95\n",
            "iteration 9648, epoch 20, batch 309/481,disc_loss 76.447, (real 77.879, fake 75.014 ) gen_loss 877.63\n",
            "iteration 9649, epoch 20, batch 310/481,disc_loss 76.113, (real 78.827, fake 73.399 ) gen_loss 905.85\n",
            "iteration 9650, epoch 20, batch 311/481,disc_loss 75.394, (real 78.007, fake 72.78 ) gen_loss 920.0\n",
            "iteration 9651, epoch 20, batch 312/481,disc_loss 78.134, (real 79.949, fake 76.319 ) gen_loss 939.14\n",
            "iteration 9652, epoch 20, batch 313/481,disc_loss 76.587, (real 78.665, fake 74.51 ) gen_loss 814.8\n",
            "iteration 9653, epoch 20, batch 314/481,disc_loss 77.157, (real 79.04, fake 75.274 ) gen_loss 927.48\n",
            "iteration 9654, epoch 20, batch 315/481,disc_loss 75.977, (real 78.001, fake 73.953 ) gen_loss 952.72\n",
            "iteration 9655, epoch 20, batch 316/481,disc_loss 74.24, (real 76.709, fake 71.771 ) gen_loss 950.02\n",
            "iteration 9656, epoch 20, batch 317/481,disc_loss 76.146, (real 79.145, fake 73.148 ) gen_loss 1023.4\n",
            "iteration 9657, epoch 20, batch 318/481,disc_loss 76.916, (real 78.655, fake 75.177 ) gen_loss 1040.0\n",
            "iteration 9658, epoch 20, batch 319/481,disc_loss 79.409, (real 81.796, fake 77.021 ) gen_loss 968.14\n",
            "iteration 9659, epoch 20, batch 320/481,disc_loss 79.04, (real 81.276, fake 76.804 ) gen_loss 871.93\n",
            "iteration 9660, epoch 20, batch 321/481,disc_loss 78.82, (real 80.709, fake 76.931 ) gen_loss 896.6\n",
            "iteration 9661, epoch 20, batch 322/481,disc_loss 74.161, (real 76.176, fake 72.147 ) gen_loss 868.01\n",
            "iteration 9662, epoch 20, batch 323/481,disc_loss 80.283, (real 82.71, fake 77.857 ) gen_loss 932.1\n",
            "iteration 9663, epoch 20, batch 324/481,disc_loss 78.362, (real 80.18, fake 76.544 ) gen_loss 842.88\n",
            "iteration 9664, epoch 20, batch 325/481,disc_loss 75.198, (real 77.348, fake 73.049 ) gen_loss 930.57\n",
            "iteration 9665, epoch 20, batch 326/481,disc_loss 75.377, (real 77.441, fake 73.313 ) gen_loss 947.64\n",
            "iteration 9666, epoch 20, batch 327/481,disc_loss 71.505, (real 73.78, fake 69.23 ) gen_loss 953.1\n",
            "iteration 9667, epoch 20, batch 328/481,disc_loss 76.86, (real 78.304, fake 75.416 ) gen_loss 874.62\n",
            "iteration 9668, epoch 20, batch 329/481,disc_loss 79.654, (real 81.442, fake 77.865 ) gen_loss 893.93\n",
            "iteration 9669, epoch 20, batch 330/481,disc_loss 74.13, (real 76.849, fake 71.411 ) gen_loss 950.19\n",
            "iteration 9670, epoch 20, batch 331/481,disc_loss 77.854, (real 79.618, fake 76.089 ) gen_loss 918.38\n",
            "iteration 9671, epoch 20, batch 332/481,disc_loss 74.182, (real 75.794, fake 72.57 ) gen_loss 946.59\n",
            "iteration 9672, epoch 20, batch 333/481,disc_loss 79.369, (real 81.421, fake 77.318 ) gen_loss 1048.9\n",
            "iteration 9673, epoch 20, batch 334/481,disc_loss 78.128, (real 80.241, fake 76.016 ) gen_loss 868.64\n",
            "iteration 9674, epoch 20, batch 335/481,disc_loss 78.635, (real 80.621, fake 76.649 ) gen_loss 914.27\n",
            "iteration 9675, epoch 20, batch 336/481,disc_loss 74.491, (real 76.526, fake 72.455 ) gen_loss 963.75\n",
            "iteration 9676, epoch 20, batch 337/481,disc_loss 74.92, (real 77.782, fake 72.059 ) gen_loss 866.23\n",
            "iteration 9677, epoch 20, batch 338/481,disc_loss 76.999, (real 79.404, fake 74.595 ) gen_loss 984.74\n",
            "iteration 9678, epoch 20, batch 339/481,disc_loss 76.696, (real 78.868, fake 74.523 ) gen_loss 1046.0\n",
            "iteration 9679, epoch 20, batch 340/481,disc_loss 78.52, (real 81.002, fake 76.037 ) gen_loss 987.09\n",
            "iteration 9680, epoch 20, batch 341/481,disc_loss 74.213, (real 76.406, fake 72.02 ) gen_loss 1024.9\n",
            "iteration 9681, epoch 20, batch 342/481,disc_loss 76.961, (real 79.307, fake 74.614 ) gen_loss 906.42\n",
            "iteration 9682, epoch 20, batch 343/481,disc_loss 72.547, (real 75.009, fake 70.085 ) gen_loss 1003.3\n",
            "iteration 9683, epoch 20, batch 344/481,disc_loss 79.462, (real 81.211, fake 77.713 ) gen_loss 933.2\n",
            "iteration 9684, epoch 20, batch 345/481,disc_loss 77.68, (real 80.057, fake 75.304 ) gen_loss 754.53\n",
            "iteration 9685, epoch 20, batch 346/481,disc_loss 74.592, (real 77.58, fake 71.604 ) gen_loss 942.78\n",
            "iteration 9686, epoch 20, batch 347/481,disc_loss 75.105, (real 76.948, fake 73.262 ) gen_loss 902.33\n",
            "iteration 9687, epoch 20, batch 348/481,disc_loss 76.083, (real 78.218, fake 73.948 ) gen_loss 890.33\n",
            "iteration 9688, epoch 20, batch 349/481,disc_loss 80.849, (real 82.132, fake 79.567 ) gen_loss 991.45\n",
            "iteration 9689, epoch 20, batch 350/481,disc_loss 79.997, (real 83.375, fake 76.618 ) gen_loss 1006.9\n",
            "iteration 9690, epoch 20, batch 351/481,disc_loss 77.504, (real 80.015, fake 74.993 ) gen_loss 1005.8\n",
            "iteration 9691, epoch 20, batch 352/481,disc_loss 78.138, (real 80.054, fake 76.221 ) gen_loss 970.79\n",
            "iteration 9692, epoch 20, batch 353/481,disc_loss 74.64, (real 76.764, fake 72.515 ) gen_loss 849.85\n",
            "iteration 9693, epoch 20, batch 354/481,disc_loss 78.957, (real 81.62, fake 76.294 ) gen_loss 842.99\n",
            "iteration 9694, epoch 20, batch 355/481,disc_loss 74.879, (real 76.811, fake 72.947 ) gen_loss 960.16\n",
            "iteration 9695, epoch 20, batch 356/481,disc_loss 75.589, (real 78.122, fake 73.057 ) gen_loss 892.14\n",
            "iteration 9696, epoch 20, batch 357/481,disc_loss 76.756, (real 78.757, fake 74.755 ) gen_loss 869.19\n",
            "iteration 9697, epoch 20, batch 358/481,disc_loss 78.32, (real 80.652, fake 75.988 ) gen_loss 883.02\n",
            "iteration 9698, epoch 20, batch 359/481,disc_loss 73.637, (real 75.428, fake 71.846 ) gen_loss 814.0\n",
            "iteration 9699, epoch 20, batch 360/481,disc_loss 80.235, (real 82.614, fake 77.857 ) gen_loss 923.03\n",
            "iteration 9700, epoch 20, batch 361/481,disc_loss 80.282, (real 82.935, fake 77.63 ) gen_loss 874.87\n",
            "iteration 9701, epoch 20, batch 362/481,disc_loss 76.762, (real 79.149, fake 74.375 ) gen_loss 908.2\n",
            "iteration 9702, epoch 20, batch 363/481,disc_loss 80.232, (real 83.408, fake 77.056 ) gen_loss 916.13\n",
            "iteration 9703, epoch 20, batch 364/481,disc_loss 73.309, (real 75.347, fake 71.272 ) gen_loss 982.47\n",
            "iteration 9704, epoch 20, batch 365/481,disc_loss 76.222, (real 78.122, fake 74.322 ) gen_loss 984.58\n",
            "iteration 9705, epoch 20, batch 366/481,disc_loss 74.862, (real 76.887, fake 72.837 ) gen_loss 909.29\n",
            "iteration 9706, epoch 20, batch 367/481,disc_loss 77.333, (real 78.979, fake 75.687 ) gen_loss 945.21\n",
            "iteration 9707, epoch 20, batch 368/481,disc_loss 76.962, (real 79.392, fake 74.532 ) gen_loss 967.67\n",
            "iteration 9708, epoch 20, batch 369/481,disc_loss 72.19, (real 74.037, fake 70.343 ) gen_loss 814.63\n",
            "iteration 9709, epoch 20, batch 370/481,disc_loss 76.22, (real 78.308, fake 74.131 ) gen_loss 831.7\n",
            "iteration 9710, epoch 20, batch 371/481,disc_loss 75.5, (real 78.284, fake 72.717 ) gen_loss 936.67\n",
            "iteration 9711, epoch 20, batch 372/481,disc_loss 84.32, (real 87.498, fake 81.142 ) gen_loss 851.97\n",
            "iteration 9712, epoch 20, batch 373/481,disc_loss 76.985, (real 79.864, fake 74.106 ) gen_loss 923.19\n",
            "iteration 9713, epoch 20, batch 374/481,disc_loss 74.711, (real 76.415, fake 73.006 ) gen_loss 948.63\n",
            "iteration 9714, epoch 20, batch 375/481,disc_loss 70.996, (real 73.104, fake 68.888 ) gen_loss 978.25\n",
            "iteration 9715, epoch 20, batch 376/481,disc_loss 75.22, (real 77.126, fake 73.314 ) gen_loss 1010.3\n",
            "iteration 9716, epoch 20, batch 377/481,disc_loss 77.765, (real 79.311, fake 76.219 ) gen_loss 871.58\n",
            "iteration 9717, epoch 20, batch 378/481,disc_loss 76.588, (real 78.249, fake 74.927 ) gen_loss 936.47\n",
            "iteration 9718, epoch 20, batch 379/481,disc_loss 78.941, (real 81.318, fake 76.564 ) gen_loss 895.46\n",
            "iteration 9719, epoch 20, batch 380/481,disc_loss 70.646, (real 72.507, fake 68.786 ) gen_loss 1002.4\n",
            "iteration 9720, epoch 20, batch 381/481,disc_loss 77.132, (real 79.006, fake 75.258 ) gen_loss 958.05\n",
            "iteration 9721, epoch 20, batch 382/481,disc_loss 76.501, (real 78.985, fake 74.018 ) gen_loss 947.01\n",
            "iteration 9722, epoch 20, batch 383/481,disc_loss 78.15, (real 80.473, fake 75.827 ) gen_loss 987.31\n",
            "iteration 9723, epoch 20, batch 384/481,disc_loss 82.234, (real 83.393, fake 81.075 ) gen_loss 894.58\n",
            "iteration 9724, epoch 20, batch 385/481,disc_loss 80.785, (real 83.131, fake 78.439 ) gen_loss 827.08\n",
            "iteration 9725, epoch 20, batch 386/481,disc_loss 75.246, (real 77.176, fake 73.317 ) gen_loss 917.45\n",
            "iteration 9726, epoch 20, batch 387/481,disc_loss 78.843, (real 81.065, fake 76.621 ) gen_loss 807.32\n",
            "iteration 9727, epoch 20, batch 388/481,disc_loss 81.13, (real 83.361, fake 78.899 ) gen_loss 883.26\n",
            "iteration 9728, epoch 20, batch 389/481,disc_loss 78.716, (real 81.006, fake 76.426 ) gen_loss 995.86\n",
            "iteration 9729, epoch 20, batch 390/481,disc_loss 78.738, (real 80.562, fake 76.915 ) gen_loss 939.72\n",
            "iteration 9730, epoch 20, batch 391/481,disc_loss 79.453, (real 82.177, fake 76.73 ) gen_loss 841.97\n",
            "iteration 9731, epoch 20, batch 392/481,disc_loss 80.274, (real 81.79, fake 78.757 ) gen_loss 952.35\n",
            "iteration 9732, epoch 20, batch 393/481,disc_loss 76.744, (real 79.057, fake 74.431 ) gen_loss 916.95\n",
            "iteration 9733, epoch 20, batch 394/481,disc_loss 75.827, (real 78.378, fake 73.275 ) gen_loss 889.35\n",
            "iteration 9734, epoch 20, batch 395/481,disc_loss 76.467, (real 78.986, fake 73.948 ) gen_loss 870.52\n",
            "iteration 9735, epoch 20, batch 396/481,disc_loss 74.373, (real 76.354, fake 72.392 ) gen_loss 808.76\n",
            "iteration 9736, epoch 20, batch 397/481,disc_loss 76.345, (real 78.567, fake 74.124 ) gen_loss 1026.5\n",
            "iteration 9737, epoch 20, batch 398/481,disc_loss 76.233, (real 78.817, fake 73.648 ) gen_loss 893.83\n",
            "iteration 9738, epoch 20, batch 399/481,disc_loss 78.763, (real 81.242, fake 76.284 ) gen_loss 1017.1\n",
            "iteration 9739, epoch 20, batch 400/481,disc_loss 73.654, (real 75.638, fake 71.67 ) gen_loss 996.25\n",
            "iteration 9740, epoch 20, batch 401/481,disc_loss 78.841, (real 81.011, fake 76.672 ) gen_loss 885.68\n",
            "iteration 9741, epoch 20, batch 402/481,disc_loss 73.739, (real 75.972, fake 71.506 ) gen_loss 894.44\n",
            "iteration 9742, epoch 20, batch 403/481,disc_loss 77.156, (real 79.838, fake 74.474 ) gen_loss 945.28\n",
            "iteration 9743, epoch 20, batch 404/481,disc_loss 76.115, (real 78.727, fake 73.503 ) gen_loss 1021.5\n",
            "iteration 9744, epoch 20, batch 405/481,disc_loss 74.332, (real 76.649, fake 72.015 ) gen_loss 984.1\n",
            "iteration 9745, epoch 20, batch 406/481,disc_loss 77.307, (real 79.693, fake 74.922 ) gen_loss 831.02\n",
            "iteration 9746, epoch 20, batch 407/481,disc_loss 77.674, (real 79.659, fake 75.688 ) gen_loss 959.61\n",
            "iteration 9747, epoch 20, batch 408/481,disc_loss 74.356, (real 77.263, fake 71.449 ) gen_loss 895.75\n",
            "iteration 9748, epoch 20, batch 409/481,disc_loss 75.577, (real 77.989, fake 73.166 ) gen_loss 938.11\n",
            "iteration 9749, epoch 20, batch 410/481,disc_loss 74.051, (real 76.071, fake 72.032 ) gen_loss 977.65\n",
            "iteration 9750, epoch 20, batch 411/481,disc_loss 75.609, (real 78.384, fake 72.834 ) gen_loss 902.94\n",
            "iteration 9751, epoch 20, batch 412/481,disc_loss 78.488, (real 80.353, fake 76.623 ) gen_loss 1009.2\n",
            "iteration 9752, epoch 20, batch 413/481,disc_loss 73.571, (real 75.783, fake 71.36 ) gen_loss 910.36\n",
            "iteration 9753, epoch 20, batch 414/481,disc_loss 77.647, (real 80.199, fake 75.096 ) gen_loss 819.49\n",
            "iteration 9754, epoch 20, batch 415/481,disc_loss 72.289, (real 74.61, fake 69.968 ) gen_loss 919.4\n",
            "iteration 9755, epoch 20, batch 416/481,disc_loss 77.389, (real 79.822, fake 74.956 ) gen_loss 1000.9\n",
            "iteration 9756, epoch 20, batch 417/481,disc_loss 82.708, (real 84.437, fake 80.979 ) gen_loss 922.51\n",
            "iteration 9757, epoch 20, batch 418/481,disc_loss 78.612, (real 81.094, fake 76.129 ) gen_loss 994.39\n",
            "iteration 9758, epoch 20, batch 419/481,disc_loss 77.558, (real 79.781, fake 75.335 ) gen_loss 997.02\n",
            "iteration 9759, epoch 20, batch 420/481,disc_loss 74.062, (real 76.526, fake 71.599 ) gen_loss 897.22\n",
            "iteration 9760, epoch 20, batch 421/481,disc_loss 74.209, (real 76.136, fake 72.282 ) gen_loss 837.91\n",
            "iteration 9761, epoch 20, batch 422/481,disc_loss 77.081, (real 79.961, fake 74.202 ) gen_loss 865.26\n",
            "iteration 9762, epoch 20, batch 423/481,disc_loss 80.867, (real 83.584, fake 78.149 ) gen_loss 890.74\n",
            "iteration 9763, epoch 20, batch 424/481,disc_loss 79.678, (real 81.743, fake 77.613 ) gen_loss 950.08\n",
            "iteration 9764, epoch 20, batch 425/481,disc_loss 77.117, (real 79.293, fake 74.941 ) gen_loss 861.07\n",
            "iteration 9765, epoch 20, batch 426/481,disc_loss 77.741, (real 79.469, fake 76.013 ) gen_loss 829.82\n",
            "iteration 9766, epoch 20, batch 427/481,disc_loss 76.094, (real 78.622, fake 73.567 ) gen_loss 907.57\n",
            "iteration 9767, epoch 20, batch 428/481,disc_loss 73.273, (real 75.804, fake 70.743 ) gen_loss 1024.5\n",
            "iteration 9768, epoch 20, batch 429/481,disc_loss 76.889, (real 79.057, fake 74.721 ) gen_loss 907.57\n",
            "iteration 9769, epoch 20, batch 430/481,disc_loss 74.177, (real 76.278, fake 72.076 ) gen_loss 1007.0\n",
            "iteration 9770, epoch 20, batch 431/481,disc_loss 76.557, (real 78.684, fake 74.431 ) gen_loss 877.5\n",
            "iteration 9771, epoch 20, batch 432/481,disc_loss 71.941, (real 73.738, fake 70.144 ) gen_loss 997.53\n",
            "iteration 9772, epoch 20, batch 433/481,disc_loss 74.664, (real 76.872, fake 72.456 ) gen_loss 1011.2\n",
            "iteration 9773, epoch 20, batch 434/481,disc_loss 75.888, (real 77.025, fake 74.75 ) gen_loss 956.22\n",
            "iteration 9774, epoch 20, batch 435/481,disc_loss 79.085, (real 80.784, fake 77.386 ) gen_loss 897.84\n",
            "iteration 9775, epoch 20, batch 436/481,disc_loss 79.382, (real 81.515, fake 77.248 ) gen_loss 933.74\n",
            "iteration 9776, epoch 20, batch 437/481,disc_loss 71.065, (real 73.068, fake 69.061 ) gen_loss 962.74\n",
            "iteration 9777, epoch 20, batch 438/481,disc_loss 77.142, (real 78.978, fake 75.305 ) gen_loss 1169.9\n",
            "iteration 9778, epoch 20, batch 439/481,disc_loss 77.354, (real 79.613, fake 75.096 ) gen_loss 970.25\n",
            "iteration 9779, epoch 20, batch 440/481,disc_loss 76.636, (real 78.61, fake 74.661 ) gen_loss 941.19\n",
            "iteration 9780, epoch 20, batch 441/481,disc_loss 77.801, (real 81.533, fake 74.069 ) gen_loss 947.11\n",
            "iteration 9781, epoch 20, batch 442/481,disc_loss 79.562, (real 81.613, fake 77.511 ) gen_loss 1001.6\n",
            "iteration 9782, epoch 20, batch 443/481,disc_loss 84.421, (real 86.842, fake 82.0 ) gen_loss 906.6\n",
            "iteration 9783, epoch 20, batch 444/481,disc_loss 72.862, (real 74.527, fake 71.197 ) gen_loss 927.56\n",
            "iteration 9784, epoch 20, batch 445/481,disc_loss 78.348, (real 80.942, fake 75.754 ) gen_loss 887.39\n",
            "iteration 9785, epoch 20, batch 446/481,disc_loss 76.639, (real 79.036, fake 74.241 ) gen_loss 938.64\n",
            "iteration 9786, epoch 20, batch 447/481,disc_loss 77.888, (real 80.446, fake 75.33 ) gen_loss 896.45\n",
            "iteration 9787, epoch 20, batch 448/481,disc_loss 76.757, (real 78.725, fake 74.79 ) gen_loss 871.73\n",
            "iteration 9788, epoch 20, batch 449/481,disc_loss 78.933, (real 81.203, fake 76.662 ) gen_loss 906.8\n",
            "iteration 9789, epoch 20, batch 450/481,disc_loss 76.547, (real 79.507, fake 73.587 ) gen_loss 917.32\n",
            "iteration 9790, epoch 20, batch 451/481,disc_loss 81.757, (real 84.242, fake 79.271 ) gen_loss 991.62\n",
            "iteration 9791, epoch 20, batch 452/481,disc_loss 78.235, (real 80.638, fake 75.831 ) gen_loss 1044.9\n",
            "iteration 9792, epoch 20, batch 453/481,disc_loss 75.164, (real 78.299, fake 72.028 ) gen_loss 913.3\n",
            "iteration 9793, epoch 20, batch 454/481,disc_loss 81.985, (real 84.068, fake 79.902 ) gen_loss 936.63\n",
            "iteration 9794, epoch 20, batch 455/481,disc_loss 79.259, (real 81.161, fake 77.356 ) gen_loss 889.35\n",
            "iteration 9795, epoch 20, batch 456/481,disc_loss 78.909, (real 81.606, fake 76.212 ) gen_loss 958.15\n",
            "iteration 9796, epoch 20, batch 457/481,disc_loss 79.218, (real 81.912, fake 76.524 ) gen_loss 985.34\n",
            "iteration 9797, epoch 20, batch 458/481,disc_loss 75.39, (real 77.426, fake 73.354 ) gen_loss 936.19\n",
            "iteration 9798, epoch 20, batch 459/481,disc_loss 75.526, (real 77.766, fake 73.285 ) gen_loss 895.85\n",
            "iteration 9799, epoch 20, batch 460/481,disc_loss 76.735, (real 79.125, fake 74.345 ) gen_loss 1005.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 9800, epoch 20, batch 461/481,disc_loss 73.625, (real 75.526, fake 71.724 ) gen_loss 983.33\n",
            "iteration 9801, epoch 20, batch 462/481,disc_loss 74.727, (real 76.174, fake 73.28 ) gen_loss 874.32\n",
            "iteration 9802, epoch 20, batch 463/481,disc_loss 77.303, (real 79.635, fake 74.972 ) gen_loss 849.53\n",
            "iteration 9803, epoch 20, batch 464/481,disc_loss 76.012, (real 79.026, fake 72.998 ) gen_loss 976.8\n",
            "iteration 9804, epoch 20, batch 465/481,disc_loss 79.658, (real 82.557, fake 76.76 ) gen_loss 1019.5\n",
            "iteration 9805, epoch 20, batch 466/481,disc_loss 75.188, (real 77.984, fake 72.391 ) gen_loss 887.47\n",
            "iteration 9806, epoch 20, batch 467/481,disc_loss 74.452, (real 76.409, fake 72.494 ) gen_loss 824.73\n",
            "iteration 9807, epoch 20, batch 468/481,disc_loss 76.04, (real 78.108, fake 73.973 ) gen_loss 877.66\n",
            "iteration 9808, epoch 20, batch 469/481,disc_loss 81.695, (real 85.735, fake 77.654 ) gen_loss 819.83\n",
            "iteration 9809, epoch 20, batch 470/481,disc_loss 74.516, (real 77.695, fake 71.336 ) gen_loss 823.4\n",
            "iteration 9810, epoch 20, batch 471/481,disc_loss 78.216, (real 80.234, fake 76.197 ) gen_loss 892.39\n",
            "iteration 9811, epoch 20, batch 472/481,disc_loss 76.98, (real 79.325, fake 74.636 ) gen_loss 937.53\n",
            "iteration 9812, epoch 20, batch 473/481,disc_loss 80.296, (real 82.212, fake 78.381 ) gen_loss 933.78\n",
            "iteration 9813, epoch 20, batch 474/481,disc_loss 77.973, (real 80.038, fake 75.909 ) gen_loss 830.44\n",
            "iteration 9814, epoch 20, batch 475/481,disc_loss 74.18, (real 76.327, fake 72.032 ) gen_loss 902.87\n",
            "iteration 9815, epoch 20, batch 476/481,disc_loss 74.943, (real 77.02, fake 72.866 ) gen_loss 882.53\n",
            "iteration 9816, epoch 20, batch 477/481,disc_loss 78.53, (real 81.399, fake 75.66 ) gen_loss 977.73\n",
            "iteration 9817, epoch 20, batch 478/481,disc_loss 72.894, (real 76.085, fake 69.702 ) gen_loss 1025.7\n",
            "iteration 9818, epoch 20, batch 479/481,disc_loss 77.33, (real 79.671, fake 74.988 ) gen_loss 945.01\n",
            "iteration 9819, epoch 20, batch 480/481,disc_loss 75.828, (real 78.198, fake 73.458 ) gen_loss 848.2\n",
            "iteration 9820, epoch 20, batch 481/481,disc_loss 79.708, (real 82.339, fake 77.077 ) gen_loss 958.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfBjaXVv7IS0"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFcJlt6Hh8Qh"
      },
      "source": [
        "!pip install line_profiler\n",
        "%load_ext line_profiler\n",
        "%lprun -f training_step training_step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPlmAHXbVZac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e912a8-ab7d-4095-bf8f-8c0c899a205e"
      },
      "source": [
        "labels_train.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnvpRlGSbVfF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ZBhhRlJ_gS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQQHP5dDJPVw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}