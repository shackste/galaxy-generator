# Cluster-based evaluation metrics and Wasserstein distance

Contained in this folder is the code for evaluation of the ditsribution of data generated by a generated model.
Images (or any other type of data) are reduced to a column vector of features.
Starting with the training data, clusters are identified in the feature space using k-means.
For features of generated data, the number of data points in each cluster are counted and compared to the reference using MSE, resulting in the **cluster error metric**.
Furthermore, the RMS distance of data points from the cluster center are computed, as well as the standard deviation from that value, resulting in the **cluster distance metric** as well as the **cluster std metric**.

Low values of the  **cluster error** indicate that the different types are generated in the correct amounts.
The **cluster distance** and **cluster std** indicate whether the diversity of generated objects matches that of the target distribution.
Interpretation requires to compare these results to metrics obtained by comparing two separate subsets of the training data.

Computation requires a pretrained autoencoder or alternative way of reducing the data to a column vector of features.

## Usage

Once the dimension reduction procedure is prepared, the metrics can be obtained using the following code

```
models = { "name": Model} ## add as many models as required

evaluate_latent_distribution(models, data_loader_test, data_loader_valid, N_cluster=10)
```

The data loaders return tuples of (images, labels).
Model is the class function of the generator, which will be loaded using the following autoencoder
```
generator = Model().cuda()
generator.load()
generator.eval()
```

The generator instance has to include a parameter z_dim that indicates the required dimension of the latent vector.
Furthermore, the generator takes as input two pytorch tensors (latent, labels).