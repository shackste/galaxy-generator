{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bb9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"/home/shackste/galaxy-generator/python_modules/\")\n",
    "\n",
    "\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2fc723",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79405ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear, MaxPool1d\n",
    "from torch import max, Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "from additional_layers import Reshape, PrintShape\n",
    "\n",
    "\n",
    "class MaxOut(Module):\n",
    "    \"\"\" Maxout Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, N_layers=2, **kwargs):\n",
    "        super(MaxOut, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dense = []\n",
    "        for i, N_label in enumerate(label_group_sizes):\n",
    "            layer = f\"dense{i}\"\n",
    "            setattr(self, layer,\n",
    "                Sequential(\n",
    "                   Linear(self.in_features, self.out_features, **kwargs),\n",
    "                )\n",
    "            )\n",
    "            self.dense.append( getattr(self, layer) )\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.dense[0](input)\n",
    "        for layer in self.dense[1:]:\n",
    "            x = max(x, layer(input))\n",
    "        return x\n",
    "\n",
    "class MaxOut(Module):\n",
    "    \"\"\" Maxout Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, N_layers=2, **kwargs):\n",
    "        super(MaxOut, self).__init__()\n",
    "        self.maxout = Sequential(\n",
    "            Linear(in_features, N_layers*out_features, **kwargs),\n",
    "            Reshape(1,N_layers*out_features),\n",
    "            MaxPool1d(N_layers),\n",
    "            Reshape(out_features),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.maxout(input)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Conv2dUntiedBias(Module):\n",
    "    def __init__(self, height, width, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias_init=0.1, weight_std=0.01):\n",
    "        super(Conv2dUntiedBias, self).__init__() \n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.weight = Parameter(Tensor(\n",
    "                out_channels, in_channels // groups, *kernel_size))\n",
    "        self.bias = Parameter(Tensor(out_channels, height, width))\n",
    "        self.reset_parameters(bias_init=bias_init, weight_std=weight_std)\n",
    "\n",
    "    def reset_parameters(self, bias_init=None, weight_std=None):\n",
    "        n = self.in_channels\n",
    "        for k in self.kernel_size:\n",
    "            n *= k\n",
    "        if weight_std is None:\n",
    "            stdv = n**-0.5\n",
    "            self.weight.data.uniform_(-stdv, stdv)\n",
    "        else:\n",
    "            self.weight.data.uniform_(-weight_std, weight_std)            \n",
    "        if bias_init is None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        else:\n",
    "            self.bias.data[:] = bias_init\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        if not self.kernel_size[0] % 2 and self.padding: ## one side padding 1 extra for even kernel size\n",
    "            input = F.pad(input=input, pad=(1,0,1,0))\n",
    "        output = F.conv2d(input, self.weight, None, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "        # add untied bias\n",
    "        output += self.bias.unsqueeze(0).repeat(input.size(0), 1, 1, 1)\n",
    "        return output\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c35f7c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!!!!!!!!!\n",
      "\n",
      "galaxyzoo_data_cropped_nonnormalized.npy and training_solutions_rev1.csv must be placed in google drive under galaxy-generator/data/\n",
      "the results will be placed there, too.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from torch import squeeze, cat, stack\n",
    "from torch.nn import Sequential, Linear, Conv2d, MaxPool2d, Softmax, Dropout, ReLU, Flatten\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, FiveCrop, Lambda\n",
    "from torchvision.transforms.functional import rotate, hflip\n",
    "\n",
    "from neuralnetwork import NeuralNetwork\n",
    "from labeling import make_galaxy_labels_hierarchical, label_group_sizes, labels_dim\n",
    "from parameter import parameter\n",
    "\n",
    "from dataset import augment\n",
    "\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "N_resnet = 512\n",
    "\n",
    "\n",
    "\n",
    "class ImageClassifier(NeuralNetwork):\n",
    "    def __init__(self):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "#        self.conv = Sequential(*(list(resnet.children())[:-1]))\n",
    "        self.conv = Sequential(\n",
    "            Conv2dUntiedBias(41, 41, 3, 32, kernel_size=6),\n",
    "            ReLU(),\n",
    "            MaxPool2d(2),\n",
    "            Conv2dUntiedBias(16, 16, 32, 64, kernel_size=5),\n",
    "            ReLU(),\n",
    "            MaxPool2d(2),\n",
    "            Conv2dUntiedBias(6, 6, 64, 128, kernel_size=3),\n",
    "            ReLU(),\n",
    "            Conv2dUntiedBias(4, 4, 128, 128, kernel_size=3, weight_std=0.1),\n",
    "            ReLU(),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "        )\n",
    "        self.dense = Sequential(\n",
    "            Dropout(p=0.5),\n",
    "            MaxOut(8192, 2048, bias=0.01),\n",
    "            Dropout(p=0.5),\n",
    "            MaxOut(2048, 2048, bias=0.01),\n",
    "            Dropout(p=0.5),\n",
    "            MaxOut(2048, 37, bias=0.1),\n",
    "            ReLU()\n",
    "        )\n",
    "        \n",
    "        self.augment = Compose([\n",
    "            Lambda(lambda img: cat([img, hflip(img)], 0)),\n",
    "            Lambda(lambda img: cat([img, rotate(img, 45)], 0)),\n",
    "            FiveCrop(45),\n",
    "            Lambda(lambda crops: cat([rotate(crop, ang) for crop, ang in zip(crops[:-1], (0, 90, 270, 180))], 0))\n",
    "        ])\n",
    "        self.N_augmentations = 16\n",
    "        self.N_conv_outputs = 512\n",
    "\n",
    "        \n",
    "        self.set_optimizer(parameter.optimizer, lr=parameter.learning_rate, betas=parameter.betas)\n",
    "        \n",
    "        self.valid_probabilities = False\n",
    "        \n",
    "\n",
    "    def update_optimizer(self, betas=parameter.betas, **kwargs):\n",
    "        self.set_optimizer(parameter.optimizer, betas=betas, **kwargs)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        batch_size = images.size(0)\n",
    "        x = self.augment(images)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        ## combine results of augmentations to a single vector \n",
    "        x = x.reshape(self.N_augmentations, batch_size, self.N_conv_outputs)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = x.reshape(batch_size, self.N_augmentations*self.N_conv_outputs)\n",
    "                \n",
    "        x = squeeze(x)\n",
    "        labels = self.dense(x)\n",
    "        if self.valid_probabilities:\n",
    "            labels = make_galaxy_labels_hierarchical(label_groups)\n",
    "        return labels\n",
    "\n",
    "image_size = 45\n",
    "\n",
    "from torch import rand\n",
    "classifier = ImageClassifier()\n",
    "img = rand(2, 3, 64, 64)\n",
    "classifier(img).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b6bf6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37083d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch \n",
    "\n",
    "from loss import mse\n",
    "from dataset import MakeDataLoader\n",
    "from neuralnetwork import update_networks_on_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "make_data_loader = MakeDataLoader()\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate_schedule = {\n",
    "    # iteration : learning_rate\n",
    "    0 : 0.04,\n",
    "    18000000 / batch_size : 0.004,\n",
    "    23000000 / batch_size : 0.0004,\n",
    "    \n",
    "}\n",
    "\n",
    "losses = []\n",
    "losses_steps = 10  # number of batches between loss tracking\n",
    "accuracy = []\n",
    "accuracy_steps = 10 # number of batches after which to measure accuracy\n",
    "\n",
    "\n",
    "classifier = ImageClassifier().to(device)\n",
    "iteration = 0\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae5b532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "test loss 0.06546, 0 iterations\n",
      "training loss 0.07434, iteration 10, 23.3 s\n",
      "test loss 0.07281, 10 iterations\n",
      "training loss 0.07074, iteration 20, 23.5 s\n",
      "test loss 0.07507, 20 iterations\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-067b3c8ed768>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlabels_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mupdate_networks_on_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/galaxy-generator/python_modules/neuralnetwork.py\u001b[0m in \u001b[0;36mupdate_networks_on_loss\u001b[0;34m(loss, *networks)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GalaxyZoo/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GalaxyZoo/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch {epoch+1}\")\n",
    "    data_loader_train = make_data_loader.get_data_loader_train(batch_size=batch_size)\n",
    "    for images, labels in data_loader_train:\n",
    "        if iteration == 10000:\n",
    "            classifier.valid_probabilities = True\n",
    "        elif iteration in learning_rate_schedule.keys():\n",
    "            classifier.update_optimizer(lr=learning_rate_schedule[iteration])\n",
    "\n",
    "        if not iteration % accuracy_steps:\n",
    "            data_loader_test = make_data_loader.get_data_loader_test(batch_size=256)\n",
    "            for images_test, labels_test in data_loader_test:\n",
    "                labels_pred = classifier(images_test)\n",
    "                break\n",
    "            loss = mse(labels_test, labels_pred)\n",
    "            print(f\"test loss {loss.item():.4}, {iteration} iterations\")\n",
    "            accuracy.append(loss)\n",
    "            \n",
    "        labels_pred = classifier(images)\n",
    "        loss = mse(labels, labels_pred)\n",
    "        update_networks_on_loss(loss, classifier)\n",
    "            \n",
    "        if not iteration % losses_steps:\n",
    "            t1 = time()\n",
    "            print(f\"training loss {loss.item():.4}, iteration {iteration+1}, {t1-t0:.3} s\")\n",
    "            t0 = t1\n",
    "\n",
    "        iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06cd8f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device == torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdef1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "for img, lab in data_loader:\n",
    "    print(img.shape)\n",
    "    print(img.permute(0,1,3,2).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sum, rand, zeros\n",
    "\n",
    "z = zeros(10,100)\n",
    "\n",
    "loss_class(z+1,z), 25000000/64*2/3600\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
